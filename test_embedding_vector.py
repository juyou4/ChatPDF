"""
æµ‹è¯•å‘é‡æ£€ç´¢å’ŒåµŒå…¥æ¨¡å‹åŠŸèƒ½
æ£€æŸ¥å‘é‡æ£€ç´¢æ˜¯å¦æ­£å¸¸å·¥ä½œ,ä»¥åŠå…è´¹åµŒå…¥æ¨¡å‹æ˜¯å¦èƒ½è‡ªåŠ¨ä¸‹è½½
"""

import sys
import os
import numpy as np
import faiss
import pickle

# æ·»åŠ backendè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'backend'))

def _run_local_embedding_model():
    """æµ‹è¯•æœ¬åœ°åµŒå…¥æ¨¡å‹ - all-MiniLM-L6-v2"""
    print("=" * 60)
    print("æµ‹è¯•1: æµ‹è¯•æœ¬åœ°åµŒå…¥æ¨¡å‹åŠ è½½å’ŒåµŒå…¥ç”Ÿæˆ")
    print("=" * 60)
    
    try:
        from sentence_transformers import SentenceTransformer
        
        # æµ‹è¯•é»˜è®¤çš„å…è´¹æ¨¡å‹
        model_name = "all-MiniLM-L6-v2"
        print(f"\næ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
        print("æ³¨æ„: å¦‚æœè¿™æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œ,æ¨¡å‹ä¼šè‡ªåŠ¨ä»Hugging Faceä¸‹è½½")
        print("ä¸‹è½½å¤§å°çº¦ä¸º 80-90 MB,è¯·è€å¿ƒç­‰å¾…...\n")
        
        model = SentenceTransformer(model_name)
        print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ!")
        
        # æµ‹è¯•åµŒå…¥ç”Ÿæˆ
        test_texts = [
            "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬",
            "This is a test document about machine learning",
            "äººå·¥æ™ºèƒ½å’Œæ·±åº¦å­¦ä¹ "
        ]
        
        print(f"\næ­£åœ¨ç”Ÿæˆ {len(test_texts)} ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡...")
        embeddings = model.encode(test_texts)
        
        print(f"âœ“ åµŒå…¥å‘é‡ç”ŸæˆæˆåŠŸ!")
        print(f"  - åµŒå…¥ç»´åº¦: {embeddings.shape[1]}")
        print(f"  - å‘é‡æ•°é‡: {embeddings.shape[0]}")
        print(f"  - æ•°æ®ç±»å‹: {embeddings.dtype}")
        
        # æ˜¾ç¤ºå‘é‡çš„å‰å‡ ä¸ªå€¼
        print(f"\nç¬¬ä¸€ä¸ªå‘é‡çš„å‰10ä¸ªå€¼:")
        print(f"  {embeddings[0][:10]}")
        return True, embeddings
        
    except Exception as e:
        print(f"âœ— é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False, None


def _run_multilingual_embedding_model():
    """æµ‹è¯•å¤šè¯­è¨€åµŒå…¥æ¨¡å‹"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•2: æµ‹è¯•å¤šè¯­è¨€åµŒå…¥æ¨¡å‹")
    print("=" * 60)
    
    try:
        from sentence_transformers import SentenceTransformer
        
        model_name = "paraphrase-multilingual-MiniLM-L12-v2"
        print(f"\næ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
        print("æ³¨æ„: å¦‚æœè¿™æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œ,æ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½")
        print("ä¸‹è½½å¤§å°çº¦ä¸º 420 MB,è¯·è€å¿ƒç­‰å¾…...\n")
        
        model = SentenceTransformer(model_name)
        print(f"âœ“ å¤šè¯­è¨€æ¨¡å‹åŠ è½½æˆåŠŸ!")
        
        # æµ‹è¯•å¤šè¯­è¨€æ–‡æœ¬
        test_texts = [
            "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ",
            "Artificial intelligence is changing the world",
            "äººå·¥çŸ¥èƒ½ã¯ä¸–ç•Œã‚’å¤‰ãˆã¦ã„ã¾ã™"
        ]
        
        print(f"\næ­£åœ¨ç”Ÿæˆå¤šè¯­è¨€æ–‡æœ¬çš„åµŒå…¥å‘é‡...")
        embeddings = model.encode(test_texts)
        
        print(f"âœ“ å¤šè¯­è¨€åµŒå…¥ç”ŸæˆæˆåŠŸ!")
        print(f"  - åµŒå…¥ç»´åº¦: {embeddings.shape[1]}")
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        from sklearn.metrics.pairwise import cosine_similarity
        similarities = cosine_similarity(embeddings)
        print(f"\nè¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µ (ç›¸åŒæ„æ€çš„ä¸åŒè¯­è¨€):")
        print(f"  ä¸­æ–‡-è‹±æ–‡: {similarities[0][1]:.4f}")
        print(f"  ä¸­æ–‡-æ—¥æ–‡: {similarities[0][2]:.4f}")
        print(f"  è‹±æ–‡-æ—¥æ–‡: {similarities[1][2]:.4f}")
        
        return True, embeddings
        
    except Exception as e:
        print(f"âœ— é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False, None


def _run_vector_search():
    """æµ‹è¯•FAISSå‘é‡æ£€ç´¢åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•3: æµ‹è¯•FAISSå‘é‡æ£€ç´¢")
    print("=" * 60)
    
    try:
        from sentence_transformers import SentenceTransformer
        
        # å‡†å¤‡æµ‹è¯•æ–‡æ¡£
        documents = [
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯",
            "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œè®­ç»ƒ",
            "è‡ªç„¶è¯­è¨€å¤„ç†ç”¨äºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€",
            "è®¡ç®—æœºè§†è§‰å¸®åŠ©æœºå™¨ç†è§£å›¾åƒ",
            "ä»Šå¤©å¤©æ°”å¾ˆå¥½,é€‚åˆå‡ºå»æ•£æ­¥",
            "æˆ‘å–œæ¬¢åƒè‹¹æœå’Œé¦™è•‰",
        ]
        
        print(f"\nå‡†å¤‡äº† {len(documents)} ä¸ªæµ‹è¯•æ–‡æ¡£")
        
        # åŠ è½½æ¨¡å‹
        model = SentenceTransformer("all-MiniLM-L6-v2")
        print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # ç”Ÿæˆæ–‡æ¡£åµŒå…¥
        print("\næ­£åœ¨ä¸ºæ–‡æ¡£ç”ŸæˆåµŒå…¥å‘é‡...")
        doc_embeddings = model.encode(documents)
        print(f"âœ“ æ–‡æ¡£åµŒå…¥ç”Ÿæˆå®Œæˆ (ç»´åº¦: {doc_embeddings.shape[1]})")
        
        # åˆ›å»ºFAISSç´¢å¼•
        print("\næ­£åœ¨åˆ›å»ºFAISSç´¢å¼•...")
        dimension = doc_embeddings.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(doc_embeddings.astype('float32'))
        print(f"âœ“ FAISSç´¢å¼•åˆ›å»ºæˆåŠŸ (ç´¢å¼•ä¸­æœ‰ {index.ntotal} ä¸ªå‘é‡)")
        
        # æµ‹è¯•æ£€ç´¢
        queries = [
            "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ?",
            "å¦‚ä½•å¤„ç†æ–‡æœ¬æ•°æ®?",
            "æ°´æœæœ‰å“ªäº›?"
        ]
        
        print("\n" + "-" * 60)
        print("å¼€å§‹æµ‹è¯•å‘é‡æ£€ç´¢:")
        print("-" * 60)
        
        for query in queries:
            print(f"\næŸ¥è¯¢: '{query}'")
            
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = model.encode([query])
            
            # æœç´¢æœ€ç›¸å…³çš„3ä¸ªæ–‡æ¡£
            k = 3
            distances, indices = index.search(query_embedding.astype('float32'), k)
            
            print(f"  æœ€ç›¸å…³çš„ {k} ä¸ªæ–‡æ¡£:")
            for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):
                print(f"    {i+1}. [è·ç¦»: {dist:.4f}] {documents[idx]}")
        
        print("\nâœ“ å‘é‡æ£€ç´¢æµ‹è¯•å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âœ— é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def _run_app_embedding_function():
    """æµ‹è¯•åº”ç”¨ä¸­çš„åµŒå…¥å‡½æ•°"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•4: æµ‹è¯•åº”ç”¨çš„åµŒå…¥å‡½æ•°æ¥å£")
    print("=" * 60)
    
    try:
        # å¯¼å…¥åº”ç”¨ä»£ç 
        from services.embedding_service import get_embedding_function
        from models.model_registry import EMBEDDING_MODELS
        
        print(f"\nå¯ç”¨çš„åµŒå…¥æ¨¡å‹:")
        for model_id, config in EMBEDDING_MODELS.items():
            print(f"  - {model_id}: {config['name']}")
            print(f"    æä¾›å•†: {config['provider']}, ç»´åº¦: {config['dimension']}, ä»·æ ¼: {config['price']}")
        
        # æµ‹è¯•æœ¬åœ°æ¨¡å‹
        print(f"\næµ‹è¯• 'local-minilm' æ¨¡å‹...")
        embed_fn = get_embedding_function("local-minilm")
        
        test_texts = ["æµ‹è¯•æ–‡æœ¬1", "æµ‹è¯•æ–‡æœ¬2", "æµ‹è¯•æ–‡æœ¬3"]
        embeddings = embed_fn(test_texts)
        
        print(f"âœ“ åµŒå…¥å‡½æ•°å·¥ä½œæ­£å¸¸!")
        print(f"  - è¾“å…¥æ–‡æœ¬æ•°: {len(test_texts)}")
        print(f"  - è¾“å‡ºå‘é‡å½¢çŠ¶: {embeddings.shape}")
        
        return True
        
    except Exception as e:
        print(f"âœ— é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def _run_build_and_search_index():
    """æµ‹è¯•å®Œæ•´çš„ç´¢å¼•æ„å»ºå’Œæ£€ç´¢æµç¨‹"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•5: æµ‹è¯•å®Œæ•´çš„ç´¢å¼•æ„å»ºå’Œæ£€ç´¢æµç¨‹")
    print("=" * 60)
    
    try:
        from services.embedding_service import build_vector_index, get_relevant_context
        import tempfile
        import shutil
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºæµ‹è¯•
        test_dir = tempfile.mkdtemp(prefix="chatpdf_test_")
        print(f"\nåˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•: {test_dir}")
        
        # ä¸´æ—¶ä¿®æ”¹å…¨å±€è·¯å¾„
        import app
        original_vector_dir = app.VECTOR_STORE_DIR
        app.VECTOR_STORE_DIR = test_dir
        
        try:
            # å‡†å¤‡æµ‹è¯•æ–‡æ¡£
            test_doc_id = "test_doc_123"
            test_text = """
            äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ã€‚
            æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸ,å®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚
            è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æ˜¯äººå·¥æ™ºèƒ½çš„å¦ä¸€ä¸ªé‡è¦åº”ç”¨é¢†åŸŸ,å®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚
            è®¡ç®—æœºè§†è§‰ä½¿æœºå™¨èƒ½å¤Ÿä»å›¾åƒå’Œè§†é¢‘ä¸­æå–ä¿¡æ¯å¹¶è¿›è¡Œç†è§£ã€‚
            å¼ºåŒ–å­¦ä¹ è®©æ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚
            """
            
            # æ„å»ºå‘é‡ç´¢å¼•
            print(f"\nä¸ºæµ‹è¯•æ–‡æ¡£æ„å»ºå‘é‡ç´¢å¼•...")
            build_vector_index(test_doc_id, test_text, vector_store_dir=test_dir, embedding_model_id="local-minilm")
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦åˆ›å»º
            index_path = os.path.join(test_dir, f"{test_doc_id}.index")
            chunks_path = os.path.join(test_dir, f"{test_doc_id}.pkl")
            
            if os.path.exists(index_path) and os.path.exists(chunks_path):
                print(f"âœ“ ç´¢å¼•æ–‡ä»¶åˆ›å»ºæˆåŠŸ!")
                print(f"  - ç´¢å¼•æ–‡ä»¶: {index_path}")
                print(f"  - åˆ†å—æ–‡ä»¶: {chunks_path}")
            else:
                print(f"âœ— ç´¢å¼•æ–‡ä»¶æœªåˆ›å»º")
                return False
            
            # æµ‹è¯•æ£€ç´¢
            test_queries = [
                "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ?",
                "NLPæ˜¯ä»€ä¹ˆ?",
                "å¼ºåŒ–å­¦ä¹ å¦‚ä½•å·¥ä½œ?"
            ]
            
            print("\n" + "-" * 60)
            print("æµ‹è¯•å‘é‡æ£€ç´¢:")
            print("-" * 60)
            
            for query in test_queries:
                print(f"\næŸ¥è¯¢: '{query}'")
                context = get_relevant_context(
                    test_doc_id,
                    query,
                    vector_store_dir=test_dir,
                    pages=[],
                    top_k=2
                )
                
                if context:
                    print(f"âœ“ æ£€ç´¢åˆ°ç›¸å…³å†…å®¹:")
                    # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦
                    preview = context[:200] + "..." if len(context) > 200 else context
                    print(f"  {preview}")
                else:
                    print(f"âœ— æœªæ£€ç´¢åˆ°å†…å®¹")
            
            print("\nâœ“ å®Œæ•´æµç¨‹æµ‹è¯•æˆåŠŸ!")
            return True
            
        finally:
            # æ¢å¤åŸå§‹è·¯å¾„å¹¶æ¸…ç†ä¸´æ—¶ç›®å½•
            app.VECTOR_STORE_DIR = original_vector_dir
            try:
                shutil.rmtree(test_dir)
                print(f"\næ¸…ç†ä¸´æ—¶ç›®å½•: {test_dir}")
            except:
                pass
        
    except Exception as e:
        print(f"âœ— é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_local_embedding_model():
    success, _ = _run_local_embedding_model()
    assert success is True


def test_multilingual_embedding_model():
    success, _ = _run_multilingual_embedding_model()
    assert success is True


def test_vector_search():
    assert _run_vector_search() is True


def test_app_embedding_function():
    assert _run_app_embedding_function() is True


def test_build_and_search_index():
    assert _run_build_and_search_index() is True


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("ChatPDF - å‘é‡æ£€ç´¢å’ŒåµŒå…¥æ¨¡å‹åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    print("\nè¿™ä¸ªæµ‹è¯•å°†éªŒè¯:")
    print("  1. æœ¬åœ°å…è´¹åµŒå…¥æ¨¡å‹èƒ½å¦æ­£å¸¸åŠ è½½(é¦–æ¬¡ä¼šè‡ªåŠ¨ä¸‹è½½)")
    print("  2. å¤šè¯­è¨€åµŒå…¥æ¨¡å‹èƒ½å¦æ­£å¸¸å·¥ä½œ")
    print("  3. FAISSå‘é‡æ£€ç´¢åŠŸèƒ½æ˜¯å¦æ­£å¸¸")
    print("  4. åº”ç”¨çš„åµŒå…¥å‡½æ•°æ¥å£æ˜¯å¦æ­£å¸¸")
    print("  5. å®Œæ•´çš„ç´¢å¼•æ„å»ºå’Œæ£€ç´¢æµç¨‹æ˜¯å¦æ­£å¸¸")
    print("\n" + "=" * 60)
    
    results = []
    
    # è¿è¡Œæµ‹è¯•
    success1, _ = _run_local_embedding_model()
    results.append(("æœ¬åœ°åµŒå…¥æ¨¡å‹", success1))
    
    success2, _ = _run_multilingual_embedding_model()
    results.append(("å¤šè¯­è¨€åµŒå…¥æ¨¡å‹", success2))
    
    success3 = _run_vector_search()
    results.append(("FAISSå‘é‡æ£€ç´¢", success3))
    
    success4 = _run_app_embedding_function()
    results.append(("åº”ç”¨åµŒå…¥å‡½æ•°", success4))
    
    success5 = _run_build_and_search_index()
    results.append(("å®Œæ•´ç´¢å¼•æµç¨‹", success5))
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 60)
    
    for name, success in results:
        status = "âœ“ é€šè¿‡" if success else "âœ— å¤±è´¥"
        print(f"{status} - {name}")
    
    all_passed = all(success for _, success in results)
    
    if all_passed:
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! å‘é‡æ£€ç´¢å’ŒåµŒå…¥æ¨¡å‹åŠŸèƒ½æ­£å¸¸!")
        print("=" * 60)
        print("\nâœ“ å…è´¹æœ¬åœ°åµŒå…¥æ¨¡å‹å·²æˆåŠŸä¸‹è½½å¹¶å¯ä»¥ä½¿ç”¨")
        print("âœ“ å‘é‡æ£€ç´¢åŠŸèƒ½å·¥ä½œæ­£å¸¸")
        print("âœ“ åº”ç”¨å¯ä»¥æ­£å¸¸è¿›è¡Œè¯­ä¹‰æœç´¢")
    else:
        print("\n" + "=" * 60)
        print("âš  éƒ¨åˆ†æµ‹è¯•å¤±è´¥,è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        print("=" * 60)
    
    return all_passed


if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\n\næµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
