{
  "filename": "DiffuLT.pdf",
  "upload_time": "2025-12-12T20:21:53.788854",
  "data": {
    "full_text": "DiffuLT: Diffusion for Long-tail Recognition Without\nExternal Knowledge\nJieShao KeZhu HanxiaoZhang JianxinWu‚àó\nNationalKeyLaboratoryforNovelSoftwareTechnology,NanjingUniversity,China\nSchoolofArtificialIntelligence,NanjingUniversity,China\n{shaoj, zhuk, zhanghx}@lamda.nju.edu.cn, wujx2001@nju.edu.cn\nAbstract\nThispaperintroducesanovelpipelineforlong-tail(LT)recognitionthatdiverges\nfromconventionalstrategies. Instead,itleveragesthelong-taileddatasetitselfto\ngenerateabalancedproxydatasetwithoututilizingexternaldataormodel. We\ndeployadiffusionmodeltrainedfromscratchononlythelong-taileddatasetto\ncreatethisproxyandverifytheeffectivenessofthedataproduced. Ouranalysis\nidentifies approximately-in-distribution (AID) samples, which slightly deviate\nfrom the real data distribution and incorporate a blend of class information, as\nthecrucialsamplesforenhancingthegenerativemodel‚Äôsperformanceinlong-tail\nclassification. We promote the generation of AID samples during the training\nof a generative model by utilizing a feature extractor to guide the process and\nfilteroutdetrimentalsamplesduringgeneration. Ourapproach,termedDiffusion\nmodel for Long-Tail recognition (DiffuLT), represents a pioneer application of\ngenerativemodelsinlong-tailrecognition. DiffuLTachievesstate-of-the-artresults\nonCIFAR10-LT,CIFAR100-LT,andImageNet-LT,surpassingleadingcompetitors\nbysignificantmargins. Comprehensiveablationsenhancetheinterpretabilityof\nourpipeline. Notably,theentiregenerativeprocessisconductedwithoutrelying\nonexternaldataorpre-trainedmodelweights,whichleadstoitsgeneralizabilityto\nreal-worldlong-tailedscenarios.\n1 Introduction\nDeeplearninghasexhibitedremarkablesuccessacrossaspectrumofcomputervisiontasks,especially\ninimageclassification, e.g., asexhibitedbyHeetal.[2016],Dosovitskiyetal.[2021],Liuetal.\n[2021]. Thesemodels,however,encounterobstacleswhenfacedwithreal-worldlong-tailed(LT)\ndata,wherethemajorityclasseshaveabundantsamplesbuttheminorityonesaresparselyrepresented.\nTheintrinsicbiasofdeeplearningarchitecturestowardsmorepopulousclassesexacerbatesthisissue,\nleadingtosub-optimalrecognitionofminorityclassesdespitetheircriticalimportanceinpractical\napplications.\nConventionallong-tailedlearningstrategiessuchasre-weighting(Linetal.[2017],Caoetal.[2019a]),\nre-sampling(Zhouetal.[2020a],Zhangetal.[2021a]),andstructuraladjustments(Wangetal.[2020],\nCuietal.[2022]),shareacommonality: theyacknowledgethedata‚Äôsimbalanceandfocusonthe\ntrainingofmodels. Theydemandmeticulousdesignandarechallengingtogeneralize. Recently,\na shift towards studying the dataset itself and involving more training samples through external\nknowledgetomitigatelong-tailedchallengeshasemerged(Zhangetal.[2021b,2024a]). Yet,inmany\nreal-worldscenarios,accesstospecializeddataislimitedandcloselyguarded,suchasinmilitaryor\nmedicalcontexts. Thispredicamentpromptsacriticalinquiry: Isitfeasibletobalancelong-tailed\ndatasetswithoutdependingonexternalresourcesormodels?\n‚àóJ.Wuisthecorrespondingauthor.\n38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).\n\n\u0000\u0018\u0000\u0013\n\u0000\u0017\u0000\u0013\n\u0000\u0016\u0000\u0013\n\u0000\u0015\u0000\u0013\n\u0000\u0014\u0000\u0013\n\u0000\u0013\n\u0000\u001a\u0000\u0011\u0000\u0018 \u0000\u001a\u0000\u0011\u0000\u0013 \u0000\u0019\u0000\u0011\u0000\u0018 \u0000\u0019\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0011\u0000\u0018\n\u0000)\u0000,\u0000'\n\u0000\f\u0000\b\u0000\u000b\u0000\u0003DIAp\n\u0000\u0018\u0000\u0013\n\u0000\u0017\u0000\u001b\n\u0000\u0017\u0000\u0019\n\u0000\u0017\u0000\u0017\n\u0000\u0017\u0000\u0015\n\u0000\u0017\u0000\u0013\n\u0000\u0016\u0000\u001b\n\u0000\f\u0000\b\u0000\u000b\u0000\u0003\u0000\u0011\u0000F\u0000F\u0000$\n\u0000'\u0000'\u00003\u00000\n\u0000&\u0000%\u0000'\u00000\n\u00002\u0000X\u0000U\u0000V\n\u0000%\u0000D\u0000V\u0000H\u0000O\u0000L\u0000Q\u0000H\nFigure1: Thesamplesgeneratedbydiffusionmodelsimprovelong-tailclassificationonCIFAR100-\nLT,showingacorrelationbetweenFIDandaccuracyandastrongercorrelationbetweentheproportion\nofAIDsamplesandaccuracy. Ourmethodsignificantlyboostsclassifieraccuracycomparedwith\nothers(left). Featurespacevisualizationrevealsthatdifferentdiffusionmodelsgeneratesampleswith\nvaryingdistributions,andourmodelbiasesthegenerativeprocesstowardAIDsamples(right).\nOuranswerisyes. Recentadvancesindiffusionmodelshavedemonstratedtheirsignificantpotential\nin generating high-quality images (Ho et al. [2020], Song et al. [2020], Rombach et al. [2022]).\nAssumingthatdiffusionmodelsareproficientatlearningdistributions,wedevelopadiffusionmodel\ntrainedfromscratchononlythelong-taildistributeddataset. Thismodelcreatesnewsamplesfor\nunderrepresentedclasses,whicharethenusedtotrainaclassifieronthere-balanceddataset,leading\ntoimprovedaccuracy. Wearethefirsttodemonstratetheeffectivenessofusinggeneratedsamples,\nwithoutrelyingonexternaldataormodels,inimprovinglong-tailclassification. Weobserveanotable\npattern: enhancingtheperformanceofthegenerativemodelwithalossmodificationcalledCBDM\n(Qinetal.[2023])alsoenhancestheclassifier‚Äôsaccuracy,asillustratedinfig.1. Thisphenomenon\nimpliesthatagenerativemodelwithbetterperformancetendstogeneratesamplesthataremore\nbeneficialfortheclassificationtask. Thisobservationraisesanimportantquestion: Whatarethemost\nvaluablegeneratedsamplesforclassification,andhowaretheygenerated? Thisquestioniscritical,\nasitdetermineswhetheradiffusionmodelisgoingtobebeneficialordetrimentalforLTrecognition.\nWeanswerthisquestionbyanalyzingfeaturesofgeneratedsamples,asvisualizedinfig.1using\nt-SNE(VanderMaatenandHinton[2008]). Wecategorizethegeneratedsamplesintothreegroups:\nin-distribution(ID),approximatelyin-distribution(AID),andout-of-distribution(OOD).Ourresearch\nindicatesthatAIDsamplesarepivotalinenhancingclassifierperformance. Throughexperiments,we\nconcludethatadiffusionmodelcanassimilatepatternsfromtheheadclassesandintegratestheminto\nthetailonestoproduceAIDsamples. Thesesamplessignificantlyenhancethequantityanddiversity\nofthetailclasses,therebysubstantiallyimprovingtheirperformance. Thentheimportantquestionto\nsolveis: HowcanwegenerateAIDsamplesefficiently?\nToencouragethemodeltopredominantlygenerateAIDsamples,weintroduceanoveltypeofloss.\nThislossemploysafeatureextractortopenalizethegenerationofIDandOODsamples. Sucha\nstrategy not only elevates the performance of the generative model on long-tail datasets but also\nrendersitmoreeffectiveandefficientinenhancingclassifierperformance.\nIngeneral,weintroduceanewpipeline,DiffuLT(DiffusionmodelforLong-Tailrecognition),for\nlong-taildatasets. Ithasthreesteps: initialtraining,samplegeneration,andretraining. Initially,we\ntrainafeatureextractorandadiffusionmodelincorporatingasupervisiontermtoencouragethe\ngenerationofAIDsamples. Subsequently,thisgenerativemodelisemployedtoaugmentthedataset\ntowardsbalance. Thefinalstepinvolvestraininganewclassifierontheenricheddataset,withaminor\nadjustmenttoreducetheimpactofsyntheticsamples. Itiscrucialtounderscoretheimportanceof\ntrainingthediffusionmodelwithoutexternaldataorknowledge,tomaintainfairnessincomparison.\nOurcontributionsaresummarizedasfollows:\n‚Ä¢ We pioneer in addressing long-tail recognition by synthesizing images using diffusion\nmodelswithoutrelyingonexternaldata.\n2\n\n‚Ä¢ Ourresearchdelvesintothemechanismsunderlyingourapproach,highlightingthesignifi-\ncanceofthegeneratedAIDsamples. Thesesamplesemergefromafusionofinformation\nfrombothheadandtailclasses,playingacrucialroleinenhancingclassifierperformance.\n‚Ä¢ Weintroduceanovellossfunctionthatenhancestheperformanceofdiffusionmodelson\nlong-taileddatasetsandbiasesthemtowardsgeneratingAIDsamples,therebymakingthe\ngenerationprocessmoreeffectiveandefficientforclassification.\nExtensiveexperimentalvalidationacrossCIFAR10-LT,CIFAR100-LT,andImageNet-LTdatasets\ndemonstratesthesuperiorperformanceofourmethodoverexistingapproaches.\n2 RelatedWork\nLong-tailed recognition Long-tailed recognition is a challenging and practical task (Cui et al.\n[2019],Zhouetal.[2020b],Caoetal.[2019b],Zhangetal.[2023a],Zhuetal.[2024]),sincenatural\ndataoftenconstituteasqueezedandimbalanceddistribution. Themajorityoftraditionallong-tailed\nlearningmethodscanbeviewedas(orspecialcases)ofre-weighting(Caoetal.[2019b],Kangetal.\n[2020],Zhongetal.[2021a],Wangetal.[2024a])andre-sampling(Cuietal.[2019]),withmore\nemphasis on the deferred tail class to seek an optimization trade-off. There are variants of them\nthatadoptself-supervisedlearning(Zhuetal.[2023],Lietal.[2021]),theoreticalanalysis(Lietal.\n[2022],Menonetal.[2021],Yangetal.[2024])anddecouplingpipeline(Kangetal.[2020],Zhou\netal.[2020b])totacklelong-tailedlearningfromvariousaspects,andtheyallachieveseemingly\ndecentperformanceindownstreamtasks.\nOneofthecoredifficultiesinlong-tailedlearningistheinsufficiencyoftailsamples. Andrecently,\nquitesomeworksstarttofocusonthisaspectbyinvolvingmoretrainingsamplesthroughexternal\nknowledge(Zhangetal.[2021b],Ramanathanetal.[2020],Dongetal.[2022],Shietal.[2023a]).\nNevertheless,themostdistinctdrawbackoftheseworksisthattheyeitherrelyonexternaldatasource\norstrongmodelweights. Thisconditioncanseldomlyholdtrueinpracticalscenarioswhereonly\nahandfulofspecializeddataareavailableandaresecretlykept(considersomeimportantmilitary\nor medical data). We thus raise a natural question about long-tailed learning: can we utilize the\nadvantageofgeneratingtailsampleswithoutresortingtoanyexternaldataormodel? Thatis,the\nwholeprocessisdoneinanin-domain(alsocalledheld-in)manner. Inthispaper,weproposeto\nadopttheoff-the-shelfdiffusionmodeltolearnandgeneratesamplesfromthedataathand.\nDiffusionmodelsandsyntheticdata Diffusionmodelshavebeenhighlycompetitiveinrecent\nyears(Hoetal.[2020],Songetal.[2020]),producingpromisingimagequalityinbothunconditional\nandconditionalsettings(DhariwalandNichol[2021],Rombachetal.[2022],Rameshetal.[2021]).\nDespitethepredominantuseincreatingdigitalart,theapplicationofdiffusionmodelsinscenariosof\nlimiteddataremainsunder-explored. Thispaperaffirmstheutilityofdiffusionmodelsinenhancing\nrepresentation learning, particularly within the long-tailed learning framework, offering a novel\ninsightintotheirapplicationbeyondconventionalgenerativetasks.\nTheintegrationofsyntheticdataintodeeplearning,generatedthroughmethodslikeGANsGoodfel-\nlowetal.[2014],Isolaetal.[2017]anddiffusionmodels(DhariwalandNichol[2021],Rombachetal.\n[2022]),hasbeenexploredtoenhanceperformanceinimageclassification(Kongetal.[2019],Azizi\netal.[2023],Zhangetal.[2024a],Trabuccoetal.[2023]),objectdetection(Zhangetal.[2023b]),and\nsemanticsegmentation(Zhangetal.[2021c,2023b]). Theseapproachesoftendependonsubstantial\nvolumesoftrainingdataorleveragepre-trainedmodels,suchasStableDiffusion,forhigh-quality\ndatageneration. Yet,theefficacyofgenerativemodelsandsyntheticdataundertheconstraintof\nlimitedavailabledataandinaddressingimbalanceddatadistributionsremainsanunresolvedinquiry.\nThis paper specifically addresses this question, evaluating the viability of generative models and\nsyntheticdatainscenarioswheredataisscarceandimbalanced.\n3 Method\n3.1 Preliminaries\nFor image classification, we have a long-tail dataset D = {(x ,y )}N ,y ‚àà C with each x\ni i i=1 i i\nrepresentinganinputimageandy representingitscorrespondinglabelfromthesetofallclasses\ni\n3\n\nTable1: FIDof different generationmodels Table2: Percentageofdifferenttypesofgener-\nandtheircorrespondingclassifiers‚Äôaccuracy. atedsamplesforeachmodel.\nModel FID Acc.(%) Model p p p\nID AID OOD\nBaseline - 38.3\nDDPM 39.1 21.2 39.7\nDDPM 7.76 43.8\nCBDM(œÑ =3) 38.6 29.1 32.3\nCBDM(œÑ =3) 7.42 44.8\nCBDM(œÑ =2) 6.82 46.0 CBDM(œÑ =2) 40.2 33.5 26.3\nCBDM(œÑ =1) 5.86 46.6 CBDM(œÑ =1) 44.8 36.3 18.9\nC. Inthelong-tailsetting,afewclassesdominatewithmanysamples,whilemostclasseshavevery\nfewimages,leadingtoasignificantclassimbalance. TheclassesinC areorderedbysamplecount\nwith|c | ‚â• |c | ‚â• ... ‚â• |c |, where|c |denotesthenumberoftrainingsamplesinclassc and\n1 2 M j j\n|c |‚â´|c |. Theratior = |c1| isdefinedasthelong-tailratio. Thegoaloflong-tailclassification\n1 M |cM|\nistolearnaclassifierf :X ‚ÜíY capableofeffectivelyhandlingthetailclasses.\nœÜ\nThenaiveideaistotrainagenerativemodelŒ∏onthelong-taildatasetDandusethetrainedmodel\ntogeneratenewsamplesandsupplementthetailclasses. Inspiredbyitssuperiorperformance,we\nselect diffusion models as the generative model in our pipeline. In our approach, we follow the\nDenoisingDiffusionProbabilisticModel(DDPMbyHoetal.[2020])framework. Givenadataset\nD = {x ,y }N , wetrainadiffusionmodeltomaximizethelikelihoodofthedataset. Atevery\ni i i=1\ntrainingstep,wesampleamini-batchofimagesx fromthedatasetandaddnoisetoobtainx ,\n0 t\n‚àö\nq(x |x )=N( Œ±¬Ø x ,(1‚àíŒ±¬Ø )I), (1)\nt 0 t 0 t\nwhereŒ±¬Ø = (cid:81)t (1‚àíŒ≤ )iscalculatedthroughpre-definedvarianceschedule{Œ≤ ‚àà (0,1)}T .\nt i=1 i t t=1\nAftertrainingadiffusionmodelŒ∏togetp (x |x ,t),wereversetheaboveprocessstepbystep\nŒ∏ t‚àí1 t\ntorecovertheoriginalimagex frompurenoisex ‚àºN(0,I). Thetrainingobjectiveistoreduce\n0 T\nthegapbetweentheaddednoiseinforwardprocessandtheestimatednoiseinreverseprocess:\n‚àö ‚àö\nL =E [‚à•œµ ‚àíœµ ( Œ±¬Ø x + 1‚àíŒ±¬Ø œµ ,t)‚à•2], (2)\nDDPM t‚àº[1,T],x0,œµt t Œ∏ t 0 t t\nwhere œµ ‚àº N(0,I) is the noise added to original images and œµ is the noise estimated by the\nt Œ∏\ntrainablemodelwithparametersŒ∏. DDPMcanbeconditionalbytransformingy intoatrainable\nclassembeddingandincorporatingthelabelydirectlyasaminput,similartotimet. Toimprovethe\nperformanceofDDPMonlong-taileddataset,severalworks(Qinetal.[2023],Zhangetal.[2024b])\nhave been proposed to adjust the distribution of generated samples. CBDM adds a distribution\nadjustmentregularizeratthelossterm. Thistermisdesignedtopromotethegenerationofsamples\nfortailclasses,whichisdefinedas(wheresgmeansstopgradient):\nœÑt (cid:88)\nL = (‚à•œµ (x ,t,y)‚àísg(œµ (x ,t,y‚Ä≤))‚à•2+Œ≥‚à•sg(œµ (x ,t,y))‚àíœµ (x ,t,y‚Ä≤)‚à•2). (3)\nCBDM |Y| Œ∏ t Œ∏ t Œ∏ t Œ∏ t\ny‚Ä≤‚ààY\n3.2 DiffuLT:DiffusionmodelforLong-Tailrecognition\nDiffusionmodelhelpslong-tailclassification. Inthisphase,arandomlyinitializeddiffusionmodel\nŒ∏istrainedtoenrichthedataset. PreliminaryexperimentsinvolvetrainingaDDPMonalong-tailed\ndatasetandusingittogenerateadditionaldata. AthresholdN isset,andforclassesc withfewer\nt j\nthan N samples, we generate the images to meet this threshold. This augmentation results in a\nt\ncollectionofsyntheticsamples, D = {(x ,y )}Ngen, whereN = (cid:80) max(0,N ‚àí|c |)\ngen i i i=1 gen cj‚ààC t j\nrepresentsthetotalnumberofgeneratedsamples. Thesegeneratedsamplesarethenintegratedwith\nthe original dataset, forming an augmented dataset D‚à™D , on which a classifier is trained to\ngen\nenhanceclassificationperformance.\nWeconductedexperimentsonCIFAR100-LTwithanimbalanceratioof100andsetN =500to\nt\nsupplementthedata. Theresults,detailedinthesecondlineoftable1,showa5.5%accuracyincrease\nfor the classifier trained on D ‚à™D compared to the baseline. This improvement underscores\ngen\ntheeffectivenessofourstraightforwardmethodinboostingoverallperformance. Consideringthe\ngeneratedsamples(especiallyfortailclasses)maybeoflowerqualityduetolimiteddataavailability,\n4\n\nFigure2:Visualizationofgeneratedsamplesforclass90infeaturespaceusingt-SNE.Theassociated\nmodelisindicatedintheupper-leftcorner.\nTable3:Quantities,overallclassifierenhancement,and Table4: Diffusiontrainedwithvarying\naverageimprovementpersamplefordifferentgroups proportionsofheadclassdataandthe\nofdatageneratedbydiffusionmodel. correspondingresultsfortailclasses.\nGroup ‚à•D gen‚à• Acc.(%) ‚àÜAcc/‚à•D gen‚à• p h p AID Acc t(%)\n- - 25.0\nBaseline - 38.3\n0 25.8 26.0\nID 21,511 44.2 2.75√ó10‚àí4\n40 33.2 29.7\nAID 11,886 45.2 5.78√ó10‚àí4 80 35.7 32.5\nOOD 5,756 36.2 ‚àí3.61√ó10‚àí4 100 39.1 32.8\nClass-BalancingDiffusionModels(CBDM)isemployedtoimprovegenerationqualityinlong-tailed\nsettings. ByintegratingL andL intrainingthemodelŒ∏onD,thedatasetisenhanced,\nDDPM CBDM\nandaclassifieristrainedasdescribedpreviously. SubsequenttestingonCIFAR100-LTrevealsthat\ntheclassifierachievesanaccuracyof46.6%,markingan8.3%increaseoverthebaseline,asnotedin\nthefinallineoftable1.\nWhat samples are helpful? AID samples! We adjusted the hyper-parameter œÑ in L and\nCBDM\nevaluatemodelswithvaryingFIDscores. Resultspresentedintable1showthataccuracyimproves\nasFIDdecreases. LowerFIDscoresindicatethatgeneratedsamplesmorecloselyresemblethereal\ndatadistribution. Notably,somegeneratedsamplesclearlyfail,whileotherscorrectlyresembletheir\nintendedclass. Thisobservationmotivatesfurtherinvestigationintotheefficacyofsamples.\nClass 90 (truck) is selected randomly as a representative example in CIFAR100-LT. A baseline\nclassifier(œÜ ),trainedexclusivelyontheoriginaldatasetD,isusedtoanalyzethegenerateddata.\n0\nThis classifier extracts features which are then visualized using t-SNE, as shown in fig. 2. The\nvisualization reveals that samples generated via CBDM tend to be more centralized. For deeper\nanalysis,wedefinethecenterf ofaclass‚Äôsfeaturesastheaverageoftherealdatainfeaturespace,\no\nandsetthemaximumEuclideandistancebetweentworealsamples‚Äôfeaturesasathresholdd . We\nf\nthendefine3typesofthegeneratedsamplesbasedontheirdistancetof :\no\n(cid:40) d ‚â§d , ID\ni f\nd =‚à•f ‚àíf ‚à• : d <d ‚â§2d , AID (4)\ni i o 2 f i f\nd >2d , OOD\ni f\nwhereIDdenotesin-distributionsamples,whichcloselymatchthepatternsoftheoriginaldata. We\ndefineandnameapproximatelyin-distribution(AID)samples,whichexhibitslightdeviations. OOD\nstandsforout-of-distributionones,whicharesignificantlydifferingfromthecenter. Wesummarize\nthecompositionofsamplesgeneratedbyeachmodelintable2. Notably,theCBDMmodelgenerates\nalowerproportionofOODsamples,consistentwithitsFIDscore. Forevaluatingtheimpactofeach\ntype,wetrainclassifiersusingonlytheID,AID,andOODsamplesgeneratedbyCBDMwithœÑ =1\nrespectivelyasD ,combinedwithD,andpresenttheresultsintable3. Surprisingly,classifiers\ngen\ntrainedwithAIDsamplesachievethehighestaccuracyandshowthegreatestaverageimprovement\nper sample. Basedon this finding, ourhypothesis is thatAID samplesarethe most beneficialin\nenhancingclassifierperformance.\nMechanismsbehindtheAIDsamples. WeconductedexperimentstoexplorehowAIDsamples\nenhanceclassifierperformanceandwheretheirnewandusefulinformationoriginates. Adiffusion\nmodel (CBDM with œÑ = 1) is trained using images from tail classes (fewer than 100 samples),\nsupplemented by a variable proportion p of head class images. This model generates samples\nh\n5\n\nID AID OOD ID AID OOD ID AID OOD\nSnake Tiger Wardrobe\nSkyscraper Train Whale\nFigure3: Examplesofthreegroupsofgeneratedsamples.\nspecificallyfortailclasseswiththeproportionofAIDsamplesp ,andgetstheperformanceof\nAID\nthecorrespondingclassifierdenotedasAcc . Theresults,presentedintable4,showthatatp =0%,\nt h\nrelyingsolelyontailclassimages,p is25.8%,andAcc improvesmarginallyto26.0%,only1%\nAID t\nabovethebaseline. Asp increases,bothp andAcc rise,peakingatp = 100%. Thistrend\nh AID t h\nillustratesthediffusionmodel‚Äôsabilitytotransferinformationfrompopuloustounderrepresented\nclasses,effectivelyblendingdataacrossdifferentclassesintoAIDsamples. Examplesofthesample\ngroupsaredisplayedinfig.3,whereIDsamplescloselyresemblerealimages,AIDsamplesblend\npatternsfrommultipleclasses,andOODsamplestypicallyexhibitanomalies.\nGeneration of AID samples. How can we efficiently generate AID samples? While a filtering\nstrategycanbeusedtocollectAIDsamples,itisnotthemostefficientmethod. Amoreeffective\napproachinvolvesencouragingthegenerationmodeltospecificallyproduceAIDsamples. Given\nthatAIDsamplesaredefinedbytheirdistancefromthecenterofrealimagesinfeaturespace,we\ncanutilizethebaselineclassifierœÜ asafeatureextractortoguidethegenerationofAIDsamples.\n0\nOurgoalistoencourageacontrolleddeviationwithinfeaturespace. AfterT denoisingsteps,the\ndeviationshouldideallybewithintherangeofd to2d . Assumingthatthedeviationineachstepis\nf f\nproportionaltothenoisestrength,weintroduceanadditionalterminthelossfunctiontoencourage\nsmall,stepwisedeviations. Wedefinethedeviationateachstepinfeaturespaceas\n‚àö\n1‚àíŒ±¬Ø\nd = ‚àö T‚à•œÜ (x )‚àíœÜ (x +œµ ‚àíœµ (x ,t,y))‚à• , (5)\nt 0 0 0 0 t Œ∏ t 2\n1‚àíŒ±¬Ø\nt\nwhereœÜ (x +œµ ‚àíœµ (x ,t,y))representsthede-noisedimages‚Äôfeature. ThenewAIDlossisthen\n0 0 t Œ∏ t\n3\nL =Œ±E ‚à•d ‚àí d ‚à•2. (6)\nAID t‚àº[1,T],x0,œµt t 2 f\nwhereŒ±isahyper-parameteranddefaultedto0.1. WeincorporatethistermintobothL and\nDDPM\nL totrainthegenerationmodel. Aftertraining,weusethismodeltogeneratedata. During\nCBDM\nthegenerationprocess, weemployœÜ tofilteroutharmfulOODsamples, resultinginD . We\n0 gen\nthentraintheclassifierusingthecombineddatasetD‚à™D . Recognizingthatgenerateddataare\ngen\nlesscrucialthanrealimages,weintroduceaweightingtermtothecross-entropylosstoadjustthe\ninfluenceofthegeneratedsamples:\nL\n=‚àí(cid:88)\n(œây +(1‚àíy ))log\nexp(f œÜ,y(x))\n, (7)\ncls\n(x,y,yg)‚ààD‚à™Dgen\ng g (cid:80)M\ni=1exp(f œÜ,ci(x))\nwhereœâcontrolstheweightofgeneratedsamplesandissetto0.3bydefault. y isanadditionallabel\ng\nassignedtoeachimagex,whichdistinguishesbetweengeneratedandoriginalsamples. Specifically,\ny =1isusedforgeneratedsamples,whiley =0markstheoriginalones.\ng g\n3.3 OverallPipelineandDiscussion\nNow we are ready propose a new pipeline called DiffuLT to address long-tail recognition. The\npipelineisshowninfig.4withfoursteps:\n‚Ä¢ Training: Initially,wetrainafeatureextractorœÜ andaconditional,AID-biaseddiffusion\n0\nmodelŒ∏usingtheoriginallong-taileddatasetDalone.\n6\n\nImbalancedDatasetùíü\nTraining ‚Ñíùëêùëôùë†\nForward only\nùúë0 ùëìùúë\nùúë0 Feature Extractor\nFinalClassifier\nTsteps\nùúë0\nùúî‚àô‚Ñíùëêùëôùë†\nGenerate\nDiffusionùúÉ\nùíügen\nFigure4: TheoverallpipelineofourmethodDiffuLT.\n‚Ä¢ Generating: We establish a threshold N and employ the trained diffusion model Œ∏ to\nt\ngenerate and supplement samples. Using œÜ , we filter out OOD samples, resulting in a\n0\nrefineddatasetD .\ngen\n‚Ä¢ Training: We then train a new classifier f on the augmented dataset D ‚à™D using\nœÜ gen\nweightedcross-entropy,formingourfinalmodel.\nComparedtotraditionalmethodsthatfocusprimarilyontraining,oursnotonlyenhancesperformance\nbutisalsoreusableformodelupdates. Ourmethodrequiresmoretrainingtime,typicallyfourtimes\nlonger,totrainthegenerationmodelandproducesamples. However,ourmethodsprovevaluable\nwhenperformanceimprovementiscritical. Unliketypicaldataexpansionmethods,ourapproach\noffersbothpracticalandtheoreticalbenefitsbecauseitdon‚Äôtrelyonanyexternaldatasetormodel.\nFordetailedanalysis,pleaserefertotheappendixB.\n4 Experiment\n4.1 Experimentalsetup\nDatasets. Our research evaluate three long-tailed datasets: CIFAR10-LT (Cao et al. [2019a]),\nCIFAR100-LT(Caoetal.[2019a]),andImageNet-LT(Liuetal.[2019a]).Followingthemethodology\ndescribedin(Caoetal.[2019a]),weconstructlong-tailedversionsofthefirsttwodatasetsbyadjusting\nthelong-tailratiorto100,50,and10totestourmethodagainstvariouslevelsofimbalance.\nBaselines. Inourcomparativeanalysis,webenchmarkagainstabroadspectrumofclassicaland\ncontemporarylong-tailedlearningstrategies. Themethodscomparedcanbeclassifiedintomultiple\ngenreslikere-weightingandre-samplingtechniques,head-to-tailknowledgetransferapproaches,data-\naugmentation,andsoon. Somemethodshaveissuessuchasunfaircomparisonsorimplementation\nproblems. WedocumentboththeirresultsandourimplementationoutcomesintheappendixA.\nImplementation. WesetŒ± = 0.1,andœâ = 0.3. ThegenerationthresholdsN forCIFAR10-LT\nt\nandCIFAR100-LTwerefixedat5000and500,respectively. WeemployResNet-32astheclassifier\nbackbone. ForImageNet-LTexperiments,wesetagenerationthresholdofN =300. Theclassifiers\nt\nwerebasedonResNet-10andResNet-50architectureswithœâ =0.5.\nMoredetailsabouttheexperimentalsetupareavailableintheappendixA.\n4.2 ExperimentalResults\nGenerativeResults. Weassesstheefficacyofourspeciallydesignedlossfunction,detailedintable5.\nThis function improves the FID by reducing OOD samples, while also increasing the number of\nAIDsamplesandtheclassifier‚Äôsaccuracy. Furtherexperimentsintable6highlightthenecessityof\nourtrainingloss. Forbenchmarking,weuseabasicfilteringstrategyforCBDM,withallmodels\ngeneratingsamplestomeetthethresholdN foreachclass. Theterms\"Kept\"and\"G-Num\"denote\nt\n7\n\nTable5: FIDofdiffusionmodel,proportionof Table6: Methodsandtypesofretainedsamples,\nsamples,andcorrespondingclassifieraccuracy pre-filteringcounts,andclassificationaccuracy.\nMethod FID p p p Acc.(%) Method Kept G-Num Acc.(%)\nID AID OOD\nCBDM All 39,153 46.6\nDDPM 7.76 39.1 21.2 39.7 43.8\nCBDM AID 108,684 48.1\nCBDM 5.86 44.8 36.3 18.9 46.6\nCBDM ID&AID 48,414 47.1\nOurs 5.37 40.7 50.1 9.2 49.7 Ours All 39,153 49.7\nTable7: ResultsonCIFAR100-LTandCIFAR10-LTdatasets. Theimbalanceratiorissetto100,50\nand10. Thehighest-performingresultsareinbold,withthesecond-bestinunderline. Additionally,\nwepresenttheresultsfordifferentgroups(many,medium,andfew)inCIFAR100-LTwithr =100.\nCIFAR100-LT CIFAR10-LT Statistics\nMethod\n100 50 10 100 50 10 Many Med. Few\nCE 38.3 43.9 55.7 70.4 74.8 86.4 65.2 37.1 9.1\nFocalLossLinetal.[2017] 38.4 44.3 55.8 70.4 76.7 86.7 65.3 38.4 8.1\nLDAM-DRWCaoetal.[2019a] 42.0 46.6 58.7 77.0 81.0 88.2 61.5 41.7 20.2\ncRTKangetal.[2019] 42.3 46.8 58.1 75.7 80.4 88.3 64.0 44.8 18.1\nBBNZhouetal.[2020a] 42.6 47.0 59.1 79.8 82.2 88.3 - - -\nRIDE(3experts)Wangetal.[2020] 48.0 - - - - - 68.1 49.2 23.9\nCAM-BSZhangetal.[2021a] 41.7 46.0 - 75.4 81.4 - - - -\nMisLASZhongetal.[2021b] 47.0 52.3 63.2 82.1 85.7 90.0 - - -\nDiVEHeetal.[2021] 45.4 51.1 62.0 - - - - - -\nCMOParketal.[2022] 47.2 51.7 58.4 - - - 70.4 42.5 14.4\nSAMRangwanietal.[2022] 45.4 - - 81.9 - - 64.4 46.2 20.8\nCUDAAhnetal.[2023] 47.6 51.1 58.4 - - - 67.3 50.4 21.4\nCSAShietal.[2023b] 46.6 51.9 62.6 82.5 86.0 90.8 64.3 49.7 18.2\nADRWWangetal.[2024b] 46.4 - 61.9 83.6 - 90.3 - - -\nH2TLietal.[2023] 48.9 53.8 - - - - - - -\nDiffuLT 51.5 56.3 63.8 84.7 86.9 90.7 69.0 51.6 29.7\nDiffuLT+BBN 51.9 56.7 64.0 85.0 87.2 90.9 69.5 51.9 30.2\nDiffuLT+RIDE(3experts) 52.4 56.9 64.2 85.3 87.3 90.9 70.3 52.1 30.7\nthetypesofsamplesretainedandthetotalnumberofsamplesgeneratedbeforefiltering,respectively.\nOurmethodsenhancethegenerationprocess‚Äôsefficiencyandachievethehighestaccuracy.\nCIFAR100-LT&CIFAR10-LT.Webenchmarkourapproachagainstarangeofmethodsonthe\nCIFAR100-LTandCIFAR10-LTdatasets, withresultsdetailedintable7. Theresultsnotshown\nin the original papers are indicated as \"-\" in the table. On CIFAR100-LT, our method surpasses\ncompetingmodels,achievingaccuracyimprovementsof13.2%,12.4%,and8.1%comparedwiththe\nbaselineforr =100,50,and10,respectively. OnCIFAR10-LT,ourmodelalsodemonstratesstrong\ncompetitiveness,enhancingaccuracyby14.3%,12.1%,and4.3%acrossthelong-tailratios,further\nvalidatingtheeffectivenessofourmethod. Sinceourmethodssolelymodifythetrainingdata,they\ncanbeeasilyintegratedwithothermethodstoachievebetterresults.\nFor CIFAR100-LT with an imbalanced ratio of 100, performance is also assessed across three\ncategories: many(classeswithover100samples),medium(classeswith20to100samples),andfew\n(classeswithfewerthan20samples). Whileourapproachdoesnotleadinthe‚ÄúMany‚Äùcategory,it\nexcelsin‚ÄúMed.‚Äù and‚ÄúFew‚Äù,significantlyoutperformingothersinthe‚ÄúFew‚Äùgroupwitha29.7%\naccuracy‚Äî8.3%abovethenearestcompetitorand20.6%beyondthebaseline.\nImageNet-LT. On the ImageNet-LT dataset, our methodology is evaluated against existing ap-\nproaches,withresultssummarizedintable8. UtilizingaResNet-10backbone,ourmethodregisters\na50.4%accuracy,outperformingthenearestcompetitorby4.5%. WithResNet-50,theaccuracy\nfurtherescalatesto56.4%,markingasubstantial14.8%enhancementoverthebaseline. Despitea\nslightdeclineinthe‚ÄúMany‚Äùcategoryrelativetothebaseline,ourapproachexcelsin‚ÄúMed.‚Äù and\n‚ÄòFew‚Äù,withthelatterwitnessingaremarkable33.6%improvementoverthebaseline. Ourmethod\ncanbecombinedwithotherstoachieveenhancedresults.\n8\n\nTable 8: Results on ImageNet-LT. We deploy ResNet-10 and ResNet-50 as classifier backbones.\nTop-performingresultsarehighlightedinbold,withsecond-bestoutcomesunderlined.\nResNet-10 ResNet-50\nAll All Many Med. Few\nCE 34.8 41.6 64.0 33.8 5.8\nFocalLossLinetal.[2017] 30.5 - - - -\nOLTRLiuetal.[2019b] 35.6 - - - -\ncRTKangetal.[2019] 41.8 47.3 58.8 44.0 26.1\nRIDE(3experts)Wangetal.[2020] 45.9 54.9 66.2 51.7 34.9\nMisLASZhongetal.[2021b] - 52.7 - - -\nCMOParketal.[2022] - 49.1 67.0 42.3 20.5\nSAMRangwanietal.[2022] 53.1 62.0 52.1 34.8\nCUDAAhnetal.[2023] - 51.4 63.1 48.0 31.1\nCSAShietal.[2023b] 42.7 49.1 62.5 46.6 24.1\nADRWWangetal.[2024b] - 54.1 62.9 52.6 37.1\nDiffuLT 50.4 56.4 63.3 55.6 39.4\nDiffuLT+RIDE(3experts) 51.1 56.9 64.1 55.8 39.9\nTable9: Ablationexperimentstoverifythe Table10: Performancewithdifferent\neffectofeachmodule. weightsœâandhyper-parameterŒ±.\nGen. L Filt. Weight Acc.(%) œâ Acc.(%) Œ± Acc.(%)\nAID\n0 38.3 0 38.3\n38.3\n0.1 49.2 0.1 49.7\n‚úì 46.6\n0.3 51.5 0.5 49.5\n‚úì ‚úì 49.7 0.5 50.1 1.0 48.3\n‚úì ‚úì ‚úì 50.3 0.7 50.3 2.0 45.1\n‚úì ‚úì ‚úì ‚úì 51.5 1.0 50.3 4.0 43.3\n4.3 AblationStudy\nDifferent modules in our pipeline. Our methodology comprises several critical components:\ngenerated samples (using CBDM), AID-biased loss, filtering, and weighted cross-entropy. We\nconduct ablation experiments on CIFAR100-LT with r = 100. The results, presented in table 9,\nhighlightthecrucialroleeachcomponentplaysinenhancingtheoverallperformance. Notably,the\ngeneratedsamplesandAID-biasedlossarethemostinfluentialfactors.\nHyper-parameters. Weadjusttheparametersœâ intheweightedcross-entropyandŒ±inL on\nAID\nCIFAR100-LTwithr =100andevaluatetheclassificationresults. Theseresultsaresummarizedin\ntable10. Throughiterativeadjustments,wefindthattheoptimalperformance,a51.5%classification\naccuracy, is achieved when œâ = 0.3. Similarly, the best setting for Œ± is determined to be 0.1.\nConsequently,weestablishœâ =0.3andŒ±=0.1asthedefaultsettingsforourmethod.\n5 Conclusion\nInthisresearch,weproposedanovel,data-centricapproachdesignedtoaddressthechallengesof\nlong-tailclassification. WedefinedandidentifiedAID(approximatelyin-distribution)samplesasthe\nimportantones. WethenrevisedadiffusionmodeltrainedwithanAID-biasedlosstermononlythe\noriginaldatasetforthepurposeofgeneratingmoreAIDsamples,therebysignificantlyenrichingthe\ndataset. Followingsamplegeneration,wetrainedaclassifieronthisenhanceddatasetandemployeda\nweightedcross-entropyloss. Ourmethodhasshowntodelivercompetitiveperformance,highlighting\nits efficacy in real-world applications. The experiments conducted as part of this study notably\nemphasizethecriticalroleplayedbyAIDsamplesandtheirsignificantimpact.\nWeproposethatthisapproachintroducesanewparadigmfortacklinglong-tailclassificationchal-\nlenges,offeringasubstantialcomplementtoexistingmethodologies. Itprovidesarobustframework\n9\n\nthatcanbeadaptedtovariousscenarioswhereperformanceisacriticalfactor. Despiteitsadvantages,\nthetrainingofthediffusionmodelandthegenerationofsamplesaretime-consuming. Theneedfor\noptimizationintrainingandgenerationspeedsrepresentsalimitationofourcurrentmethod. Wewill\nleavethispointasfutureworktofurtherimprovetheeffectivenessandefficiencyofourmethod.\nAcknowledgements\nWeacknowledgethefundingprovidedbytheNationalNaturalScienceFoundationofChinaunder\nGrant62276123andGrant61921006. J.Wuisthecorrespondingauthor.\nReferences\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,\npages770‚Äì778,2016.\nAlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas\nUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,\nandNeilHoulsby. Animageisworth16x16words: Transformersforimagerecognitionatscale.\nInICLR,2021.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swintransformer: Hierarchicalvisiontransformerusingshiftedwindows. InICCV,pages\n10012‚Äì10022,2021.\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll√°r. Focal loss for dense\nobjectdetection. InProceedingsoftheIEEEinternationalconferenceoncomputervision,pages\n2980‚Äì2988,2017.\nKaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced\ndatasetswithlabel-distribution-awaremarginloss. Advancesinneuralinformationprocessing\nsystems,32,2019a.\nBoyanZhou,QuanCui,Xiu-ShenWei,andZhao-MinChen. Bbn: Bilateral-branchnetworkwith\ncumulativelearningforlong-tailedvisualrecognition. InProceedingsoftheIEEE/CVFconference\noncomputervisionandpatternrecognition,pages9719‚Äì9728,2020a.\nYongshunZhang,Xiu-ShenWei,BoyanZhou,andJianxinWu. Bagoftricksforlong-tailedvisual\nrecognitionwithdeepconvolutionalneuralnetworks. InProceedingsoftheAAAIconferenceon\nartificialintelligence,volume35,pages3447‚Äì3455,2021a.\nXudongWang,LongLian,ZhongqiMiao,ZiweiLiu,andStellaYu. Long-tailedrecognitionbyrout-\ningdiversedistribution-awareexperts. InInternationalConferenceonLearningRepresentations,\n2020.\nJiequanCui,ShuLiu,ZhuotaoTian,ZhishengZhong,andJiayaJia. Reslt: Residuallearningfor\nlong-tailedrecognition. IEEEtransactionsonpatternanalysisandmachineintelligence,45(3):\n3695‚Äì3706,2022.\nChengZhang,Tai-YuPan,YandongLi,HexiangHu,DongXuan,SoravitChangpinyo,BoqingGong,\nandWei-LunChao. Mosaicos: asimpleandeffectiveuseofobject-centricimagesforlong-tailed\nobjectdetection. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,\npages417‚Äì427,2021b.\nYifanZhang,DaquanZhou,BryanHooi,KaiWang,andJiashiFeng. Expandingsmall-scaledatasets\nwithguidedimagination. AdvancesinNeuralInformationProcessingSystems,36,2024a.\nJonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin\nneuralinformationprocessingsystems,33:6840‚Äì6851,2020.\nYangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBen\nPoole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXivpreprint\narXiv:2011.13456,2020.\n10\n\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-\nresolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-\nenceoncomputervisionandpatternrecognition,pages10684‚Äì10695,2022.\nYiming Qin, Huangjie Zheng, Jiangchao Yao, Mingyuan Zhou, and Ya Zhang. Class-balancing\ndiffusionmodels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern\nRecognition,pages18434‚Äì18443,2023.\nLaurensVanderMaatenandGeoffreyHinton. Visualizingdatausingt-sne. Journalofmachine\nlearningresearch,9(11),2008.\nYinCui,MenglinJia,Tsung-YiLin,YangSong,andSergeBelongie. Class-balancedlossbasedon\neffectivenumberofsamples. InCVPR,pages9268‚Äì9277,2019.\nBoyanZhou,QuanCui,Xiu-ShenWei,andZhao-MinChen. Bbn: Bilateral-branchnetworkwith\ncumulativelearningforlong-tailedvisualrecognition. InCVPR,pages9716‚Äì9725,2020b.\nKaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced\ndatasetswithlabel-distribution-awaremarginloss. InNeurIPS,pages1565‚Äì1576,2019b.\nYifanZhang,BingyiKang,BryanHooi,ShuichengYan,andJiashiFeng. Deeplong-tailedlearning:\nAsurvey. IEEETransactionsonPatternAnalysisandMachineIntelligence,45(9):10795‚Äì10816,\n2023a.\nKeZhu,MinghaoFu,JieShao,TianyuLiu,andJianxinWu. Rectifytheregressionbiasinlong-tailed\nobjectdetection. arXivpreprintarXiv:2401.15885,2024.\nBingyiKang,SainingXie,MarcusRohrbach,ZhichengYan,AlbertGordo,JiashiFeng,andYannis\nKalantidis. Decouplingrepresentationandclassifierforlong-tailedrecognition. InICLR,2020.\nZhishengZhong,JiequanCui,ShuLiu,andJiayaJia. Improvingcalibrationforlong-tailedrecogni-\ntion. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition\n(CVPR),pages16489‚Äì16498,2021a.\nZitaiWang,QianqianXu,ZhiyongYang,YuanHe,XiaochunCao,andQingmingHuang. Aunified\ngeneralizationanalysisofre-weightingandlogit-adjustmentforimbalancedlearning. Advancesin\nNeuralInformationProcessingSystems,36,2024a.\nKeZhu,MinghaoFu,andJianxinWu. Multi-labelself-supervisedlearningwithsceneimages. In\nICCV,pages6694‚Äì6703,2023.\nTianhaoLi,LiminWang,andGangshanWu. Selfsupervisiontodistillationforlong-tailedvisual\nrecognition. InICCV,pages630‚Äì639,2021.\nMengkeLi,Yiu-mingCheung,andYangLu. Long-tailedvisualrecognitionviagaussianclouded\nlogitadjustment. InCVPR,pages6929‚Äì6938,2022.\nAdityaKrishnaMenon,SadeepJayasumana,AnkitSinghRawat,HimanshuJain,AndreasVeit,and\nSanjivKumar. Long-taillearningvialogitadjustment. InICLR,2021.\nZhiyongYang,QianqianXu,ZitaiWang,SicongLi,BoyuHan,ShilongBao,XiaochunCao,and\nQingmingHuang. Harnessinghierarchicallabeldistributionvariationsintestagnosticlong-tail\nrecognition. arXivpreprintarXiv:2405.07780,2024.\nVigneshRamanathan,RuiWang,andDhruvMahajan. Dlwl: Improvingdetectionforlowshotclasses\nwithweaklylabelleddata. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand\npatternrecognition,pages9342‚Äì9352,2020.\nBowenDong,PanZhou,ShuichengYan,andWangmengZuo. Lpt: Long-tailedprompttuningfor\nimageclassification. arXivpreprintarXiv:2210.01033,2022.\nJiang-XinShi,TongWei,ZhiZhou,Xin-YanHan,Jie-JingShao,andYu-FengLi. Parameter-efficient\nlong-tailedrecognition. arXivpreprintarXiv:2309.10019,2023a.\n11\n\nPrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. Advances\ninneuralinformationprocessingsystems,34:8780‚Äì8794,2021.\nAdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,MarkChen,\nandIlyaSutskever. Zero-shottext-to-imagegeneration. InInternationalConferenceonMachine\nLearning,pages8821‚Äì8831.PMLR,2021.\nIanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,\nAaronCourville,andYoshuaBengio. Generativeadversarialnets. Advancesinneuralinformation\nprocessingsystems,27,2014.\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with\nconditionaladversarialnetworks. InProceedingsoftheIEEEconferenceoncomputervisionand\npatternrecognition,pages1125‚Äì1134,2017.\nQuanKong,BinTong,MartinKlinkigt,YukiWatanabe,NaotoAkira,andTomokazuMurakami.\nActive generative adversarial network for image classification. In Proceedings of the AAAI\nconferenceonartificialintelligence,volume33,pages4090‚Äì4097,2019.\nShekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J Fleet.\nSynthetic data from diffusion models improves imagenet classification. arXiv preprint\narXiv:2304.08466,2023.\nBrandonTrabucco,KyleDoherty,MaxGurinas,andRuslanSalakhutdinov. Effectivedataaugmenta-\ntionwithdiffusionmodels. arXivpreprintarXiv:2302.07944,2023.\nManlinZhang,JieWu,YuxiRen,MingLi,JieQin,XuefengXiao,WeiLiu,RuiWang,MinZheng,\nandAndyJMa. Diffusionengine: Diffusionmodelisscalabledataengineforobjectdetection.\narXivpreprintarXiv:2309.03893,2023b.\nYuxuanZhang,HuanLing,JunGao,KangxueYin,Jean-FrancoisLafleche,AdelaBarriuso,Antonio\nTorralba,andSanjaFidler. Datasetgan: Efficientlabeleddatafactorywithminimalhumaneffort.\nInProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages\n10145‚Äì10155,2021c.\nTianjiaoZhang,HuangjieZheng,JiangchaoYao,XiangfengWang,MingyuanZhou,YaZhang,and\nYanfengWang.Long-taileddiffusionmodelswithorientedcalibration.InTheTwelfthInternational\nConferenceonLearningRepresentations,2024b.\nZiweiLiu,ZhongqiMiao,XiaohangZhan,JiayunWang,BoqingGong,andStellaX.Yu. Large-scale\nlong-tailedrecognitioninanopenworld. InCVPR,pages2537‚Äì2546,2019a.\nBingyiKang,SainingXie,MarcusRohrbach,ZhichengYan,AlbertGordo,JiashiFeng,andYannis\nKalantidis. Decouplingrepresentationandclassifierforlong-tailedrecognition. arXivpreprint\narXiv:1910.09217,2019.\nZhishengZhong,JiequanCui,ShuLiu,andJiayaJia. Improvingcalibrationforlong-tailedrecog-\nnition. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,\npages16489‚Äì16498,2021b.\nYin-YinHe,JianxinWu,andXiu-ShenWei. Distillingvirtualexamplesforlong-tailedrecognition.\nInProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages235‚Äì244,\n2021.\nSeulkiPark,YoungkyuHong,ByeonghoHeo,SangdooYun,andJinYoungChoi. Themajoritycan\nhelptheminority:Context-richminorityoversamplingforlong-tailedclassification.InProceedings\noftheIEEE/CVFConferenceonComputerVisionandPatternRecognition, pages6887‚Äì6896,\n2022.\nHarsh Rangwani, Sumukh K Aithal, Mayank Mishra, et al. Escaping saddle points for effective\ngeneralizationonclass-imbalanceddata. AdvancesinNeuralInformationProcessingSystems,35:\n22791‚Äì22805,2022.\n12\n\nSumyeong Ahn, Jongwoo Ko, and Se-Young Yun. Cuda: Curriculum of data augmentation for\nlong-tailedrecognition. arXivpreprintarXiv:2302.05499,2023.\nJiang-XinShi,TongWei,YukeXiang,andYu-FengLi. Howre-samplinghelpsforlong-taillearning?\nAdvancesinNeuralInformationProcessingSystems,36,2023b.\nZitaiWang,QianqianXu,ZhiyongYang,YuanHe,XiaochunCao,andQingmingHuang. Aunified\ngeneralizationanalysisofre-weightingandlogit-adjustmentforimbalancedlearning. Advancesin\nNeuralInformationProcessingSystems,36,2024b.\nMengkeLi,ZhikaiHu,YangLu,WeichaoLan,Yiu-mingCheung,andHuiHuang. Featurefusion\nfromheadtotail:anextremeaugmentingstrategyforlong-tailedvisualrecognition. arXivpreprint\narXiv:2306.06963,2023.\nZiweiLiu,ZhongqiMiao,XiaohangZhan,JiayunWang,BoqingGong,andStellaXYu. Large-scale\nlong-tailedrecognitioninanopenworld. InProceedingsoftheIEEE/CVFconferenceoncomputer\nvisionandpatternrecognition,pages2537‚Äì2546,2019b.\nAlexKrizhevsky. Learningmultiplelayersoffeaturesfromtinyimages. Technicalreport,University\nofToronto,2009.\nTeroKarras,MiikaAittala,JanneHellsten,SamuliLaine,JaakkoLehtinen,andTimoAila. Training\ngenerativeadversarialnetworkswithlimiteddata. Advancesinneuralinformationprocessing\nsystems,33:12104‚Äì12114,2020.\nOlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,ZhihengHuang,\nAndrejKarpathy,AdityaKhosla,MichaelBernstein,AlexanderBerg,andLiFei-Fei. ImageNet\nlargescalevisualrecognitionchallenge.InternationalJournalofComputerVision,115(3):211‚Äì252,\n2015.\nJiaweiRen,CunjunYu,shunansheng,XiaoMa,HaiyuZhao,ShuaiYi,andhongshengLi. Balanced\nmeta-softmaxforlong-tailedvisualrecognition. InNeurIPS,pages4175‚Äì4186,2020.\nJiequanCui,ZhishengZhong,ShuLiu,BeiYu,andJiayaJia. Parametriccontrastivelearning. In\nProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages715‚Äì724,2021.\nJiequanCui,ZhishengZhong,ZhuotaoTian,ShuLiu,BeiYu,andJiayaJia. Generalizedparametric\ncontrastivelearning. IEEETransactionsonPatternAnalysisandMachineIntelligence,2023.\nShiranZada,ItayBenou,andMichalIrani. Purenoisetotherescueofinsufficientdata: Improving\nimbalancedclassificationbytrainingonrandomnoiseimages. InInternationalConferenceon\nMachineLearning,pages25817‚Äì25833.PMLR,2022.\nEkinDCubuk, BarretZoph, DandelionMane, VijayVasudevan, andQuocVLe. Autoaugment:\nLearningaugmentationpoliciesfromdata. arXivpreprintarXiv:1805.09501,2018.\n13\n\nA Experiment\nA.1 Experimentalsettings\nDuetospaceconstraintsinthemainpaper,weonlyincludeessentialinformationabouttheexperi-\nmentalsetup. Additionaldetailsareprovidedhere.\nCIFAR100-LT&CIFAR10-LT.TheoriginalCIFAR100andCIFAR10datasetseachconsistof\na training set with 50,000 images evenly distributed across 100 or 10 classes, respectively. The\nCIFAR100-LT and CIFAR10-LT datasets, derived from CIFAR100 Krizhevsky [2009] and CI-\nFAR10 Krizhevsky [2009], feature a long-tail distribution where the class frequency decreases\nexponentiallyfromclass0tothelastclass. Commonlyusedlong-tailratiosare100, 50, and10.\nSpecifically,theCIFAR100-LTsubsetscontain10,847,12,608,and19,573images,withthelargest\nclasscontaining500samplesandthesmallesthaving5,10,and50samples,respectively. Similarly,\nthe CIFAR10-LT subsets consist of 12,406, 13,996, and 20,431 images, with the largest classes\ncontaining5,000samplesandthesmallest50,100,and500samples,respectively.\nExperimentsonCIFAR100-LTandCIFAR10-LTutilizetheframeworkandtrainingmethodologies\nfrom Qin et al. [2023], incorporating the CBDM Qin et al. [2023] loss function and Adaptive\nAugmentation Karras et al. [2020]. We set the training duration to 500,000 steps, with hyper-\nparametersœÑ andŒ≥ fixedat1and0.25,asperthecitedstudy. Thebatchsizeismaintainedat128,\nthediffusionprocessrunsfor1,000timesteps,andthelearningrateissetat0.0002usinganAdam\noptimizer.\nForclassifiertraining,wefollowthecodeandprotocolsfromZhouetal.[2020a],whichprescribe\na 200-epoch training regimen. The classifier training also employs a batch size of 128, utilizing\nanSGDoptimizerwithalearningrateof0.1. ThefeatureextractorœÜ istrainedusingthissetup\n0\nwithoutanyadditionalmethodsordata. Thefinalclassifieristrainedsimilarlybutincorporatesboth\ngeneratedandoriginalsamples. Alltrainingtasksareconductedon8√óNVIDIAGeForceRTX3090\nGPUs,withfurtherdiscussedinappendixB\nImageNet-LT.ImageNet-LT,comprising115,846imagesacross1,000classeswithamaximumof\n1,280imagesperclassandaminimumof5,followsthespecificationssetin(Liuetal.[2019a]). This\ndatasetisderivedfromImageNetRussakovskyetal.[2015]bysamplingasubsetaccordingtoa\nParetodistributionwithapowervalueofŒ±=6. Forperspective,theoriginalImageNettrainingset\ncontains1,281,167images,makingImageNet-LTlessthan10%thesizeoftheoriginaldataset. The\ntestsetofImageNet-LTmirrorsthatofImageNet,containing100,000images.\nDuetoitsconsiderablesizeandimageresolution,ImageNet-LTnecessitatesmodificationsfromthe\nstandardCBDMframeworktoaddressinefficiencies. WeadaptthecodebasefromDhariwaland\nNichol[2021]forthispurpose,extendingthetrainingto1,980,000iterations. Thissetupusesthe\nAdamWoptimizerwithalearningrateof3e-4andabatchsizeof64,adjustedfromtheoriginal256\nduetoGPUmemoryconstraints. Moreover,themodelistrainedatanimageresolutionof256,and\nthetotaldiffusiontimestepissetto1,000.\nClassifiertrainingforImageNet-LTemploystheframeworkfromZhangetal.[2021a]. Specifically,\nthemodelistrainedover100epochswithabatchsizeof512usingtheSGDoptimizeratalearning\nrateof0.2. Alltrainingtasksarecarriedouton8√óNVIDIAGeForceRTX3090GPUs.\nA.2 GeneratedImages\nWesynthesizeimagesforCIFAR-100,CIFAR-10,andImageNet-LTtodemonstratetheirutilityin\nenhancinglong-tailrecognition. Randonlyselectedexamplesofthegeneratedsamplesaredisplayed,\nparticularlyforCIFAR100-LTinfig.5,whereimageshavearesolutionof32√ó32andfocuson\n\"few-shot\" classes‚Äîthose with fewer than 20 instances. Despite the limited examples available\nintheseclasses,ourdiffusionmodeleffectivelyutilizestheentiredatasettoproducehigh-quality\nsamples. Whilethesesynthesizedimagesmaintainsomesimilaritieswiththeiroriginalcounterparts,\nthenotablevariationsmakethemvaluableformodeltraining.However,becausethegeneratedimages\nmaysometimesdistortessentialfeaturesorintroduceinaccuracies,itbecomescrucialtofilteroutthe\nOODsamplestomaintaintheirusefulness.\nInfig.6,weshowcasethegeneratedimagesforCIFAR10-LT,whichdemonstratesuperiorquality\ncomparedtothosefromCIFAR100-LT,duetoamoreabundanttrainingdataset. InCIFAR10-LT,\n14\n\nFigure5: GeneratedimagesforCIFAR100-LT\nFigure6: GeneratedimagesforCIFAR10-LT\nTable11: RepeatedexperimentsonCIFAR100-LTandCIFAR10-LTtotesttherobustnessofour\nmethods.\nCIFAR100-LT CIFAR10-LT\nMethod\n100 50 10 100 50 10\nDiffuLT(1) 51.5 56.3 63.8 84.7 86.9 90.7\nDiffuLT(2) 51.7 56.3 63.7 84.9 86.8 90.7\nDiffuLT(3) 51.5 56.3 63.3 84.9 86.3 90.6\nTable12: RepeatedexperimentsonImageNet-LTtotesttherobustnessofmethods.\nResNet-10 ResNet-50\nDiffuLT(1) 50.4 56.4\nDiffuLT(2) 50.4 56.5\nDiffuLT(3) 50.5 56.5\neachclassgenerallycontainstentimesmoreimagesthaninCIFAR100-LT.Thesmallestclassin\nCIFAR10-LThas50images,greatlyexceedingtheminimumof5inCIFAR100-LT.Thisincreasein\nsamplesize,however,requiresamorestringentfilteringprocesstopreventpotentialinformationloss\nduetothelargervolumeofimagesgenerated.\nFor ImageNet-LT, the generated images, displayed in fig. 7, feature a resolution of 224 √ó 224,\nwhichissignificantlyclearerthanthosefromCIFAR-10andCIFAR-100. Whilesomefinerdetails,\nsuchastextwithintheimageortextureslikefur,maynotbefullydistinct,thegeneratedimages\neffectivelycapturetheessentialpatternsoftheclasses.Theseimagescansignificantlyaidthelong-tail\nrecognitiontask,particularlyforclasseswithfeweroriginalsamples.\nA.3 Robustnessanlysis\nSinceCIFAR100-LTandCIFAR10-LTaretypicallysampledrandomlyfromtheiroriginaldatasets,\nwetestedourmethodsacrossvarioussampledsetstoassesstheireffectivenessandrobustness. The\nresults,displayedintable11,demonstratethatourmethod‚Äôsperformanceisstable,exhibitingonly\nminimalvariations. ForImageNet-LT,whichisafixeddataset,wegeneratedsamplesthreetimesto\n15\n\nFigure7: GeneratedimagesforImageNet-LT\nexamineconsistency. Thevariationsamongdifferentsamplesetsareminimal,asshownintable12.\nTherefore,ourmethodprovestoberobustinenhancinglong-tailclassificationperformance.\nA.4 DetailsofBaselineMethods.\nIn the main paper, we outline various approaches to long-tail classification and benchmark our\nmethodsagainstthesestrategies. Here,weprovideaconciseoverviewofthemethodswithineach\ncategory. Forre-weightingandre-samplingtechniques,weexamineCross-Entropy(CE),FocalLoss\n(Linetal.[2017]),LDAM-DRW(Caoetal.[2019a]),cRT(Kangetal.[2019]),BBN(Zhouetal.\n[2020a]),CSA(Shietal.[2023b]),andADRW(Wangetal.[2024b]). Intherealmofhead-to-tail\nknowledgetransfer,weincludemethodssuchasOLTR(Liuetal.[2019b])andH2T(Lietal.[2023]).\nLabel-smoothingstrategiesarerepresentedbyMisLAS(Zhongetal.[2021b])andDiVE(Heetal.\n[2021]),whileindataaugmentation,wecompareourapproachwithCAM-BS(Zhangetal.[2021a]),\nCMO(Parketal.[2022]),andCUDA(Ahnetal.[2023]). Lastly,SAM(Rangwanietal.[2022])\nexemplifiesanadvancedoptimizationtechnique,andRIDE(Wangetal.[2020])showcasesamixture\nofexperttechniqueinourcomparison.\n16\n\nTable 13: Results on CIFAR100-LT using an alternative pipeline based on the implementation\nguidelinesfromBSCERenetal.[2020],withimbalanceratiosrsetat100,50,and10.\nCIFAR100-LT\nMethod\n100 50 10\nBaseline 38.3 43.9 55.7\nBaseline‚àó 45.3 50.3 61.9\nBSCERenetal.[2020] 50.8 - 63.0\nPaCoCuietal.[2021] 52.0 56.0 64.2\nGPaCoCuietal.[2023] 52.3 56.4 65.4\nDiffuLT 54.7 58.9 66.1\nDiffuLT+GPaCo 55.4 59.5 66.4\nA.5 OtherMethods.\nForvariousreasons,somemethodsarenotincludedinourexperimentalcomparisons. Thisdecision\nprimarilystemsfromtwofactors. Firstly,severalmethods,despitedemonstratingimpressiveresults,\ndo not have publicly available code (Zada et al. [2022]), limiting our ability to perform direct\ncomparisons. Secondly, methodssuchasthoseinCuietal.[2021]andCuietal.[2023]achieve\ncommendableresultsandcanbereplicated. However,theircomparisonsmaybeconsideredunfair.\nThesemethodsutilizeAutoAugment(Cubuketal.[2018]),withparametersoptimizedacrossthe\nentiredataset,ratherthanspecificallyforthelong-tailedsegment. Thisapproachsignificantlyboosts\ntheirbaselineperformance,asreportedinRenetal.[2020]. Forinstance,onCIFAR100-LTwith\nanimbalanceratioof0.1,baselineaccuracyimprovesfrom38.3to45.3,asshownintable13. The\nfirst \"Baseline\" line reflects the standard settings, while entries marked with ‚àó use the enhanced\nsettings, demonstrating a substantial improvement. Comparing these results with those obtained\nunderstandardconditionswouldbeunfair. Ourmethodologycouldalsobeadaptedtosuchsettings\nandcombinedwiththesetechniquestoachieveexcellentresults,asillustratedintable13. However,\ntheseresultsareomittedfromthemainpapertomaintainafaircomparison.\nB Discussion\nB.1 Comparisonofourmethodwithothertypeofmethods.\nThecomparisonisdividedintotwoparts:comparingourmethodswithtraditionallong-tailrecognition\napproachesandcontrastingthemwithdatasynthesismethods,asshowninfig.8.\nComparedtotraditionallong-tailclassificationmethodsthatpredominantlyfocusontraining,our\napproachoffersanovelperspective. Ratherthandesigningintricatemethodstofacilitatetrainingon\nlong-taileddatasets,westraightforwardlyenhancethedatasetusingagenerativemodel.Thisapproach\nisnotonlyinnovativebutalsocompatiblewithexistingtrainingmethodologiesanddemonstrates\nimprovedperformance. However,themainlimitationisthetrainingofthegenerativemodel,which\nistime-consumingandchallengingtoscale. Thesepointswillbefurtherdiscussedinthesubsequent\nsubsection.\nWhencomparingwithdatasynthesismethods,itisclearthatourapproachdoesnotsurpassthose\nemployingtechnologieslikeStableDiffusionorCLIPonlong-taileddatasets.Nevertheless,theuseof\nsuchlargemodelstrainedonextensivedataeliminatesthecorechallengeoflong-tailproblems‚Äîdata\nscarcity.Forinstance,theclass\"train\"inCIFAR100-LThasonlytenimages,whereasStableDiffusion\nhasbeentrainedonthousandsoftrainimages. Inpracticalsettings,accessingsuchexpansivemodels\ntailoredtospecificdatasetsisunrealistic. Thesemethodsmainlybenefitfromdataleakage. Our\napproach, in contrast, is designed for real long-tail scenarios without reliance on external data\nor models, providing practical and theoretical value. We address several previously unanswered\nquestions:\n‚Ä¢ Isadiffusionmodeltrainedfromscratchbeneficialforlong-tailrecognition? Yes.\n17\n\nTrain\nùúë\n0 Generate\nStable Diffusion\nClassifier\n(a)Previouslong-tailrecognitionmethods. (b)Datasynthesismethods.\nTsteps\nTrain Generate\nDiffusionùúÉ\n(c)Ours\nFigure8: Maincaptiondescribingallimages\n‚Ä¢ Whichgeneratedsamplesareusefulforlong-tailrecognition? AIDsamples.\n‚Ä¢ Whydoesdiffusionworkforlong-tailrecognition? Itblendsclassinformationtogenerate\nbeneficialandnovelsamples.\nB.2 Limitation\nThe primary limitation of our methods is the extensive training time required for the generative\nmodel. Forinstance,trainingadiffusionmodelonCIFAR100-LTtakes24hours,whileImageNet-LT\nrequiresapproximatelysixdays. Asthequalityandquantityofdataincrease,thetrainingcostsscale\nupsignificantly,makingitchallengingtoapplyourmethodstolargerdatasetssuchasiNaturalistand\nPlaces-LTduetoresourceandtimeconstraints.\nDespitethesechallenges,ourmethodsarehighlyeffectiveinaddressingthelong-tailrecognition\nproblem. Toimprovetrainingefficiency,weareexploringtwopotentialsolutions. Thefirstinvolves\nadoptingtechniquesthatacceleratethetrainingandinferenceprocessesofdiffusionmodels. The\nsecondstrategyconsiderstheuseofpre-trainedgenerativemodelsinreallong-tailscenarios. This\napproachdoesnotcontradictourpreviousassertionthatusinglarge,pre-trainedmodelsonfamiliar\nlong-taileddataisunfairandnottrulyrepresentativeoflong-tailchallenges. Instead,weadvocatefor\ntheuseofpre-trainedmodelsonlong-taildatasetstheyhavenotpreviouslyencountered,ensuring\nfairnessandpracticalapplicability. Employingapre-trainedmodelcouldsignificantlyexpediteour\npipelinebyeliminatingtheneedtotrainfromscratch. Thistopicextendsbeyondthescopeofthis\npaperandwillbeaddressedinfutureresearch.\n18\n\nNeurIPSPaperChecklist\n1. Claims\nQuestion: Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthe\npaper‚Äôscontributionsandscope?\nAnswer: [Yes]\nJustification: The conclusions and methods in the abstract and introduction accurately\nencapsulatethecontributionsofourpaper,asdetailedinsection3. Theexperimentalresults\ndiscussedarefullydocumentedinsection4,confirmingtheclaims‚Äôvalidity.\nGuidelines:\n‚Ä¢ The answer NA means that the abstract and introduction do not include the claims\nmadeinthepaper.\n‚Ä¢ Theabstractand/orintroductionshouldclearlystatetheclaimsmade,includingthe\ncontributionsmadeinthepaperandimportantassumptionsandlimitations. ANoor\nNAanswertothisquestionwillnotbeperceivedwellbythereviewers.\n‚Ä¢ Theclaimsmadeshouldmatchtheoreticalandexperimentalresults,andreflecthow\nmuchtheresultscanbeexpectedtogeneralizetoothersettings.\n‚Ä¢ Itisfinetoincludeaspirationalgoalsasmotivationaslongasitisclearthatthesegoals\narenotattainedbythepaper.\n2. Limitations\nQuestion: Doesthepaperdiscussthelimitationsoftheworkperformedbytheauthors?\nAnswer: [Yes]\nJustification: The paper transparently acknowledges the main limitations in section 3.3\nandsection5. Additionally,potentiallimitationsareexploredintheappendixB,ensuringa\ncomprehensivediscussionoftheconstraintsandchallengesencountered.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperhasnolimitationwhiletheanswerNomeansthat\nthepaperhaslimitations,butthosearenotdiscussedinthepaper.\n‚Ä¢ Theauthorsareencouragedtocreateaseparate\"Limitations\"sectionintheirpaper.\n‚Ä¢ Thepapershouldpointoutanystrongassumptionsandhowrobusttheresultsareto\nviolationsoftheseassumptions(e.g.,independenceassumptions,noiselesssettings,\nmodelwell-specification,asymptoticapproximationsonlyholdinglocally).Theauthors\nshouldreflectonhowtheseassumptionsmightbeviolatedinpracticeandwhatthe\nimplicationswouldbe.\n‚Ä¢ Theauthorsshouldreflectonthescopeoftheclaimsmade,e.g.,iftheapproachwas\nonlytestedonafewdatasetsorwithafewruns. Ingeneral,empiricalresultsoften\ndependonimplicitassumptions,whichshouldbearticulated.\n‚Ä¢ Theauthorsshouldreflectonthefactorsthatinfluencetheperformanceoftheapproach.\nForexample,afacialrecognitionalgorithmmayperformpoorlywhenimageresolution\nisloworimagesaretakeninlowlighting. Oraspeech-to-textsystemmightnotbe\nusedreliablytoprovideclosedcaptionsforonlinelecturesbecauseitfailstohandle\ntechnicaljargon.\n‚Ä¢ Theauthorsshoulddiscussthecomputationalefficiencyoftheproposedalgorithms\nandhowtheyscalewithdatasetsize.\n‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to\naddressproblemsofprivacyandfairness.\n‚Ä¢ Whiletheauthorsmightfearthatcompletehonestyaboutlimitationsmightbeusedby\nreviewersasgroundsforrejection,aworseoutcomemightbethatreviewersdiscover\nlimitationsthataren‚Äôtacknowledgedinthepaper. Theauthorsshouldusetheirbest\njudgmentandrecognizethatindividualactionsinfavoroftransparencyplayanimpor-\ntantroleindevelopingnormsthatpreservetheintegrityofthecommunity. Reviewers\nwillbespecificallyinstructedtonotpenalizehonestyconcerninglimitations.\n3. TheoryAssumptionsandProofs\n19\n\nQuestion: Foreachtheoreticalresult,doesthepaperprovidethefullsetofassumptionsand\nacomplete(andcorrect)proof?\nAnswer: [NA]\nJustification: Thefocusofpaperisprimarilyempirical;thus,ourconclusionsandmethods\narederivedfromandvalidatedbyexperimentalresultsratherthantheoreticalproofs.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludetheoreticalresults.\n‚Ä¢ Allthetheorems, formulas, andproofsinthepapershouldbenumberedandcross-\nreferenced.\n‚Ä¢ Allassumptionsshouldbeclearlystatedorreferencedinthestatementofanytheorems.\n‚Ä¢ Theproofscaneitherappearinthemainpaperorthesupplementalmaterial, butif\ntheyappearinthesupplementalmaterial,theauthorsareencouragedtoprovideashort\nproofsketchtoprovideintuition.\n‚Ä¢ Inversely,anyinformalproofprovidedinthecoreofthepapershouldbecomplemented\nbyformalproofsprovidedinappendixorsupplementalmaterial.\n‚Ä¢ TheoremsandLemmasthattheproofreliesuponshouldbeproperlyreferenced.\n4. ExperimentalResultReproducibility\nQuestion: Doesthepaperfullydisclosealltheinformationneededtoreproducethemainex-\nperimentalresultsofthepapertotheextentthatitaffectsthemainclaimsand/orconclusions\nofthepaper(regardlessofwhetherthecodeanddataareprovidedornot)?\nAnswer: [Yes]\nJustification: We have developed a straightforward and easily replicable pipeline, fully\ndetailedinsection3. Allnecessaryhyper-parametersandexperimentalsettingsareoutlined\nin section 4.1. Additional information required for reproduction is provided in the ap-\npendixA.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n‚Ä¢ Ifthepaperincludesexperiments,aNoanswertothisquestionwillnotbeperceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhetherthecodeanddataareprovidedornot.\n‚Ä¢ Ifthecontributionisadatasetand/ormodel,theauthorsshoulddescribethestepstaken\ntomaketheirresultsreproducibleorverifiable.\n‚Ä¢ Dependingonthecontribution,reproducibilitycanbeaccomplishedinvariousways.\nForexample,ifthecontributionisanovelarchitecture,describingthearchitecturefully\nmightsuffice,orifthecontributionisaspecificmodelandempiricalevaluation,itmay\nbenecessarytoeithermakeitpossibleforotherstoreplicatethemodelwiththesame\ndataset,orprovideaccesstothemodel. Ingeneral. releasingcodeanddataisoften\nonegoodwaytoaccomplishthis,butreproducibilitycanalsobeprovidedviadetailed\ninstructionsforhowtoreplicatetheresults,accesstoahostedmodel(e.g.,inthecase\nofalargelanguagemodel),releasingofamodelcheckpoint,orothermeansthatare\nappropriatetotheresearchperformed.\n‚Ä¢ WhileNeurIPSdoesnotrequirereleasingcode,theconferencedoesrequireallsubmis-\nsionstoprovidesomereasonableavenueforreproducibility,whichmaydependonthe\nnatureofthecontribution. Forexample\n(a) Ifthecontributionisprimarilyanewalgorithm,thepapershouldmakeitclearhow\ntoreproducethatalgorithm.\n(b) Ifthecontributionisprimarilyanewmodelarchitecture,thepapershoulddescribe\nthearchitectureclearlyandfully.\n(c) Ifthecontributionisanewmodel(e.g.,alargelanguagemodel),thenthereshould\neitherbeawaytoaccessthismodelforreproducingtheresultsorawaytoreproduce\nthemodel(e.g.,withanopen-sourcedatasetorinstructionsforhowtoconstruct\nthedataset).\n20\n\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthorsarewelcometodescribetheparticularwaytheyprovideforreproducibility.\nInthecaseofclosed-sourcemodels,itmaybethataccesstothemodelislimitedin\nsomeway(e.g.,toregisteredusers),butitshouldbepossibleforotherresearchers\ntohavesomepathtoreproducingorverifyingtheresults.\n5. Openaccesstodataandcode\nQuestion: Doesthepaperprovideopenaccesstothedataandcode,withsufficientinstruc-\ntionstofaithfullyreproducethemainexperimentalresults,asdescribedinsupplemental\nmaterial?\nAnswer: [No]\nJustification: The code related to this paper will be released as open-source after it is\naccepted.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatpaperdoesnotincludeexperimentsrequiringcode.\n‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy)formoredetails.\n‚Ä¢ Whileweencouragethereleaseofcodeanddata,weunderstandthatthismightnotbe\npossible,so‚ÄúNo‚Äùisanacceptableanswer. Paperscannotberejectedsimplyfornot\nincludingcode,unlessthisiscentraltothecontribution(e.g.,foranewopen-source\nbenchmark).\n‚Ä¢ Theinstructionsshouldcontaintheexactcommandandenvironmentneededtorunto\nreproducetheresults. SeetheNeurIPScodeanddatasubmissionguidelines(https:\n//nips.cc/public/guides/CodeSubmissionPolicy)formoredetails.\n‚Ä¢ Theauthorsshouldprovideinstructionsondataaccessandpreparation,includinghow\ntoaccesstherawdata,preprocesseddata,intermediatedata,andgenerateddata,etc.\n‚Ä¢ Theauthorsshouldprovidescriptstoreproduceallexperimentalresultsforthenew\nproposedmethodandbaselines. Ifonlyasubsetofexperimentsarereproducible,they\nshouldstatewhichonesareomittedfromthescriptandwhy.\n‚Ä¢ Atsubmissiontime, topreserveanonymity, theauthorsshouldreleaseanonymized\nversions(ifapplicable).\n‚Ä¢ Providingasmuchinformationaspossibleinsupplementalmaterial(appendedtothe\npaper)isrecommended,butincludingURLstodataandcodeispermitted.\n6. ExperimentalSetting/Details\nQuestion: Doesthepaperspecifyallthetrainingandtestdetails(e.g.,datasplits,hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: Allessentialexperimentalsettings,includingdataset,hyperparameters,selec-\ntioncriteriafortheseparametersarethoroughlydetailedinsection4. Additionalexperimen-\ntaldetailsareprovidedintheappendixA.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n‚Ä¢ Theexperimentalsettingshouldbepresentedinthecoreofthepapertoalevelofdetail\nthatisnecessarytoappreciatetheresultsandmakesenseofthem.\n‚Ä¢ Thefulldetailscanbeprovidedeitherwiththecode,inappendix,orassupplemental\nmaterial.\n7. ExperimentStatisticalSignificance\nQuestion:Doesthepaperreporterrorbarssuitablyandcorrectlydefinedorotherappropriate\ninformationaboutthestatisticalsignificanceoftheexperiments?\nAnswer: [Yes]\nJustification: Thepaperprovidesdetailedinformationonthestatisticalsignificanceofthe\nexperiments. ThesedetailsareavailableintheappendixappendixA.\n21\n\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n‚Ä¢ Theauthorsshouldanswer\"Yes\"iftheresultsareaccompaniedbyerrorbars,confi-\ndenceintervals,orstatisticalsignificancetests,atleastfortheexperimentsthatsupport\nthemainclaimsofthepaper.\n‚Ä¢ Thefactorsofvariabilitythattheerrorbarsarecapturingshouldbeclearlystated(for\nexample,train/testsplit,initialization,randomdrawingofsomeparameter,oroverall\nrunwithgivenexperimentalconditions).\n‚Ä¢ Themethodforcalculatingtheerrorbarsshouldbeexplained(closedformformula,\ncalltoalibraryfunction,bootstrap,etc.)\n‚Ä¢ Theassumptionsmadeshouldbegiven(e.g.,Normallydistributederrors).\n‚Ä¢ Itshouldbeclearwhethertheerrorbaristhestandarddeviationorthestandarderror\nofthemean.\n‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferablyreporta2-sigmaerrorbarthanstatethattheyhavea96%CI,ifthehypothesis\nofNormalityoferrorsisnotverified.\n‚Ä¢ Forasymmetricdistributions,theauthorsshouldbecarefulnottoshowintablesor\nfiguressymmetricerrorbarsthatwouldyieldresultsthatareoutofrange(e.g. negative\nerrorrates).\n‚Ä¢ Iferrorbarsarereportedintablesorplots,Theauthorsshouldexplaininthetexthow\ntheywerecalculatedandreferencethecorrespondingfiguresortablesinthetext.\n8. ExperimentsComputeResources\nQuestion: Foreachexperiment,doesthepaperprovidesufficientinformationonthecom-\nputerresources(typeofcomputeworkers,memory,timeofexecution)neededtoreproduce\ntheexperiments?\nAnswer: [Yes]\nJustification: Detailedinformationaboutthecomputationalresourcesrequired,including\nthetypeofGPUandexecutiontimes,isprovidedintheappendixappendixA.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n‚Ä¢ ThepapershouldindicatethetypeofcomputeworkersCPUorGPU,internalcluster,\norcloudprovider,includingrelevantmemoryandstorage.\n‚Ä¢ Thepapershouldprovidetheamountofcomputerequiredforeachoftheindividual\nexperimentalrunsaswellasestimatethetotalcompute.\n‚Ä¢ Thepapershoulddisclosewhetherthefullresearchprojectrequiredmorecompute\nthantheexperimentsreportedinthepaper(e.g.,preliminaryorfailedexperimentsthat\ndidn‚Äôtmakeitintothepaper).\n9. CodeOfEthics\nQuestion: Doestheresearchconductedinthepaperconform, ineveryrespect, withthe\nNeurIPSCodeofEthicshttps://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: We have thoroughly reviewed and adhered to the NeurIPS Code of Ethics\nthroughoutourresearch.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthattheauthorshavenotreviewedtheNeurIPSCodeofEthics.\n‚Ä¢ IftheauthorsanswerNo,theyshouldexplainthespecialcircumstancesthatrequirea\ndeviationfromtheCodeofEthics.\n‚Ä¢ Theauthorsshouldmakesuretopreserveanonymity(e.g.,ifthereisaspecialconsid-\nerationduetolawsorregulationsintheirjurisdiction).\n10. BroaderImpacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietalimpactsoftheworkperformed?\n22\n\nAnswer: [NA]\nJustification: Our research concentrates on the long-tail recognition issue, a technical\nchallengewithinthefieldofcomputervisionthattypicallyhasminimalsocietalimpact. We\nutilizeopen-sourcedatasetsandensurethatnoharmfulinformationisgenerated.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthereisnosocietalimpactoftheworkperformed.\n‚Ä¢ IftheauthorsanswerNAorNo,theyshouldexplainwhytheirworkhasnosocietal\nimpactorwhythepaperdoesnotaddresssocietalimpact.\n‚Ä¢ Examplesofnegativesocietalimpactsincludepotentialmaliciousorunintendeduses\n(e.g.,disinformation,generatingfakeprofiles,surveillance),fairnessconsiderations\n(e.g.,deploymentoftechnologiesthatcouldmakedecisionsthatunfairlyimpactspecific\ngroups),privacyconsiderations,andsecurityconsiderations.\n‚Ä¢ Theconferenceexpectsthatmanypaperswillbefoundationalresearchandnottied\ntoparticularapplications,letalonedeployments. However,ifthereisadirectpathto\nanynegativeapplications,theauthorsshouldpointitout. Forexample,itislegitimate\ntopointoutthatanimprovementinthequalityofgenerativemodelscouldbeusedto\ngeneratedeepfakesfordisinformation. Ontheotherhand,itisnotneededtopointout\nthatagenericalgorithmforoptimizingneuralnetworkscouldenablepeopletotrain\nmodelsthatgenerateDeepfakesfaster.\n‚Ä¢ Theauthorsshouldconsiderpossibleharmsthatcouldarisewhenthetechnologyis\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnologyisbeingusedasintendedbutgivesincorrectresults,andharmsfollowing\nfrom(intentionalorunintentional)misuseofthetechnology.\n‚Ä¢ Iftherearenegativesocietalimpacts,theauthorscouldalsodiscusspossiblemitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanismsformonitoringmisuse,mechanismstomonitorhowasystemlearnsfrom\nfeedbackovertime,improvingtheefficiencyandaccessibilityofML).\n11. Safeguards\nQuestion: Doesthepaperdescribesafeguardsthathavebeenputinplaceforresponsible\nreleaseofdataormodelsthathaveahighriskformisuse(e.g.,pretrainedlanguagemodels,\nimagegenerators,orscrapeddatasets)?\nAnswer: [NA]\nJustification: Ourresearchdoesn‚Äôtinvolvemodelscapableofgeneratinghigh-riskcontent\nnordoesitcollectdatafromtheinternet,thuseliminatingtheneedforsuchsafeguards.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperposesnosuchrisks.\n‚Ä¢ Releasedmodelsthathaveahighriskformisuseordual-useshouldbereleasedwith\nnecessarysafeguardstoallowforcontrolleduseofthemodel,forexamplebyrequiring\nthatusersadheretousageguidelinesorrestrictionstoaccessthemodelorimplementing\nsafetyfilters.\n‚Ä¢ DatasetsthathavebeenscrapedfromtheInternetcouldposesafetyrisks. Theauthors\nshoulddescribehowtheyavoidedreleasingunsafeimages.\n‚Ä¢ Werecognizethatprovidingeffectivesafeguardsischallenging,andmanypapersdo\nnotrequirethis,butweencourageauthorstotakethisintoaccountandmakeabest\nfaitheffort.\n12. Licensesforexistingassets\nQuestion: Arethecreatorsororiginalownersofassets(e.g.,code,data,models),usedin\nthepaper,properlycreditedandarethelicenseandtermsofuseexplicitlymentionedand\nproperlyrespected?\nAnswer: [Yes]\nJustification: Allthedata,code,andmethodsemployedinthispaperareopen-source. We\nensurepropercitationoftheseresourcesandadherencetotheirlicensesandtermsofuse.\nGuidelines:\n23\n\n‚Ä¢ TheanswerNAmeansthatthepaperdoesnotuseexistingassets.\n‚Ä¢ Theauthorsshouldcitetheoriginalpaperthatproducedthecodepackageordataset.\n‚Ä¢ Theauthorsshouldstatewhichversionoftheassetisusedand,ifpossible,includea\nURL.\n‚Ä¢ Thenameofthelicense(e.g.,CC-BY4.0)shouldbeincludedforeachasset.\n‚Ä¢ Forscrapeddatafromaparticularsource(e.g.,website),thecopyrightandtermsof\nserviceofthatsourceshouldbeprovided.\n‚Ä¢ If assets are released, the license, copyright information, and terms of use in the\npackageshouldbeprovided. Forpopulardatasets,paperswithcode.com/datasets\nhascuratedlicensesforsomedatasets. Theirlicensingguidecanhelpdeterminethe\nlicenseofadataset.\n‚Ä¢ Forexistingdatasetsthatarere-packaged,boththeoriginallicenseandthelicenseof\nthederivedasset(ifithaschanged)shouldbeprovided.\n‚Ä¢ Ifthisinformationisnotavailableonline,theauthorsareencouragedtoreachoutto\ntheasset‚Äôscreators.\n13. NewAssets\nQuestion:Arenewassetsintroducedinthepaperwelldocumentedandisthedocumentation\nprovidedalongsidetheassets?\nAnswer: [NA]\nJustification: Thispaperdoesnotintroduceanynewassets,hencethereisnoassociated\ndocumentation.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperdoesnotreleasenewassets.\n‚Ä¢ Researchersshouldcommunicatethedetailsofthedataset/code/modelaspartoftheir\nsubmissions via structured templates. This includes details about training, license,\nlimitations,etc.\n‚Ä¢ Thepapershoulddiscusswhetherandhowconsentwasobtainedfrompeoplewhose\nassetisused.\n‚Ä¢ Atsubmissiontime,remembertoanonymizeyourassets(ifapplicable). Youcaneither\ncreateananonymizedURLorincludeananonymizedzipfile.\n14. CrowdsourcingandResearchwithHumanSubjects\nQuestion: Forcrowdsourcingexperimentsandresearchwithhumansubjects,doesthepaper\nincludethefulltextofinstructionsgiventoparticipantsandscreenshots,ifapplicable,as\nwellasdetailsaboutcompensation(ifany)?\nAnswer: [NA]\nJustification: Thispaperdoesnotinvolveanycrowdsourcingexperimentsorresearchwith\nhumansubjects,thereforethisquestionisnotapplicable.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith\nhumansubjects.\n‚Ä¢ Includingthisinformationinthesupplementalmaterialisfine,butifthemaincontribu-\ntionofthepaperinvolveshumansubjects,thenasmuchdetailaspossibleshouldbe\nincludedinthemainpaper.\n‚Ä¢ AccordingtotheNeurIPSCodeofEthics,workersinvolvedindatacollection,curation,\norotherlaborshouldbepaidatleasttheminimumwageinthecountryofthedata\ncollector.\n15. InstitutionalReviewBoard(IRB)ApprovalsorEquivalentforResearchwithHuman\nSubjects\nQuestion: Doesthepaperdescribepotentialrisksincurredbystudyparticipants,whether\nsuchrisksweredisclosedtothesubjects,andwhetherInstitutionalReviewBoard(IRB)\napprovals(oranequivalentapproval/reviewbasedontherequirementsofyourcountryor\ninstitution)wereobtained?\n24\n\nAnswer: [NA]\nJustification: Thispaperdoesnotengageinresearchwithhumansubjectsorcrowdsourcing;\nthus,therearenostudyparticipants,potentialrisks,orrequirementsforIRBapproval.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith\nhumansubjects.\n‚Ä¢ Dependingonthecountryinwhichresearchisconducted,IRBapproval(orequivalent)\nmayberequiredforanyhumansubjectsresearch. IfyouobtainedIRBapproval,you\nshouldclearlystatethisinthepaper.\n‚Ä¢ Werecognizethattheproceduresforthismayvarysignificantlybetweeninstitutions\nandlocations,andweexpectauthorstoadheretotheNeurIPSCodeofEthicsandthe\nguidelinesfortheirinstitution.\n‚Ä¢ Forinitialsubmissions,donotincludeanyinformationthatwouldbreakanonymity(if\napplicable),suchastheinstitutionconductingthereview.\n25\n\n",
    "total_pages": 25,
    "pages": [
      {
        "page": 1,
        "content": "DiffuLT: Diffusion for Long-tail Recognition Without\nExternal Knowledge\nJieShao KeZhu HanxiaoZhang JianxinWu‚àó\nNationalKeyLaboratoryforNovelSoftwareTechnology,NanjingUniversity,China\nSchoolofArtificialIntelligence,NanjingUniversity,China\n{shaoj, zhuk, zhanghx}@lamda.nju.edu.cn, wujx2001@nju.edu.cn\nAbstract\nThispaperintroducesanovelpipelineforlong-tail(LT)recognitionthatdiverges\nfromconventionalstrategies. Instead,itleveragesthelong-taileddatasetitselfto\ngenerateabalancedproxydatasetwithoututilizingexternaldataormodel. We\ndeployadiffusionmodeltrainedfromscratchononlythelong-taileddatasetto\ncreatethisproxyandverifytheeffectivenessofthedataproduced. Ouranalysis\nidentifies approximately-in-distribution (AID) samples, which slightly deviate\nfrom the real data distribution and incorporate a blend of class information, as\nthecrucialsamplesforenhancingthegenerativemodel‚Äôsperformanceinlong-tail\nclassification. We promote the generation of AID samples during the training\nof a generative model by utilizing a feature extractor to guide the process and\nfilteroutdetrimentalsamplesduringgeneration. Ourapproach,termedDiffusion\nmodel for Long-Tail recognition (DiffuLT), represents a pioneer application of\ngenerativemodelsinlong-tailrecognition. DiffuLTachievesstate-of-the-artresults\nonCIFAR10-LT,CIFAR100-LT,andImageNet-LT,surpassingleadingcompetitors\nbysignificantmargins. Comprehensiveablationsenhancetheinterpretabilityof\nourpipeline. Notably,theentiregenerativeprocessisconductedwithoutrelying\nonexternaldataorpre-trainedmodelweights,whichleadstoitsgeneralizabilityto\nreal-worldlong-tailedscenarios.\n1 Introduction\nDeeplearninghasexhibitedremarkablesuccessacrossaspectrumofcomputervisiontasks,especially\ninimageclassification, e.g., asexhibitedbyHeetal.[2016],Dosovitskiyetal.[2021],Liuetal.\n[2021]. Thesemodels,however,encounterobstacleswhenfacedwithreal-worldlong-tailed(LT)\ndata,wherethemajorityclasseshaveabundantsamplesbuttheminorityonesaresparselyrepresented.\nTheintrinsicbiasofdeeplearningarchitecturestowardsmorepopulousclassesexacerbatesthisissue,\nleadingtosub-optimalrecognitionofminorityclassesdespitetheircriticalimportanceinpractical\napplications.\nConventionallong-tailedlearningstrategiessuchasre-weighting(Linetal.[2017],Caoetal.[2019a]),\nre-sampling(Zhouetal.[2020a],Zhangetal.[2021a]),andstructuraladjustments(Wangetal.[2020],\nCuietal.[2022]),shareacommonality: theyacknowledgethedata‚Äôsimbalanceandfocusonthe\ntrainingofmodels. Theydemandmeticulousdesignandarechallengingtogeneralize. Recently,\na shift towards studying the dataset itself and involving more training samples through external\nknowledgetomitigatelong-tailedchallengeshasemerged(Zhangetal.[2021b,2024a]). Yet,inmany\nreal-worldscenarios,accesstospecializeddataislimitedandcloselyguarded,suchasinmilitaryor\nmedicalcontexts. Thispredicamentpromptsacriticalinquiry: Isitfeasibletobalancelong-tailed\ndatasetswithoutdependingonexternalresourcesormodels?\n‚àóJ.Wuisthecorrespondingauthor.\n38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024)."
      },
      {
        "page": 2,
        "content": "\u0000\u0018\u0000\u0013\n\u0000\u0017\u0000\u0013\n\u0000\u0016\u0000\u0013\n\u0000\u0015\u0000\u0013\n\u0000\u0014\u0000\u0013\n\u0000\u0013\n\u0000\u001a\u0000\u0011\u0000\u0018 \u0000\u001a\u0000\u0011\u0000\u0013 \u0000\u0019\u0000\u0011\u0000\u0018 \u0000\u0019\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0011\u0000\u0018\n\u0000)\u0000,\u0000'\n\u0000\f\u0000\b\u0000\u000b\u0000\u0003DIAp\n\u0000\u0018\u0000\u0013\n\u0000\u0017\u0000\u001b\n\u0000\u0017\u0000\u0019\n\u0000\u0017\u0000\u0017\n\u0000\u0017\u0000\u0015\n\u0000\u0017\u0000\u0013\n\u0000\u0016\u0000\u001b\n\u0000\f\u0000\b\u0000\u000b\u0000\u0003\u0000\u0011\u0000F\u0000F\u0000$\n\u0000'\u0000'\u00003\u00000\n\u0000&\u0000%\u0000'\u00000\n\u00002\u0000X\u0000U\u0000V\n\u0000%\u0000D\u0000V\u0000H\u0000O\u0000L\u0000Q\u0000H\nFigure1: Thesamplesgeneratedbydiffusionmodelsimprovelong-tailclassificationonCIFAR100-\nLT,showingacorrelationbetweenFIDandaccuracyandastrongercorrelationbetweentheproportion\nofAIDsamplesandaccuracy. Ourmethodsignificantlyboostsclassifieraccuracycomparedwith\nothers(left). Featurespacevisualizationrevealsthatdifferentdiffusionmodelsgeneratesampleswith\nvaryingdistributions,andourmodelbiasesthegenerativeprocesstowardAIDsamples(right).\nOuranswerisyes. Recentadvancesindiffusionmodelshavedemonstratedtheirsignificantpotential\nin generating high-quality images (Ho et al. [2020], Song et al. [2020], Rombach et al. [2022]).\nAssumingthatdiffusionmodelsareproficientatlearningdistributions,wedevelopadiffusionmodel\ntrainedfromscratchononlythelong-taildistributeddataset. Thismodelcreatesnewsamplesfor\nunderrepresentedclasses,whicharethenusedtotrainaclassifieronthere-balanceddataset,leading\ntoimprovedaccuracy. Wearethefirsttodemonstratetheeffectivenessofusinggeneratedsamples,\nwithoutrelyingonexternaldataormodels,inimprovinglong-tailclassification. Weobserveanotable\npattern: enhancingtheperformanceofthegenerativemodelwithalossmodificationcalledCBDM\n(Qinetal.[2023])alsoenhancestheclassifier‚Äôsaccuracy,asillustratedinfig.1. Thisphenomenon\nimpliesthatagenerativemodelwithbetterperformancetendstogeneratesamplesthataremore\nbeneficialfortheclassificationtask. Thisobservationraisesanimportantquestion: Whatarethemost\nvaluablegeneratedsamplesforclassification,andhowaretheygenerated? Thisquestioniscritical,\nasitdetermineswhetheradiffusionmodelisgoingtobebeneficialordetrimentalforLTrecognition.\nWeanswerthisquestionbyanalyzingfeaturesofgeneratedsamples,asvisualizedinfig.1using\nt-SNE(VanderMaatenandHinton[2008]). Wecategorizethegeneratedsamplesintothreegroups:\nin-distribution(ID),approximatelyin-distribution(AID),andout-of-distribution(OOD).Ourresearch\nindicatesthatAIDsamplesarepivotalinenhancingclassifierperformance. Throughexperiments,we\nconcludethatadiffusionmodelcanassimilatepatternsfromtheheadclassesandintegratestheminto\nthetailonestoproduceAIDsamples. Thesesamplessignificantlyenhancethequantityanddiversity\nofthetailclasses,therebysubstantiallyimprovingtheirperformance. Thentheimportantquestionto\nsolveis: HowcanwegenerateAIDsamplesefficiently?\nToencouragethemodeltopredominantlygenerateAIDsamples,weintroduceanoveltypeofloss.\nThislossemploysafeatureextractortopenalizethegenerationofIDandOODsamples. Sucha\nstrategy not only elevates the performance of the generative model on long-tail datasets but also\nrendersitmoreeffectiveandefficientinenhancingclassifierperformance.\nIngeneral,weintroduceanewpipeline,DiffuLT(DiffusionmodelforLong-Tailrecognition),for\nlong-taildatasets. Ithasthreesteps: initialtraining,samplegeneration,andretraining. Initially,we\ntrainafeatureextractorandadiffusionmodelincorporatingasupervisiontermtoencouragethe\ngenerationofAIDsamples. Subsequently,thisgenerativemodelisemployedtoaugmentthedataset\ntowardsbalance. Thefinalstepinvolvestraininganewclassifierontheenricheddataset,withaminor\nadjustmenttoreducetheimpactofsyntheticsamples. Itiscrucialtounderscoretheimportanceof\ntrainingthediffusionmodelwithoutexternaldataorknowledge,tomaintainfairnessincomparison.\nOurcontributionsaresummarizedasfollows:\n‚Ä¢ We pioneer in addressing long-tail recognition by synthesizing images using diffusion\nmodelswithoutrelyingonexternaldata.\n2"
      },
      {
        "page": 3,
        "content": "‚Ä¢ Ourresearchdelvesintothemechanismsunderlyingourapproach,highlightingthesignifi-\ncanceofthegeneratedAIDsamples. Thesesamplesemergefromafusionofinformation\nfrombothheadandtailclasses,playingacrucialroleinenhancingclassifierperformance.\n‚Ä¢ Weintroduceanovellossfunctionthatenhancestheperformanceofdiffusionmodelson\nlong-taileddatasetsandbiasesthemtowardsgeneratingAIDsamples,therebymakingthe\ngenerationprocessmoreeffectiveandefficientforclassification.\nExtensiveexperimentalvalidationacrossCIFAR10-LT,CIFAR100-LT,andImageNet-LTdatasets\ndemonstratesthesuperiorperformanceofourmethodoverexistingapproaches.\n2 RelatedWork\nLong-tailed recognition Long-tailed recognition is a challenging and practical task (Cui et al.\n[2019],Zhouetal.[2020b],Caoetal.[2019b],Zhangetal.[2023a],Zhuetal.[2024]),sincenatural\ndataoftenconstituteasqueezedandimbalanceddistribution. Themajorityoftraditionallong-tailed\nlearningmethodscanbeviewedas(orspecialcases)ofre-weighting(Caoetal.[2019b],Kangetal.\n[2020],Zhongetal.[2021a],Wangetal.[2024a])andre-sampling(Cuietal.[2019]),withmore\nemphasis on the deferred tail class to seek an optimization trade-off. There are variants of them\nthatadoptself-supervisedlearning(Zhuetal.[2023],Lietal.[2021]),theoreticalanalysis(Lietal.\n[2022],Menonetal.[2021],Yangetal.[2024])anddecouplingpipeline(Kangetal.[2020],Zhou\netal.[2020b])totacklelong-tailedlearningfromvariousaspects,andtheyallachieveseemingly\ndecentperformanceindownstreamtasks.\nOneofthecoredifficultiesinlong-tailedlearningistheinsufficiencyoftailsamples. Andrecently,\nquitesomeworksstarttofocusonthisaspectbyinvolvingmoretrainingsamplesthroughexternal\nknowledge(Zhangetal.[2021b],Ramanathanetal.[2020],Dongetal.[2022],Shietal.[2023a]).\nNevertheless,themostdistinctdrawbackoftheseworksisthattheyeitherrelyonexternaldatasource\norstrongmodelweights. Thisconditioncanseldomlyholdtrueinpracticalscenarioswhereonly\nahandfulofspecializeddataareavailableandaresecretlykept(considersomeimportantmilitary\nor medical data). We thus raise a natural question about long-tailed learning: can we utilize the\nadvantageofgeneratingtailsampleswithoutresortingtoanyexternaldataormodel? Thatis,the\nwholeprocessisdoneinanin-domain(alsocalledheld-in)manner. Inthispaper,weproposeto\nadopttheoff-the-shelfdiffusionmodeltolearnandgeneratesamplesfromthedataathand.\nDiffusionmodelsandsyntheticdata Diffusionmodelshavebeenhighlycompetitiveinrecent\nyears(Hoetal.[2020],Songetal.[2020]),producingpromisingimagequalityinbothunconditional\nandconditionalsettings(DhariwalandNichol[2021],Rombachetal.[2022],Rameshetal.[2021]).\nDespitethepredominantuseincreatingdigitalart,theapplicationofdiffusionmodelsinscenariosof\nlimiteddataremainsunder-explored. Thispaperaffirmstheutilityofdiffusionmodelsinenhancing\nrepresentation learning, particularly within the long-tailed learning framework, offering a novel\ninsightintotheirapplicationbeyondconventionalgenerativetasks.\nTheintegrationofsyntheticdataintodeeplearning,generatedthroughmethodslikeGANsGoodfel-\nlowetal.[2014],Isolaetal.[2017]anddiffusionmodels(DhariwalandNichol[2021],Rombachetal.\n[2022]),hasbeenexploredtoenhanceperformanceinimageclassification(Kongetal.[2019],Azizi\netal.[2023],Zhangetal.[2024a],Trabuccoetal.[2023]),objectdetection(Zhangetal.[2023b]),and\nsemanticsegmentation(Zhangetal.[2021c,2023b]). Theseapproachesoftendependonsubstantial\nvolumesoftrainingdataorleveragepre-trainedmodels,suchasStableDiffusion,forhigh-quality\ndatageneration. Yet,theefficacyofgenerativemodelsandsyntheticdataundertheconstraintof\nlimitedavailabledataandinaddressingimbalanceddatadistributionsremainsanunresolvedinquiry.\nThis paper specifically addresses this question, evaluating the viability of generative models and\nsyntheticdatainscenarioswheredataisscarceandimbalanced.\n3 Method\n3.1 Preliminaries\nFor image classification, we have a long-tail dataset D = {(x ,y )}N ,y ‚àà C with each x\ni i i=1 i i\nrepresentinganinputimageandy representingitscorrespondinglabelfromthesetofallclasses\ni\n3"
      },
      {
        "page": 4,
        "content": "Table1: FIDof different generationmodels Table2: Percentageofdifferenttypesofgener-\nandtheircorrespondingclassifiers‚Äôaccuracy. atedsamplesforeachmodel.\nModel FID Acc.(%) Model p p p\nID AID OOD\nBaseline - 38.3\nDDPM 39.1 21.2 39.7\nDDPM 7.76 43.8\nCBDM(œÑ =3) 38.6 29.1 32.3\nCBDM(œÑ =3) 7.42 44.8\nCBDM(œÑ =2) 6.82 46.0 CBDM(œÑ =2) 40.2 33.5 26.3\nCBDM(œÑ =1) 5.86 46.6 CBDM(œÑ =1) 44.8 36.3 18.9\nC. Inthelong-tailsetting,afewclassesdominatewithmanysamples,whilemostclasseshavevery\nfewimages,leadingtoasignificantclassimbalance. TheclassesinC areorderedbysamplecount\nwith|c | ‚â• |c | ‚â• ... ‚â• |c |, where|c |denotesthenumberoftrainingsamplesinclassc and\n1 2 M j j\n|c |‚â´|c |. Theratior = |c1| isdefinedasthelong-tailratio. Thegoaloflong-tailclassification\n1 M |cM|\nistolearnaclassifierf :X ‚ÜíY capableofeffectivelyhandlingthetailclasses.\nœÜ\nThenaiveideaistotrainagenerativemodelŒ∏onthelong-taildatasetDandusethetrainedmodel\ntogeneratenewsamplesandsupplementthetailclasses. Inspiredbyitssuperiorperformance,we\nselect diffusion models as the generative model in our pipeline. In our approach, we follow the\nDenoisingDiffusionProbabilisticModel(DDPMbyHoetal.[2020])framework. Givenadataset\nD = {x ,y }N , wetrainadiffusionmodeltomaximizethelikelihoodofthedataset. Atevery\ni i i=1\ntrainingstep,wesampleamini-batchofimagesx fromthedatasetandaddnoisetoobtainx ,\n0 t\n‚àö\nq(x |x )=N( Œ±¬Ø x ,(1‚àíŒ±¬Ø )I), (1)\nt 0 t 0 t\nwhereŒ±¬Ø = (cid:81)t (1‚àíŒ≤ )iscalculatedthroughpre-definedvarianceschedule{Œ≤ ‚àà (0,1)}T .\nt i=1 i t t=1\nAftertrainingadiffusionmodelŒ∏togetp (x |x ,t),wereversetheaboveprocessstepbystep\nŒ∏ t‚àí1 t\ntorecovertheoriginalimagex frompurenoisex ‚àºN(0,I). Thetrainingobjectiveistoreduce\n0 T\nthegapbetweentheaddednoiseinforwardprocessandtheestimatednoiseinreverseprocess:\n‚àö ‚àö\nL =E [‚à•œµ ‚àíœµ ( Œ±¬Ø x + 1‚àíŒ±¬Ø œµ ,t)‚à•2], (2)\nDDPM t‚àº[1,T],x0,œµt t Œ∏ t 0 t t\nwhere œµ ‚àº N(0,I) is the noise added to original images and œµ is the noise estimated by the\nt Œ∏\ntrainablemodelwithparametersŒ∏. DDPMcanbeconditionalbytransformingy intoatrainable\nclassembeddingandincorporatingthelabelydirectlyasaminput,similartotimet. Toimprovethe\nperformanceofDDPMonlong-taileddataset,severalworks(Qinetal.[2023],Zhangetal.[2024b])\nhave been proposed to adjust the distribution of generated samples. CBDM adds a distribution\nadjustmentregularizeratthelossterm. Thistermisdesignedtopromotethegenerationofsamples\nfortailclasses,whichisdefinedas(wheresgmeansstopgradient):\nœÑt (cid:88)\nL = (‚à•œµ (x ,t,y)‚àísg(œµ (x ,t,y‚Ä≤))‚à•2+Œ≥‚à•sg(œµ (x ,t,y))‚àíœµ (x ,t,y‚Ä≤)‚à•2). (3)\nCBDM |Y| Œ∏ t Œ∏ t Œ∏ t Œ∏ t\ny‚Ä≤‚ààY\n3.2 DiffuLT:DiffusionmodelforLong-Tailrecognition\nDiffusionmodelhelpslong-tailclassification. Inthisphase,arandomlyinitializeddiffusionmodel\nŒ∏istrainedtoenrichthedataset. PreliminaryexperimentsinvolvetrainingaDDPMonalong-tailed\ndatasetandusingittogenerateadditionaldata. AthresholdN isset,andforclassesc withfewer\nt j\nthan N samples, we generate the images to meet this threshold. This augmentation results in a\nt\ncollectionofsyntheticsamples, D = {(x ,y )}Ngen, whereN = (cid:80) max(0,N ‚àí|c |)\ngen i i i=1 gen cj‚ààC t j\nrepresentsthetotalnumberofgeneratedsamples. Thesegeneratedsamplesarethenintegratedwith\nthe original dataset, forming an augmented dataset D‚à™D , on which a classifier is trained to\ngen\nenhanceclassificationperformance.\nWeconductedexperimentsonCIFAR100-LTwithanimbalanceratioof100andsetN =500to\nt\nsupplementthedata. Theresults,detailedinthesecondlineoftable1,showa5.5%accuracyincrease\nfor the classifier trained on D ‚à™D compared to the baseline. This improvement underscores\ngen\ntheeffectivenessofourstraightforwardmethodinboostingoverallperformance. Consideringthe\ngeneratedsamples(especiallyfortailclasses)maybeoflowerqualityduetolimiteddataavailability,\n4"
      },
      {
        "page": 5,
        "content": "Figure2:Visualizationofgeneratedsamplesforclass90infeaturespaceusingt-SNE.Theassociated\nmodelisindicatedintheupper-leftcorner.\nTable3:Quantities,overallclassifierenhancement,and Table4: Diffusiontrainedwithvarying\naverageimprovementpersamplefordifferentgroups proportionsofheadclassdataandthe\nofdatageneratedbydiffusionmodel. correspondingresultsfortailclasses.\nGroup ‚à•D gen‚à• Acc.(%) ‚àÜAcc/‚à•D gen‚à• p h p AID Acc t(%)\n- - 25.0\nBaseline - 38.3\n0 25.8 26.0\nID 21,511 44.2 2.75√ó10‚àí4\n40 33.2 29.7\nAID 11,886 45.2 5.78√ó10‚àí4 80 35.7 32.5\nOOD 5,756 36.2 ‚àí3.61√ó10‚àí4 100 39.1 32.8\nClass-BalancingDiffusionModels(CBDM)isemployedtoimprovegenerationqualityinlong-tailed\nsettings. ByintegratingL andL intrainingthemodelŒ∏onD,thedatasetisenhanced,\nDDPM CBDM\nandaclassifieristrainedasdescribedpreviously. SubsequenttestingonCIFAR100-LTrevealsthat\ntheclassifierachievesanaccuracyof46.6%,markingan8.3%increaseoverthebaseline,asnotedin\nthefinallineoftable1.\nWhat samples are helpful? AID samples! We adjusted the hyper-parameter œÑ in L and\nCBDM\nevaluatemodelswithvaryingFIDscores. Resultspresentedintable1showthataccuracyimproves\nasFIDdecreases. LowerFIDscoresindicatethatgeneratedsamplesmorecloselyresemblethereal\ndatadistribution. Notably,somegeneratedsamplesclearlyfail,whileotherscorrectlyresembletheir\nintendedclass. Thisobservationmotivatesfurtherinvestigationintotheefficacyofsamples.\nClass 90 (truck) is selected randomly as a representative example in CIFAR100-LT. A baseline\nclassifier(œÜ ),trainedexclusivelyontheoriginaldatasetD,isusedtoanalyzethegenerateddata.\n0\nThis classifier extracts features which are then visualized using t-SNE, as shown in fig. 2. The\nvisualization reveals that samples generated via CBDM tend to be more centralized. For deeper\nanalysis,wedefinethecenterf ofaclass‚Äôsfeaturesastheaverageoftherealdatainfeaturespace,\no\nandsetthemaximumEuclideandistancebetweentworealsamples‚Äôfeaturesasathresholdd . We\nf\nthendefine3typesofthegeneratedsamplesbasedontheirdistancetof :\no\n(cid:40) d ‚â§d , ID\ni f\nd =‚à•f ‚àíf ‚à• : d <d ‚â§2d , AID (4)\ni i o 2 f i f\nd >2d , OOD\ni f\nwhereIDdenotesin-distributionsamples,whichcloselymatchthepatternsoftheoriginaldata. We\ndefineandnameapproximatelyin-distribution(AID)samples,whichexhibitslightdeviations. OOD\nstandsforout-of-distributionones,whicharesignificantlydifferingfromthecenter. Wesummarize\nthecompositionofsamplesgeneratedbyeachmodelintable2. Notably,theCBDMmodelgenerates\nalowerproportionofOODsamples,consistentwithitsFIDscore. Forevaluatingtheimpactofeach\ntype,wetrainclassifiersusingonlytheID,AID,andOODsamplesgeneratedbyCBDMwithœÑ =1\nrespectivelyasD ,combinedwithD,andpresenttheresultsintable3. Surprisingly,classifiers\ngen\ntrainedwithAIDsamplesachievethehighestaccuracyandshowthegreatestaverageimprovement\nper sample. Basedon this finding, ourhypothesis is thatAID samplesarethe most beneficialin\nenhancingclassifierperformance.\nMechanismsbehindtheAIDsamples. WeconductedexperimentstoexplorehowAIDsamples\nenhanceclassifierperformanceandwheretheirnewandusefulinformationoriginates. Adiffusion\nmodel (CBDM with œÑ = 1) is trained using images from tail classes (fewer than 100 samples),\nsupplemented by a variable proportion p of head class images. This model generates samples\nh\n5"
      },
      {
        "page": 6,
        "content": "ID AID OOD ID AID OOD ID AID OOD\nSnake Tiger Wardrobe\nSkyscraper Train Whale\nFigure3: Examplesofthreegroupsofgeneratedsamples.\nspecificallyfortailclasseswiththeproportionofAIDsamplesp ,andgetstheperformanceof\nAID\nthecorrespondingclassifierdenotedasAcc . Theresults,presentedintable4,showthatatp =0%,\nt h\nrelyingsolelyontailclassimages,p is25.8%,andAcc improvesmarginallyto26.0%,only1%\nAID t\nabovethebaseline. Asp increases,bothp andAcc rise,peakingatp = 100%. Thistrend\nh AID t h\nillustratesthediffusionmodel‚Äôsabilitytotransferinformationfrompopuloustounderrepresented\nclasses,effectivelyblendingdataacrossdifferentclassesintoAIDsamples. Examplesofthesample\ngroupsaredisplayedinfig.3,whereIDsamplescloselyresemblerealimages,AIDsamplesblend\npatternsfrommultipleclasses,andOODsamplestypicallyexhibitanomalies.\nGeneration of AID samples. How can we efficiently generate AID samples? While a filtering\nstrategycanbeusedtocollectAIDsamples,itisnotthemostefficientmethod. Amoreeffective\napproachinvolvesencouragingthegenerationmodeltospecificallyproduceAIDsamples. Given\nthatAIDsamplesaredefinedbytheirdistancefromthecenterofrealimagesinfeaturespace,we\ncanutilizethebaselineclassifierœÜ asafeatureextractortoguidethegenerationofAIDsamples.\n0\nOurgoalistoencourageacontrolleddeviationwithinfeaturespace. AfterT denoisingsteps,the\ndeviationshouldideallybewithintherangeofd to2d . Assumingthatthedeviationineachstepis\nf f\nproportionaltothenoisestrength,weintroduceanadditionalterminthelossfunctiontoencourage\nsmall,stepwisedeviations. Wedefinethedeviationateachstepinfeaturespaceas\n‚àö\n1‚àíŒ±¬Ø\nd = ‚àö T‚à•œÜ (x )‚àíœÜ (x +œµ ‚àíœµ (x ,t,y))‚à• , (5)\nt 0 0 0 0 t Œ∏ t 2\n1‚àíŒ±¬Ø\nt\nwhereœÜ (x +œµ ‚àíœµ (x ,t,y))representsthede-noisedimages‚Äôfeature. ThenewAIDlossisthen\n0 0 t Œ∏ t\n3\nL =Œ±E ‚à•d ‚àí d ‚à•2. (6)\nAID t‚àº[1,T],x0,œµt t 2 f\nwhereŒ±isahyper-parameteranddefaultedto0.1. WeincorporatethistermintobothL and\nDDPM\nL totrainthegenerationmodel. Aftertraining,weusethismodeltogeneratedata. During\nCBDM\nthegenerationprocess, weemployœÜ tofilteroutharmfulOODsamples, resultinginD . We\n0 gen\nthentraintheclassifierusingthecombineddatasetD‚à™D . Recognizingthatgenerateddataare\ngen\nlesscrucialthanrealimages,weintroduceaweightingtermtothecross-entropylosstoadjustthe\ninfluenceofthegeneratedsamples:\nL\n=‚àí(cid:88)\n(œây +(1‚àíy ))log\nexp(f œÜ,y(x))\n, (7)\ncls\n(x,y,yg)‚ààD‚à™Dgen\ng g (cid:80)M\ni=1exp(f œÜ,ci(x))\nwhereœâcontrolstheweightofgeneratedsamplesandissetto0.3bydefault. y isanadditionallabel\ng\nassignedtoeachimagex,whichdistinguishesbetweengeneratedandoriginalsamples. Specifically,\ny =1isusedforgeneratedsamples,whiley =0markstheoriginalones.\ng g\n3.3 OverallPipelineandDiscussion\nNow we are ready propose a new pipeline called DiffuLT to address long-tail recognition. The\npipelineisshowninfig.4withfoursteps:\n‚Ä¢ Training: Initially,wetrainafeatureextractorœÜ andaconditional,AID-biaseddiffusion\n0\nmodelŒ∏usingtheoriginallong-taileddatasetDalone.\n6"
      },
      {
        "page": 7,
        "content": "ImbalancedDatasetùíü\nTraining ‚Ñíùëêùëôùë†\nForward only\nùúë0 ùëìùúë\nùúë0 Feature Extractor\nFinalClassifier\nTsteps\nùúë0\nùúî‚àô‚Ñíùëêùëôùë†\nGenerate\nDiffusionùúÉ\nùíügen\nFigure4: TheoverallpipelineofourmethodDiffuLT.\n‚Ä¢ Generating: We establish a threshold N and employ the trained diffusion model Œ∏ to\nt\ngenerate and supplement samples. Using œÜ , we filter out OOD samples, resulting in a\n0\nrefineddatasetD .\ngen\n‚Ä¢ Training: We then train a new classifier f on the augmented dataset D ‚à™D using\nœÜ gen\nweightedcross-entropy,formingourfinalmodel.\nComparedtotraditionalmethodsthatfocusprimarilyontraining,oursnotonlyenhancesperformance\nbutisalsoreusableformodelupdates. Ourmethodrequiresmoretrainingtime,typicallyfourtimes\nlonger,totrainthegenerationmodelandproducesamples. However,ourmethodsprovevaluable\nwhenperformanceimprovementiscritical. Unliketypicaldataexpansionmethods,ourapproach\noffersbothpracticalandtheoreticalbenefitsbecauseitdon‚Äôtrelyonanyexternaldatasetormodel.\nFordetailedanalysis,pleaserefertotheappendixB.\n4 Experiment\n4.1 Experimentalsetup\nDatasets. Our research evaluate three long-tailed datasets: CIFAR10-LT (Cao et al. [2019a]),\nCIFAR100-LT(Caoetal.[2019a]),andImageNet-LT(Liuetal.[2019a]).Followingthemethodology\ndescribedin(Caoetal.[2019a]),weconstructlong-tailedversionsofthefirsttwodatasetsbyadjusting\nthelong-tailratiorto100,50,and10totestourmethodagainstvariouslevelsofimbalance.\nBaselines. Inourcomparativeanalysis,webenchmarkagainstabroadspectrumofclassicaland\ncontemporarylong-tailedlearningstrategies. Themethodscomparedcanbeclassifiedintomultiple\ngenreslikere-weightingandre-samplingtechniques,head-to-tailknowledgetransferapproaches,data-\naugmentation,andsoon. Somemethodshaveissuessuchasunfaircomparisonsorimplementation\nproblems. WedocumentboththeirresultsandourimplementationoutcomesintheappendixA.\nImplementation. WesetŒ± = 0.1,andœâ = 0.3. ThegenerationthresholdsN forCIFAR10-LT\nt\nandCIFAR100-LTwerefixedat5000and500,respectively. WeemployResNet-32astheclassifier\nbackbone. ForImageNet-LTexperiments,wesetagenerationthresholdofN =300. Theclassifiers\nt\nwerebasedonResNet-10andResNet-50architectureswithœâ =0.5.\nMoredetailsabouttheexperimentalsetupareavailableintheappendixA.\n4.2 ExperimentalResults\nGenerativeResults. Weassesstheefficacyofourspeciallydesignedlossfunction,detailedintable5.\nThis function improves the FID by reducing OOD samples, while also increasing the number of\nAIDsamplesandtheclassifier‚Äôsaccuracy. Furtherexperimentsintable6highlightthenecessityof\nourtrainingloss. Forbenchmarking,weuseabasicfilteringstrategyforCBDM,withallmodels\ngeneratingsamplestomeetthethresholdN foreachclass. Theterms\"Kept\"and\"G-Num\"denote\nt\n7"
      },
      {
        "page": 8,
        "content": "Table5: FIDofdiffusionmodel,proportionof Table6: Methodsandtypesofretainedsamples,\nsamples,andcorrespondingclassifieraccuracy pre-filteringcounts,andclassificationaccuracy.\nMethod FID p p p Acc.(%) Method Kept G-Num Acc.(%)\nID AID OOD\nCBDM All 39,153 46.6\nDDPM 7.76 39.1 21.2 39.7 43.8\nCBDM AID 108,684 48.1\nCBDM 5.86 44.8 36.3 18.9 46.6\nCBDM ID&AID 48,414 47.1\nOurs 5.37 40.7 50.1 9.2 49.7 Ours All 39,153 49.7\nTable7: ResultsonCIFAR100-LTandCIFAR10-LTdatasets. Theimbalanceratiorissetto100,50\nand10. Thehighest-performingresultsareinbold,withthesecond-bestinunderline. Additionally,\nwepresenttheresultsfordifferentgroups(many,medium,andfew)inCIFAR100-LTwithr =100.\nCIFAR100-LT CIFAR10-LT Statistics\nMethod\n100 50 10 100 50 10 Many Med. Few\nCE 38.3 43.9 55.7 70.4 74.8 86.4 65.2 37.1 9.1\nFocalLossLinetal.[2017] 38.4 44.3 55.8 70.4 76.7 86.7 65.3 38.4 8.1\nLDAM-DRWCaoetal.[2019a] 42.0 46.6 58.7 77.0 81.0 88.2 61.5 41.7 20.2\ncRTKangetal.[2019] 42.3 46.8 58.1 75.7 80.4 88.3 64.0 44.8 18.1\nBBNZhouetal.[2020a] 42.6 47.0 59.1 79.8 82.2 88.3 - - -\nRIDE(3experts)Wangetal.[2020] 48.0 - - - - - 68.1 49.2 23.9\nCAM-BSZhangetal.[2021a] 41.7 46.0 - 75.4 81.4 - - - -\nMisLASZhongetal.[2021b] 47.0 52.3 63.2 82.1 85.7 90.0 - - -\nDiVEHeetal.[2021] 45.4 51.1 62.0 - - - - - -\nCMOParketal.[2022] 47.2 51.7 58.4 - - - 70.4 42.5 14.4\nSAMRangwanietal.[2022] 45.4 - - 81.9 - - 64.4 46.2 20.8\nCUDAAhnetal.[2023] 47.6 51.1 58.4 - - - 67.3 50.4 21.4\nCSAShietal.[2023b] 46.6 51.9 62.6 82.5 86.0 90.8 64.3 49.7 18.2\nADRWWangetal.[2024b] 46.4 - 61.9 83.6 - 90.3 - - -\nH2TLietal.[2023] 48.9 53.8 - - - - - - -\nDiffuLT 51.5 56.3 63.8 84.7 86.9 90.7 69.0 51.6 29.7\nDiffuLT+BBN 51.9 56.7 64.0 85.0 87.2 90.9 69.5 51.9 30.2\nDiffuLT+RIDE(3experts) 52.4 56.9 64.2 85.3 87.3 90.9 70.3 52.1 30.7\nthetypesofsamplesretainedandthetotalnumberofsamplesgeneratedbeforefiltering,respectively.\nOurmethodsenhancethegenerationprocess‚Äôsefficiencyandachievethehighestaccuracy.\nCIFAR100-LT&CIFAR10-LT.Webenchmarkourapproachagainstarangeofmethodsonthe\nCIFAR100-LTandCIFAR10-LTdatasets, withresultsdetailedintable7. Theresultsnotshown\nin the original papers are indicated as \"-\" in the table. On CIFAR100-LT, our method surpasses\ncompetingmodels,achievingaccuracyimprovementsof13.2%,12.4%,and8.1%comparedwiththe\nbaselineforr =100,50,and10,respectively. OnCIFAR10-LT,ourmodelalsodemonstratesstrong\ncompetitiveness,enhancingaccuracyby14.3%,12.1%,and4.3%acrossthelong-tailratios,further\nvalidatingtheeffectivenessofourmethod. Sinceourmethodssolelymodifythetrainingdata,they\ncanbeeasilyintegratedwithothermethodstoachievebetterresults.\nFor CIFAR100-LT with an imbalanced ratio of 100, performance is also assessed across three\ncategories: many(classeswithover100samples),medium(classeswith20to100samples),andfew\n(classeswithfewerthan20samples). Whileourapproachdoesnotleadinthe‚ÄúMany‚Äùcategory,it\nexcelsin‚ÄúMed.‚Äù and‚ÄúFew‚Äù,significantlyoutperformingothersinthe‚ÄúFew‚Äùgroupwitha29.7%\naccuracy‚Äî8.3%abovethenearestcompetitorand20.6%beyondthebaseline.\nImageNet-LT. On the ImageNet-LT dataset, our methodology is evaluated against existing ap-\nproaches,withresultssummarizedintable8. UtilizingaResNet-10backbone,ourmethodregisters\na50.4%accuracy,outperformingthenearestcompetitorby4.5%. WithResNet-50,theaccuracy\nfurtherescalatesto56.4%,markingasubstantial14.8%enhancementoverthebaseline. Despitea\nslightdeclineinthe‚ÄúMany‚Äùcategoryrelativetothebaseline,ourapproachexcelsin‚ÄúMed.‚Äù and\n‚ÄòFew‚Äù,withthelatterwitnessingaremarkable33.6%improvementoverthebaseline. Ourmethod\ncanbecombinedwithotherstoachieveenhancedresults.\n8"
      },
      {
        "page": 9,
        "content": "Table 8: Results on ImageNet-LT. We deploy ResNet-10 and ResNet-50 as classifier backbones.\nTop-performingresultsarehighlightedinbold,withsecond-bestoutcomesunderlined.\nResNet-10 ResNet-50\nAll All Many Med. Few\nCE 34.8 41.6 64.0 33.8 5.8\nFocalLossLinetal.[2017] 30.5 - - - -\nOLTRLiuetal.[2019b] 35.6 - - - -\ncRTKangetal.[2019] 41.8 47.3 58.8 44.0 26.1\nRIDE(3experts)Wangetal.[2020] 45.9 54.9 66.2 51.7 34.9\nMisLASZhongetal.[2021b] - 52.7 - - -\nCMOParketal.[2022] - 49.1 67.0 42.3 20.5\nSAMRangwanietal.[2022] 53.1 62.0 52.1 34.8\nCUDAAhnetal.[2023] - 51.4 63.1 48.0 31.1\nCSAShietal.[2023b] 42.7 49.1 62.5 46.6 24.1\nADRWWangetal.[2024b] - 54.1 62.9 52.6 37.1\nDiffuLT 50.4 56.4 63.3 55.6 39.4\nDiffuLT+RIDE(3experts) 51.1 56.9 64.1 55.8 39.9\nTable9: Ablationexperimentstoverifythe Table10: Performancewithdifferent\neffectofeachmodule. weightsœâandhyper-parameterŒ±.\nGen. L Filt. Weight Acc.(%) œâ Acc.(%) Œ± Acc.(%)\nAID\n0 38.3 0 38.3\n38.3\n0.1 49.2 0.1 49.7\n‚úì 46.6\n0.3 51.5 0.5 49.5\n‚úì ‚úì 49.7 0.5 50.1 1.0 48.3\n‚úì ‚úì ‚úì 50.3 0.7 50.3 2.0 45.1\n‚úì ‚úì ‚úì ‚úì 51.5 1.0 50.3 4.0 43.3\n4.3 AblationStudy\nDifferent modules in our pipeline. Our methodology comprises several critical components:\ngenerated samples (using CBDM), AID-biased loss, filtering, and weighted cross-entropy. We\nconduct ablation experiments on CIFAR100-LT with r = 100. The results, presented in table 9,\nhighlightthecrucialroleeachcomponentplaysinenhancingtheoverallperformance. Notably,the\ngeneratedsamplesandAID-biasedlossarethemostinfluentialfactors.\nHyper-parameters. Weadjusttheparametersœâ intheweightedcross-entropyandŒ±inL on\nAID\nCIFAR100-LTwithr =100andevaluatetheclassificationresults. Theseresultsaresummarizedin\ntable10. Throughiterativeadjustments,wefindthattheoptimalperformance,a51.5%classification\naccuracy, is achieved when œâ = 0.3. Similarly, the best setting for Œ± is determined to be 0.1.\nConsequently,weestablishœâ =0.3andŒ±=0.1asthedefaultsettingsforourmethod.\n5 Conclusion\nInthisresearch,weproposedanovel,data-centricapproachdesignedtoaddressthechallengesof\nlong-tailclassification. WedefinedandidentifiedAID(approximatelyin-distribution)samplesasthe\nimportantones. WethenrevisedadiffusionmodeltrainedwithanAID-biasedlosstermononlythe\noriginaldatasetforthepurposeofgeneratingmoreAIDsamples,therebysignificantlyenrichingthe\ndataset. Followingsamplegeneration,wetrainedaclassifieronthisenhanceddatasetandemployeda\nweightedcross-entropyloss. Ourmethodhasshowntodelivercompetitiveperformance,highlighting\nits efficacy in real-world applications. The experiments conducted as part of this study notably\nemphasizethecriticalroleplayedbyAIDsamplesandtheirsignificantimpact.\nWeproposethatthisapproachintroducesanewparadigmfortacklinglong-tailclassificationchal-\nlenges,offeringasubstantialcomplementtoexistingmethodologies. Itprovidesarobustframework\n9"
      },
      {
        "page": 10,
        "content": "thatcanbeadaptedtovariousscenarioswhereperformanceisacriticalfactor. Despiteitsadvantages,\nthetrainingofthediffusionmodelandthegenerationofsamplesaretime-consuming. Theneedfor\noptimizationintrainingandgenerationspeedsrepresentsalimitationofourcurrentmethod. Wewill\nleavethispointasfutureworktofurtherimprovetheeffectivenessandefficiencyofourmethod.\nAcknowledgements\nWeacknowledgethefundingprovidedbytheNationalNaturalScienceFoundationofChinaunder\nGrant62276123andGrant61921006. J.Wuisthecorrespondingauthor.\nReferences\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,\npages770‚Äì778,2016.\nAlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas\nUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,\nandNeilHoulsby. Animageisworth16x16words: Transformersforimagerecognitionatscale.\nInICLR,2021.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swintransformer: Hierarchicalvisiontransformerusingshiftedwindows. InICCV,pages\n10012‚Äì10022,2021.\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll√°r. Focal loss for dense\nobjectdetection. InProceedingsoftheIEEEinternationalconferenceoncomputervision,pages\n2980‚Äì2988,2017.\nKaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced\ndatasetswithlabel-distribution-awaremarginloss. Advancesinneuralinformationprocessing\nsystems,32,2019a.\nBoyanZhou,QuanCui,Xiu-ShenWei,andZhao-MinChen. Bbn: Bilateral-branchnetworkwith\ncumulativelearningforlong-tailedvisualrecognition. InProceedingsoftheIEEE/CVFconference\noncomputervisionandpatternrecognition,pages9719‚Äì9728,2020a.\nYongshunZhang,Xiu-ShenWei,BoyanZhou,andJianxinWu. Bagoftricksforlong-tailedvisual\nrecognitionwithdeepconvolutionalneuralnetworks. InProceedingsoftheAAAIconferenceon\nartificialintelligence,volume35,pages3447‚Äì3455,2021a.\nXudongWang,LongLian,ZhongqiMiao,ZiweiLiu,andStellaYu. Long-tailedrecognitionbyrout-\ningdiversedistribution-awareexperts. InInternationalConferenceonLearningRepresentations,\n2020.\nJiequanCui,ShuLiu,ZhuotaoTian,ZhishengZhong,andJiayaJia. Reslt: Residuallearningfor\nlong-tailedrecognition. IEEEtransactionsonpatternanalysisandmachineintelligence,45(3):\n3695‚Äì3706,2022.\nChengZhang,Tai-YuPan,YandongLi,HexiangHu,DongXuan,SoravitChangpinyo,BoqingGong,\nandWei-LunChao. Mosaicos: asimpleandeffectiveuseofobject-centricimagesforlong-tailed\nobjectdetection. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,\npages417‚Äì427,2021b.\nYifanZhang,DaquanZhou,BryanHooi,KaiWang,andJiashiFeng. Expandingsmall-scaledatasets\nwithguidedimagination. AdvancesinNeuralInformationProcessingSystems,36,2024a.\nJonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin\nneuralinformationprocessingsystems,33:6840‚Äì6851,2020.\nYangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBen\nPoole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXivpreprint\narXiv:2011.13456,2020.\n10"
      },
      {
        "page": 11,
        "content": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-\nresolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-\nenceoncomputervisionandpatternrecognition,pages10684‚Äì10695,2022.\nYiming Qin, Huangjie Zheng, Jiangchao Yao, Mingyuan Zhou, and Ya Zhang. Class-balancing\ndiffusionmodels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern\nRecognition,pages18434‚Äì18443,2023.\nLaurensVanderMaatenandGeoffreyHinton. Visualizingdatausingt-sne. Journalofmachine\nlearningresearch,9(11),2008.\nYinCui,MenglinJia,Tsung-YiLin,YangSong,andSergeBelongie. Class-balancedlossbasedon\neffectivenumberofsamples. InCVPR,pages9268‚Äì9277,2019.\nBoyanZhou,QuanCui,Xiu-ShenWei,andZhao-MinChen. Bbn: Bilateral-branchnetworkwith\ncumulativelearningforlong-tailedvisualrecognition. InCVPR,pages9716‚Äì9725,2020b.\nKaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced\ndatasetswithlabel-distribution-awaremarginloss. InNeurIPS,pages1565‚Äì1576,2019b.\nYifanZhang,BingyiKang,BryanHooi,ShuichengYan,andJiashiFeng. Deeplong-tailedlearning:\nAsurvey. IEEETransactionsonPatternAnalysisandMachineIntelligence,45(9):10795‚Äì10816,\n2023a.\nKeZhu,MinghaoFu,JieShao,TianyuLiu,andJianxinWu. Rectifytheregressionbiasinlong-tailed\nobjectdetection. arXivpreprintarXiv:2401.15885,2024.\nBingyiKang,SainingXie,MarcusRohrbach,ZhichengYan,AlbertGordo,JiashiFeng,andYannis\nKalantidis. Decouplingrepresentationandclassifierforlong-tailedrecognition. InICLR,2020.\nZhishengZhong,JiequanCui,ShuLiu,andJiayaJia. Improvingcalibrationforlong-tailedrecogni-\ntion. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition\n(CVPR),pages16489‚Äì16498,2021a.\nZitaiWang,QianqianXu,ZhiyongYang,YuanHe,XiaochunCao,andQingmingHuang. Aunified\ngeneralizationanalysisofre-weightingandlogit-adjustmentforimbalancedlearning. Advancesin\nNeuralInformationProcessingSystems,36,2024a.\nKeZhu,MinghaoFu,andJianxinWu. Multi-labelself-supervisedlearningwithsceneimages. In\nICCV,pages6694‚Äì6703,2023.\nTianhaoLi,LiminWang,andGangshanWu. Selfsupervisiontodistillationforlong-tailedvisual\nrecognition. InICCV,pages630‚Äì639,2021.\nMengkeLi,Yiu-mingCheung,andYangLu. Long-tailedvisualrecognitionviagaussianclouded\nlogitadjustment. InCVPR,pages6929‚Äì6938,2022.\nAdityaKrishnaMenon,SadeepJayasumana,AnkitSinghRawat,HimanshuJain,AndreasVeit,and\nSanjivKumar. Long-taillearningvialogitadjustment. InICLR,2021.\nZhiyongYang,QianqianXu,ZitaiWang,SicongLi,BoyuHan,ShilongBao,XiaochunCao,and\nQingmingHuang. Harnessinghierarchicallabeldistributionvariationsintestagnosticlong-tail\nrecognition. arXivpreprintarXiv:2405.07780,2024.\nVigneshRamanathan,RuiWang,andDhruvMahajan. Dlwl: Improvingdetectionforlowshotclasses\nwithweaklylabelleddata. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand\npatternrecognition,pages9342‚Äì9352,2020.\nBowenDong,PanZhou,ShuichengYan,andWangmengZuo. Lpt: Long-tailedprompttuningfor\nimageclassification. arXivpreprintarXiv:2210.01033,2022.\nJiang-XinShi,TongWei,ZhiZhou,Xin-YanHan,Jie-JingShao,andYu-FengLi. Parameter-efficient\nlong-tailedrecognition. arXivpreprintarXiv:2309.10019,2023a.\n11"
      },
      {
        "page": 12,
        "content": "PrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. Advances\ninneuralinformationprocessingsystems,34:8780‚Äì8794,2021.\nAdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,MarkChen,\nandIlyaSutskever. Zero-shottext-to-imagegeneration. InInternationalConferenceonMachine\nLearning,pages8821‚Äì8831.PMLR,2021.\nIanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,\nAaronCourville,andYoshuaBengio. Generativeadversarialnets. Advancesinneuralinformation\nprocessingsystems,27,2014.\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with\nconditionaladversarialnetworks. InProceedingsoftheIEEEconferenceoncomputervisionand\npatternrecognition,pages1125‚Äì1134,2017.\nQuanKong,BinTong,MartinKlinkigt,YukiWatanabe,NaotoAkira,andTomokazuMurakami.\nActive generative adversarial network for image classification. In Proceedings of the AAAI\nconferenceonartificialintelligence,volume33,pages4090‚Äì4097,2019.\nShekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J Fleet.\nSynthetic data from diffusion models improves imagenet classification. arXiv preprint\narXiv:2304.08466,2023.\nBrandonTrabucco,KyleDoherty,MaxGurinas,andRuslanSalakhutdinov. Effectivedataaugmenta-\ntionwithdiffusionmodels. arXivpreprintarXiv:2302.07944,2023.\nManlinZhang,JieWu,YuxiRen,MingLi,JieQin,XuefengXiao,WeiLiu,RuiWang,MinZheng,\nandAndyJMa. Diffusionengine: Diffusionmodelisscalabledataengineforobjectdetection.\narXivpreprintarXiv:2309.03893,2023b.\nYuxuanZhang,HuanLing,JunGao,KangxueYin,Jean-FrancoisLafleche,AdelaBarriuso,Antonio\nTorralba,andSanjaFidler. Datasetgan: Efficientlabeleddatafactorywithminimalhumaneffort.\nInProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages\n10145‚Äì10155,2021c.\nTianjiaoZhang,HuangjieZheng,JiangchaoYao,XiangfengWang,MingyuanZhou,YaZhang,and\nYanfengWang.Long-taileddiffusionmodelswithorientedcalibration.InTheTwelfthInternational\nConferenceonLearningRepresentations,2024b.\nZiweiLiu,ZhongqiMiao,XiaohangZhan,JiayunWang,BoqingGong,andStellaX.Yu. Large-scale\nlong-tailedrecognitioninanopenworld. InCVPR,pages2537‚Äì2546,2019a.\nBingyiKang,SainingXie,MarcusRohrbach,ZhichengYan,AlbertGordo,JiashiFeng,andYannis\nKalantidis. Decouplingrepresentationandclassifierforlong-tailedrecognition. arXivpreprint\narXiv:1910.09217,2019.\nZhishengZhong,JiequanCui,ShuLiu,andJiayaJia. Improvingcalibrationforlong-tailedrecog-\nnition. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,\npages16489‚Äì16498,2021b.\nYin-YinHe,JianxinWu,andXiu-ShenWei. Distillingvirtualexamplesforlong-tailedrecognition.\nInProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages235‚Äì244,\n2021.\nSeulkiPark,YoungkyuHong,ByeonghoHeo,SangdooYun,andJinYoungChoi. Themajoritycan\nhelptheminority:Context-richminorityoversamplingforlong-tailedclassification.InProceedings\noftheIEEE/CVFConferenceonComputerVisionandPatternRecognition, pages6887‚Äì6896,\n2022.\nHarsh Rangwani, Sumukh K Aithal, Mayank Mishra, et al. Escaping saddle points for effective\ngeneralizationonclass-imbalanceddata. AdvancesinNeuralInformationProcessingSystems,35:\n22791‚Äì22805,2022.\n12"
      },
      {
        "page": 13,
        "content": "Sumyeong Ahn, Jongwoo Ko, and Se-Young Yun. Cuda: Curriculum of data augmentation for\nlong-tailedrecognition. arXivpreprintarXiv:2302.05499,2023.\nJiang-XinShi,TongWei,YukeXiang,andYu-FengLi. Howre-samplinghelpsforlong-taillearning?\nAdvancesinNeuralInformationProcessingSystems,36,2023b.\nZitaiWang,QianqianXu,ZhiyongYang,YuanHe,XiaochunCao,andQingmingHuang. Aunified\ngeneralizationanalysisofre-weightingandlogit-adjustmentforimbalancedlearning. Advancesin\nNeuralInformationProcessingSystems,36,2024b.\nMengkeLi,ZhikaiHu,YangLu,WeichaoLan,Yiu-mingCheung,andHuiHuang. Featurefusion\nfromheadtotail:anextremeaugmentingstrategyforlong-tailedvisualrecognition. arXivpreprint\narXiv:2306.06963,2023.\nZiweiLiu,ZhongqiMiao,XiaohangZhan,JiayunWang,BoqingGong,andStellaXYu. Large-scale\nlong-tailedrecognitioninanopenworld. InProceedingsoftheIEEE/CVFconferenceoncomputer\nvisionandpatternrecognition,pages2537‚Äì2546,2019b.\nAlexKrizhevsky. Learningmultiplelayersoffeaturesfromtinyimages. Technicalreport,University\nofToronto,2009.\nTeroKarras,MiikaAittala,JanneHellsten,SamuliLaine,JaakkoLehtinen,andTimoAila. Training\ngenerativeadversarialnetworkswithlimiteddata. Advancesinneuralinformationprocessing\nsystems,33:12104‚Äì12114,2020.\nOlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,ZhihengHuang,\nAndrejKarpathy,AdityaKhosla,MichaelBernstein,AlexanderBerg,andLiFei-Fei. ImageNet\nlargescalevisualrecognitionchallenge.InternationalJournalofComputerVision,115(3):211‚Äì252,\n2015.\nJiaweiRen,CunjunYu,shunansheng,XiaoMa,HaiyuZhao,ShuaiYi,andhongshengLi. Balanced\nmeta-softmaxforlong-tailedvisualrecognition. InNeurIPS,pages4175‚Äì4186,2020.\nJiequanCui,ZhishengZhong,ShuLiu,BeiYu,andJiayaJia. Parametriccontrastivelearning. In\nProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages715‚Äì724,2021.\nJiequanCui,ZhishengZhong,ZhuotaoTian,ShuLiu,BeiYu,andJiayaJia. Generalizedparametric\ncontrastivelearning. IEEETransactionsonPatternAnalysisandMachineIntelligence,2023.\nShiranZada,ItayBenou,andMichalIrani. Purenoisetotherescueofinsufficientdata: Improving\nimbalancedclassificationbytrainingonrandomnoiseimages. InInternationalConferenceon\nMachineLearning,pages25817‚Äì25833.PMLR,2022.\nEkinDCubuk, BarretZoph, DandelionMane, VijayVasudevan, andQuocVLe. Autoaugment:\nLearningaugmentationpoliciesfromdata. arXivpreprintarXiv:1805.09501,2018.\n13"
      },
      {
        "page": 14,
        "content": "A Experiment\nA.1 Experimentalsettings\nDuetospaceconstraintsinthemainpaper,weonlyincludeessentialinformationabouttheexperi-\nmentalsetup. Additionaldetailsareprovidedhere.\nCIFAR100-LT&CIFAR10-LT.TheoriginalCIFAR100andCIFAR10datasetseachconsistof\na training set with 50,000 images evenly distributed across 100 or 10 classes, respectively. The\nCIFAR100-LT and CIFAR10-LT datasets, derived from CIFAR100 Krizhevsky [2009] and CI-\nFAR10 Krizhevsky [2009], feature a long-tail distribution where the class frequency decreases\nexponentiallyfromclass0tothelastclass. Commonlyusedlong-tailratiosare100, 50, and10.\nSpecifically,theCIFAR100-LTsubsetscontain10,847,12,608,and19,573images,withthelargest\nclasscontaining500samplesandthesmallesthaving5,10,and50samples,respectively. Similarly,\nthe CIFAR10-LT subsets consist of 12,406, 13,996, and 20,431 images, with the largest classes\ncontaining5,000samplesandthesmallest50,100,and500samples,respectively.\nExperimentsonCIFAR100-LTandCIFAR10-LTutilizetheframeworkandtrainingmethodologies\nfrom Qin et al. [2023], incorporating the CBDM Qin et al. [2023] loss function and Adaptive\nAugmentation Karras et al. [2020]. We set the training duration to 500,000 steps, with hyper-\nparametersœÑ andŒ≥ fixedat1and0.25,asperthecitedstudy. Thebatchsizeismaintainedat128,\nthediffusionprocessrunsfor1,000timesteps,andthelearningrateissetat0.0002usinganAdam\noptimizer.\nForclassifiertraining,wefollowthecodeandprotocolsfromZhouetal.[2020a],whichprescribe\na 200-epoch training regimen. The classifier training also employs a batch size of 128, utilizing\nanSGDoptimizerwithalearningrateof0.1. ThefeatureextractorœÜ istrainedusingthissetup\n0\nwithoutanyadditionalmethodsordata. Thefinalclassifieristrainedsimilarlybutincorporatesboth\ngeneratedandoriginalsamples. Alltrainingtasksareconductedon8√óNVIDIAGeForceRTX3090\nGPUs,withfurtherdiscussedinappendixB\nImageNet-LT.ImageNet-LT,comprising115,846imagesacross1,000classeswithamaximumof\n1,280imagesperclassandaminimumof5,followsthespecificationssetin(Liuetal.[2019a]). This\ndatasetisderivedfromImageNetRussakovskyetal.[2015]bysamplingasubsetaccordingtoa\nParetodistributionwithapowervalueofŒ±=6. Forperspective,theoriginalImageNettrainingset\ncontains1,281,167images,makingImageNet-LTlessthan10%thesizeoftheoriginaldataset. The\ntestsetofImageNet-LTmirrorsthatofImageNet,containing100,000images.\nDuetoitsconsiderablesizeandimageresolution,ImageNet-LTnecessitatesmodificationsfromthe\nstandardCBDMframeworktoaddressinefficiencies. WeadaptthecodebasefromDhariwaland\nNichol[2021]forthispurpose,extendingthetrainingto1,980,000iterations. Thissetupusesthe\nAdamWoptimizerwithalearningrateof3e-4andabatchsizeof64,adjustedfromtheoriginal256\nduetoGPUmemoryconstraints. Moreover,themodelistrainedatanimageresolutionof256,and\nthetotaldiffusiontimestepissetto1,000.\nClassifiertrainingforImageNet-LTemploystheframeworkfromZhangetal.[2021a]. Specifically,\nthemodelistrainedover100epochswithabatchsizeof512usingtheSGDoptimizeratalearning\nrateof0.2. Alltrainingtasksarecarriedouton8√óNVIDIAGeForceRTX3090GPUs.\nA.2 GeneratedImages\nWesynthesizeimagesforCIFAR-100,CIFAR-10,andImageNet-LTtodemonstratetheirutilityin\nenhancinglong-tailrecognition. Randonlyselectedexamplesofthegeneratedsamplesaredisplayed,\nparticularlyforCIFAR100-LTinfig.5,whereimageshavearesolutionof32√ó32andfocuson\n\"few-shot\" classes‚Äîthose with fewer than 20 instances. Despite the limited examples available\nintheseclasses,ourdiffusionmodeleffectivelyutilizestheentiredatasettoproducehigh-quality\nsamples. Whilethesesynthesizedimagesmaintainsomesimilaritieswiththeiroriginalcounterparts,\nthenotablevariationsmakethemvaluableformodeltraining.However,becausethegeneratedimages\nmaysometimesdistortessentialfeaturesorintroduceinaccuracies,itbecomescrucialtofilteroutthe\nOODsamplestomaintaintheirusefulness.\nInfig.6,weshowcasethegeneratedimagesforCIFAR10-LT,whichdemonstratesuperiorquality\ncomparedtothosefromCIFAR100-LT,duetoamoreabundanttrainingdataset. InCIFAR10-LT,\n14"
      },
      {
        "page": 15,
        "content": "Figure5: GeneratedimagesforCIFAR100-LT\nFigure6: GeneratedimagesforCIFAR10-LT\nTable11: RepeatedexperimentsonCIFAR100-LTandCIFAR10-LTtotesttherobustnessofour\nmethods.\nCIFAR100-LT CIFAR10-LT\nMethod\n100 50 10 100 50 10\nDiffuLT(1) 51.5 56.3 63.8 84.7 86.9 90.7\nDiffuLT(2) 51.7 56.3 63.7 84.9 86.8 90.7\nDiffuLT(3) 51.5 56.3 63.3 84.9 86.3 90.6\nTable12: RepeatedexperimentsonImageNet-LTtotesttherobustnessofmethods.\nResNet-10 ResNet-50\nDiffuLT(1) 50.4 56.4\nDiffuLT(2) 50.4 56.5\nDiffuLT(3) 50.5 56.5\neachclassgenerallycontainstentimesmoreimagesthaninCIFAR100-LT.Thesmallestclassin\nCIFAR10-LThas50images,greatlyexceedingtheminimumof5inCIFAR100-LT.Thisincreasein\nsamplesize,however,requiresamorestringentfilteringprocesstopreventpotentialinformationloss\nduetothelargervolumeofimagesgenerated.\nFor ImageNet-LT, the generated images, displayed in fig. 7, feature a resolution of 224 √ó 224,\nwhichissignificantlyclearerthanthosefromCIFAR-10andCIFAR-100. Whilesomefinerdetails,\nsuchastextwithintheimageortextureslikefur,maynotbefullydistinct,thegeneratedimages\neffectivelycapturetheessentialpatternsoftheclasses.Theseimagescansignificantlyaidthelong-tail\nrecognitiontask,particularlyforclasseswithfeweroriginalsamples.\nA.3 Robustnessanlysis\nSinceCIFAR100-LTandCIFAR10-LTaretypicallysampledrandomlyfromtheiroriginaldatasets,\nwetestedourmethodsacrossvarioussampledsetstoassesstheireffectivenessandrobustness. The\nresults,displayedintable11,demonstratethatourmethod‚Äôsperformanceisstable,exhibitingonly\nminimalvariations. ForImageNet-LT,whichisafixeddataset,wegeneratedsamplesthreetimesto\n15"
      },
      {
        "page": 16,
        "content": "Figure7: GeneratedimagesforImageNet-LT\nexamineconsistency. Thevariationsamongdifferentsamplesetsareminimal,asshownintable12.\nTherefore,ourmethodprovestoberobustinenhancinglong-tailclassificationperformance.\nA.4 DetailsofBaselineMethods.\nIn the main paper, we outline various approaches to long-tail classification and benchmark our\nmethodsagainstthesestrategies. Here,weprovideaconciseoverviewofthemethodswithineach\ncategory. Forre-weightingandre-samplingtechniques,weexamineCross-Entropy(CE),FocalLoss\n(Linetal.[2017]),LDAM-DRW(Caoetal.[2019a]),cRT(Kangetal.[2019]),BBN(Zhouetal.\n[2020a]),CSA(Shietal.[2023b]),andADRW(Wangetal.[2024b]). Intherealmofhead-to-tail\nknowledgetransfer,weincludemethodssuchasOLTR(Liuetal.[2019b])andH2T(Lietal.[2023]).\nLabel-smoothingstrategiesarerepresentedbyMisLAS(Zhongetal.[2021b])andDiVE(Heetal.\n[2021]),whileindataaugmentation,wecompareourapproachwithCAM-BS(Zhangetal.[2021a]),\nCMO(Parketal.[2022]),andCUDA(Ahnetal.[2023]). Lastly,SAM(Rangwanietal.[2022])\nexemplifiesanadvancedoptimizationtechnique,andRIDE(Wangetal.[2020])showcasesamixture\nofexperttechniqueinourcomparison.\n16"
      },
      {
        "page": 17,
        "content": "Table 13: Results on CIFAR100-LT using an alternative pipeline based on the implementation\nguidelinesfromBSCERenetal.[2020],withimbalanceratiosrsetat100,50,and10.\nCIFAR100-LT\nMethod\n100 50 10\nBaseline 38.3 43.9 55.7\nBaseline‚àó 45.3 50.3 61.9\nBSCERenetal.[2020] 50.8 - 63.0\nPaCoCuietal.[2021] 52.0 56.0 64.2\nGPaCoCuietal.[2023] 52.3 56.4 65.4\nDiffuLT 54.7 58.9 66.1\nDiffuLT+GPaCo 55.4 59.5 66.4\nA.5 OtherMethods.\nForvariousreasons,somemethodsarenotincludedinourexperimentalcomparisons. Thisdecision\nprimarilystemsfromtwofactors. Firstly,severalmethods,despitedemonstratingimpressiveresults,\ndo not have publicly available code (Zada et al. [2022]), limiting our ability to perform direct\ncomparisons. Secondly, methodssuchasthoseinCuietal.[2021]andCuietal.[2023]achieve\ncommendableresultsandcanbereplicated. However,theircomparisonsmaybeconsideredunfair.\nThesemethodsutilizeAutoAugment(Cubuketal.[2018]),withparametersoptimizedacrossthe\nentiredataset,ratherthanspecificallyforthelong-tailedsegment. Thisapproachsignificantlyboosts\ntheirbaselineperformance,asreportedinRenetal.[2020]. Forinstance,onCIFAR100-LTwith\nanimbalanceratioof0.1,baselineaccuracyimprovesfrom38.3to45.3,asshownintable13. The\nfirst \"Baseline\" line reflects the standard settings, while entries marked with ‚àó use the enhanced\nsettings, demonstrating a substantial improvement. Comparing these results with those obtained\nunderstandardconditionswouldbeunfair. Ourmethodologycouldalsobeadaptedtosuchsettings\nandcombinedwiththesetechniquestoachieveexcellentresults,asillustratedintable13. However,\ntheseresultsareomittedfromthemainpapertomaintainafaircomparison.\nB Discussion\nB.1 Comparisonofourmethodwithothertypeofmethods.\nThecomparisonisdividedintotwoparts:comparingourmethodswithtraditionallong-tailrecognition\napproachesandcontrastingthemwithdatasynthesismethods,asshowninfig.8.\nComparedtotraditionallong-tailclassificationmethodsthatpredominantlyfocusontraining,our\napproachoffersanovelperspective. Ratherthandesigningintricatemethodstofacilitatetrainingon\nlong-taileddatasets,westraightforwardlyenhancethedatasetusingagenerativemodel.Thisapproach\nisnotonlyinnovativebutalsocompatiblewithexistingtrainingmethodologiesanddemonstrates\nimprovedperformance. However,themainlimitationisthetrainingofthegenerativemodel,which\nistime-consumingandchallengingtoscale. Thesepointswillbefurtherdiscussedinthesubsequent\nsubsection.\nWhencomparingwithdatasynthesismethods,itisclearthatourapproachdoesnotsurpassthose\nemployingtechnologieslikeStableDiffusionorCLIPonlong-taileddatasets.Nevertheless,theuseof\nsuchlargemodelstrainedonextensivedataeliminatesthecorechallengeoflong-tailproblems‚Äîdata\nscarcity.Forinstance,theclass\"train\"inCIFAR100-LThasonlytenimages,whereasStableDiffusion\nhasbeentrainedonthousandsoftrainimages. Inpracticalsettings,accessingsuchexpansivemodels\ntailoredtospecificdatasetsisunrealistic. Thesemethodsmainlybenefitfromdataleakage. Our\napproach, in contrast, is designed for real long-tail scenarios without reliance on external data\nor models, providing practical and theoretical value. We address several previously unanswered\nquestions:\n‚Ä¢ Isadiffusionmodeltrainedfromscratchbeneficialforlong-tailrecognition? Yes.\n17"
      },
      {
        "page": 18,
        "content": "Train\nùúë\n0 Generate\nStable Diffusion\nClassifier\n(a)Previouslong-tailrecognitionmethods. (b)Datasynthesismethods.\nTsteps\nTrain Generate\nDiffusionùúÉ\n(c)Ours\nFigure8: Maincaptiondescribingallimages\n‚Ä¢ Whichgeneratedsamplesareusefulforlong-tailrecognition? AIDsamples.\n‚Ä¢ Whydoesdiffusionworkforlong-tailrecognition? Itblendsclassinformationtogenerate\nbeneficialandnovelsamples.\nB.2 Limitation\nThe primary limitation of our methods is the extensive training time required for the generative\nmodel. Forinstance,trainingadiffusionmodelonCIFAR100-LTtakes24hours,whileImageNet-LT\nrequiresapproximatelysixdays. Asthequalityandquantityofdataincrease,thetrainingcostsscale\nupsignificantly,makingitchallengingtoapplyourmethodstolargerdatasetssuchasiNaturalistand\nPlaces-LTduetoresourceandtimeconstraints.\nDespitethesechallenges,ourmethodsarehighlyeffectiveinaddressingthelong-tailrecognition\nproblem. Toimprovetrainingefficiency,weareexploringtwopotentialsolutions. Thefirstinvolves\nadoptingtechniquesthatacceleratethetrainingandinferenceprocessesofdiffusionmodels. The\nsecondstrategyconsiderstheuseofpre-trainedgenerativemodelsinreallong-tailscenarios. This\napproachdoesnotcontradictourpreviousassertionthatusinglarge,pre-trainedmodelsonfamiliar\nlong-taileddataisunfairandnottrulyrepresentativeoflong-tailchallenges. Instead,weadvocatefor\ntheuseofpre-trainedmodelsonlong-taildatasetstheyhavenotpreviouslyencountered,ensuring\nfairnessandpracticalapplicability. Employingapre-trainedmodelcouldsignificantlyexpediteour\npipelinebyeliminatingtheneedtotrainfromscratch. Thistopicextendsbeyondthescopeofthis\npaperandwillbeaddressedinfutureresearch.\n18"
      },
      {
        "page": 19,
        "content": "NeurIPSPaperChecklist\n1. Claims\nQuestion: Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthe\npaper‚Äôscontributionsandscope?\nAnswer: [Yes]\nJustification: The conclusions and methods in the abstract and introduction accurately\nencapsulatethecontributionsofourpaper,asdetailedinsection3. Theexperimentalresults\ndiscussedarefullydocumentedinsection4,confirmingtheclaims‚Äôvalidity.\nGuidelines:\n‚Ä¢ The answer NA means that the abstract and introduction do not include the claims\nmadeinthepaper.\n‚Ä¢ Theabstractand/orintroductionshouldclearlystatetheclaimsmade,includingthe\ncontributionsmadeinthepaperandimportantassumptionsandlimitations. ANoor\nNAanswertothisquestionwillnotbeperceivedwellbythereviewers.\n‚Ä¢ Theclaimsmadeshouldmatchtheoreticalandexperimentalresults,andreflecthow\nmuchtheresultscanbeexpectedtogeneralizetoothersettings.\n‚Ä¢ Itisfinetoincludeaspirationalgoalsasmotivationaslongasitisclearthatthesegoals\narenotattainedbythepaper.\n2. Limitations\nQuestion: Doesthepaperdiscussthelimitationsoftheworkperformedbytheauthors?\nAnswer: [Yes]\nJustification: The paper transparently acknowledges the main limitations in section 3.3\nandsection5. Additionally,potentiallimitationsareexploredintheappendixB,ensuringa\ncomprehensivediscussionoftheconstraintsandchallengesencountered.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperhasnolimitationwhiletheanswerNomeansthat\nthepaperhaslimitations,butthosearenotdiscussedinthepaper.\n‚Ä¢ Theauthorsareencouragedtocreateaseparate\"Limitations\"sectionintheirpaper.\n‚Ä¢ Thepapershouldpointoutanystrongassumptionsandhowrobusttheresultsareto\nviolationsoftheseassumptions(e.g.,independenceassumptions,noiselesssettings,\nmodelwell-specification,asymptoticapproximationsonlyholdinglocally).Theauthors\nshouldreflectonhowtheseassumptionsmightbeviolatedinpracticeandwhatthe\nimplicationswouldbe.\n‚Ä¢ Theauthorsshouldreflectonthescopeoftheclaimsmade,e.g.,iftheapproachwas\nonlytestedonafewdatasetsorwithafewruns. Ingeneral,empiricalresultsoften\ndependonimplicitassumptions,whichshouldbearticulated.\n‚Ä¢ Theauthorsshouldreflectonthefactorsthatinfluencetheperformanceoftheapproach.\nForexample,afacialrecognitionalgorithmmayperformpoorlywhenimageresolution\nisloworimagesaretakeninlowlighting. Oraspeech-to-textsystemmightnotbe\nusedreliablytoprovideclosedcaptionsforonlinelecturesbecauseitfailstohandle\ntechnicaljargon.\n‚Ä¢ Theauthorsshoulddiscussthecomputationalefficiencyoftheproposedalgorithms\nandhowtheyscalewithdatasetsize.\n‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to\naddressproblemsofprivacyandfairness.\n‚Ä¢ Whiletheauthorsmightfearthatcompletehonestyaboutlimitationsmightbeusedby\nreviewersasgroundsforrejection,aworseoutcomemightbethatreviewersdiscover\nlimitationsthataren‚Äôtacknowledgedinthepaper. Theauthorsshouldusetheirbest\njudgmentandrecognizethatindividualactionsinfavoroftransparencyplayanimpor-\ntantroleindevelopingnormsthatpreservetheintegrityofthecommunity. Reviewers\nwillbespecificallyinstructedtonotpenalizehonestyconcerninglimitations.\n3. TheoryAssumptionsandProofs\n19"
      },
      {
        "page": 20,
        "content": "Question: Foreachtheoreticalresult,doesthepaperprovidethefullsetofassumptionsand\nacomplete(andcorrect)proof?\nAnswer: [NA]\nJustification: Thefocusofpaperisprimarilyempirical;thus,ourconclusionsandmethods\narederivedfromandvalidatedbyexperimentalresultsratherthantheoreticalproofs.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludetheoreticalresults.\n‚Ä¢ Allthetheorems, formulas, andproofsinthepapershouldbenumberedandcross-\nreferenced.\n‚Ä¢ Allassumptionsshouldbeclearlystatedorreferencedinthestatementofanytheorems.\n‚Ä¢ Theproofscaneitherappearinthemainpaperorthesupplementalmaterial, butif\ntheyappearinthesupplementalmaterial,theauthorsareencouragedtoprovideashort\nproofsketchtoprovideintuition.\n‚Ä¢ Inversely,anyinformalproofprovidedinthecoreofthepapershouldbecomplemented\nbyformalproofsprovidedinappendixorsupplementalmaterial.\n‚Ä¢ TheoremsandLemmasthattheproofreliesuponshouldbeproperlyreferenced.\n4. ExperimentalResultReproducibility\nQuestion: Doesthepaperfullydisclosealltheinformationneededtoreproducethemainex-\nperimentalresultsofthepapertotheextentthatitaffectsthemainclaimsand/orconclusions\nofthepaper(regardlessofwhetherthecodeanddataareprovidedornot)?\nAnswer: [Yes]\nJustification: We have developed a straightforward and easily replicable pipeline, fully\ndetailedinsection3. Allnecessaryhyper-parametersandexperimentalsettingsareoutlined\nin section 4.1. Additional information required for reproduction is provided in the ap-\npendixA.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n‚Ä¢ Ifthepaperincludesexperiments,aNoanswertothisquestionwillnotbeperceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhetherthecodeanddataareprovidedornot.\n‚Ä¢ Ifthecontributionisadatasetand/ormodel,theauthorsshoulddescribethestepstaken\ntomaketheirresultsreproducibleorverifiable.\n‚Ä¢ Dependingonthecontribution,reproducibilitycanbeaccomplishedinvariousways.\nForexample,ifthecontributionisanovelarchitecture,describingthearchitecturefully\nmightsuffice,orifthecontributionisaspecificmodelandempiricalevaluation,itmay\nbenecessarytoeithermakeitpossibleforotherstoreplicatethemodelwiththesame\ndataset,orprovideaccesstothemodel. Ingeneral. releasingcodeanddataisoften\nonegoodwaytoaccomplishthis,butreproducibilitycanalsobeprovidedviadetailed\ninstructionsforhowtoreplicatetheresults,accesstoahostedmodel(e.g.,inthecase\nofalargelanguagemodel),releasingofamodelcheckpoint,orothermeansthatare\nappropriatetotheresearchperformed.\n‚Ä¢ WhileNeurIPSdoesnotrequirereleasingcode,theconferencedoesrequireallsubmis-\nsionstoprovidesomereasonableavenueforreproducibility,whichmaydependonthe\nnatureofthecontribution. Forexample\n(a) Ifthecontributionisprimarilyanewalgorithm,thepapershouldmakeitclearhow\ntoreproducethatalgorithm.\n(b) Ifthecontributionisprimarilyanewmodelarchitecture,thepapershoulddescribe\nthearchitectureclearlyandfully.\n(c) Ifthecontributionisanewmodel(e.g.,alargelanguagemodel),thenthereshould\neitherbeawaytoaccessthismodelforreproducingtheresultsorawaytoreproduce\nthemodel(e.g.,withanopen-sourcedatasetorinstructionsforhowtoconstruct\nthedataset).\n20"
      },
      {
        "page": 21,
        "content": "(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthorsarewelcometodescribetheparticularwaytheyprovideforreproducibility.\nInthecaseofclosed-sourcemodels,itmaybethataccesstothemodelislimitedin\nsomeway(e.g.,toregisteredusers),butitshouldbepossibleforotherresearchers\ntohavesomepathtoreproducingorverifyingtheresults.\n5. Openaccesstodataandcode\nQuestion: Doesthepaperprovideopenaccesstothedataandcode,withsufficientinstruc-\ntionstofaithfullyreproducethemainexperimentalresults,asdescribedinsupplemental\nmaterial?\nAnswer: [No]\nJustification: The code related to this paper will be released as open-source after it is\naccepted.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatpaperdoesnotincludeexperimentsrequiringcode.\n‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy)formoredetails.\n‚Ä¢ Whileweencouragethereleaseofcodeanddata,weunderstandthatthismightnotbe\npossible,so‚ÄúNo‚Äùisanacceptableanswer. Paperscannotberejectedsimplyfornot\nincludingcode,unlessthisiscentraltothecontribution(e.g.,foranewopen-source\nbenchmark).\n‚Ä¢ Theinstructionsshouldcontaintheexactcommandandenvironmentneededtorunto\nreproducetheresults. SeetheNeurIPScodeanddatasubmissionguidelines(https:\n//nips.cc/public/guides/CodeSubmissionPolicy)formoredetails.\n‚Ä¢ Theauthorsshouldprovideinstructionsondataaccessandpreparation,includinghow\ntoaccesstherawdata,preprocesseddata,intermediatedata,andgenerateddata,etc.\n‚Ä¢ Theauthorsshouldprovidescriptstoreproduceallexperimentalresultsforthenew\nproposedmethodandbaselines. Ifonlyasubsetofexperimentsarereproducible,they\nshouldstatewhichonesareomittedfromthescriptandwhy.\n‚Ä¢ Atsubmissiontime, topreserveanonymity, theauthorsshouldreleaseanonymized\nversions(ifapplicable).\n‚Ä¢ Providingasmuchinformationaspossibleinsupplementalmaterial(appendedtothe\npaper)isrecommended,butincludingURLstodataandcodeispermitted.\n6. ExperimentalSetting/Details\nQuestion: Doesthepaperspecifyallthetrainingandtestdetails(e.g.,datasplits,hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: Allessentialexperimentalsettings,includingdataset,hyperparameters,selec-\ntioncriteriafortheseparametersarethoroughlydetailedinsection4. Additionalexperimen-\ntaldetailsareprovidedintheappendixA.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n‚Ä¢ Theexperimentalsettingshouldbepresentedinthecoreofthepapertoalevelofdetail\nthatisnecessarytoappreciatetheresultsandmakesenseofthem.\n‚Ä¢ Thefulldetailscanbeprovidedeitherwiththecode,inappendix,orassupplemental\nmaterial.\n7. ExperimentStatisticalSignificance\nQuestion:Doesthepaperreporterrorbarssuitablyandcorrectlydefinedorotherappropriate\ninformationaboutthestatisticalsignificanceoftheexperiments?\nAnswer: [Yes]\nJustification: Thepaperprovidesdetailedinformationonthestatisticalsignificanceofthe\nexperiments. ThesedetailsareavailableintheappendixappendixA.\n21"
      },
      {
        "page": 22,
        "content": "Guidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n‚Ä¢ Theauthorsshouldanswer\"Yes\"iftheresultsareaccompaniedbyerrorbars,confi-\ndenceintervals,orstatisticalsignificancetests,atleastfortheexperimentsthatsupport\nthemainclaimsofthepaper.\n‚Ä¢ Thefactorsofvariabilitythattheerrorbarsarecapturingshouldbeclearlystated(for\nexample,train/testsplit,initialization,randomdrawingofsomeparameter,oroverall\nrunwithgivenexperimentalconditions).\n‚Ä¢ Themethodforcalculatingtheerrorbarsshouldbeexplained(closedformformula,\ncalltoalibraryfunction,bootstrap,etc.)\n‚Ä¢ Theassumptionsmadeshouldbegiven(e.g.,Normallydistributederrors).\n‚Ä¢ Itshouldbeclearwhethertheerrorbaristhestandarddeviationorthestandarderror\nofthemean.\n‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferablyreporta2-sigmaerrorbarthanstatethattheyhavea96%CI,ifthehypothesis\nofNormalityoferrorsisnotverified.\n‚Ä¢ Forasymmetricdistributions,theauthorsshouldbecarefulnottoshowintablesor\nfiguressymmetricerrorbarsthatwouldyieldresultsthatareoutofrange(e.g. negative\nerrorrates).\n‚Ä¢ Iferrorbarsarereportedintablesorplots,Theauthorsshouldexplaininthetexthow\ntheywerecalculatedandreferencethecorrespondingfiguresortablesinthetext.\n8. ExperimentsComputeResources\nQuestion: Foreachexperiment,doesthepaperprovidesufficientinformationonthecom-\nputerresources(typeofcomputeworkers,memory,timeofexecution)neededtoreproduce\ntheexperiments?\nAnswer: [Yes]\nJustification: Detailedinformationaboutthecomputationalresourcesrequired,including\nthetypeofGPUandexecutiontimes,isprovidedintheappendixappendixA.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n‚Ä¢ ThepapershouldindicatethetypeofcomputeworkersCPUorGPU,internalcluster,\norcloudprovider,includingrelevantmemoryandstorage.\n‚Ä¢ Thepapershouldprovidetheamountofcomputerequiredforeachoftheindividual\nexperimentalrunsaswellasestimatethetotalcompute.\n‚Ä¢ Thepapershoulddisclosewhetherthefullresearchprojectrequiredmorecompute\nthantheexperimentsreportedinthepaper(e.g.,preliminaryorfailedexperimentsthat\ndidn‚Äôtmakeitintothepaper).\n9. CodeOfEthics\nQuestion: Doestheresearchconductedinthepaperconform, ineveryrespect, withthe\nNeurIPSCodeofEthicshttps://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: We have thoroughly reviewed and adhered to the NeurIPS Code of Ethics\nthroughoutourresearch.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthattheauthorshavenotreviewedtheNeurIPSCodeofEthics.\n‚Ä¢ IftheauthorsanswerNo,theyshouldexplainthespecialcircumstancesthatrequirea\ndeviationfromtheCodeofEthics.\n‚Ä¢ Theauthorsshouldmakesuretopreserveanonymity(e.g.,ifthereisaspecialconsid-\nerationduetolawsorregulationsintheirjurisdiction).\n10. BroaderImpacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietalimpactsoftheworkperformed?\n22"
      },
      {
        "page": 23,
        "content": "Answer: [NA]\nJustification: Our research concentrates on the long-tail recognition issue, a technical\nchallengewithinthefieldofcomputervisionthattypicallyhasminimalsocietalimpact. We\nutilizeopen-sourcedatasetsandensurethatnoharmfulinformationisgenerated.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthereisnosocietalimpactoftheworkperformed.\n‚Ä¢ IftheauthorsanswerNAorNo,theyshouldexplainwhytheirworkhasnosocietal\nimpactorwhythepaperdoesnotaddresssocietalimpact.\n‚Ä¢ Examplesofnegativesocietalimpactsincludepotentialmaliciousorunintendeduses\n(e.g.,disinformation,generatingfakeprofiles,surveillance),fairnessconsiderations\n(e.g.,deploymentoftechnologiesthatcouldmakedecisionsthatunfairlyimpactspecific\ngroups),privacyconsiderations,andsecurityconsiderations.\n‚Ä¢ Theconferenceexpectsthatmanypaperswillbefoundationalresearchandnottied\ntoparticularapplications,letalonedeployments. However,ifthereisadirectpathto\nanynegativeapplications,theauthorsshouldpointitout. Forexample,itislegitimate\ntopointoutthatanimprovementinthequalityofgenerativemodelscouldbeusedto\ngeneratedeepfakesfordisinformation. Ontheotherhand,itisnotneededtopointout\nthatagenericalgorithmforoptimizingneuralnetworkscouldenablepeopletotrain\nmodelsthatgenerateDeepfakesfaster.\n‚Ä¢ Theauthorsshouldconsiderpossibleharmsthatcouldarisewhenthetechnologyis\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnologyisbeingusedasintendedbutgivesincorrectresults,andharmsfollowing\nfrom(intentionalorunintentional)misuseofthetechnology.\n‚Ä¢ Iftherearenegativesocietalimpacts,theauthorscouldalsodiscusspossiblemitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanismsformonitoringmisuse,mechanismstomonitorhowasystemlearnsfrom\nfeedbackovertime,improvingtheefficiencyandaccessibilityofML).\n11. Safeguards\nQuestion: Doesthepaperdescribesafeguardsthathavebeenputinplaceforresponsible\nreleaseofdataormodelsthathaveahighriskformisuse(e.g.,pretrainedlanguagemodels,\nimagegenerators,orscrapeddatasets)?\nAnswer: [NA]\nJustification: Ourresearchdoesn‚Äôtinvolvemodelscapableofgeneratinghigh-riskcontent\nnordoesitcollectdatafromtheinternet,thuseliminatingtheneedforsuchsafeguards.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperposesnosuchrisks.\n‚Ä¢ Releasedmodelsthathaveahighriskformisuseordual-useshouldbereleasedwith\nnecessarysafeguardstoallowforcontrolleduseofthemodel,forexamplebyrequiring\nthatusersadheretousageguidelinesorrestrictionstoaccessthemodelorimplementing\nsafetyfilters.\n‚Ä¢ DatasetsthathavebeenscrapedfromtheInternetcouldposesafetyrisks. Theauthors\nshoulddescribehowtheyavoidedreleasingunsafeimages.\n‚Ä¢ Werecognizethatprovidingeffectivesafeguardsischallenging,andmanypapersdo\nnotrequirethis,butweencourageauthorstotakethisintoaccountandmakeabest\nfaitheffort.\n12. Licensesforexistingassets\nQuestion: Arethecreatorsororiginalownersofassets(e.g.,code,data,models),usedin\nthepaper,properlycreditedandarethelicenseandtermsofuseexplicitlymentionedand\nproperlyrespected?\nAnswer: [Yes]\nJustification: Allthedata,code,andmethodsemployedinthispaperareopen-source. We\nensurepropercitationoftheseresourcesandadherencetotheirlicensesandtermsofuse.\nGuidelines:\n23"
      },
      {
        "page": 24,
        "content": "‚Ä¢ TheanswerNAmeansthatthepaperdoesnotuseexistingassets.\n‚Ä¢ Theauthorsshouldcitetheoriginalpaperthatproducedthecodepackageordataset.\n‚Ä¢ Theauthorsshouldstatewhichversionoftheassetisusedand,ifpossible,includea\nURL.\n‚Ä¢ Thenameofthelicense(e.g.,CC-BY4.0)shouldbeincludedforeachasset.\n‚Ä¢ Forscrapeddatafromaparticularsource(e.g.,website),thecopyrightandtermsof\nserviceofthatsourceshouldbeprovided.\n‚Ä¢ If assets are released, the license, copyright information, and terms of use in the\npackageshouldbeprovided. Forpopulardatasets,paperswithcode.com/datasets\nhascuratedlicensesforsomedatasets. Theirlicensingguidecanhelpdeterminethe\nlicenseofadataset.\n‚Ä¢ Forexistingdatasetsthatarere-packaged,boththeoriginallicenseandthelicenseof\nthederivedasset(ifithaschanged)shouldbeprovided.\n‚Ä¢ Ifthisinformationisnotavailableonline,theauthorsareencouragedtoreachoutto\ntheasset‚Äôscreators.\n13. NewAssets\nQuestion:Arenewassetsintroducedinthepaperwelldocumentedandisthedocumentation\nprovidedalongsidetheassets?\nAnswer: [NA]\nJustification: Thispaperdoesnotintroduceanynewassets,hencethereisnoassociated\ndocumentation.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperdoesnotreleasenewassets.\n‚Ä¢ Researchersshouldcommunicatethedetailsofthedataset/code/modelaspartoftheir\nsubmissions via structured templates. This includes details about training, license,\nlimitations,etc.\n‚Ä¢ Thepapershoulddiscusswhetherandhowconsentwasobtainedfrompeoplewhose\nassetisused.\n‚Ä¢ Atsubmissiontime,remembertoanonymizeyourassets(ifapplicable). Youcaneither\ncreateananonymizedURLorincludeananonymizedzipfile.\n14. CrowdsourcingandResearchwithHumanSubjects\nQuestion: Forcrowdsourcingexperimentsandresearchwithhumansubjects,doesthepaper\nincludethefulltextofinstructionsgiventoparticipantsandscreenshots,ifapplicable,as\nwellasdetailsaboutcompensation(ifany)?\nAnswer: [NA]\nJustification: Thispaperdoesnotinvolveanycrowdsourcingexperimentsorresearchwith\nhumansubjects,thereforethisquestionisnotapplicable.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith\nhumansubjects.\n‚Ä¢ Includingthisinformationinthesupplementalmaterialisfine,butifthemaincontribu-\ntionofthepaperinvolveshumansubjects,thenasmuchdetailaspossibleshouldbe\nincludedinthemainpaper.\n‚Ä¢ AccordingtotheNeurIPSCodeofEthics,workersinvolvedindatacollection,curation,\norotherlaborshouldbepaidatleasttheminimumwageinthecountryofthedata\ncollector.\n15. InstitutionalReviewBoard(IRB)ApprovalsorEquivalentforResearchwithHuman\nSubjects\nQuestion: Doesthepaperdescribepotentialrisksincurredbystudyparticipants,whether\nsuchrisksweredisclosedtothesubjects,andwhetherInstitutionalReviewBoard(IRB)\napprovals(oranequivalentapproval/reviewbasedontherequirementsofyourcountryor\ninstitution)wereobtained?\n24"
      },
      {
        "page": 25,
        "content": "Answer: [NA]\nJustification: Thispaperdoesnotengageinresearchwithhumansubjectsorcrowdsourcing;\nthus,therearenostudyparticipants,potentialrisks,orrequirementsforIRBapproval.\nGuidelines:\n‚Ä¢ TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith\nhumansubjects.\n‚Ä¢ Dependingonthecountryinwhichresearchisconducted,IRBapproval(orequivalent)\nmayberequiredforanyhumansubjectsresearch. IfyouobtainedIRBapproval,you\nshouldclearlystatethisinthepaper.\n‚Ä¢ Werecognizethattheproceduresforthismayvarysignificantlybetweeninstitutions\nandlocations,andweexpectauthorstoadheretotheNeurIPSCodeofEthicsandthe\nguidelinesfortheirinstitution.\n‚Ä¢ Forinitialsubmissions,donotincludeanyinformationthatwouldbreakanonymity(if\napplicable),suchastheinstitutionconductingthereview.\n25"
      }
    ]
  },
  "pdf_url": "/uploads/7e2f35823f1ccc8999efe0aaf72765c9.pdf"
}