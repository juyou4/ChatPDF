{
  "filename": "Efficient Diffusion Training via Min-SNR Weighting Strategy.pdf",
  "upload_time": "2025-12-12T20:25:48.274484",
  "data": {
    "full_text": "Efficient Diffusion Training via Min-SNR Weighting Strategy\nTiankaiHang1,ShuyangGu2*,ChenLi3,JianminBao2,DongChen2,\nHanHu2,XinGeng1,BainingGuo1*\n1SoutheastUniversity,2MicrosoftResearchAsia,\n3NationalKeyLaboratoryofHuman-MachineHybridAugmentedIntelligence,\nNationalEngineeringResearchCenterforVisualInformationandApplications,\nandInstituteofArtificialIntelligenceandRobotics,Xi’anJiaotongUniversity\n{tkhang,xgeng,307000167}@seu.edu.cn,{shuyanggu,t-chenli1,jianmin.bao,doch,hanhu}@microsoft.com\nAbstract 30\nDenoising diffusion models have been a mainstream\napproach for image generation, however, training these 20\nmodels often suffers from slow convergence. In this pa-\nper, we discovered that the slow convergence is partly due\nto conflicting optimization directions between timesteps. 10\nTo address this issue, we treat the diffusion training as\na multi-task learning problem, and introduce a simple\nyet effective approach referred to as Min-SNR-γ. This 200 400 600 800 1000\nmethodadaptslossweightsoftimestepsbasedonclamped TrainingIterations(K)\nsignal-to-noise ratios, which effectively balances the con-\nflicts among timesteps. Our results demonstrate a signif-\nicant improvement in converging speed, 3.4× faster than\nprevious weighting strategies. It is also more effective,\nachieving a new record FID score of 2.06 on the Ima-\ngeNet 256 × 256 benchmark using smaller architectures\nthan that employed in previous state-of-the-art. The code\nis available at https://github.com/TiankaiHang/Min-SNR-\nDiffusion-Training.\n1.Introduction\nIn recent years, denoising diffusion models [48, 19, 57,\n36]haveemergedasapromisingnewclassofdeepgener-\nativemodelsduetotheirremarkableabilitytomodelcom-\nplicated distributions. Compared to prior Generative Ad-\nversarialNetworks(GANs),diffusionmodelshavedemon-\nstrated superior performance across a range of generation\ntasks in various modalities, including text-to-image gener-\nation [40, 43, 41, 17], image manipulation [26, 34, 4, 56],\nvideo synthesis [18, 47, 22], text generation [28, 16, 59],\n3D avatar synthesis [39, 53], etc. A key limitation of\npresent denoising diffusion models is their slow conver-\ngencerate,requiringsubstantialamountsofGPUhoursfor\n*Correspondingauthors.\nDIF\nThisICCVpaperistheOpenAccessversion,providedbytheComputerVisionFoundation.\nExceptforthiswatermark,itisidenticaltotheacceptedversion;\nthefinalpublishedversionoftheproceedingsisavailableonIEEEXplore.\nbaseline\nours\n3.4×speedup\nFigure 1: By leveraging a non-conflicting weighting strat-\negy,ourmethodcanconverge3.4timesfasterthanbaseline,\nresultinginsuperiorperformance.\ntraining[41,40]. Thisconstitutesaconsiderablechallenge\nforresearchersseekingtoeffectivelyexperimentwiththese\nmodels.\nIn this paper, we first conducted a thorough examina-\ntion of this issue, revealing that the slow convergence rate\nlikelyarisesfromconflictingoptimizationdirectionsfordif-\nferent timesteps during training. In fact, we find that by\ndedicatedlyoptimizingthedenoisingfunctionforaspecific\nnoise level can even harm the reconstruction performance\nfor other noise levels, as shown in Figure 2. This indi-\ncates that the optimal weight gradients for different noise\nlevels are in conflict with one another. Given that cur-\nrent denoising diffusion models [19, 12, 36, 41] employ\nsharedmodelweightsforvariousnoiselevels,theconflict-\ning weight gradients will impede the overall convergence\nrate,ifwithoutcarefulconsiderationonthebalanceofthese\nnoisetimesteps.\nTotacklethisproblem,weproposetheMin-SNR-γloss\nweighting strategy. This strategy treats the denoising pro-\ncess of each timestep as an individual task, thus diffusion\n7441\n\ntrainingcanbeconsideredasamulti-tasklearningproblem. tion of the gradient conflicting issue. As a result, the pro-\nTo balance various tasks, we assign loss weights for each posed weighting strategy not only converges much faster\ntask according to their difficulty. Specifically, we adopt a than previous approaches, but is also effective and general\nclampedsignal-to-noiseratio(SNR)aslossweighttoalle- for various generation scenarios. It achieves a new record\nviatetheconflictinggradientsissue. Byorganizingvarious of FID score 2.06 on the ImageNet 256×256 benchmark,\ntimesteps using this new weighting strategy, the diffusion and proves to also improve models using other prediction\ntrainingprocesscanconvergemuchfasterthanpreviousap- targetsandnetworkarchitectures.\nproaches,asillustratedinFigure1. Ourcontributionsaresummarizedasfollows:\nGenericmulti-tasklearningmethodsusuallyseektomit-\n• We have uncovered a compelling explanation for the\nigate conflicts between tasks by adjusting the loss weight\nslowconvergenceissueindiffusiontraining:aconflict\nof each task based on their gradients. One classical ap-\ningradientsacrossvarioustimesteps.\nproach[11,46],Paretooptimization,aimstoseekagradient\ndescent direction to improve all the tasks. However, these • We have proposed a new loss weighting strategy for\napproachesdifferfromourMin-SNR-γ weightingstrategy diffusion model training, which greatly mitigates the\ninthreeaspects: 1)Sparsity. Mostpreviousstudiesinthe conflicting gradients across timesteps and results in a\ngenericmulti-tasklearningfieldhavefocusedonscenarios markedaccelerationofconvergencespeed.\nwithasmallnumberoftasks,whichdiffersfromthediffu-\nsiontrainingwherethenumberoftaskscanbeuptothou- • We have established a new FID score record on the\nsands. As in our experiments, Pareto optimal solutions in\nImageNet256×256imagegenerationbenchmark.\ndiffusiontrainingtendtosetlossweightsofmosttimesteps\n2.RelatedWorks\nas 0. In this way, many timesteps will be left without any\nlearning, and thus harm the entire denoising process. 2)\nDenoising Diffusion Models. Diffusion models [19, 50,\nInstability. The gradients computed for each timestep in\n12]arestronggenerativemodels,particularlyinthefieldof\neachiterationareoftennoisy,owingtoalimitednumberof\nimagegeneration,duetotheirabilitytomodelcomplexdis-\nsamplesforeachtimestep. Thishamperstheaccuratecom-\ntributions. Thisadvantagehasledtosuperiorityoverprevi-\nputation of Pareto optimal solutions. 3) Inefficiency. The\nousGANmodelsintermsofbothhigh-fidelityanddiversity\ncalculation of Pareto optimal solutions is time-consuming,\nofgeneratedimages[12,24,35,40,41,43]. Besides,diffu-\nsignificantlyslowingdowntheoveralltraining.\nsionmodelsalsoshowgreatsuccessintext-to-videogener-\nOurproposedMin-SNR-γstrategyisapredefinedglobal\nation[18,47,52],3DAvatargeneration[39,53],imageto\nstep-wise loss weighting setting, instead of run-time adap- image translation [37], image manipulation [4, 26], music\ntivelossweightsforeachiterationasintheoriginalPareto generation [23], and even drug discovery [55]. The most\noptimization, thus avoiding the sparsity issue. Moreover, widely used network structure for diffusion models in the\nthe global loss weighting strategy eliminates the need for fieldofimagegenerationisUNet[19,12,35,36].Recently,\nnoisy computation of gradients and the time-consuming researchershavealsoexploredtheuseofVisionTransform-\nPareto optimization process, making it more efficient and ers[13]asanalternative,withU-ViT[2]borrowingtheskip\nstable. Thoughsuboptimal, theglobalstrategycanbealso connection design from UNet [42] and DiT [38] leverag-\nalmost as effective: Firstly, the optimization dynamics of ingAdaptiveLayerNormanddiscoveringthatthezeroini-\neach denoising task are largely shaped by the task’s noise tialization strategy is critical for achieving state-of-the-art\nlevel, without the need to account for individual samples class-conditionalImageNetgenerationresults.\ntoomuch. Secondly,afteramoderatenumberofiterations, ImprovedDiffusionModels. Recentstudieshavetriedto\nthe gradients of the majority subsequent training process improve the diffusion models from different perspectives.\nbecomemorestable, thusitcanbeapproximated byasta- Some works aim to improve the quality of generated im-\ntioneryweightingstrategy. agesbyguidingthesamplingprocess[12,21]. Otherstud-\nTovalidatetheeffectivenessoftheMin-SNR-γ weight- iesproposefastsamplingmethodsthatrequireonlyadozen\ningstrategy,wefirstcomputeitsParetoobjectivevalueand steps [49, 29, 32, 24] to generating high-quality images.\ncompareitwiththeoptimalstep-wiselossweightsobtained Someworkshavefurtherdistilledthediffusionmodelsfor\nby directly solving the Pareto problem. Together, we also even fewer steps in the sampling process [44, 33]. Mean-\ncompareitwithseveralconventionallossweightingstrate- while, some researchers [19, 24, 6] have noticed that the\ngies, including constant weighting, SNR weighting, and noise schedule is important for diffusion models. Other\nSNR with an lower bound. Figure 4 shows that our Min- works [36, 44] have found that different predicting targets\nSNR-γ weighting strategy produces Pareto objective val- from denoising networks affect the training stability and\nues almost as low as the optimal one, significantly better final performance. Finally, some works [14, 1] have pro-\nthan other existing works, indicating a significant allevia- posedusingtheMixtureofExperts(MoE)approachtohan-\n7442\n\ndlenoisefromdifferentlevels,whichcanboosttheperfor- 0.305\nmance of diffusion models, but require a larger number of\n0.165\nparametersandlongertrainingtime.\nMulti-task Learning. The goal of Multi-task learning 0.041\n(MTL) is to learn multiple related tasks jointly so that the\nknowledge contained in a task can be leveraged by other -0.001\ntasks. One of the main challenges in MTL is negative\n-0.002\ntransfer [9], means the joint training of tasks hurts learn-\ninginsteadofhelpingit. Fromanoptimizationperspective, -0.003\n0 100 200 300 400 500 600\nit manifests as the presence of conflicting task gradients.\ntimesteps\nTo address this issue, some previous works [58, 54, 8] try\nto modulate the gradient to prevent conflicts. Meanwhile,\notherworksattempttobalancedifferenttasksthroughcare-\nfullydesignthelossweights[7,25]. GradNorm[7]consid-\ners loss weight as learnable parameters and updates them\nthroughgradientdescent. AnotherapproachMTO[11,46]\nregardsthemulti-tasklearningproblemasamulti-objective\noptimizationproblemandobtainsthelossweightsbysolv-\ningaquadraticprogrammingproblem.\n3.Method\n3.1.Preliminary\nDiffusion models consist of two processes: a forward\nnoising process and a reverse denoising process. We de-\nnotethedistributionoftrainingdataasp(x 0). Theforward\nprocessisaGaussiantransition, graduallyaddsnoisewith\ndifferentscalestoarealdatapointx 0 ∼ p(x 0)toobtaina\nseriesofnoisylatentvariables{x 1,x 2,...,x T}:\nq(x t|x 0)=N(x t;α tx 0,σ t2I) (1)\nx t =α tx 0+σ t(cid:2) (2)\nwhere (cid:2) is the noise sampled from Gaussian distribution\nN(0,I). The noise schedule σ t denotes the magnitude of\nnoise added to the clean data at t timestep. It increases\nmonotonically with t. In this paper, we adopt the stan-\n(cid:2)dard variance-preserving diffusion process, where α t =\n1−σ2.\nt\nThe reverse process is parameterized by another Gaus-\nsian transition, gradually denoises the latent variables and\nrestorestherealdatax 0fromaGaussiannoise:\np θ(x t−1|x t)=N(x t−1;μˆ θ(x t),Σˆ θ(x t)). (3)\nμˆ θ andΣˆ θ arepredictedstatistics. Hoetal.[19]setΣˆ θ(x t)\nto the constant σ t2I, and μˆθ can be decomposed into the\nlinearcombinationofx t andanoiseapproximationmodel\n(cid:5)ˆθ. Theyfindusinganetworktopredictnoise(cid:5)workswell,\nespecially when combined with a simple re-weighted loss\nfunction:\n(cid:3) (cid:4)\nLt simple(θ)=E x0,(cid:3) (cid:3)(cid:5)−(cid:5)ˆθ(α tx 0+σ t(cid:5))(cid:3)2 2 . (4)\nssolESMΔ\n[100,200)\n[200,300)\n[300,400)\nbaseline\nFigure2:Wefinetunethediffusionmodelinspecificranges\nof timesteps:[100, 200), [200, 300), and [300, 400), then\nweinvestigatehowitaffectsthelossindifferenttimesteps.\nThesurroundingtimestepsmayderivebenefitfromit,while\nothersmayexperienceadverseeffects.\nMost previous works [36, 12, 35] follow this strategy and\npredict the noise. Later works [17, 44] use another re-\nparameterizationthatpredictsthenoiselessstatex 0:\n(cid:3) (cid:4)\nLt simple(θ)=E x0,(cid:3) (cid:3)x 0−xˆθ(α tx 0+σ t(cid:5))(cid:3)2 2 . (5)\nAndsomeotherworks[44,41]evenemploythenetworkto\ndirectly predict velocity v. Despite their prediction targets\nbeingdifferent,wecanderivethattheyaremathematically\nequivalentbymodifyingtheirlossweights.\n3.2.DiffusionTrainingasMulti-TaskLearning\nTo reduce the number of parameters, previous stud-\nies[19,36,12]oftensharetheparametersofthedenoising\nmodelsacrossallsteps. However,it’simportanttokeepin\nmindthatdifferentstepsmayhavevastlydifferentrequire-\nments. At each step of a diffusion model, the strength of\nthe denoising varies. For example, easier denoising tasks\n(when t → 0) may require simple reconstructions of the\ninput in order to achieve lower denoising loss. This strat-\negy, unfortunately, does not work as well for noisier tasks\n(when t → T). Thus, it’s extremely important to analyze\nthecorrelationbetweendifferenttimesteps.\nInthisregard,weconductasimpleexperiment. Webe-\nginbyclusteringthedenoisingprocessintoseveralseparate\nbins. Then we finetune the diffusion model by sampling\ntimestepsineachbin. Lastly, weevaluateitseffectiveness\nby looking at how it impacted the loss of other bins. As\nshown in Figure 2, we canobserve that finetuning specific\nstepsbenefitedthosesurroundingsteps. However,it’soften\ndetrimental for other steps that are far away. This inspires\nustoconsiderwhetherwecanfindamoreefficientsolution\nthatbenefitsalltimestepssimultaneously.\nWe re-organized our goal from the perspective of mul-\ntitask learning. The training process of denoising diffu-\nsion models contains T different tasks, each task repre-\n7443\n\nsentsanindividualtimestep. Wedenotethe modelparam-\netersasθ andthecorrespondingtraininglossisLt(θ),t ∈\n0.15\n{1,2,...,T}. Ourgoalistofindaupdatedirectionδ (cid:6)= 0,\nthatsatisfies:\n0.10\nLt(θ+δ)≤Lt(θ),∀t∈{1,2,...,T}. (6)\n0.05\nWeconsiderthefirst-orderTaylorexpansion:\n0.00\n(cid:5) (cid:6)\nLt(θ+δ)≈Lt(θ)+ δ,∇ θLt(θ) . (7) 0 1000 2000 3000\nNumber of samples\nThus,theidealupdatedirectionisequivalenttosatisfy:\n(cid:5) (cid:6)\nδ,∇ θLt(θ) ≤0,∀t∈{1,2,...,T}. (8)\n3.3.Paretooptimalityofdiffusionmodels\nTheorem1 Consideraupdatedirectionδ∗:\n(cid:7)T\nδ∗ =− w t∇ θLt(θ), (9)\nt=1\nofwhichw tisthesolutiontotheoptimizationproblem:\n(cid:8) (cid:9)\n(cid:7)T (cid:7)T\nmin (cid:3) w t∇ θLt(θ)(cid:3)2| w t =1,w t ≥0 (10)\nwt\nt=1 t=1\nIf the optimal solution to the Equation 8 exists, then δ∗\nshould satisfy it. Otherwise, it means that we must sacri-\nficeacertaintaskinexchangeforthelossdecreaseofother\ntasks. Inotherwords,wehavereachedtheParetoStation-\naryandthetraininghasconverged.\nAmoregeneralformofthistheoremwasfirstproposed\nin[11]andweleaveasuccinctproofinthesupplementary\nmaterial. Sincediffusionmodelsarerequiredtogothrough\nallthetimestepswhengeneratingimages. Soanytimestep\nshould not be ignored during training. Consequently, a\nregularization term is included to prevent the loss weights\nfrombecomingexcessivelysmall. Theoptimizationgoalin\nEquation10becomes:\n(cid:8) (cid:9)\n(cid:7)T (cid:7)T\nmin (cid:3) w t∇ θLt(θ)(cid:3)2 2+λ (cid:3)w t(cid:3)2 2 (11)\nwt\nt=1 t=1\nwhereλcontrolstheregularizationstrength.\nTo solve Equation 11, [46] leverages the Frank-\nWolfe [15] algorithm to obtain the weight {w t} through\niterative optimization. Another approach is to adopt Un-\nconstrained Gradient Descent(UGD). Specifically, we re-\nparameterizew tthroughβ t:\neβt (cid:7)\nw t = Z ,Z = eβt,β t ∈R. (12)\nt\nthgiew\nssol\nFigure3: Demonstrationoftheinstabilityofoptimization-\nbased weighting strategy. As the number of samples in-\ncreases, the loss weight becomes stable, while the compu-\ntationcostincreases.\nCombinedwithEquation11,wecanusegradientdescentto\noptimizeeachtermindependently:\n1 (cid:7)T λ (cid:7)T\nm βi tn Z2(cid:3) t=1eβt∇ θL t(θ)(cid:3)2 2+ Z2 t=1(cid:3)eβt(cid:3)2 2 (13)\nHowever, whether leveraging the Frank-Wolfe or the\nUGD algorithm, there are two disadvantages: 1) Ineffi-\nciency. Both of these two methods need additional opti-\nmization at each training iteration, it greatly increases the\ntrainingcost. 2)Instability. Inpractice,byusingalimited\nnumberofsamplestocalculatethegradientterm∇ θLt(θ),\ntheoptimizationresultsareunstable(asshowninFigure3).\nInotherwords,thelossweightsforeachdenoisingtaskvary\ngreatlyduringtraining,makingtheentirediffusiontraining\ninefficient.\n3.4.Min-SNR-γ LossWeightStrategy\nIn order to avoid the inefficiency and instability caused\nbytheiterativeoptimizationineachiteration,onepossible\nattemptistoadoptastationerylossweightstrategy.\nTosimplifythediscussion, weassumethatthenetwork\nisreparameteredtopredictthenoiselessstatex 0. However,\nit’sworthnotingthatdifferentpredictionobjectivescanbe\ntransformed into one another, we will delve into it in Sec-\ntion4.2. Now, weconsiderthefollowingalternativetrain-\ninglossweights:\n• Constant weighting. w t = 1. Which treats different\ntasks as equally weighted and has been used in both\ndiscretediffusionmodels[17,51]andcontinuousdif-\nfusionmodels[5].\n• SNR weighting. w t = SNR(t), where SNR(t) =\nα2/σ2. It’s the most widely used weighting strat-\nt t\negy [33, 22, 12, 41]. By combining with Equation 2,\nwecanfindit’snumericallyequivalenttotheconstant weightingstrategywhenthepredictingtargetisnoise.\n7444\n\n0.4\n0.3\n0.2\n0.1\n0.0\n60 80 100 120 140\nTrainingIterations(K)\neulaVevitcejbO\nhuman faces, is a widely-used unconditional image gener-\nConst\nation benchmark. We follow ScoreSDE [57] for data pre- SNR\nMin-SNR-5 processing,whichinvolvescentercroppingeachimagetoa\nUGD resolutionof140×140andthenresizingitto64×64. For\nclassconditionalgeneration,weadoptImageNet[10]with\na total of 1.3 million images from 1000 different classes.\nWetesttheperformanceonboth642and2562resolutions.\nTrainingDetails.Forlowresolution(64×64)imagegener-\nation,wefollowADM[12]anddirectlytrainthediffusion\nmodel on the pixel-level. For high-resolution image gen-\nFigure4:ComparisonoftheobjectivevaluesinEquation11 eration, we utilize LDM [41] approach by first compress-\nondifferentweightingstrategies. ing the images into latent space, then training a diffusion\nmodel to model the latent distributions. To obtain the la-\n• Max-SNR-γ weighting. w t = max{SNR(t),γ}. This tent for images, we employ VAE from Stable Diffusion1,\nmodification of SNR weighting is first proposed in\nwhichencodesahigh-resolutionimage(256×256×3)into\n[44] to avoid a weight of zero with zero SNR steps.\n32×32×4latentcodes.\nTheyset γ = 1astheirdefaultsetting. However, the In our experiments, we employ both ViT and UNet as\nweightsstillconcentrateonsmallnoiselevels. our diffusion model backbones. We adopt a vanilla ViT\nstructurewithoutanymodifications[13]asourdefaultset-\n• Min-SNR-γ weighting. w t = min{SNR(t),γ}. We ting. we incorporate the timestep t and class condition c\nproposethisweightingstrategytoavoidthemodelfo- as learnable input tokens to the model. Although further\ncusingtoomuchonsmallnoiselevels. customization of the network structure may improve per-\nformance, our focus in this paper is to analyze the general\n• UGD optimization weighting. w t is optimized from propertiesofdiffusionmodels. FortheUNetstructure, we\nEquation13ineachtimestep. Comparedwiththepre-\nfollowADM[12]andkeeptheFLOPssimilartotheViT-B\nvioussetting,thisstrategychangesduringtraining. model, whichhas1.5×parameters. Additionaldetailscan\nbefoundinthesupplementarymaterial.\nFirst, wecombinetheseweightingstrategiesintoEqua-\nFor the diffusion settings, we use a cosine noise sched-\ntion11tovalidatewhethertheyareapproachtothePareto\nulerfollowingtheapproachin[36,12].Thetotalnumberof\noptimalitystate. AsshowninFigure4,theUGDoptimiza- timesteps is standardized to T = 1000 across all datasets.\ntionweightingstrategycanachievethelowestscoreonour\nWeadoptAdamW[27,31]asouroptimizer.FortheCelebA\noptimizationtarget. Inaddition,theMin-SNR-γ weighting\ndataset,wetrainourmodelfor500Kiterationswithabatch\nstrategyistheclosesttotheoptimum,demonstratingithas\nsizeof128. Duringthefirst5,000iterations,weimplement\nthepropertytooptimizedifferenttimestepssimultaneously. alinearwarm-upandkeepthelearningrateat1×10−4for\nInthefollowingsection,wepresentexperimentalresults\nthe remaining training. For the ImageNet dataset, the de-\ntodemonstratetheeffectivenessofourMin-SNR-γweight-\nfaultlearningrateisfixedat1×10−4. Thebatchsizeisset\ningstrategyinbalancingdiversenoiselevels. Ourapproach to1024for642resolutionand256for2562resolution.\naimstoachievefasterconvergenceandstrongperformance.\nEvaluation Settings. We utilize an Exponential Moving\nAverage (EMA) model with a rate of 0.9999 for evalua-\n4.Experiments\ntion. Duringtheevaluationphase,wegenerateimageswith\nthe Heun sampler from EDM [24]. For conditional image\nIn this section, we first provide an overview of the ex-\ngeneration, we also implement the classifier-free sampling\nperimentalsetup.Subsequently,weconductcomprehensive\nstrategy[21]toachievebetterresults. Finally,wemeasure\nablation studies to show that our method is versatile and\nthequalityofthegeneratedimagesusingtheFIDscorecal-\nsuitableforvariouspredictiontargetsandnetworkarchitec-\nculatedon50Kimages.\ntures. Finally, wecompareourapproachwiththestate-of-\nthe-artmethodsacrossmultiplebenchmarks,demonstrating 4.2.AnalysisoftheProposedMin-SNR-γ\nnotonlyitsacceleratedconvergencebutalsoitssuperiorca-\npabilityingeneratinghigh-qualityimages. Comparison of Different Weighting Strategies. To\ndemonstrate the significance of the loss weighting strat-\n4.1.Setup egy,weconductexperimentswithdifferentlossweightset-\ntings for predicting x 0. These settings include: 1) con-\nDatasets. We perform experiments on both uncondi-\nstant weighting, where w t = 1, 2) SNR weighting, with\ntional CelebA dataset [30] and the conditional ImageNet\ndataset[10].TheCelebAdataset,whichcomprises162,770 1https://huggingface.co/stabilityai/sd-vae-ft-mse-original\n7445\n\n40\n30\n20\n10\n200 400 600 800 1000\nTrainingIterations(K)\nDIF\n40\nx0+Const\nx0+SNR\nx0+Max-SNR-1 30\nx0+Min-SNR-5\n20\n3.4×speedup\n10\n200 400 600 800 1000\nTrainingIterations(K)\nDIF\n40\n(cid:2)+SNR\n(cid:2)+Min-SNR-5\n30\n20\n10\n200 400 600 800 1000\nTrainingIterations(K)\nDIF\nv+Const\nv+SNR\nv+Max-SNR-1\nv+Min-SNR-5\nFigure5: Comparingdifferentlossweightingdesignsonpredictingx 0,(cid:5),v. Takingtheneuralnetworkoutputasnoisewith\nconstorMax-SNR-γ strategyleadtodivergence. Min-SNR-γ strategyconvergesthefastestunderallthesesettings.\n×10−3 ×10−2 ×10−1 ×10−1\n9.0 Const 8.5 Const 3.0 Const 4.45 Const\nSNR SNR SNR SNR\n8.5 Max-SNR-1 8.0 Max-SNR-1 Max-SNR-1\n4.40\nMax-SNR-1\nMin-SNR-5 Min-SNR-5\n2.9\nMin-SNR-5 4.35 Min-SNR-5\n8.0\n7.5 4.30\n2.8\n7.5 4.25\n250 500 750 1000 250 500 750 1000 250 500 750 1000 250 500 750 1000\nTrainingIterations(K) TrainingIterations(K) TrainingIterations(K) TrainingIterations(K)\nFigure 6: Unweighted loss in different ranges of timesteps. From left to right, each figure represents a specific range of\ntimesteps: [0,100),[200,300),[600,700),[800,900). They-axisrepresentstheMeanSquaredError(MSE),averagedover\neachrangeoftimesteps.\nw t = SNR(t), 3) truncated SNR weighting, with w t = ple images from training iteration 50K, 200K, 400K, and\nmax{SNR(t),γ}(following[44]withasetvalueofγ =1), 1Mwithdifferentlossweights. OurresultsshowthatMin-\nand 4) our proposed Min-SNR-γ weighting strategy, with SNR-γ generates a clear object with only 200K iterations,\nw t =min{SNR(t),γ},wesetγ =5asthedefaultvalue. whichissignificantlybetterinqualitythanothermethods.\nThe ViT-B serves as our default backbone and experi- Min-SNR-γ for Different Prediction Targets. Instead of\nmentsareperformedonImageNet256×256.Asillustrated predictingtheoriginalsignalx 0fromthenetwork,somere-\ninFigure5,weobservethatallresultsimproveasthenum- centworkshaveemployedalternativere-parameterizations,\nber of training iterations increases. However, our method such as predicting noise (cid:5), or velocity v [44]. To verify\ndemonstrates a significantly faster convergence compared the applicability of our weighting strategy to these predic-\ntoothermethods.Specifically,itexhibitsa3.4×speedupin tion targets, we conduct experiments comparing the four\nreachinganFIDscoreof10. Itisworthmentioningthatthe aforementioned weighting strategies across these different\nSNRweightingstrategyperformedtheworst, whichcould re-parameterizations.\nbeduetoitsdisproportionatefocusonlessnoisystages. As we discussed in Section 3.4, predicting noise (cid:5) is\nFor a deeper understanding of the reasons behind the mathematicallyequivalenttopredictingx 0 byintrinsically\nvaryingconvergencerates,weanalyzedtheirtraininglossat involvingSignal-to-NoiseRatioasaweightfactor,thuswe\ndifferentnoiselevels.Forafaircomparison,weexcludethe divide the SNR term in practice. For example, the Min-\nlossweighttermbyonlycalculating(cid:3)x 0−xˆ θ(cid:3)2 2. Consider- SNR-γ strategy in predicting noise can be expressed as\ningthatthelossofdifferentnoiselevelsvariesgreatly, we w t = min{ SS NN RR (( t)t),γ} = min{ SNRγ (t),1}. AndtheSNRstrat-\ncalculatethelossindifferentbinsandpresenttheresultsin egy in predicting noise is equivalent to a “constant strat-\nFigure6. Theresultsshowthatwhiletheconstantweight- egy”. Forsimplicityandconsistency,westillrefertothem\ning strategy is effective for high noise intensities, it per- asMin-SNR-γandSNRstrategies.Similarly,wecanderive\nformspoorlyatlownoiseintensities. Conversely,theSNR thatwhenpredictingvelocityv,thelossweightfactormust\nweighting strategy exhibits the opposite behavior. In con- bedividedby(SNR+1). Thesestrategiesarestillreferred\ntrast, our proposed Min-SNR-γ strategy achieves a lower tobytheiroriginalnamesforeaseofreference.\ntraining loss across almost all cases, and indicates quicker We conduct experiments on these two variants in Fig-\nconvergencethroughtheFIDmetric. ure5. NoiseoutputwithconstorMax-SNR-γ settingleads\nFurthermore, we present visual results in Figure 7 to todivergence. Meanwhile,ourproposedMin-SNR-γ strat-\ndemonstratethefastconvergenceofMin-SNR-γ. Wesam- egyconvergesfasterthanotherlossweightingstrategiesfor\n7446\n\nLonger Training\nRNS\ntsnoC\n5-RNS-niM\nFigure 7: Qualitative comparison of the generation results from different weighting strategies on ImageNet-256 dataset.\nImagesineachcolumnaresampledfrom50K,200K,400K,and1Miterations. OurMin-SNR-5strategyyieldssignificant\nimprovementsinvisualfidelityfromthesameiteration.\nbothpredictionnoiseandpredictingvelocity.Thesedemon- TrainingIterations 200K 400K 600K 800K 1M\nstratethatbalancingthelossweightsfordifferenttimesteps\nisintrinsic,independentofanyre-parameterization.\nBaseline(x 0) 25.93 15.41 11.54 9.52 8.33\nMin-SNR-γ on Different Network Architectures. The +Min-SNR-5 7.99 5.34 4.69 4.41 4.28\nMin-SNR-γ strategy is versatile and robust for different Baseline((cid:5)) 8.55 5.43 4.64 4.35 4.21\nprediction targets and network structures. We conduct ex- +Min-SNR-5 7.32 4.98 4.48 4.24 4.14\nperiments on the widely used UNet and keep the number\nTable1: AblationstudiesontheUNetbackbone. Whether\nof parameters close to the ViT-B model. For each experi-\nment, models are trained for 1 million iterations and their\nthenetworkpredictsx 0or(cid:5),theMin-SNR-5weightingde-\nsignconvergesfasterandachievesbetterFIDscore.\nFIDscoresarecalculatedatmultipleiterations. Theresults\ninTable1indicatethattheMin-SNR-γ strategyconverges\nandmodifyingthenetworkstructuretoUNet. Wefindthat\nsignificantlyfasterthanthebaselineandprovidesbetterper-\nthe results were also consistently stable. Our results indi-\nformanceforbothpredictingx 0andpredicting(cid:5).\ncatethatgoodperformancecanusuallybeachievedwhenγ\nRobustnessAnalysis. Weutilizeasinglehyper-parameter,\nissetto5,makingittheestablisheddefaultsetting.\nγ, as the truncate value. To assess its robustness, we con-\n4.3.Comparisonwithstate-of-the-artMethods\nductthoroughanalysisinvarioussettings. Ourexperiments\nwere performed on ImageNet-256 using the ViT-B model CelebA-64.WeconductexperimentsonCelebA64×64for\nandpredictingx 0. Wevarythetruncatevalueγ bysetting unconditional generation. Both UNet and ViT are trained\nitto1,5,10,and20. TheresultsareshowninTable2. We for 500K iterations. During the evaluation, we use Heun\nfindthereareonlyminorvariationsintheFIDscorewhen sampler[24]togenerate50Ksamples. TheFIDresultsare\nγ issmallerthan20. Additionally,weconductedmoreex- summarizedinTable3. OurViT-Small[13]modeloutper-\nperimentsbymodifyingthepredictingtargettothenoise(cid:5), formspreviousViT-basedmodelswithanFIDscoreof2.14.\n7447\n\nγ 1 5 10 20 Method #Params FID\nViT(x 0) 4.98 4.92 5.34 5.45 BigGAN-deep[3] 340M 6.95\nViT((cid:5)) 4.89 4.84 4.94 5.41 StyleGAN-XL[45] 2.30\nUNet(x 0) 4.49 4.28 4.32 4.37\nImprovedVQ-Diffusion[17] 460M 4.83\nUNet((cid:5)) 4.30 4.14 4.14 4.12\nIDDPM[36] 270M 12.26\nTable2: Ablationstudyonγ. Theresultsarerobusttothe CDM[20] 4.88\nhyper-parameterγ indifferentsettings. ADM-U,ADM-G[12] 608M 3.94\nLDM[41] 400M 3.60\nMethod #Params FID UNet(ours) 395M 2.81†\nDDIM[49] 79M 3.26\nU-ViT-L[2] 287M 3.40\nSoftTruncation[49] 62M 1.90\nDiT-XL-2[38] 675M 2.27\nOurUNet 59M 1.60\nViT-XL(ours) 451M 2.06\nU-ViT-Small[2] 44M 2.87\nTable5:FIDresultsonImageNet256×256.†denotesonly\nViT-Small(ours) 43M 2.14\ntrain1.4Miterations. OurmodelwithaViT-XLbackbone\nTable3: FIDresultsonunconditionalCelebA64×64[30] achievesanewrecordFIDscoreof2.06.\nbenchmark. WeexperimentwithbothUNetandViT.\nImageNet-256. Wealsoapplydiffusionmodelsforhigher-\nMethod #Params FID resolution image generation on the ImageNet 256 × 256\nBigGAN-deep[3] 4.06 benchmark. To enhance training efficiency, we first com-\nStyleGAN-XL[45] 1.51\npress256×256×3imagesinto32×32×4latentcodes\nusing the encoder from LDM [41]. During the sampling\nIDDPM(small)[36] 100M 6.92\nprocess,weemploytheHeunsamplerandtheclassifier-free\nIDDPM(large)[36] 270M 2.92\nguidanceCFG=1.5. TheFIDcomparisonispresentedin\nCDM[20] 1.48\nTable 5. Under the setting of predicting (cid:5) with Min-SNR-\nADM[12] 296M 2.61\n5, our ViT-XL model achieves the FID of 2.08 for only\nEDM[24] 296M 1.36\n2.1Miterations,whichis3.3×fasterthanDiTandoutper-\nU-ViT-Mid[2] 131M 5.85 formsthepreviousstate-of-the-artFIDof2.27. Moreover,\nU-ViT-Large[2] 287M 4.26 withlongertraining(about7Miterationsasin[38]),weare\nViT-L(ours) 269M 2.28 abletoachieveFID2.06bypredictingx 0withMin-SNR-5.\nOurUNet-basedmodelwith395Mparametersistrainedfor\nTable 4: FID results on ImageNet 64 × 64. We conduct\nabout1.4MiterationsandachievesFIDscoreof2.81.\nexperiments using the ViT-L backbone which significantly\nimprovesuponpreviousmethods.\n5.Conclusion\nInthispaper,wepointoutthattheconflictingoptimiza-\nIt is worth mentioning that no modifications are made to tiondirectionsbetweendifferenttimestepsmaycauseslow\nthe naive network structure, demonstrating that the results convergenceindiffusiontraining. Toaddressit,weregard\ncould still be improved further. Meanwhile, our method thediffusiontrainingprocessasamulti-tasklearningprob-\nusing the UNet [12] structure achieves an even better FID lemandintroduceanovelweightingstrategy, namedMin-\nscoreof1.60,outperformingpreviousUNetmethods. SNR-γ, to effectively balance different timesteps. Experi-\nImageNet-64. We also validate our method on class- mentsdemonstrateourmethodcanboostdiffusiontraining\nconditional ImageNet 64 × 64 benchmark. During train- several times faster, and achieves the state-of-the-art FID\ning,theclasslabelisdroppedwiththeprobability0.15for\nscoreonImageNet-256dataset.\nclassifier-freeinference[21].Themodelistrainedfor800K\niterations and images are synthesized using classifier-free Acknowledgments\nguidance scale of 1.5. For a fair comparison, we adopt a\n21-layer ViT-Large model without additional architecture We sincerely thank Yixuan Wei, Zheng Zhang, and\ndesigns, which has a similar number of parameters to U- StephenLinforhelpfuldiscussion.Thisresearchwaspartly\nViT-Large [2]. The results presented in Table 4 show that supported by the National Key Research & Development\nourmethodachievesanFIDscoreof2.28,significantlyim- Plan of China (No. 2018AAA0100104), the National Sci-\nprovingupontheU-ViT-Largemodel. enceFoundationofChina(62125602,62076063).\n7448\n\nReferences Yin,ShikunFeng,etal. Ernie-vilg2.0: Improvingtext-to-\nimage diffusion model with knowledge-enhanced mixture-\n[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nof-denoising-experts. arXiv preprint arXiv:2210.15257,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\n2022. 2\nSamuliLaine,BryanCatanzaro,TeroKarras,andMing-Yu\n[15] Marguerite Frank and Philip Wolfe. An algorithm for\nLiu. ediff-i: Text-to-imagediffusionmodelswithensemble\nquadraticprogramming. Navalresearchlogisticsquarterly,\nofexpertdenoisers.arXivpreprintarXiv:2211.01324,2022.\n3(1-2):95–110,1956. 4\n2\n[16] ShansanGong,MukaiLi,JiangtaoFeng,ZhiyongWu,and\n[2] FanBao,ShenNie,KaiwenXue,YueCao,ChongxuanLi,\nLingpengKong. Sequencetosequencetextgenerationwith\nHang Su, and Jun Zhu. All are worth words: A vit back-\ndiffusionmodels. InInternationalConferenceonLearning\nbonefordiffusionmodels.arXivpreprintarXiv:2209.12152,\nRepresentations,2023. 1\n2022. 2,8\n[17] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\n[3] AndrewBrock,JeffDonahue,andKarenSimonyan. Large\nZhang, DongdongChen, LuYuan, andBainingGuo. Vec-\nscale gan training for high fidelity natural image synthe-\ntorquantizeddiffusionmodelfortext-to-imagesynthesis.In\nsis. InternationalConferenceonLearningRepresentations,\nProceedingsoftheIEEE/CVFConferenceonComputerVi-\n2019. 8\nsionandPatternRecognition,pages10696–10706,2022. 1,\n[4] TimBrooks,AleksanderHolynski,andAlexeiAEfros. In-\n3,4,8\nstructpix2pix:Learningtofollowimageeditinginstructions.\n[18] Jonathan Ho, WilliamChan, ChitwanSaharia, JayWhang,\nIn Proceedings of the IEEE/CVF Conference on Computer\nRuiqiGao, AlexeyA.Gritsenko, DiederikP.Kingma, Ben\nVisionandPatternRecognition,pages18392–18402,2023.\nPoole, Mohammad Norouzi, David J. Fleet, and Tim Sali-\n1,2\nmans. Imagenvideo: Highdefinitionvideogenerationwith\n[5] Hanqun Cao, Cheng Tan, Zhangyang Gao, Guangyong\ndiffusionmodels. ArXiv,abs/2210.02303,2022. 1,2\nChen, Pheng-Ann Heng, and Stan Z Li. A survey on gen-\n[19] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu-\nerative diffusion model. arXiv preprint arXiv:2209.02646,\nsionprobabilisticmodels. AdvancesinNeuralInformation\n2022. 4\nProcessingSystems,33:6840–6851,2020. 1,2,3\n[6] TingChen. Ontheimportanceofnoiseschedulingfordiffu-\nsionmodels. arXivpreprintarXiv:2301.10972,2023. 2 [20] JonathanHo,ChitwanSaharia,WilliamChan,DavidJFleet,\nMohammadNorouzi,andTimSalimans.Cascadeddiffusion\n[7] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and An-\nmodelsforhighfidelityimagegeneration. J.Mach.Learn.\ndrew Rabinovich. Gradnorm: Gradient normalization for\nRes.,23:47–1,2022. 8\nadaptive loss balancing in deep multitask networks. In In-\nternationalconferenceonmachinelearning,pages794–803. [21] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nPMLR,2018. 3 guidance. InNeurIPS2021WorkshoponDeepGenerative\n[8] Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, ModelsandDownstreamApplications,2021. 2,5,8\nHenrikKretzschmar,YuningChai,andDragomirAnguelov. [22] JonathanHo, TimSalimans, AlexeyA.Gritsenko, William\nJustpickasign:Optimizingdeepmultitaskmodelswithgra- Chan,MohammadNorouzi,andDavidJ.Fleet. Videodif-\ndientsigndropout.AdvancesinNeuralInformationProcess- fusion models. In Alice H. Oh, Alekh Agarwal, Danielle\ningSystems,33:2039–2050,2020. 3 Belgrave,andKyunghyunCho,editors,AdvancesinNeural\n[9] Michael Crawshaw. Multi-task learning with deep neural InformationProcessingSystems,2022. 1,4\nnetworks:Asurvey.arXivpreprintarXiv:2009.09796,2020. [23] Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk,\n3 AndyLy,NanxinChen,ZhengdongZhang,ZhishuaiZhang,\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Jiahui Yu, Christian Frank, et al. Noise2music: Text-\nandLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage conditionedmusicgenerationwithdiffusionmodels. arXiv\ndatabase. In2009IEEEconferenceoncomputervisionand preprintarXiv:2302.03917,2023. 2\npatternrecognition,pages248–255.Ieee,2009. 5 [24] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\n[11] Jean-AntoineDe´side´ri. Multiple-gradientdescentalgorithm Elucidating the design space of diffusion-based generative\n(mgda) for multiobjective optimization. Comptes Rendus models. InAliceH.Oh,AlekhAgarwal,DanielleBelgrave,\nMathematique,350(5-6):313–318,2012. 2,3,4 andKyunghyunCho,editors,AdvancesinNeuralInforma-\n[12] PrafullaDhariwalandAlexanderNichol. Diffusionmodels tionProcessingSystems,2022. 2,5,7,8\nbeatgansonimagesynthesis. AdvancesinNeuralInforma- [25] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task\ntionProcessingSystems,34:8780–8794,2021. 1,2,3,4,5, learningusinguncertaintytoweighlossesforscenegeome-\n8 tryandsemantics.InProceedingsoftheIEEEconferenceon\n[13] A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X. computervisionandpatternrecognition,pages7482–7491,\nZhai,T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold, 2018. 3\nandS.Gelly.Animageisworth16x16words:Transformers [26] GwanghyunKim, TaesungKwon, andJongChulYe. Dif-\nforimagerecognitionatscale. InInternationalConference fusionclip: Text-guided diffusion models for robust image\nonLearningRepresentations,2021. 2,5,7 manipulation. InProceedingsoftheIEEE/CVFConference\n[14] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, on Computer Vision and Pattern Recognition, pages 2426–\nLanxinLi,XuyiChen,YuxiangLu,JiaxiangLiu,Weichong 2435,2022. 1,2\n7449\n\n[27] D. P. Kingma and J. Ba. Adam: A method for stochastic theIEEE/CVFConferenceonComputerVisionandPattern\noptimization. InInternationalConferenceonLearningRep- Recognition,pages10684–10695,2022. 1,2,3,4,5,8\nresentations,2014. 5 [42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\n[28] Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy net: Convolutionalnetworksforbiomedicalimagesegmen-\nLiang, and Tatsunori B Hashimoto. Diffusion-lm im- tation.InMedicalImageComputingandComputer-Assisted\nproves controllable text generation. arXiv preprint Intervention–MICCAI2015:18thInternationalConference,\narXiv:2205.14217,2022. 1 Munich,Germany,October5-9,2015,Proceedings,PartIII\n[29] LupingLiu,YiRen,ZhijieLin,andZhouZhao. Pseudonu- 18,pages234–241.Springer,2015. 2\nmericalmethodsfordiffusionmodelsonmanifolds. ArXiv, [43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nabs/2202.09778,2022. 2 Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed\n[30] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Ghasemipour,RaphaelGontijo-Lopes,BurcuKaragolAyan,\nDeeplearningfaceattributesinthewild. InProceedingsof TimSalimans,JonathanHo,DavidJ.Fleet,andMohammad\nInternational Conference on Computer Vision (ICCV), De- Norouzi. Photorealistictext-to-imagediffusionmodelswith\ncember2015. 5,8 deeplanguageunderstanding. InAliceH.Oh,AlekhAgar-\nwal, Danielle Belgrave, and Kyunghyun Cho, editors, Ad-\n[31] I.LoshchilovandF.Hutter. Decoupledweightdecayregu-\nvancesinNeuralInformationProcessingSystems,2022. 1,\nlarization. InInternationalConferenceonLearningRepre-\n2\nsentations,2018. 5\n[44] TimSalimansandJonathanHo. Progressivedistillationfor\n[32] ChengLu,YuhaoZhou,FanBao,JianfeiChen,Chongxuan\nfastsamplingofdiffusionmodels. InInternationalConfer-\nLi, andJun Zhu. Dpm-solver: A fastode solverfor diffu-\nenceonLearningRepresentations,2022. 2,3,5,6\nsionprobabilisticmodelsamplinginaround10steps.ArXiv,\n[45] AxelSauer,KatjaSchwarz,andAndreasGeiger. Stylegan-\nabs/2206.00927,2022. 2\nxl: Scalingstylegantolargediversedatasets. InACMSIG-\n[33] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik\nGRAPH2022conferenceproceedings,pages1–10,2022. 8\nKingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\n[46] Ozan Sener and Vladlen Koltun. Multi-task learning as\nOndistillationofguideddiffusionmodels. InProceedings\nmulti-objective optimization. Advances in neural informa-\noftheIEEE/CVFConferenceonComputerVisionandPat-\ntionprocessingsystems,31,2018. 2,3,4\nternRecognition,pages14297–14306,2023. 2,4\n[47] UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn,\n[34] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nYanZhu,andStefanoErmon. Sdedit: Imagesynthesisand\nOranGafni,DeviParikh,SonalGupta,andYanivTaigman.\neditingwithstochasticdifferentialequations. arXivpreprint\nMake-a-video: Text-to-video generation without text-video\narXiv:2108.01073,2021. 1\ndata. InTheEleventhInternationalConferenceonLearning\n[35] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nRepresentations,2023. 1,2\nShyam,PamelaMishkin,BobMcGrew,IlyaSutskever,and\n[48] JaschaSohl-Dickstein, EricWeiss, NiruMaheswaranathan,\nMarkChen. Glide:Towardsphotorealisticimagegeneration\nand Surya Ganguli. Deep unsupervised learning using\nand editing with text-guided diffusion models. In Interna-\nnonequilibrium thermodynamics. In International Confer-\ntionalConferenceonMachineLearning,2021. 2,3\nenceonMachineLearning,pages2256–2265.PMLR,2015.\n[36] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\n1\ndenoising diffusion probabilistic models. In International\n[49] JiamingSong,ChenlinMeng,andStefanoErmon. Denois-\nConferenceonMachineLearning,pages8162–8171.PMLR,\ning diffusion implicit models. In International Conference\n2021. 1,2,3,5,8\nonLearningRepresentations,2021. 2,8\n[37] GauravParmar,KrishnaKumarSingh,RichardZhang,Yijun\n[50] YangSongandStefanoErmon.Generativemodelingbyesti-\nLi,JingwanLu,andJun-YanZhu.Zero-shotimage-to-image\nmatinggradientsofthedatadistribution.Advancesinneural\ntranslation. InACMSIGGRAPH2023ConferenceProceed-\ninformationprocessingsystems,32,2019. 2\nings,pages1–11,2023. 2\n[51] ZhicongTang,ShuyangGu,JianminBao,DongChen,and\n[38] WilliamPeeblesandSainingXie. Scalablediffusionmodels Fang Wen. Improved vector quantized diffusion models.\nwithtransformers. arXivpreprintarXiv:2212.09748,2022. arXivpreprintarXiv:2205.16007,2022. 4\n2,8 [52] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\n[39] BenPoole,AjayJain,JonathanTBarron,andBenMilden- dermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nEleventhInternationalConferenceonLearningRepresenta- Phenaki:Variablelengthvideogenerationfromopendomain\ntions,2022. 1,2 textualdescriptions. InInternationalConferenceonLearn-\n[40] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu, ingRepresentations,2023. 2\nand Mark Chen. Hierarchical text-conditional image gen- [53] TengfeiWang,BoZhang,TingZhang,ShuyangGu,Jianmin\nerationwithcliplatents. arXivpreprintarXiv:2204.06125, Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang\n2022. 1,2 Wen, Qifeng Chen, et al. Rodin: A generative model for\n[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, sculpting3ddigitalavatarsusingdiffusion. InProceedings\nPatrick Esser, and Bjo¨rn Ommer. High-resolution image oftheIEEE/CVFConferenceonComputerVisionandPat-\nsynthesis with latent diffusion models. In Proceedings of ternRecognition,pages4563–4573,2023. 1,2\n7450\n\n[54] Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao.\nGradient vaccine: Investigating and improving multi-task\noptimization in massively multilingual models. arXiv\npreprintarXiv:2010.05874,2020. 3\n[55] MinkaiXu,LantaoYu,YangSong,ChenceShi,StefanoEr-\nmon,andJianTang. Geodiff: Ageometricdiffusionmodel\nfor molecular conformation generation. arXiv preprint\narXiv:2203.02923,2022. 2\n[56] BinxinYang, Shuyang Gu, Bo Zhang, TingZhang, Xuejin\nChen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by\nexample:Exemplar-basedimageeditingwithdiffusionmod-\nels. InProceedingsoftheIEEE/CVFConferenceonCom-\nputerVisionandPatternRecognition, pages18381–18391,\n2023. 1\n[57] S. Yang, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S.\nErmon, and B. Poole. Score-based generative modeling\nthrough stochastic differential equations. In International\nConferenceonLearningRepresentations,2021. 1,5\n[58] TianheYu,SaurabhKumar,AbhishekGupta,SergeyLevine,\nKarol Hausman, and Chelsea Finn. Gradient surgery for\nmulti-task learning. Advances in Neural Information Pro-\ncessingSystems,33:5824–5836,2020. 3\n[59] Zixin Zhu, Yixuan Wei, Jianfeng Wang, Zhe Gan, Zheng\nZhang,LeWang,GangHua,LijuanWang,ZichengLiu,and\nHanHu. Exploringdiscretediffusionmodelsforimagecap-\ntioning. arXivpreprintarXiv:2211.11694,2022. 1\n7451\n\n",
    "total_pages": 11,
    "pages": [
      {
        "page": 1,
        "content": "Efficient Diffusion Training via Min-SNR Weighting Strategy\nTiankaiHang1,ShuyangGu2*,ChenLi3,JianminBao2,DongChen2,\nHanHu2,XinGeng1,BainingGuo1*\n1SoutheastUniversity,2MicrosoftResearchAsia,\n3NationalKeyLaboratoryofHuman-MachineHybridAugmentedIntelligence,\nNationalEngineeringResearchCenterforVisualInformationandApplications,\nandInstituteofArtificialIntelligenceandRobotics,Xi’anJiaotongUniversity\n{tkhang,xgeng,307000167}@seu.edu.cn,{shuyanggu,t-chenli1,jianmin.bao,doch,hanhu}@microsoft.com\nAbstract 30\nDenoising diffusion models have been a mainstream\napproach for image generation, however, training these 20\nmodels often suffers from slow convergence. In this pa-\nper, we discovered that the slow convergence is partly due\nto conflicting optimization directions between timesteps. 10\nTo address this issue, we treat the diffusion training as\na multi-task learning problem, and introduce a simple\nyet effective approach referred to as Min-SNR-γ. This 200 400 600 800 1000\nmethodadaptslossweightsoftimestepsbasedonclamped TrainingIterations(K)\nsignal-to-noise ratios, which effectively balances the con-\nflicts among timesteps. Our results demonstrate a signif-\nicant improvement in converging speed, 3.4× faster than\nprevious weighting strategies. It is also more effective,\nachieving a new record FID score of 2.06 on the Ima-\ngeNet 256 × 256 benchmark using smaller architectures\nthan that employed in previous state-of-the-art. The code\nis available at https://github.com/TiankaiHang/Min-SNR-\nDiffusion-Training.\n1.Introduction\nIn recent years, denoising diffusion models [48, 19, 57,\n36]haveemergedasapromisingnewclassofdeepgener-\nativemodelsduetotheirremarkableabilitytomodelcom-\nplicated distributions. Compared to prior Generative Ad-\nversarialNetworks(GANs),diffusionmodelshavedemon-\nstrated superior performance across a range of generation\ntasks in various modalities, including text-to-image gener-\nation [40, 43, 41, 17], image manipulation [26, 34, 4, 56],\nvideo synthesis [18, 47, 22], text generation [28, 16, 59],\n3D avatar synthesis [39, 53], etc. A key limitation of\npresent denoising diffusion models is their slow conver-\ngencerate,requiringsubstantialamountsofGPUhoursfor\n*Correspondingauthors.\nDIF\nThisICCVpaperistheOpenAccessversion,providedbytheComputerVisionFoundation.\nExceptforthiswatermark,itisidenticaltotheacceptedversion;\nthefinalpublishedversionoftheproceedingsisavailableonIEEEXplore.\nbaseline\nours\n3.4×speedup\nFigure 1: By leveraging a non-conflicting weighting strat-\negy,ourmethodcanconverge3.4timesfasterthanbaseline,\nresultinginsuperiorperformance.\ntraining[41,40]. Thisconstitutesaconsiderablechallenge\nforresearchersseekingtoeffectivelyexperimentwiththese\nmodels.\nIn this paper, we first conducted a thorough examina-\ntion of this issue, revealing that the slow convergence rate\nlikelyarisesfromconflictingoptimizationdirectionsfordif-\nferent timesteps during training. In fact, we find that by\ndedicatedlyoptimizingthedenoisingfunctionforaspecific\nnoise level can even harm the reconstruction performance\nfor other noise levels, as shown in Figure 2. This indi-\ncates that the optimal weight gradients for different noise\nlevels are in conflict with one another. Given that cur-\nrent denoising diffusion models [19, 12, 36, 41] employ\nsharedmodelweightsforvariousnoiselevels,theconflict-\ning weight gradients will impede the overall convergence\nrate,ifwithoutcarefulconsiderationonthebalanceofthese\nnoisetimesteps.\nTotacklethisproblem,weproposetheMin-SNR-γloss\nweighting strategy. This strategy treats the denoising pro-\ncess of each timestep as an individual task, thus diffusion\n7441"
      },
      {
        "page": 2,
        "content": "trainingcanbeconsideredasamulti-tasklearningproblem. tion of the gradient conflicting issue. As a result, the pro-\nTo balance various tasks, we assign loss weights for each posed weighting strategy not only converges much faster\ntask according to their difficulty. Specifically, we adopt a than previous approaches, but is also effective and general\nclampedsignal-to-noiseratio(SNR)aslossweighttoalle- for various generation scenarios. It achieves a new record\nviatetheconflictinggradientsissue. Byorganizingvarious of FID score 2.06 on the ImageNet 256×256 benchmark,\ntimesteps using this new weighting strategy, the diffusion and proves to also improve models using other prediction\ntrainingprocesscanconvergemuchfasterthanpreviousap- targetsandnetworkarchitectures.\nproaches,asillustratedinFigure1. Ourcontributionsaresummarizedasfollows:\nGenericmulti-tasklearningmethodsusuallyseektomit-\n• We have uncovered a compelling explanation for the\nigate conflicts between tasks by adjusting the loss weight\nslowconvergenceissueindiffusiontraining:aconflict\nof each task based on their gradients. One classical ap-\ningradientsacrossvarioustimesteps.\nproach[11,46],Paretooptimization,aimstoseekagradient\ndescent direction to improve all the tasks. However, these • We have proposed a new loss weighting strategy for\napproachesdifferfromourMin-SNR-γ weightingstrategy diffusion model training, which greatly mitigates the\ninthreeaspects: 1)Sparsity. Mostpreviousstudiesinthe conflicting gradients across timesteps and results in a\ngenericmulti-tasklearningfieldhavefocusedonscenarios markedaccelerationofconvergencespeed.\nwithasmallnumberoftasks,whichdiffersfromthediffu-\nsiontrainingwherethenumberoftaskscanbeuptothou- • We have established a new FID score record on the\nsands. As in our experiments, Pareto optimal solutions in\nImageNet256×256imagegenerationbenchmark.\ndiffusiontrainingtendtosetlossweightsofmosttimesteps\n2.RelatedWorks\nas 0. In this way, many timesteps will be left without any\nlearning, and thus harm the entire denoising process. 2)\nDenoising Diffusion Models. Diffusion models [19, 50,\nInstability. The gradients computed for each timestep in\n12]arestronggenerativemodels,particularlyinthefieldof\neachiterationareoftennoisy,owingtoalimitednumberof\nimagegeneration,duetotheirabilitytomodelcomplexdis-\nsamplesforeachtimestep. Thishamperstheaccuratecom-\ntributions. Thisadvantagehasledtosuperiorityoverprevi-\nputation of Pareto optimal solutions. 3) Inefficiency. The\nousGANmodelsintermsofbothhigh-fidelityanddiversity\ncalculation of Pareto optimal solutions is time-consuming,\nofgeneratedimages[12,24,35,40,41,43]. Besides,diffu-\nsignificantlyslowingdowntheoveralltraining.\nsionmodelsalsoshowgreatsuccessintext-to-videogener-\nOurproposedMin-SNR-γstrategyisapredefinedglobal\nation[18,47,52],3DAvatargeneration[39,53],imageto\nstep-wise loss weighting setting, instead of run-time adap- image translation [37], image manipulation [4, 26], music\ntivelossweightsforeachiterationasintheoriginalPareto generation [23], and even drug discovery [55]. The most\noptimization, thus avoiding the sparsity issue. Moreover, widely used network structure for diffusion models in the\nthe global loss weighting strategy eliminates the need for fieldofimagegenerationisUNet[19,12,35,36].Recently,\nnoisy computation of gradients and the time-consuming researchershavealsoexploredtheuseofVisionTransform-\nPareto optimization process, making it more efficient and ers[13]asanalternative,withU-ViT[2]borrowingtheskip\nstable. Thoughsuboptimal, theglobalstrategycanbealso connection design from UNet [42] and DiT [38] leverag-\nalmost as effective: Firstly, the optimization dynamics of ingAdaptiveLayerNormanddiscoveringthatthezeroini-\neach denoising task are largely shaped by the task’s noise tialization strategy is critical for achieving state-of-the-art\nlevel, without the need to account for individual samples class-conditionalImageNetgenerationresults.\ntoomuch. Secondly,afteramoderatenumberofiterations, ImprovedDiffusionModels. Recentstudieshavetriedto\nthe gradients of the majority subsequent training process improve the diffusion models from different perspectives.\nbecomemorestable, thusitcanbeapproximated byasta- Some works aim to improve the quality of generated im-\ntioneryweightingstrategy. agesbyguidingthesamplingprocess[12,21]. Otherstud-\nTovalidatetheeffectivenessoftheMin-SNR-γ weight- iesproposefastsamplingmethodsthatrequireonlyadozen\ningstrategy,wefirstcomputeitsParetoobjectivevalueand steps [49, 29, 32, 24] to generating high-quality images.\ncompareitwiththeoptimalstep-wiselossweightsobtained Someworkshavefurtherdistilledthediffusionmodelsfor\nby directly solving the Pareto problem. Together, we also even fewer steps in the sampling process [44, 33]. Mean-\ncompareitwithseveralconventionallossweightingstrate- while, some researchers [19, 24, 6] have noticed that the\ngies, including constant weighting, SNR weighting, and noise schedule is important for diffusion models. Other\nSNR with an lower bound. Figure 4 shows that our Min- works [36, 44] have found that different predicting targets\nSNR-γ weighting strategy produces Pareto objective val- from denoising networks affect the training stability and\nues almost as low as the optimal one, significantly better final performance. Finally, some works [14, 1] have pro-\nthan other existing works, indicating a significant allevia- posedusingtheMixtureofExperts(MoE)approachtohan-\n7442"
      },
      {
        "page": 3,
        "content": "dlenoisefromdifferentlevels,whichcanboosttheperfor- 0.305\nmance of diffusion models, but require a larger number of\n0.165\nparametersandlongertrainingtime.\nMulti-task Learning. The goal of Multi-task learning 0.041\n(MTL) is to learn multiple related tasks jointly so that the\nknowledge contained in a task can be leveraged by other -0.001\ntasks. One of the main challenges in MTL is negative\n-0.002\ntransfer [9], means the joint training of tasks hurts learn-\ninginsteadofhelpingit. Fromanoptimizationperspective, -0.003\n0 100 200 300 400 500 600\nit manifests as the presence of conflicting task gradients.\ntimesteps\nTo address this issue, some previous works [58, 54, 8] try\nto modulate the gradient to prevent conflicts. Meanwhile,\notherworksattempttobalancedifferenttasksthroughcare-\nfullydesignthelossweights[7,25]. GradNorm[7]consid-\ners loss weight as learnable parameters and updates them\nthroughgradientdescent. AnotherapproachMTO[11,46]\nregardsthemulti-tasklearningproblemasamulti-objective\noptimizationproblemandobtainsthelossweightsbysolv-\ningaquadraticprogrammingproblem.\n3.Method\n3.1.Preliminary\nDiffusion models consist of two processes: a forward\nnoising process and a reverse denoising process. We de-\nnotethedistributionoftrainingdataasp(x 0). Theforward\nprocessisaGaussiantransition, graduallyaddsnoisewith\ndifferentscalestoarealdatapointx 0 ∼ p(x 0)toobtaina\nseriesofnoisylatentvariables{x 1,x 2,...,x T}:\nq(x t|x 0)=N(x t;α tx 0,σ t2I) (1)\nx t =α tx 0+σ t(cid:2) (2)\nwhere (cid:2) is the noise sampled from Gaussian distribution\nN(0,I). The noise schedule σ t denotes the magnitude of\nnoise added to the clean data at t timestep. It increases\nmonotonically with t. In this paper, we adopt the stan-\n(cid:2)dard variance-preserving diffusion process, where α t =\n1−σ2.\nt\nThe reverse process is parameterized by another Gaus-\nsian transition, gradually denoises the latent variables and\nrestorestherealdatax 0fromaGaussiannoise:\np θ(x t−1|x t)=N(x t−1;μˆ θ(x t),Σˆ θ(x t)). (3)\nμˆ θ andΣˆ θ arepredictedstatistics. Hoetal.[19]setΣˆ θ(x t)\nto the constant σ t2I, and μˆθ can be decomposed into the\nlinearcombinationofx t andanoiseapproximationmodel\n(cid:5)ˆθ. Theyfindusinganetworktopredictnoise(cid:5)workswell,\nespecially when combined with a simple re-weighted loss\nfunction:\n(cid:3) (cid:4)\nLt simple(θ)=E x0,(cid:3) (cid:3)(cid:5)−(cid:5)ˆθ(α tx 0+σ t(cid:5))(cid:3)2 2 . (4)\nssolESMΔ\n[100,200)\n[200,300)\n[300,400)\nbaseline\nFigure2:Wefinetunethediffusionmodelinspecificranges\nof timesteps:[100, 200), [200, 300), and [300, 400), then\nweinvestigatehowitaffectsthelossindifferenttimesteps.\nThesurroundingtimestepsmayderivebenefitfromit,while\nothersmayexperienceadverseeffects.\nMost previous works [36, 12, 35] follow this strategy and\npredict the noise. Later works [17, 44] use another re-\nparameterizationthatpredictsthenoiselessstatex 0:\n(cid:3) (cid:4)\nLt simple(θ)=E x0,(cid:3) (cid:3)x 0−xˆθ(α tx 0+σ t(cid:5))(cid:3)2 2 . (5)\nAndsomeotherworks[44,41]evenemploythenetworkto\ndirectly predict velocity v. Despite their prediction targets\nbeingdifferent,wecanderivethattheyaremathematically\nequivalentbymodifyingtheirlossweights.\n3.2.DiffusionTrainingasMulti-TaskLearning\nTo reduce the number of parameters, previous stud-\nies[19,36,12]oftensharetheparametersofthedenoising\nmodelsacrossallsteps. However,it’simportanttokeepin\nmindthatdifferentstepsmayhavevastlydifferentrequire-\nments. At each step of a diffusion model, the strength of\nthe denoising varies. For example, easier denoising tasks\n(when t → 0) may require simple reconstructions of the\ninput in order to achieve lower denoising loss. This strat-\negy, unfortunately, does not work as well for noisier tasks\n(when t → T). Thus, it’s extremely important to analyze\nthecorrelationbetweendifferenttimesteps.\nInthisregard,weconductasimpleexperiment. Webe-\nginbyclusteringthedenoisingprocessintoseveralseparate\nbins. Then we finetune the diffusion model by sampling\ntimestepsineachbin. Lastly, weevaluateitseffectiveness\nby looking at how it impacted the loss of other bins. As\nshown in Figure 2, we canobserve that finetuning specific\nstepsbenefitedthosesurroundingsteps. However,it’soften\ndetrimental for other steps that are far away. This inspires\nustoconsiderwhetherwecanfindamoreefficientsolution\nthatbenefitsalltimestepssimultaneously.\nWe re-organized our goal from the perspective of mul-\ntitask learning. The training process of denoising diffu-\nsion models contains T different tasks, each task repre-\n7443"
      },
      {
        "page": 4,
        "content": "sentsanindividualtimestep. Wedenotethe modelparam-\netersasθ andthecorrespondingtraininglossisLt(θ),t ∈\n0.15\n{1,2,...,T}. Ourgoalistofindaupdatedirectionδ (cid:6)= 0,\nthatsatisfies:\n0.10\nLt(θ+δ)≤Lt(θ),∀t∈{1,2,...,T}. (6)\n0.05\nWeconsiderthefirst-orderTaylorexpansion:\n0.00\n(cid:5) (cid:6)\nLt(θ+δ)≈Lt(θ)+ δ,∇ θLt(θ) . (7) 0 1000 2000 3000\nNumber of samples\nThus,theidealupdatedirectionisequivalenttosatisfy:\n(cid:5) (cid:6)\nδ,∇ θLt(θ) ≤0,∀t∈{1,2,...,T}. (8)\n3.3.Paretooptimalityofdiffusionmodels\nTheorem1 Consideraupdatedirectionδ∗:\n(cid:7)T\nδ∗ =− w t∇ θLt(θ), (9)\nt=1\nofwhichw tisthesolutiontotheoptimizationproblem:\n(cid:8) (cid:9)\n(cid:7)T (cid:7)T\nmin (cid:3) w t∇ θLt(θ)(cid:3)2| w t =1,w t ≥0 (10)\nwt\nt=1 t=1\nIf the optimal solution to the Equation 8 exists, then δ∗\nshould satisfy it. Otherwise, it means that we must sacri-\nficeacertaintaskinexchangeforthelossdecreaseofother\ntasks. Inotherwords,wehavereachedtheParetoStation-\naryandthetraininghasconverged.\nAmoregeneralformofthistheoremwasfirstproposed\nin[11]andweleaveasuccinctproofinthesupplementary\nmaterial. Sincediffusionmodelsarerequiredtogothrough\nallthetimestepswhengeneratingimages. Soanytimestep\nshould not be ignored during training. Consequently, a\nregularization term is included to prevent the loss weights\nfrombecomingexcessivelysmall. Theoptimizationgoalin\nEquation10becomes:\n(cid:8) (cid:9)\n(cid:7)T (cid:7)T\nmin (cid:3) w t∇ θLt(θ)(cid:3)2 2+λ (cid:3)w t(cid:3)2 2 (11)\nwt\nt=1 t=1\nwhereλcontrolstheregularizationstrength.\nTo solve Equation 11, [46] leverages the Frank-\nWolfe [15] algorithm to obtain the weight {w t} through\niterative optimization. Another approach is to adopt Un-\nconstrained Gradient Descent(UGD). Specifically, we re-\nparameterizew tthroughβ t:\neβt (cid:7)\nw t = Z ,Z = eβt,β t ∈R. (12)\nt\nthgiew\nssol\nFigure3: Demonstrationoftheinstabilityofoptimization-\nbased weighting strategy. As the number of samples in-\ncreases, the loss weight becomes stable, while the compu-\ntationcostincreases.\nCombinedwithEquation11,wecanusegradientdescentto\noptimizeeachtermindependently:\n1 (cid:7)T λ (cid:7)T\nm βi tn Z2(cid:3) t=1eβt∇ θL t(θ)(cid:3)2 2+ Z2 t=1(cid:3)eβt(cid:3)2 2 (13)\nHowever, whether leveraging the Frank-Wolfe or the\nUGD algorithm, there are two disadvantages: 1) Ineffi-\nciency. Both of these two methods need additional opti-\nmization at each training iteration, it greatly increases the\ntrainingcost. 2)Instability. Inpractice,byusingalimited\nnumberofsamplestocalculatethegradientterm∇ θLt(θ),\ntheoptimizationresultsareunstable(asshowninFigure3).\nInotherwords,thelossweightsforeachdenoisingtaskvary\ngreatlyduringtraining,makingtheentirediffusiontraining\ninefficient.\n3.4.Min-SNR-γ LossWeightStrategy\nIn order to avoid the inefficiency and instability caused\nbytheiterativeoptimizationineachiteration,onepossible\nattemptistoadoptastationerylossweightstrategy.\nTosimplifythediscussion, weassumethatthenetwork\nisreparameteredtopredictthenoiselessstatex 0. However,\nit’sworthnotingthatdifferentpredictionobjectivescanbe\ntransformed into one another, we will delve into it in Sec-\ntion4.2. Now, weconsiderthefollowingalternativetrain-\ninglossweights:\n• Constant weighting. w t = 1. Which treats different\ntasks as equally weighted and has been used in both\ndiscretediffusionmodels[17,51]andcontinuousdif-\nfusionmodels[5].\n• SNR weighting. w t = SNR(t), where SNR(t) =\nα2/σ2. It’s the most widely used weighting strat-\nt t\negy [33, 22, 12, 41]. By combining with Equation 2,\nwecanfindit’snumericallyequivalenttotheconstant weightingstrategywhenthepredictingtargetisnoise.\n7444"
      },
      {
        "page": 5,
        "content": "0.4\n0.3\n0.2\n0.1\n0.0\n60 80 100 120 140\nTrainingIterations(K)\neulaVevitcejbO\nhuman faces, is a widely-used unconditional image gener-\nConst\nation benchmark. We follow ScoreSDE [57] for data pre- SNR\nMin-SNR-5 processing,whichinvolvescentercroppingeachimagetoa\nUGD resolutionof140×140andthenresizingitto64×64. For\nclassconditionalgeneration,weadoptImageNet[10]with\na total of 1.3 million images from 1000 different classes.\nWetesttheperformanceonboth642and2562resolutions.\nTrainingDetails.Forlowresolution(64×64)imagegener-\nation,wefollowADM[12]anddirectlytrainthediffusion\nmodel on the pixel-level. For high-resolution image gen-\nFigure4:ComparisonoftheobjectivevaluesinEquation11 eration, we utilize LDM [41] approach by first compress-\nondifferentweightingstrategies. ing the images into latent space, then training a diffusion\nmodel to model the latent distributions. To obtain the la-\n• Max-SNR-γ weighting. w t = max{SNR(t),γ}. This tent for images, we employ VAE from Stable Diffusion1,\nmodification of SNR weighting is first proposed in\nwhichencodesahigh-resolutionimage(256×256×3)into\n[44] to avoid a weight of zero with zero SNR steps.\n32×32×4latentcodes.\nTheyset γ = 1astheirdefaultsetting. However, the In our experiments, we employ both ViT and UNet as\nweightsstillconcentrateonsmallnoiselevels. our diffusion model backbones. We adopt a vanilla ViT\nstructurewithoutanymodifications[13]asourdefaultset-\n• Min-SNR-γ weighting. w t = min{SNR(t),γ}. We ting. we incorporate the timestep t and class condition c\nproposethisweightingstrategytoavoidthemodelfo- as learnable input tokens to the model. Although further\ncusingtoomuchonsmallnoiselevels. customization of the network structure may improve per-\nformance, our focus in this paper is to analyze the general\n• UGD optimization weighting. w t is optimized from propertiesofdiffusionmodels. FortheUNetstructure, we\nEquation13ineachtimestep. Comparedwiththepre-\nfollowADM[12]andkeeptheFLOPssimilartotheViT-B\nvioussetting,thisstrategychangesduringtraining. model, whichhas1.5×parameters. Additionaldetailscan\nbefoundinthesupplementarymaterial.\nFirst, wecombinetheseweightingstrategiesintoEqua-\nFor the diffusion settings, we use a cosine noise sched-\ntion11tovalidatewhethertheyareapproachtothePareto\nulerfollowingtheapproachin[36,12].Thetotalnumberof\noptimalitystate. AsshowninFigure4,theUGDoptimiza- timesteps is standardized to T = 1000 across all datasets.\ntionweightingstrategycanachievethelowestscoreonour\nWeadoptAdamW[27,31]asouroptimizer.FortheCelebA\noptimizationtarget. Inaddition,theMin-SNR-γ weighting\ndataset,wetrainourmodelfor500Kiterationswithabatch\nstrategyistheclosesttotheoptimum,demonstratingithas\nsizeof128. Duringthefirst5,000iterations,weimplement\nthepropertytooptimizedifferenttimestepssimultaneously. alinearwarm-upandkeepthelearningrateat1×10−4for\nInthefollowingsection,wepresentexperimentalresults\nthe remaining training. For the ImageNet dataset, the de-\ntodemonstratetheeffectivenessofourMin-SNR-γweight-\nfaultlearningrateisfixedat1×10−4. Thebatchsizeisset\ningstrategyinbalancingdiversenoiselevels. Ourapproach to1024for642resolutionand256for2562resolution.\naimstoachievefasterconvergenceandstrongperformance.\nEvaluation Settings. We utilize an Exponential Moving\nAverage (EMA) model with a rate of 0.9999 for evalua-\n4.Experiments\ntion. Duringtheevaluationphase,wegenerateimageswith\nthe Heun sampler from EDM [24]. For conditional image\nIn this section, we first provide an overview of the ex-\ngeneration, we also implement the classifier-free sampling\nperimentalsetup.Subsequently,weconductcomprehensive\nstrategy[21]toachievebetterresults. Finally,wemeasure\nablation studies to show that our method is versatile and\nthequalityofthegeneratedimagesusingtheFIDscorecal-\nsuitableforvariouspredictiontargetsandnetworkarchitec-\nculatedon50Kimages.\ntures. Finally, wecompareourapproachwiththestate-of-\nthe-artmethodsacrossmultiplebenchmarks,demonstrating 4.2.AnalysisoftheProposedMin-SNR-γ\nnotonlyitsacceleratedconvergencebutalsoitssuperiorca-\npabilityingeneratinghigh-qualityimages. Comparison of Different Weighting Strategies. To\ndemonstrate the significance of the loss weighting strat-\n4.1.Setup egy,weconductexperimentswithdifferentlossweightset-\ntings for predicting x 0. These settings include: 1) con-\nDatasets. We perform experiments on both uncondi-\nstant weighting, where w t = 1, 2) SNR weighting, with\ntional CelebA dataset [30] and the conditional ImageNet\ndataset[10].TheCelebAdataset,whichcomprises162,770 1https://huggingface.co/stabilityai/sd-vae-ft-mse-original\n7445"
      },
      {
        "page": 6,
        "content": "40\n30\n20\n10\n200 400 600 800 1000\nTrainingIterations(K)\nDIF\n40\nx0+Const\nx0+SNR\nx0+Max-SNR-1 30\nx0+Min-SNR-5\n20\n3.4×speedup\n10\n200 400 600 800 1000\nTrainingIterations(K)\nDIF\n40\n(cid:2)+SNR\n(cid:2)+Min-SNR-5\n30\n20\n10\n200 400 600 800 1000\nTrainingIterations(K)\nDIF\nv+Const\nv+SNR\nv+Max-SNR-1\nv+Min-SNR-5\nFigure5: Comparingdifferentlossweightingdesignsonpredictingx 0,(cid:5),v. Takingtheneuralnetworkoutputasnoisewith\nconstorMax-SNR-γ strategyleadtodivergence. Min-SNR-γ strategyconvergesthefastestunderallthesesettings.\n×10−3 ×10−2 ×10−1 ×10−1\n9.0 Const 8.5 Const 3.0 Const 4.45 Const\nSNR SNR SNR SNR\n8.5 Max-SNR-1 8.0 Max-SNR-1 Max-SNR-1\n4.40\nMax-SNR-1\nMin-SNR-5 Min-SNR-5\n2.9\nMin-SNR-5 4.35 Min-SNR-5\n8.0\n7.5 4.30\n2.8\n7.5 4.25\n250 500 750 1000 250 500 750 1000 250 500 750 1000 250 500 750 1000\nTrainingIterations(K) TrainingIterations(K) TrainingIterations(K) TrainingIterations(K)\nFigure 6: Unweighted loss in different ranges of timesteps. From left to right, each figure represents a specific range of\ntimesteps: [0,100),[200,300),[600,700),[800,900). They-axisrepresentstheMeanSquaredError(MSE),averagedover\neachrangeoftimesteps.\nw t = SNR(t), 3) truncated SNR weighting, with w t = ple images from training iteration 50K, 200K, 400K, and\nmax{SNR(t),γ}(following[44]withasetvalueofγ =1), 1Mwithdifferentlossweights. OurresultsshowthatMin-\nand 4) our proposed Min-SNR-γ weighting strategy, with SNR-γ generates a clear object with only 200K iterations,\nw t =min{SNR(t),γ},wesetγ =5asthedefaultvalue. whichissignificantlybetterinqualitythanothermethods.\nThe ViT-B serves as our default backbone and experi- Min-SNR-γ for Different Prediction Targets. Instead of\nmentsareperformedonImageNet256×256.Asillustrated predictingtheoriginalsignalx 0fromthenetwork,somere-\ninFigure5,weobservethatallresultsimproveasthenum- centworkshaveemployedalternativere-parameterizations,\nber of training iterations increases. However, our method such as predicting noise (cid:5), or velocity v [44]. To verify\ndemonstrates a significantly faster convergence compared the applicability of our weighting strategy to these predic-\ntoothermethods.Specifically,itexhibitsa3.4×speedupin tion targets, we conduct experiments comparing the four\nreachinganFIDscoreof10. Itisworthmentioningthatthe aforementioned weighting strategies across these different\nSNRweightingstrategyperformedtheworst, whichcould re-parameterizations.\nbeduetoitsdisproportionatefocusonlessnoisystages. As we discussed in Section 3.4, predicting noise (cid:5) is\nFor a deeper understanding of the reasons behind the mathematicallyequivalenttopredictingx 0 byintrinsically\nvaryingconvergencerates,weanalyzedtheirtraininglossat involvingSignal-to-NoiseRatioasaweightfactor,thuswe\ndifferentnoiselevels.Forafaircomparison,weexcludethe divide the SNR term in practice. For example, the Min-\nlossweighttermbyonlycalculating(cid:3)x 0−xˆ θ(cid:3)2 2. Consider- SNR-γ strategy in predicting noise can be expressed as\ningthatthelossofdifferentnoiselevelsvariesgreatly, we w t = min{ SS NN RR (( t)t),γ} = min{ SNRγ (t),1}. AndtheSNRstrat-\ncalculatethelossindifferentbinsandpresenttheresultsin egy in predicting noise is equivalent to a “constant strat-\nFigure6. Theresultsshowthatwhiletheconstantweight- egy”. Forsimplicityandconsistency,westillrefertothem\ning strategy is effective for high noise intensities, it per- asMin-SNR-γandSNRstrategies.Similarly,wecanderive\nformspoorlyatlownoiseintensities. Conversely,theSNR thatwhenpredictingvelocityv,thelossweightfactormust\nweighting strategy exhibits the opposite behavior. In con- bedividedby(SNR+1). Thesestrategiesarestillreferred\ntrast, our proposed Min-SNR-γ strategy achieves a lower tobytheiroriginalnamesforeaseofreference.\ntraining loss across almost all cases, and indicates quicker We conduct experiments on these two variants in Fig-\nconvergencethroughtheFIDmetric. ure5. NoiseoutputwithconstorMax-SNR-γ settingleads\nFurthermore, we present visual results in Figure 7 to todivergence. Meanwhile,ourproposedMin-SNR-γ strat-\ndemonstratethefastconvergenceofMin-SNR-γ. Wesam- egyconvergesfasterthanotherlossweightingstrategiesfor\n7446"
      },
      {
        "page": 7,
        "content": "Longer Training\nRNS\ntsnoC\n5-RNS-niM\nFigure 7: Qualitative comparison of the generation results from different weighting strategies on ImageNet-256 dataset.\nImagesineachcolumnaresampledfrom50K,200K,400K,and1Miterations. OurMin-SNR-5strategyyieldssignificant\nimprovementsinvisualfidelityfromthesameiteration.\nbothpredictionnoiseandpredictingvelocity.Thesedemon- TrainingIterations 200K 400K 600K 800K 1M\nstratethatbalancingthelossweightsfordifferenttimesteps\nisintrinsic,independentofanyre-parameterization.\nBaseline(x 0) 25.93 15.41 11.54 9.52 8.33\nMin-SNR-γ on Different Network Architectures. The +Min-SNR-5 7.99 5.34 4.69 4.41 4.28\nMin-SNR-γ strategy is versatile and robust for different Baseline((cid:5)) 8.55 5.43 4.64 4.35 4.21\nprediction targets and network structures. We conduct ex- +Min-SNR-5 7.32 4.98 4.48 4.24 4.14\nperiments on the widely used UNet and keep the number\nTable1: AblationstudiesontheUNetbackbone. Whether\nof parameters close to the ViT-B model. For each experi-\nment, models are trained for 1 million iterations and their\nthenetworkpredictsx 0or(cid:5),theMin-SNR-5weightingde-\nsignconvergesfasterandachievesbetterFIDscore.\nFIDscoresarecalculatedatmultipleiterations. Theresults\ninTable1indicatethattheMin-SNR-γ strategyconverges\nandmodifyingthenetworkstructuretoUNet. Wefindthat\nsignificantlyfasterthanthebaselineandprovidesbetterper-\nthe results were also consistently stable. Our results indi-\nformanceforbothpredictingx 0andpredicting(cid:5).\ncatethatgoodperformancecanusuallybeachievedwhenγ\nRobustnessAnalysis. Weutilizeasinglehyper-parameter,\nissetto5,makingittheestablisheddefaultsetting.\nγ, as the truncate value. To assess its robustness, we con-\n4.3.Comparisonwithstate-of-the-artMethods\nductthoroughanalysisinvarioussettings. Ourexperiments\nwere performed on ImageNet-256 using the ViT-B model CelebA-64.WeconductexperimentsonCelebA64×64for\nandpredictingx 0. Wevarythetruncatevalueγ bysetting unconditional generation. Both UNet and ViT are trained\nitto1,5,10,and20. TheresultsareshowninTable2. We for 500K iterations. During the evaluation, we use Heun\nfindthereareonlyminorvariationsintheFIDscorewhen sampler[24]togenerate50Ksamples. TheFIDresultsare\nγ issmallerthan20. Additionally,weconductedmoreex- summarizedinTable3. OurViT-Small[13]modeloutper-\nperimentsbymodifyingthepredictingtargettothenoise(cid:5), formspreviousViT-basedmodelswithanFIDscoreof2.14.\n7447"
      },
      {
        "page": 8,
        "content": "γ 1 5 10 20 Method #Params FID\nViT(x 0) 4.98 4.92 5.34 5.45 BigGAN-deep[3] 340M 6.95\nViT((cid:5)) 4.89 4.84 4.94 5.41 StyleGAN-XL[45] 2.30\nUNet(x 0) 4.49 4.28 4.32 4.37\nImprovedVQ-Diffusion[17] 460M 4.83\nUNet((cid:5)) 4.30 4.14 4.14 4.12\nIDDPM[36] 270M 12.26\nTable2: Ablationstudyonγ. Theresultsarerobusttothe CDM[20] 4.88\nhyper-parameterγ indifferentsettings. ADM-U,ADM-G[12] 608M 3.94\nLDM[41] 400M 3.60\nMethod #Params FID UNet(ours) 395M 2.81†\nDDIM[49] 79M 3.26\nU-ViT-L[2] 287M 3.40\nSoftTruncation[49] 62M 1.90\nDiT-XL-2[38] 675M 2.27\nOurUNet 59M 1.60\nViT-XL(ours) 451M 2.06\nU-ViT-Small[2] 44M 2.87\nTable5:FIDresultsonImageNet256×256.†denotesonly\nViT-Small(ours) 43M 2.14\ntrain1.4Miterations. OurmodelwithaViT-XLbackbone\nTable3: FIDresultsonunconditionalCelebA64×64[30] achievesanewrecordFIDscoreof2.06.\nbenchmark. WeexperimentwithbothUNetandViT.\nImageNet-256. Wealsoapplydiffusionmodelsforhigher-\nMethod #Params FID resolution image generation on the ImageNet 256 × 256\nBigGAN-deep[3] 4.06 benchmark. To enhance training efficiency, we first com-\nStyleGAN-XL[45] 1.51\npress256×256×3imagesinto32×32×4latentcodes\nusing the encoder from LDM [41]. During the sampling\nIDDPM(small)[36] 100M 6.92\nprocess,weemploytheHeunsamplerandtheclassifier-free\nIDDPM(large)[36] 270M 2.92\nguidanceCFG=1.5. TheFIDcomparisonispresentedin\nCDM[20] 1.48\nTable 5. Under the setting of predicting (cid:5) with Min-SNR-\nADM[12] 296M 2.61\n5, our ViT-XL model achieves the FID of 2.08 for only\nEDM[24] 296M 1.36\n2.1Miterations,whichis3.3×fasterthanDiTandoutper-\nU-ViT-Mid[2] 131M 5.85 formsthepreviousstate-of-the-artFIDof2.27. Moreover,\nU-ViT-Large[2] 287M 4.26 withlongertraining(about7Miterationsasin[38]),weare\nViT-L(ours) 269M 2.28 abletoachieveFID2.06bypredictingx 0withMin-SNR-5.\nOurUNet-basedmodelwith395Mparametersistrainedfor\nTable 4: FID results on ImageNet 64 × 64. We conduct\nabout1.4MiterationsandachievesFIDscoreof2.81.\nexperiments using the ViT-L backbone which significantly\nimprovesuponpreviousmethods.\n5.Conclusion\nInthispaper,wepointoutthattheconflictingoptimiza-\nIt is worth mentioning that no modifications are made to tiondirectionsbetweendifferenttimestepsmaycauseslow\nthe naive network structure, demonstrating that the results convergenceindiffusiontraining. Toaddressit,weregard\ncould still be improved further. Meanwhile, our method thediffusiontrainingprocessasamulti-tasklearningprob-\nusing the UNet [12] structure achieves an even better FID lemandintroduceanovelweightingstrategy, namedMin-\nscoreof1.60,outperformingpreviousUNetmethods. SNR-γ, to effectively balance different timesteps. Experi-\nImageNet-64. We also validate our method on class- mentsdemonstrateourmethodcanboostdiffusiontraining\nconditional ImageNet 64 × 64 benchmark. During train- several times faster, and achieves the state-of-the-art FID\ning,theclasslabelisdroppedwiththeprobability0.15for\nscoreonImageNet-256dataset.\nclassifier-freeinference[21].Themodelistrainedfor800K\niterations and images are synthesized using classifier-free Acknowledgments\nguidance scale of 1.5. For a fair comparison, we adopt a\n21-layer ViT-Large model without additional architecture We sincerely thank Yixuan Wei, Zheng Zhang, and\ndesigns, which has a similar number of parameters to U- StephenLinforhelpfuldiscussion.Thisresearchwaspartly\nViT-Large [2]. The results presented in Table 4 show that supported by the National Key Research & Development\nourmethodachievesanFIDscoreof2.28,significantlyim- Plan of China (No. 2018AAA0100104), the National Sci-\nprovingupontheU-ViT-Largemodel. enceFoundationofChina(62125602,62076063).\n7448"
      },
      {
        "page": 9,
        "content": "References Yin,ShikunFeng,etal. Ernie-vilg2.0: Improvingtext-to-\nimage diffusion model with knowledge-enhanced mixture-\n[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nof-denoising-experts. arXiv preprint arXiv:2210.15257,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\n2022. 2\nSamuliLaine,BryanCatanzaro,TeroKarras,andMing-Yu\n[15] Marguerite Frank and Philip Wolfe. An algorithm for\nLiu. ediff-i: Text-to-imagediffusionmodelswithensemble\nquadraticprogramming. Navalresearchlogisticsquarterly,\nofexpertdenoisers.arXivpreprintarXiv:2211.01324,2022.\n3(1-2):95–110,1956. 4\n2\n[16] ShansanGong,MukaiLi,JiangtaoFeng,ZhiyongWu,and\n[2] FanBao,ShenNie,KaiwenXue,YueCao,ChongxuanLi,\nLingpengKong. Sequencetosequencetextgenerationwith\nHang Su, and Jun Zhu. All are worth words: A vit back-\ndiffusionmodels. InInternationalConferenceonLearning\nbonefordiffusionmodels.arXivpreprintarXiv:2209.12152,\nRepresentations,2023. 1\n2022. 2,8\n[17] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\n[3] AndrewBrock,JeffDonahue,andKarenSimonyan. Large\nZhang, DongdongChen, LuYuan, andBainingGuo. Vec-\nscale gan training for high fidelity natural image synthe-\ntorquantizeddiffusionmodelfortext-to-imagesynthesis.In\nsis. InternationalConferenceonLearningRepresentations,\nProceedingsoftheIEEE/CVFConferenceonComputerVi-\n2019. 8\nsionandPatternRecognition,pages10696–10706,2022. 1,\n[4] TimBrooks,AleksanderHolynski,andAlexeiAEfros. In-\n3,4,8\nstructpix2pix:Learningtofollowimageeditinginstructions.\n[18] Jonathan Ho, WilliamChan, ChitwanSaharia, JayWhang,\nIn Proceedings of the IEEE/CVF Conference on Computer\nRuiqiGao, AlexeyA.Gritsenko, DiederikP.Kingma, Ben\nVisionandPatternRecognition,pages18392–18402,2023.\nPoole, Mohammad Norouzi, David J. Fleet, and Tim Sali-\n1,2\nmans. Imagenvideo: Highdefinitionvideogenerationwith\n[5] Hanqun Cao, Cheng Tan, Zhangyang Gao, Guangyong\ndiffusionmodels. ArXiv,abs/2210.02303,2022. 1,2\nChen, Pheng-Ann Heng, and Stan Z Li. A survey on gen-\n[19] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu-\nerative diffusion model. arXiv preprint arXiv:2209.02646,\nsionprobabilisticmodels. AdvancesinNeuralInformation\n2022. 4\nProcessingSystems,33:6840–6851,2020. 1,2,3\n[6] TingChen. Ontheimportanceofnoiseschedulingfordiffu-\nsionmodels. arXivpreprintarXiv:2301.10972,2023. 2 [20] JonathanHo,ChitwanSaharia,WilliamChan,DavidJFleet,\nMohammadNorouzi,andTimSalimans.Cascadeddiffusion\n[7] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and An-\nmodelsforhighfidelityimagegeneration. J.Mach.Learn.\ndrew Rabinovich. Gradnorm: Gradient normalization for\nRes.,23:47–1,2022. 8\nadaptive loss balancing in deep multitask networks. In In-\nternationalconferenceonmachinelearning,pages794–803. [21] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nPMLR,2018. 3 guidance. InNeurIPS2021WorkshoponDeepGenerative\n[8] Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, ModelsandDownstreamApplications,2021. 2,5,8\nHenrikKretzschmar,YuningChai,andDragomirAnguelov. [22] JonathanHo, TimSalimans, AlexeyA.Gritsenko, William\nJustpickasign:Optimizingdeepmultitaskmodelswithgra- Chan,MohammadNorouzi,andDavidJ.Fleet. Videodif-\ndientsigndropout.AdvancesinNeuralInformationProcess- fusion models. In Alice H. Oh, Alekh Agarwal, Danielle\ningSystems,33:2039–2050,2020. 3 Belgrave,andKyunghyunCho,editors,AdvancesinNeural\n[9] Michael Crawshaw. Multi-task learning with deep neural InformationProcessingSystems,2022. 1,4\nnetworks:Asurvey.arXivpreprintarXiv:2009.09796,2020. [23] Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk,\n3 AndyLy,NanxinChen,ZhengdongZhang,ZhishuaiZhang,\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Jiahui Yu, Christian Frank, et al. Noise2music: Text-\nandLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage conditionedmusicgenerationwithdiffusionmodels. arXiv\ndatabase. In2009IEEEconferenceoncomputervisionand preprintarXiv:2302.03917,2023. 2\npatternrecognition,pages248–255.Ieee,2009. 5 [24] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\n[11] Jean-AntoineDe´side´ri. Multiple-gradientdescentalgorithm Elucidating the design space of diffusion-based generative\n(mgda) for multiobjective optimization. Comptes Rendus models. InAliceH.Oh,AlekhAgarwal,DanielleBelgrave,\nMathematique,350(5-6):313–318,2012. 2,3,4 andKyunghyunCho,editors,AdvancesinNeuralInforma-\n[12] PrafullaDhariwalandAlexanderNichol. Diffusionmodels tionProcessingSystems,2022. 2,5,7,8\nbeatgansonimagesynthesis. AdvancesinNeuralInforma- [25] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task\ntionProcessingSystems,34:8780–8794,2021. 1,2,3,4,5, learningusinguncertaintytoweighlossesforscenegeome-\n8 tryandsemantics.InProceedingsoftheIEEEconferenceon\n[13] A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X. computervisionandpatternrecognition,pages7482–7491,\nZhai,T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold, 2018. 3\nandS.Gelly.Animageisworth16x16words:Transformers [26] GwanghyunKim, TaesungKwon, andJongChulYe. Dif-\nforimagerecognitionatscale. InInternationalConference fusionclip: Text-guided diffusion models for robust image\nonLearningRepresentations,2021. 2,5,7 manipulation. InProceedingsoftheIEEE/CVFConference\n[14] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, on Computer Vision and Pattern Recognition, pages 2426–\nLanxinLi,XuyiChen,YuxiangLu,JiaxiangLiu,Weichong 2435,2022. 1,2\n7449"
      },
      {
        "page": 10,
        "content": "[27] D. P. Kingma and J. Ba. Adam: A method for stochastic theIEEE/CVFConferenceonComputerVisionandPattern\noptimization. InInternationalConferenceonLearningRep- Recognition,pages10684–10695,2022. 1,2,3,4,5,8\nresentations,2014. 5 [42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\n[28] Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy net: Convolutionalnetworksforbiomedicalimagesegmen-\nLiang, and Tatsunori B Hashimoto. Diffusion-lm im- tation.InMedicalImageComputingandComputer-Assisted\nproves controllable text generation. arXiv preprint Intervention–MICCAI2015:18thInternationalConference,\narXiv:2205.14217,2022. 1 Munich,Germany,October5-9,2015,Proceedings,PartIII\n[29] LupingLiu,YiRen,ZhijieLin,andZhouZhao. Pseudonu- 18,pages234–241.Springer,2015. 2\nmericalmethodsfordiffusionmodelsonmanifolds. ArXiv, [43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nabs/2202.09778,2022. 2 Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed\n[30] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Ghasemipour,RaphaelGontijo-Lopes,BurcuKaragolAyan,\nDeeplearningfaceattributesinthewild. InProceedingsof TimSalimans,JonathanHo,DavidJ.Fleet,andMohammad\nInternational Conference on Computer Vision (ICCV), De- Norouzi. Photorealistictext-to-imagediffusionmodelswith\ncember2015. 5,8 deeplanguageunderstanding. InAliceH.Oh,AlekhAgar-\nwal, Danielle Belgrave, and Kyunghyun Cho, editors, Ad-\n[31] I.LoshchilovandF.Hutter. Decoupledweightdecayregu-\nvancesinNeuralInformationProcessingSystems,2022. 1,\nlarization. InInternationalConferenceonLearningRepre-\n2\nsentations,2018. 5\n[44] TimSalimansandJonathanHo. Progressivedistillationfor\n[32] ChengLu,YuhaoZhou,FanBao,JianfeiChen,Chongxuan\nfastsamplingofdiffusionmodels. InInternationalConfer-\nLi, andJun Zhu. Dpm-solver: A fastode solverfor diffu-\nenceonLearningRepresentations,2022. 2,3,5,6\nsionprobabilisticmodelsamplinginaround10steps.ArXiv,\n[45] AxelSauer,KatjaSchwarz,andAndreasGeiger. Stylegan-\nabs/2206.00927,2022. 2\nxl: Scalingstylegantolargediversedatasets. InACMSIG-\n[33] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik\nGRAPH2022conferenceproceedings,pages1–10,2022. 8\nKingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\n[46] Ozan Sener and Vladlen Koltun. Multi-task learning as\nOndistillationofguideddiffusionmodels. InProceedings\nmulti-objective optimization. Advances in neural informa-\noftheIEEE/CVFConferenceonComputerVisionandPat-\ntionprocessingsystems,31,2018. 2,3,4\nternRecognition,pages14297–14306,2023. 2,4\n[47] UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn,\n[34] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nYanZhu,andStefanoErmon. Sdedit: Imagesynthesisand\nOranGafni,DeviParikh,SonalGupta,andYanivTaigman.\neditingwithstochasticdifferentialequations. arXivpreprint\nMake-a-video: Text-to-video generation without text-video\narXiv:2108.01073,2021. 1\ndata. InTheEleventhInternationalConferenceonLearning\n[35] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nRepresentations,2023. 1,2\nShyam,PamelaMishkin,BobMcGrew,IlyaSutskever,and\n[48] JaschaSohl-Dickstein, EricWeiss, NiruMaheswaranathan,\nMarkChen. Glide:Towardsphotorealisticimagegeneration\nand Surya Ganguli. Deep unsupervised learning using\nand editing with text-guided diffusion models. In Interna-\nnonequilibrium thermodynamics. In International Confer-\ntionalConferenceonMachineLearning,2021. 2,3\nenceonMachineLearning,pages2256–2265.PMLR,2015.\n[36] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\n1\ndenoising diffusion probabilistic models. In International\n[49] JiamingSong,ChenlinMeng,andStefanoErmon. Denois-\nConferenceonMachineLearning,pages8162–8171.PMLR,\ning diffusion implicit models. In International Conference\n2021. 1,2,3,5,8\nonLearningRepresentations,2021. 2,8\n[37] GauravParmar,KrishnaKumarSingh,RichardZhang,Yijun\n[50] YangSongandStefanoErmon.Generativemodelingbyesti-\nLi,JingwanLu,andJun-YanZhu.Zero-shotimage-to-image\nmatinggradientsofthedatadistribution.Advancesinneural\ntranslation. InACMSIGGRAPH2023ConferenceProceed-\ninformationprocessingsystems,32,2019. 2\nings,pages1–11,2023. 2\n[51] ZhicongTang,ShuyangGu,JianminBao,DongChen,and\n[38] WilliamPeeblesandSainingXie. Scalablediffusionmodels Fang Wen. Improved vector quantized diffusion models.\nwithtransformers. arXivpreprintarXiv:2212.09748,2022. arXivpreprintarXiv:2205.16007,2022. 4\n2,8 [52] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\n[39] BenPoole,AjayJain,JonathanTBarron,andBenMilden- dermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nEleventhInternationalConferenceonLearningRepresenta- Phenaki:Variablelengthvideogenerationfromopendomain\ntions,2022. 1,2 textualdescriptions. InInternationalConferenceonLearn-\n[40] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu, ingRepresentations,2023. 2\nand Mark Chen. Hierarchical text-conditional image gen- [53] TengfeiWang,BoZhang,TingZhang,ShuyangGu,Jianmin\nerationwithcliplatents. arXivpreprintarXiv:2204.06125, Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang\n2022. 1,2 Wen, Qifeng Chen, et al. Rodin: A generative model for\n[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, sculpting3ddigitalavatarsusingdiffusion. InProceedings\nPatrick Esser, and Bjo¨rn Ommer. High-resolution image oftheIEEE/CVFConferenceonComputerVisionandPat-\nsynthesis with latent diffusion models. In Proceedings of ternRecognition,pages4563–4573,2023. 1,2\n7450"
      },
      {
        "page": 11,
        "content": "[54] Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao.\nGradient vaccine: Investigating and improving multi-task\noptimization in massively multilingual models. arXiv\npreprintarXiv:2010.05874,2020. 3\n[55] MinkaiXu,LantaoYu,YangSong,ChenceShi,StefanoEr-\nmon,andJianTang. Geodiff: Ageometricdiffusionmodel\nfor molecular conformation generation. arXiv preprint\narXiv:2203.02923,2022. 2\n[56] BinxinYang, Shuyang Gu, Bo Zhang, TingZhang, Xuejin\nChen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by\nexample:Exemplar-basedimageeditingwithdiffusionmod-\nels. InProceedingsoftheIEEE/CVFConferenceonCom-\nputerVisionandPatternRecognition, pages18381–18391,\n2023. 1\n[57] S. Yang, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S.\nErmon, and B. Poole. Score-based generative modeling\nthrough stochastic differential equations. In International\nConferenceonLearningRepresentations,2021. 1,5\n[58] TianheYu,SaurabhKumar,AbhishekGupta,SergeyLevine,\nKarol Hausman, and Chelsea Finn. Gradient surgery for\nmulti-task learning. Advances in Neural Information Pro-\ncessingSystems,33:5824–5836,2020. 3\n[59] Zixin Zhu, Yixuan Wei, Jianfeng Wang, Zhe Gan, Zheng\nZhang,LeWang,GangHua,LijuanWang,ZichengLiu,and\nHanHu. Exploringdiscretediffusionmodelsforimagecap-\ntioning. arXivpreprintarXiv:2211.11694,2022. 1\n7451"
      }
    ]
  },
  "pdf_url": "/uploads/d081346df054112de43cceaf86414e67.pdf"
}