from datetime import datetime
from typing import Optional, List
import json

from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from services.chat_service import call_ai_api, call_ai_api_stream, extract_reasoning_content
from services.vector_service import vector_context
from services.glossary_service import glossary_service, build_glossary_prompt
from services.table_service import protect_markdown_tables, restore_markdown_tables
from services.query_analyzer import get_retrieval_strategy
from services.preset_service import get_generation_prompt
from services.context_builder import ContextBuilder
from models.provider_registry import PROVIDER_CONFIG
from utils.middleware import (
    LoggingMiddleware,
    RetryMiddleware,
    ErrorCaptureMiddleware,
    DegradeOnErrorMiddleware,
    TimeoutMiddleware,
    FallbackMiddleware,
)
from config import settings

router = APIRouter()

# ä¸Šä¸‹æ–‡æ„å»ºå™¨å®ä¾‹ï¼Œç”¨äºç”Ÿæˆå¼•æ–‡æŒ‡ç¤ºæç¤ºè¯
_context_builder = ContextBuilder()


def build_chat_middlewares():
    middlewares = []
    if settings.enable_chat_logging:
        middlewares.append(LoggingMiddleware())
    middlewares.append(RetryMiddleware(retries=settings.chat_retry_retries, delay=settings.chat_retry_delay))
    middlewares.append(ErrorCaptureMiddleware(log_path=settings.error_log_path))
    middlewares.append(TimeoutMiddleware(timeout=settings.chat_timeout))
    if settings.chat_fallback_provider or settings.chat_fallback_model:
        middlewares.append(FallbackMiddleware(settings.chat_fallback_provider, settings.chat_fallback_model))
    if settings.enable_chat_degrade:
        middlewares.append(DegradeOnErrorMiddleware(fallback_content=settings.degrade_message))
    return middlewares


class ChatRequest(BaseModel):
    doc_id: str
    question: str
    api_key: Optional[str] = None
    model: str
    api_provider: str
    selected_text: Optional[str] = None
    enable_vector_search: bool = True
    image_base64: Optional[str] = None
    top_k: int = 10  # å¢åŠ åˆ°10ï¼Œè·å–æ›´å¤šä¸Šä¸‹æ–‡
    candidate_k: int = 20
    use_rerank: bool = False
    reranker_model: Optional[str] = None
    rerank_provider: Optional[str] = None
    rerank_api_key: Optional[str] = None
    rerank_endpoint: Optional[str] = None
    doc_store_key: Optional[str] = None
    # æ–°å¢ï¼šæœ¯è¯­åº“å’Œè¡¨æ ¼ä¿æŠ¤é€‰é¡¹
    enable_glossary: bool = True  # æ˜¯å¦å¯ç”¨æœ¯è¯­åº“
    protect_tables: bool = True   # æ˜¯å¦ä¿æŠ¤è¡¨æ ¼ç»“æ„
    # æ·±åº¦æ€è€ƒæ¨¡å¼
    enable_thinking: bool = False  # æ˜¯å¦å¼€å¯æ·±åº¦æ€è€ƒ
    # æ¨¡å‹å‚æ•°ï¼ˆå‰ç«¯å¯è°ƒï¼‰
    max_tokens: int = 8192  # æœ€å¤§è¾“å‡º token æ•°
    temperature: float = 0.7  # æ¸©åº¦å‚æ•°
    top_p: float = 1.0  # æ ¸é‡‡æ ·å‚æ•°
    stream_output: bool = True  # æ˜¯å¦æµå¼è¾“å‡º
    # å¤šè½®å¯¹è¯å†å²ï¼ˆéœ€æ±‚ 3.2ï¼‰
    chat_history: Optional[List[dict]] = None  # [{"role": "user"|"assistant", "content": "..."}]


class ChatVisionRequest(BaseModel):
    doc_id: str
    question: str
    api_key: Optional[str] = None
    model: str
    api_provider: str
    image_base64: Optional[str] = None
    selected_text: Optional[str] = None


def _validate_rerank_request(req):
    provider = getattr(req, "rerank_provider", None)
    api_key = getattr(req, "rerank_api_key", None)
    use_rerank = getattr(req, "use_rerank", False)
    cloud_providers = {"cohere", "jina", "silicon", "aliyun", "openai", "moonshot", "deepseek", "zhipu", "minimax"}
    if use_rerank and provider and provider.lower() in cloud_providers and not api_key:
        raise HTTPException(status_code=400, detail=f"ä½¿ç”¨ {provider} rerank éœ€è¦æä¾› rerank_api_key")


@router.post("/chat")
async def chat_with_pdf(request: ChatRequest):
    if not hasattr(router, "documents_store"):
        raise HTTPException(status_code=500, detail="æ–‡æ¡£å­˜å‚¨æœªåˆå§‹åŒ–")
    store = router.documents_store if not request.doc_store_key else router.documents_store.get(request.doc_store_key, {})
    if request.doc_id not in store:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°")

    doc = store[request.doc_id]

    context = ""
    retrieval_meta = {}

    # æˆªå›¾æ¨¡å¼ï¼šè·³è¿‡å‘é‡æ£€ç´¢ï¼Œä½¿ç”¨ vision ä¸“ç”¨ç²¾ç®€ promptï¼Œè®©æ¨¡å‹ä¸“æ³¨åˆ†æå›¾ç‰‡
    if request.image_base64:
        print(f"[Chat] ğŸ“¸ æˆªå›¾æ¨¡å¼ï¼šè·³è¿‡å‘é‡æ£€ç´¢ï¼Œä½¿ç”¨ vision ä¸“ç”¨ prompt (model={request.model}, provider={request.api_provider})")
        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„PDFæ–‡æ¡£æ™ºèƒ½åŠ©æ‰‹ã€‚ç”¨æˆ·æ­£åœ¨æŸ¥çœ‹æ–‡æ¡£"{doc["filename"]}"ã€‚
ç”¨æˆ·ä»æ–‡æ¡£ä¸­æˆªå–äº†ä¸€å¼ å›¾ç‰‡å¹¶å‘é€ç»™ä½ ã€‚è¯·ä»”ç»†åˆ†æç”¨æˆ·å‘é€çš„å›¾ç‰‡å†…å®¹å¹¶å›ç­”é—®é¢˜ã€‚

å›ç­”è§„åˆ™ï¼š
1. ä»¥ç”¨æˆ·å‘é€çš„å›¾ç‰‡ä¸ºæ ¸å¿ƒä¾æ®è¿›è¡Œå›ç­”ï¼Œä¸è¦å‚è€ƒå…¶ä»–å†…å®¹ã€‚
2. å¦‚æœå›¾ç‰‡åŒ…å«å›¾è¡¨ï¼Œè¯·åˆ†ææ•°æ®è¶‹åŠ¿å’Œå…³é”®ä¿¡æ¯ã€‚
3. å¦‚æœå›¾ç‰‡åŒ…å«å…¬å¼ï¼Œè¯·ä½¿ç”¨ LaTeX æ ¼å¼ï¼ˆ$å…¬å¼$ï¼‰å±•ç¤ºã€‚
4. å¦‚æœå›¾ç‰‡åŒ…å«è¡¨æ ¼ï¼Œè¯·è½¬æ¢ä¸º Markdown æ ¼å¼ã€‚
5. ç®€æ´æ¸…æ™°ï¼Œå­¦æœ¯å‡†ç¡®ã€‚"""

        user_content = [
            {"type": "text", "text": request.question or "è¯·åˆ†æè¿™å¼ å›¾ç‰‡"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{request.image_base64}"}}
        ]
    else:
        # éæˆªå›¾æ¨¡å¼ï¼šæ­£å¸¸çš„æ–‡æœ¬æ£€ç´¢æµç¨‹
        if request.selected_text:
            context = f"ç”¨æˆ·é€‰ä¸­çš„æ–‡æœ¬ï¼š\n{request.selected_text}\n\n"
        elif request.enable_vector_search:
            _validate_rerank_request(request)
            
            # æ™ºèƒ½åˆ†ææŸ¥è¯¢ç±»å‹ï¼ŒåŠ¨æ€è°ƒæ•´top_k
            strategy = get_retrieval_strategy(request.question)
            dynamic_top_k = strategy['top_k']
            
            print(f"[Chat] æŸ¥è¯¢ç±»å‹: {strategy['query_type']}, åŠ¨æ€top_k: {dynamic_top_k}, åŸå› : {strategy['reasoning']}")
            
            # vector_context è¿”å›åŒ…å« context å’Œ retrieval_meta çš„å­—å…¸
            context_result = await vector_context(
                request.doc_id,
                request.question,
                vector_store_dir=router.vector_store_dir,
                pages=doc.get("data", {}).get("pages", []),
                api_key=request.api_key,
                top_k=dynamic_top_k,  # ä½¿ç”¨åŠ¨æ€è®¡ç®—çš„top_k
                candidate_k=max(request.candidate_k, dynamic_top_k),
                use_rerank=request.use_rerank,
                reranker_model=request.reranker_model,
                rerank_provider=request.rerank_provider,
                rerank_api_key=request.rerank_api_key,
                rerank_endpoint=request.rerank_endpoint,
                middlewares=[
                    *( [LoggingMiddleware()] if settings.enable_search_logging else [] ),
                    RetryMiddleware(retries=settings.search_retry_retries, delay=settings.search_retry_delay),
                    ErrorCaptureMiddleware()
                ]
            )
            relevant_text = context_result.get("context", "")
            retrieval_meta = context_result.get("retrieval_meta", {})
            if relevant_text:
                context = f"æ ¹æ®ç”¨æˆ·é—®é¢˜æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼š\n\n{relevant_text}\n\n"
            else:
                context = doc["data"]["full_text"][:8000]
        else:
            context = doc["data"]["full_text"][:8000]

        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„PDFæ–‡æ¡£æ™ºèƒ½åŠ©æ‰‹ã€‚ç”¨æˆ·æ­£åœ¨æŸ¥çœ‹æ–‡æ¡£"{doc["filename"]}"ã€‚
æ–‡æ¡£æ€»é¡µæ•°ï¼š{doc["data"]["total_pages"]}

æ–‡æ¡£å†…å®¹ï¼š
{context}

å›ç­”è§„åˆ™ï¼š
1. åŸºäºæ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”ï¼Œç®€æ´æ¸…æ™°ï¼Œå­¦æœ¯å‡†ç¡®ã€‚
2. ä¸è¦å£°æ˜ä½ æ˜¯å¦å…·å¤‡å¤–éƒ¨å·¥å…·/è”ç½‘ç­‰èƒ½åŠ›ï¼Œä¸è¦è¾“å‡ºä¸å›ç­”æ— å…³çš„å…è´£å£°æ˜ã€‚
3. ä¼˜å…ˆä¾æ®æ–‡æ¡£å†…å®¹å›ç­”ï¼›è‹¥æ–‡æ¡£ä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºå¸¸è¯†ç»™å‡ºæ¦‚è§ˆæ€§è§£ç­”å¹¶æ˜ç¡®ä¸ç¡®å®šä¹‹å¤„ã€‚
4. é‡åˆ°å…¬å¼ã€æ•°æ®ã€å›¾è¡¨ç­‰å…³é”®ä¿¡æ¯æ—¶ï¼Œå¿…é¡»ç›´æ¥å¼•ç”¨åŸæ–‡å±•ç¤ºå®Œæ•´å†…å®¹ï¼Œä¸è¦ä»…æ¦‚æ‹¬æè¿°ã€‚
   - ä¾‹å¦‚ç”¨æˆ·é—®"æœ‰ä»€ä¹ˆå…¬å¼"æ—¶ï¼Œåº”ç›´æ¥å±•ç¤ºå…¬å¼çš„å®Œæ•´è¡¨è¾¾å¼ã€‚
   - å¯¹äºæ•°å­¦å…¬å¼ï¼Œä¼˜å…ˆä½¿ç”¨LaTeXæ ¼å¼å±•ç¤ºï¼ˆ$å…¬å¼$ï¼‰ã€‚
5. ä¸è¦è¯´"æ ¹æ®æ‚¨æä¾›çš„æœ‰é™ç‰‡æ®µ"ã€"åŸºäºç‰‡æ®µ"ç­‰æš—ç¤ºä¿¡æ¯ä¸è¶³çš„æªè¾ï¼Œç›´æ¥å›ç­”é—®é¢˜ã€‚"""

        # é›†æˆæœ¯è¯­åº“ - åœ¨ system_prompt ä¸­æ³¨å…¥æœ¯è¯­æŒ‡ä»¤
        if request.enable_glossary:
            glossary_instruction = build_glossary_prompt(context)
            if glossary_instruction:
                system_prompt += f"\n\n{glossary_instruction}"

        # æ£€æµ‹ç”Ÿæˆç±»æŸ¥è¯¢ï¼ˆæ€ç»´å¯¼å›¾/æµç¨‹å›¾ï¼‰ï¼Œæ³¨å…¥å¯¹åº”ç³»ç»Ÿæç¤ºè¯
        generation_prompt = get_generation_prompt(request.question)
        if generation_prompt:
            system_prompt += f"\n\n{generation_prompt}"

        # å¼•æ–‡è¿½è¸ªï¼šå¦‚æœ retrieval_meta ä¸­åŒ…å« citationsï¼Œè¿½åŠ å¼•æ–‡æŒ‡ç¤ºæç¤ºè¯
        citations = retrieval_meta.get("citations", [])
        if citations:
            citation_prompt = _context_builder.build_citation_prompt(citations)
            if citation_prompt:
                system_prompt += f"\n\n{citation_prompt}"

        user_content = request.question

    messages = [
        {"role": "system", "content": system_prompt},
    ]
    # æ’å…¥å¤šè½®å¯¹è¯å†å²ï¼ˆéœ€æ±‚ 3.2ï¼šä½äº system prompt ä¹‹åã€å½“å‰ç”¨æˆ·æ¶ˆæ¯ä¹‹å‰ï¼‰
    if request.chat_history:
        for hist_msg in request.chat_history:
            if isinstance(hist_msg, dict) and hist_msg.get("role") in ("user", "assistant") and hist_msg.get("content"):
                messages.append({"role": hist_msg["role"], "content": hist_msg["content"]})
    messages.append({"role": "user", "content": user_content})

    middlewares = build_chat_middlewares()
    try:
        response = await call_ai_api(
            messages,
            request.api_key,
            request.model,
            request.api_provider,
            endpoint=PROVIDER_CONFIG.get(request.api_provider, {}).get("endpoint", ""),
            middlewares=middlewares,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p
        )
        message = response["choices"][0]["message"]
        answer = message["content"]
        reasoning_content = extract_reasoning_content(message)

        return {
            "answer": answer,
            "reasoning_content": reasoning_content,
            "doc_id": request.doc_id,
            "question": request.question,
            "timestamp": datetime.now().isoformat(),
            "used_provider": response.get("_used_provider"),
            "used_model": response.get("_used_model"),
            "fallback_used": response.get("_fallback_used", False),
            "retrieval_meta": retrieval_meta
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"AIè°ƒç”¨å¤±è´¥: {str(e)}")


@router.post("/chat/stream")
async def chat_with_pdf_stream(request: ChatRequest):
    if not hasattr(router, "documents_store"):
        raise HTTPException(status_code=500, detail="æ–‡æ¡£å­˜å‚¨æœªåˆå§‹åŒ–")
    store = router.documents_store if not request.doc_store_key else router.documents_store.get(request.doc_store_key, {})
    if request.doc_id not in store:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°")

    doc = store[request.doc_id]

    context = ""
    retrieval_meta = {}

    # æˆªå›¾æ¨¡å¼ï¼šè·³è¿‡å‘é‡æ£€ç´¢ï¼Œä½¿ç”¨ vision ä¸“ç”¨ç²¾ç®€ promptï¼Œè®©æ¨¡å‹ä¸“æ³¨åˆ†æå›¾ç‰‡
    if request.image_base64:
        print(f"[Chat Stream] ğŸ“¸ æˆªå›¾æ¨¡å¼ï¼šè·³è¿‡å‘é‡æ£€ç´¢ï¼Œä½¿ç”¨ vision ä¸“ç”¨ prompt (model={request.model}, provider={request.api_provider})")
        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„PDFæ–‡æ¡£æ™ºèƒ½åŠ©æ‰‹ã€‚ç”¨æˆ·æ­£åœ¨æŸ¥çœ‹æ–‡æ¡£"{doc["filename"]}"ã€‚
ç”¨æˆ·ä»æ–‡æ¡£ä¸­æˆªå–äº†ä¸€å¼ å›¾ç‰‡å¹¶å‘é€ç»™ä½ ã€‚è¯·ä»”ç»†åˆ†æç”¨æˆ·å‘é€çš„å›¾ç‰‡å†…å®¹å¹¶å›ç­”é—®é¢˜ã€‚

å›ç­”è§„åˆ™ï¼š
1. ä»¥ç”¨æˆ·å‘é€çš„å›¾ç‰‡ä¸ºæ ¸å¿ƒä¾æ®è¿›è¡Œå›ç­”ï¼Œä¸è¦å‚è€ƒå…¶ä»–å†…å®¹ã€‚
2. å¦‚æœå›¾ç‰‡åŒ…å«å›¾è¡¨ï¼Œè¯·åˆ†ææ•°æ®å’Œå…³é”®ä¿¡æ¯ã€‚
3. å¦‚æœå›¾ç‰‡åŒ…å«å…¬å¼ï¼Œè¯·ä½¿ç”¨ LaTeX æ ¼å¼ï¼ˆ$å…¬å¼$ï¼‰å±•ç¤ºã€‚
4. å¦‚æœå›¾ç‰‡åŒ…å«è¡¨æ ¼ï¼Œè¯·è½¬æ¢ä¸º Markdown æ ¼å¼ã€‚
5. ç®€æ´æ¸…æ™°ï¼Œå­¦æœ¯å‡†ç¡®ã€‚"""

        user_content = [
            {"type": "text", "text": request.question or "è¯·åˆ†æè¿™å¼ å›¾ç‰‡"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{request.image_base64}"}}
        ]
    else:
        # éæˆªå›¾æ¨¡å¼ï¼šæ­£å¸¸çš„æ–‡æœ¬æ£€ç´¢æµç¨‹
        if request.selected_text:
            context = f"ç”¨æˆ·é€‰ä¸­çš„æ–‡æœ¬ï¼š\n{request.selected_text}\n\n"
        elif request.enable_vector_search:
            _validate_rerank_request(request)
            
            # æ™ºèƒ½åˆ†ææŸ¥è¯¢ç±»å‹ï¼ŒåŠ¨æ€è°ƒæ•´top_k
            strategy = get_retrieval_strategy(request.question)
            dynamic_top_k = strategy['top_k']
            
            print(f"[Chat Stream] æŸ¥è¯¢ç±»å‹: {strategy['query_type']}, åŠ¨æ€top_k: {dynamic_top_k}, åŸå› : {strategy['reasoning']}")
            
            # vector_context è¿”å›åŒ…å« context å’Œ retrieval_meta çš„å­—å…¸
            context_result = await vector_context(
                request.doc_id,
                request.question,
                vector_store_dir=router.vector_store_dir,
                pages=doc.get("data", {}).get("pages", []),
                api_key=request.api_key,
                top_k=dynamic_top_k,  # ä½¿ç”¨åŠ¨æ€è®¡ç®—çš„top_k
                candidate_k=max(request.candidate_k, dynamic_top_k),
                use_rerank=request.use_rerank,
                reranker_model=request.reranker_model,
                rerank_provider=request.rerank_provider,
                rerank_api_key=request.rerank_api_key,
                rerank_endpoint=request.rerank_endpoint,
                middlewares=[
                    *( [LoggingMiddleware()] if settings.enable_search_logging else [] ),
                    RetryMiddleware(retries=settings.search_retry_retries, delay=settings.search_retry_delay)
                ]
            )
            relevant_text = context_result.get("context", "")
            retrieval_meta = context_result.get("retrieval_meta", {})
            if relevant_text:
                context = f"æ ¹æ®ç”¨æˆ·é—®é¢˜æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼š\n\n{relevant_text}\n\n"
            else:
                context = doc["data"]["full_text"][:8000]
        else:
            context = doc["data"]["full_text"][:8000]

        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„PDFæ–‡æ¡£æ™ºèƒ½åŠ©æ‰‹ã€‚ç”¨æˆ·æ­£åœ¨æŸ¥çœ‹æ–‡æ¡£"{doc["filename"]}"ã€‚
æ–‡æ¡£æ€»é¡µæ•°ï¼š{doc["data"]["total_pages"]}

æ–‡æ¡£å†…å®¹ï¼š
{context}

å›ç­”è§„åˆ™ï¼š
1. åŸºäºæ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”ï¼Œç®€æ´æ¸…æ™°ï¼Œå­¦æœ¯å‡†ç¡®ã€‚
2. ä¸è¦å£°æ˜ä½ æ˜¯å¦å…·å¤‡å¤–éƒ¨å·¥å…·/è”ç½‘ç­‰èƒ½åŠ›ï¼Œä¸è¦è¾“å‡ºä¸å›ç­”æ— å…³çš„å…è´£å£°æ˜ã€‚
3. ä¼˜å…ˆä¾æ®æ–‡æ¡£å†…å®¹å›ç­”ï¼›è‹¥æ–‡æ¡£ä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºå¸¸è¯†ç»™å‡ºæ¦‚è§ˆæ€§è§£ç­”å¹¶æ˜ç¡®ä¸ç¡®å®šä¹‹å¤„ã€‚
4. é‡åˆ°å…¬å¼ã€æ•°æ®ã€å›¾è¡¨ç­‰å…³é”®ä¿¡æ¯æ—¶ï¼Œå¿…é¡»ç›´æ¥å¼•ç”¨åŸæ–‡å±•ç¤ºå®Œæ•´å†…å®¹ï¼Œä¸è¦ä»…æ¦‚æ‹¬æè¿°ã€‚
   - ä¾‹å¦‚ç”¨æˆ·é—®"æœ‰ä»€ä¹ˆå…¬å¼"æ—¶ï¼Œåº”ç›´æ¥å±•ç¤ºå…¬å¼çš„å®Œæ•´è¡¨è¾¾å¼ã€‚
   - å¯¹äºæ•°å­¦å…¬å¼ï¼Œä¼˜å…ˆä½¿ç”¨LaTeXæ ¼å¼å±•ç¤ºï¼ˆ$å…¬å¼$ï¼‰ã€‚
5. ä¸è¦è¯´"æ ¹æ®æ‚¨æä¾›çš„æœ‰é™ç‰‡æ®µ"ã€"åŸºäºç‰‡æ®µ"ç­‰æš—ç¤ºä¿¡æ¯ä¸è¶³çš„æªè¾ï¼Œç›´æ¥å›ç­”é—®é¢˜ã€‚"""

        # æ£€æµ‹ç”Ÿæˆç±»æŸ¥è¯¢ï¼ˆæ€ç»´å¯¼å›¾/æµç¨‹å›¾ï¼‰ï¼Œæ³¨å…¥å¯¹åº”ç³»ç»Ÿæç¤ºè¯
        generation_prompt = get_generation_prompt(request.question)
        if generation_prompt:
            system_prompt += f"\n\n{generation_prompt}"

        # å¼•æ–‡è¿½è¸ªï¼šå¦‚æœ retrieval_meta ä¸­åŒ…å« citationsï¼Œè¿½åŠ å¼•æ–‡æŒ‡ç¤ºæç¤ºè¯
        citations = retrieval_meta.get("citations", [])
        if citations:
            citation_prompt = _context_builder.build_citation_prompt(citations)
            if citation_prompt:
                system_prompt += f"\n\n{citation_prompt}"

        user_content = request.question

    messages = [
        {"role": "system", "content": system_prompt},
    ]
    # æ’å…¥å¤šè½®å¯¹è¯å†å²ï¼ˆéœ€æ±‚ 3.2ï¼šä½äº system prompt ä¹‹åã€å½“å‰ç”¨æˆ·æ¶ˆæ¯ä¹‹å‰ï¼‰
    if request.chat_history:
        for hist_msg in request.chat_history:
            if isinstance(hist_msg, dict) and hist_msg.get("role") in ("user", "assistant") and hist_msg.get("content"):
                messages.append({"role": hist_msg["role"], "content": hist_msg["content"]})
    messages.append({"role": "user", "content": user_content})

    async def event_generator():
        try:
            middlewares = build_chat_middlewares()
            async for chunk in call_ai_api_stream(
                messages,
                request.api_key,
                request.model,
                request.api_provider,
                endpoint=PROVIDER_CONFIG.get(request.api_provider, {}).get("endpoint", ""),
                middlewares=middlewares,
                enable_thinking=request.enable_thinking,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p
            ):
                if chunk.get("error"):
                    yield f"data: {json.dumps({'error': chunk['error']})}\n\n"
                    break
                chunk_data = {
                    'content': chunk.get('content', ''),
                    'reasoning_content': chunk.get('reasoning_content', ''),
                    'done': chunk.get('done', False),
                    'used_provider': chunk.get('used_provider'),
                    'used_model': chunk.get('used_model'),
                    'fallback_used': chunk.get('fallback_used'),
                }
                # åœ¨æœ€åä¸€ä¸ª chunk ä¸­é™„å¸¦ retrieval_metaï¼ˆå« citationsï¼‰
                if chunk.get("done"):
                    chunk_data['retrieval_meta'] = retrieval_meta
                yield f"data: {json.dumps(chunk_data)}\n\n"
                if chunk.get("done"):
                    yield "data: [DONE]\n\n"
                    break

        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")
