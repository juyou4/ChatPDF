from datetime import datetime
from typing import Optional, List
import json
import logging
import threading

from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from services.chat_service import call_ai_api, call_ai_api_stream, extract_reasoning_content
from services.vector_service import vector_context
from services.retrieval_agent import RetrievalAgent
from services.retrieval_tools import DocContext
from services.glossary_service import glossary_service, build_glossary_prompt
from services.table_service import protect_markdown_tables, restore_markdown_tables
from services.query_analyzer import get_retrieval_strategy
from services.preset_service import get_generation_prompt
from services.context_builder import ContextBuilder
from models.provider_registry import PROVIDER_CONFIG
from utils.middleware import (
    LoggingMiddleware,
    RetryMiddleware,
    ErrorCaptureMiddleware,
    DegradeOnErrorMiddleware,
    TimeoutMiddleware,
    FallbackMiddleware,
)
from config import settings

logger = logging.getLogger(__name__)

router = APIRouter()

# ä¸Šä¸‹æ–‡æ„å»ºå™¨å®ä¾‹ï¼Œç”¨äºç”Ÿæˆå¼•æ–‡æŒ‡ç¤ºæç¤ºè¯
_context_builder = ContextBuilder()

# æ¨¡å—çº§å˜é‡ï¼Œç”± app.py æ³¨å…¥ MemoryService å®ä¾‹
memory_service = None


def build_chat_middlewares():
    middlewares = []
    if settings.enable_chat_logging:
        middlewares.append(LoggingMiddleware())
    middlewares.append(RetryMiddleware(retries=settings.chat_retry_retries, delay=settings.chat_retry_delay))
    middlewares.append(ErrorCaptureMiddleware(log_path=settings.error_log_path))
    middlewares.append(TimeoutMiddleware(timeout=settings.chat_timeout))
    if settings.chat_fallback_provider or settings.chat_fallback_model:
        middlewares.append(FallbackMiddleware(settings.chat_fallback_provider, settings.chat_fallback_model))
    if settings.enable_chat_degrade:
        middlewares.append(DegradeOnErrorMiddleware(fallback_content=settings.degrade_message))
    return middlewares


class ChatRequest(BaseModel):
    doc_id: str
    question: str
    api_key: Optional[str] = None
    model: str
    api_provider: str
    selected_text: Optional[str] = None
    enable_vector_search: bool = True
    image_base64: Optional[str] = None
    top_k: int = 10  # å¢åŠ åˆ°10ï¼Œè·å–æ›´å¤šä¸Šä¸‹æ–‡
    candidate_k: int = 20
    use_rerank: bool = False
    reranker_model: Optional[str] = None
    rerank_provider: Optional[str] = None
    rerank_api_key: Optional[str] = None
    rerank_endpoint: Optional[str] = None
    doc_store_key: Optional[str] = None
    # æ–°å¢ï¼šæœ¯è¯­åº“å’Œè¡¨æ ¼ä¿æŠ¤é€‰é¡¹
    enable_glossary: bool = True  # æ˜¯å¦å¯ç”¨æœ¯è¯­åº“
    protect_tables: bool = True   # æ˜¯å¦ä¿æŠ¤è¡¨æ ¼ç»“æ„
    # æ·±åº¦æ€è€ƒæ¨¡å¼
    enable_thinking: bool = False  # æ˜¯å¦å¼€å¯æ·±åº¦æ€è€ƒ
    # æ¨¡å‹å‚æ•°ï¼ˆå‰ç«¯å¯è°ƒï¼ŒNone è¡¨ç¤ºä¸ä¼ ï¼Œç”±æ¨¡å‹ä½¿ç”¨é»˜è®¤å€¼ï¼‰
    max_tokens: Optional[int] = None  # æœ€å¤§è¾“å‡º token æ•°
    temperature: Optional[float] = None  # æ¸©åº¦å‚æ•°
    top_p: Optional[float] = None  # æ ¸é‡‡æ ·å‚æ•°
    custom_params: Optional[dict] = None  # è‡ªå®šä¹‰å‚æ•° {key: value}ï¼Œç›´æ¥é€ä¼ ç»™ API
    reasoning_effort: Optional[str] = None  # æ·±åº¦æ€è€ƒåŠ›åº¦ï¼ˆ'low'|'medium'|'high'ï¼‰
    stream_output: bool = True  # æ˜¯å¦æµå¼è¾“å‡º
    # å¤šè½®å¯¹è¯å†å²ï¼ˆéœ€æ±‚ 3.2ï¼‰
    chat_history: Optional[List[dict]] = None  # [{"role": "user"|"assistant", "content": "..."}]
    # è®°å¿†åŠŸèƒ½å¼€å…³ï¼ˆéœ€æ±‚ 5.4ï¼‰
    enable_memory: bool = True  # æ˜¯å¦å¯ç”¨è®°å¿†åŠŸèƒ½
    # Agent å¤šè½®æ£€ç´¢ï¼ˆéœ€æ±‚ P0ï¼‰
    enable_agent_retrieval: bool = False  # æ˜¯å¦å¯ç”¨å¤šè½® Agent æ£€ç´¢


class ChatVisionRequest(BaseModel):
    doc_id: str
    question: str
    api_key: Optional[str] = None
    model: str
    api_provider: str
    image_base64: Optional[str] = None
    selected_text: Optional[str] = None


def _validate_rerank_request(req):
    provider = getattr(req, "rerank_provider", None)
    api_key = getattr(req, "rerank_api_key", None)
    use_rerank = getattr(req, "use_rerank", False)
    cloud_providers = {"cohere", "jina", "silicon", "aliyun", "openai", "moonshot", "deepseek", "zhipu", "minimax"}
    if use_rerank and provider and provider.lower() in cloud_providers and not api_key:
        raise HTTPException(status_code=400, detail=f"ä½¿ç”¨ {provider} rerank éœ€è¦æä¾› rerank_api_key")


def _retrieve_memory_context(question: str, api_key: str = None, doc_id: str = None) -> str:
    """æ£€ç´¢è®°å¿†ä¸Šä¸‹æ–‡ï¼Œå¼‚å¸¸æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²ï¼ˆéœ€æ±‚ 5.1, 5.5ï¼‰
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        api_key: API å¯†é’¥
        doc_id: å½“å‰æ–‡æ¡£ IDï¼Œç”¨äºæ–‡æ¡£ç›¸å…³æ€§åŠ æƒ
    """
    if memory_service is None:
        return ""
    try:
        # ä½¿ç”¨æ–‡æ¡£ç›¸å…³æ€§åŠ æƒï¼Œä½†ä¸è¿‡æ»¤ï¼ˆä¿ç•™è·¨æ–‡æ¡£è®°å¿†ï¼‰
        return memory_service.retrieve_memories(
            question, api_key=api_key, doc_id=doc_id, filter_by_doc=False
        )
    except Exception as e:
        logger.error(f"è®°å¿†æ£€ç´¢å¤±è´¥: {e}")
        return ""


def _async_memory_write(svc, request):
    """å¼‚æ­¥è®°å¿†å†™å…¥ï¼šæå– QA æ‘˜è¦ + æ›´æ–°å…³é”®è¯ï¼ˆéœ€æ±‚ 5.3ï¼‰"""
    try:
        # æå– QA æ‘˜è¦ï¼ˆä¼ å…¥ LLM å‚æ•°ç”¨äºè®°å¿†æç‚¼ï¼‰
        if request.doc_id:
            # æ„å»ºå®Œæ•´å¯¹è¯å†å²ï¼ˆåŒ…å«å½“å‰é—®é¢˜ï¼‰
            history = list(request.chat_history or [])
            history.append({"role": "user", "content": request.question})
            svc.save_qa_summary(
                request.doc_id,
                history,
                api_key=getattr(request, "api_key", None),
                model=getattr(request, "model", None),
                api_provider=getattr(request, "api_provider", None),
            )
        # æ›´æ–°å…³é”®è¯ç»Ÿè®¡
        svc.update_keywords(request.question)
    except Exception as e:
        logger.error(f"å¼‚æ­¥è®°å¿†å†™å…¥å¤±è´¥: {e}")


# è·Ÿè¸ªå·² flush è¿‡çš„ doc_idï¼Œé˜²æ­¢åŒä¸€ä¼šè¯é‡å¤ flush
_flushed_sessions: set = set()


def _maybe_flush_memory(request) -> None:
    """å½“ chat_history è¾ƒé•¿æ—¶ï¼Œæå‰è§¦å‘ä¸€æ¬¡è®°å¿†å†™å…¥ï¼ˆå€Ÿé‰´ OpenClaw memoryFlushï¼‰

    é˜²æ­¢é•¿ä¼šè¯ä¸­é—´è½®æ¬¡çš„é‡è¦ä¿¡æ¯ä¸¢å¤±ã€‚æ¯ä¸ª doc_id æ¯æ¬¡ä¼šè¯åª flush ä¸€æ¬¡ã€‚
    
    ä¼˜åŒ–ç‚¹ï¼š
    1. ä½¿ç”¨ç²¾ç¡®çš„ token ä¼°ç®—ï¼ˆè€ƒè™‘ä¸­è‹±æ–‡å·®å¼‚ï¼‰
    2. åŸºäºé…ç½®åŒ–é˜ˆå€¼è§¦å‘
    3. æ”¯æŒç¦ç”¨å¼€å…³
    """
    if memory_service is None:
        return
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨è®°å¿†åˆ·æ–°
    if not settings.memory_flush_enabled:
        return
    
    history = getattr(request, "chat_history", None)
    if not history:
        return
    
    doc_id = getattr(request, "doc_id", "")
    if not doc_id or doc_id in _flushed_sessions:
        return

    # ä½¿ç”¨ç²¾ç¡®çš„ token ä¼°ç®—ï¼ˆè€ƒè™‘ä¸­è‹±æ–‡å·®å¼‚ï¼‰
    from services.token_budget import TokenBudget
    budget = TokenBudget()
    
    total_tokens = 0
    for msg in history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if content:
                total_tokens += budget.estimate_tokens(content)
    
    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
    threshold = settings.memory_flush_threshold_tokens
    if total_tokens < threshold:
        return

    _flushed_sessions.add(doc_id)
    logger.info(f"[Memory] Compaction flush è§¦å‘: doc_id={doc_id}, tokens={total_tokens}, threshold={threshold}")
    threading.Thread(
        target=_async_memory_write,
        args=(memory_service, request),
        daemon=True,
    ).start()


def _should_use_memory(request) -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”å¯ç”¨è®°å¿†åŠŸèƒ½ï¼ˆéœ€æ±‚ 5.4ï¼‰"""
    return (
        settings.memory_enabled
        and getattr(request, "enable_memory", True)
        and memory_service is not None
    )


def _inject_memory_context(system_prompt: str, memory_context: str) -> str:
    """å°†è®°å¿†ä¸Šä¸‹æ–‡æ³¨å…¥ system promptï¼ˆéœ€æ±‚ 5.2ï¼‰
    æ ¼å¼ï¼šåœ¨æ–‡æ¡£å†…å®¹ä¹‹åã€å›ç­”è§„åˆ™ä¹‹å‰æ’å…¥è®°å¿†æ®µè½"""
    if not memory_context:
        return system_prompt
    # åœ¨"å›ç­”è§„åˆ™ï¼š"ä¹‹å‰æ’å…¥è®°å¿†ä¸Šä¸‹æ–‡
    marker = "\nå›ç­”è§„åˆ™ï¼š"
    if marker in system_prompt:
        idx = system_prompt.index(marker)
        return (
            system_prompt[:idx]
            + f"\n\nç”¨æˆ·å†å²è®°å¿†ï¼š\n{memory_context}"
            + system_prompt[idx:]
        )
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡è®°ï¼Œè¿½åŠ åˆ°æœ«å°¾
    return system_prompt + f"\n\nç”¨æˆ·å†å²è®°å¿†ï¼š\n{memory_context}"


@router.post("/chat")
async def chat_with_pdf(request: ChatRequest):
    if not hasattr(router, "documents_store"):
        raise HTTPException(status_code=500, detail="æ–‡æ¡£å­˜å‚¨æœªåˆå§‹åŒ–")
    store = router.documents_store if not request.doc_store_key else router.documents_store.get(request.doc_store_key, {})
    if request.doc_id not in store:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°")

    doc = store[request.doc_id]

    context = ""
    retrieval_meta = {}
    use_memory = _should_use_memory(request)

    # Compaction å‰è‡ªåŠ¨ flushï¼šé•¿ä¼šè¯æå‰ä¿å­˜è®°å¿†ï¼ˆå€Ÿé‰´ OpenClawï¼‰
    if use_memory:
        _maybe_flush_memory(request)

    # è®°å¿†æ£€ç´¢ï¼šåœ¨æ„å»º system prompt ä¹‹å‰æ‰§è¡Œï¼ˆéœ€æ±‚ 5.1ï¼‰
    memory_context = ""
    if use_memory:
        memory_context = _retrieve_memory_context(
            request.question, api_key=request.api_key, doc_id=request.doc_id
        )

    # æˆªå›¾æ¨¡å¼ï¼šè·³è¿‡å‘é‡æ£€ç´¢ï¼Œä½¿ç”¨ vision ä¸“ç”¨ç²¾ç®€ promptï¼Œè®©æ¨¡å‹ä¸“æ³¨åˆ†æå›¾ç‰‡
    if request.image_base64:
        print(f"[Chat] ğŸ“¸ æˆªå›¾æ¨¡å¼ï¼šè·³è¿‡å‘é‡æ£€ç´¢ï¼Œä½¿ç”¨ vision ä¸“ç”¨ prompt (model={request.model}, provider={request.api_provider})")
        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„PDFæ–‡æ¡£æ™ºèƒ½åŠ©æ‰‹ã€‚ç”¨æˆ·æ­£åœ¨æŸ¥çœ‹æ–‡æ¡£"{doc["filename"]}"ã€‚
ç”¨æˆ·ä»æ–‡æ¡£ä¸­æˆªå–äº†ä¸€å¼ å›¾ç‰‡å¹¶å‘é€ç»™ä½ ã€‚è¯·ä»”ç»†åˆ†æç”¨æˆ·å‘é€çš„å›¾ç‰‡å†…å®¹å¹¶å›ç­”é—®é¢˜ã€‚

å›ç­”è§„åˆ™ï¼š
1. ä»¥ç”¨æˆ·å‘é€çš„å›¾ç‰‡ä¸ºæ ¸å¿ƒä¾æ®è¿›è¡Œå›ç­”ï¼Œä¸è¦å‚è€ƒå…¶ä»–å†…å®¹ã€‚
2. å¦‚æœå›¾ç‰‡åŒ…å«å›¾è¡¨ï¼Œè¯·åˆ†ææ•°æ®è¶‹åŠ¿å’Œå…³é”®ä¿¡æ¯ã€‚
3. å¦‚æœå›¾ç‰‡åŒ…å«å…¬å¼ï¼Œè¯·ä½¿ç”¨ LaTeX æ ¼å¼ï¼ˆ$å…¬å¼$ï¼‰å±•ç¤ºã€‚
4. å¦‚æœå›¾ç‰‡åŒ…å«è¡¨æ ¼ï¼Œè¯·è½¬æ¢ä¸º Markdown æ ¼å¼ã€‚
5. ç®€æ´æ¸…æ™°ï¼Œå­¦æœ¯å‡†ç¡®ã€‚"""

        # æˆªå›¾æ¨¡å¼ä¹Ÿæ³¨å…¥è®°å¿†ä¸Šä¸‹æ–‡ï¼ˆéœ€æ±‚ 5.2ï¼‰
        system_prompt = _inject_memory_context(system_prompt, memory_context)

        user_content = [
            {"type": "text", "text": request.question or "è¯·åˆ†æè¿™å¼ å›¾ç‰‡"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{request.image_base64}"}}
        ]
    else:
        # éæˆªå›¾æ¨¡å¼ï¼šæ­£å¸¸çš„æ–‡æœ¬æ£€ç´¢æµç¨‹
        if request.selected_text:
            context = f"ç”¨æˆ·é€‰ä¸­çš„æ–‡æœ¬ï¼š\n{request.selected_text}\n\n"
        elif request.enable_vector_search:
            _validate_rerank_request(request)
            
            # æ™ºèƒ½åˆ†ææŸ¥è¯¢ç±»å‹ï¼ŒåŠ¨æ€è°ƒæ•´top_k
            strategy = get_retrieval_strategy(request.question)
            dynamic_top_k = strategy['top_k']
            
            print(f"[Chat] æŸ¥è¯¢ç±»å‹: {strategy['query_type']}, åŠ¨æ€top_k: {dynamic_top_k}, åŸå› : {strategy['reasoning']}")
            
            # vector_context è¿”å›åŒ…å« context å’Œ retrieval_meta çš„å­—å…¸
            context_result = await vector_context(
                request.doc_id,
                request.question,
                vector_store_dir=router.vector_store_dir,
                pages=doc.get("data", {}).get("pages", []),
                api_key=request.api_key,
                top_k=dynamic_top_k,  # ä½¿ç”¨åŠ¨æ€è®¡ç®—çš„top_k
                candidate_k=max(request.candidate_k, dynamic_top_k),
                use_rerank=request.use_rerank,
                reranker_model=request.reranker_model,
                rerank_provider=request.rerank_provider,
                rerank_api_key=request.rerank_api_key,
                rerank_endpoint=request.rerank_endpoint,
                middlewares=[
                    *( [LoggingMiddleware()] if settings.enable_search_logging else [] ),
                    RetryMiddleware(retries=settings.search_retry_retries, delay=settings.search_retry_delay),
                    ErrorCaptureMiddleware()
                ]
            )
            relevant_text = context_result.get("context", "")
            retrieval_meta = context_result.get("retrieval_meta", {})
            if relevant_text:
                context = f"æ ¹æ®ç”¨æˆ·é—®é¢˜æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼š\n\n{relevant_text}\n\n"
            else:
                context = doc["data"]["full_text"][:8000]
        else:
            context = doc["data"]["full_text"][:8000]

        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„PDFæ–‡æ¡£æ™ºèƒ½åŠ©æ‰‹ã€‚ç”¨æˆ·æ­£åœ¨æŸ¥çœ‹æ–‡æ¡£"{doc["filename"]}"ã€‚
æ–‡æ¡£æ€»é¡µæ•°ï¼š{doc["data"]["total_pages"]}

æ–‡æ¡£å†…å®¹ï¼š
{context}

å›ç­”è§„åˆ™ï¼š
1. åŸºäºæ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”ï¼Œç®€æ´æ¸…æ™°ï¼Œå­¦æœ¯å‡†ç¡®ã€‚
2. ä¸è¦å£°æ˜ä½ æ˜¯å¦å…·å¤‡å¤–éƒ¨å·¥å…·/è”ç½‘ç­‰èƒ½åŠ›ï¼Œä¸è¦è¾“å‡ºä¸å›ç­”æ— å…³çš„å…è´£å£°æ˜ã€‚
3. ä¼˜å…ˆä¾æ®æ–‡æ¡£å†…å®¹å›ç­”ï¼›è‹¥æ–‡æ¡£ä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºå¸¸è¯†ç»™å‡ºæ¦‚è§ˆæ€§è§£ç­”å¹¶æ˜ç¡®ä¸ç¡®å®šä¹‹å¤„ã€‚
4. é‡åˆ°å…¬å¼ã€æ•°æ®ã€å›¾è¡¨ç­‰å…³é”®ä¿¡æ¯æ—¶ï¼Œå¿…é¡»ç›´æ¥å¼•ç”¨åŸæ–‡å±•ç¤ºå®Œæ•´å†…å®¹ï¼Œä¸è¦ä»…æ¦‚æ‹¬æè¿°ã€‚
   - ä¾‹å¦‚ç”¨æˆ·é—®"æœ‰ä»€ä¹ˆå…¬å¼"æ—¶ï¼Œåº”ç›´æ¥å±•ç¤ºå…¬å¼çš„å®Œæ•´è¡¨è¾¾å¼ã€‚
   - å¯¹äºæ•°å­¦å…¬å¼ï¼Œä¼˜å…ˆä½¿ç”¨LaTeXæ ¼å¼å±•ç¤ºï¼ˆ$å…¬å¼$ï¼‰ã€‚
5. ä¸è¦è¯´"æ ¹æ®æ‚¨æä¾›çš„æœ‰é™ç‰‡æ®µ"ã€"åŸºäºç‰‡æ®µ"ç­‰æš—ç¤ºä¿¡æ¯ä¸è¶³çš„æªè¾ï¼Œç›´æ¥å›ç­”é—®é¢˜ã€‚"""

        # é›†æˆæœ¯è¯­åº“ - åœ¨ system_prompt ä¸­æ³¨å…¥æœ¯è¯­æŒ‡ä»¤
        if request.enable_glossary:
            glossary_instruction = build_glossary_prompt(context)
            if glossary_instruction:
                system_prompt += f"\n\n{glossary_instruction}"

        # æ£€æµ‹ç”Ÿæˆç±»æŸ¥è¯¢ï¼ˆæ€ç»´å¯¼å›¾/æµç¨‹å›¾ï¼‰ï¼Œæ³¨å…¥å¯¹åº”ç³»ç»Ÿæç¤ºè¯
        generation_prompt = get_generation_prompt(request.question)
        if generation_prompt:
            system_prompt += f"\n\n{generation_prompt}"

        # å¼•æ–‡è¿½è¸ªï¼šå¦‚æœ retrieval_meta ä¸­åŒ…å« citationsï¼Œè¿½åŠ å¼•æ–‡æŒ‡ç¤ºæç¤ºè¯
        citations = retrieval_meta.get("citations", [])
        if citations:
            citation_prompt = _context_builder.build_citation_prompt(citations)
            if citation_prompt:
                system_prompt += f"\n\n{citation_prompt}"

        # æ³¨å…¥è®°å¿†ä¸Šä¸‹æ–‡åˆ° system promptï¼ˆéœ€æ±‚ 5.2ï¼‰
        system_prompt = _inject_memory_context(system_prompt, memory_context)

        user_content = request.question

    messages = [
        {"role": "system", "content": system_prompt},
    ]
    # æ’å…¥å¤šè½®å¯¹è¯å†å²ï¼ˆéœ€æ±‚ 3.2ï¼šä½äº system prompt ä¹‹åã€å½“å‰ç”¨æˆ·æ¶ˆæ¯ä¹‹å‰ï¼‰
    if request.chat_history:
        for hist_msg in request.chat_history:
            if isinstance(hist_msg, dict) and hist_msg.get("role") in ("user", "assistant") and hist_msg.get("content"):
                messages.append({"role": hist_msg["role"], "content": hist_msg["content"]})
    messages.append({"role": "user", "content": user_content})

    middlewares = build_chat_middlewares()
    try:
        response = await call_ai_api(
            messages,
            request.api_key,
            request.model,
            request.api_provider,
            endpoint=PROVIDER_CONFIG.get(request.api_provider, {}).get("endpoint", ""),
            middlewares=middlewares,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
            custom_params=request.custom_params,
            reasoning_effort=request.reasoning_effort,
        )
        message = response["choices"][0]["message"]
        answer = message["content"]
        reasoning_content = extract_reasoning_content(message)

        # å¼‚æ­¥è§¦å‘è®°å¿†å†™å…¥ï¼ˆéœ€æ±‚ 5.3ï¼‰
        if use_memory:
            threading.Thread(
                target=_async_memory_write,
                args=(memory_service, request),
                daemon=True,
            ).start()

        return {
            "answer": answer,
            "reasoning_content": reasoning_content,
            "doc_id": request.doc_id,
            "question": request.question,
            "timestamp": datetime.now().isoformat(),
            "used_provider": response.get("_used_provider"),
            "used_model": response.get("_used_model"),
            "fallback_used": response.get("_fallback_used", False),
            "retrieval_meta": retrieval_meta
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"AIè°ƒç”¨å¤±è´¥: {str(e)}")


@router.post("/chat/stream")
async def chat_with_pdf_stream(request: ChatRequest):
    if not hasattr(router, "documents_store"):
        raise HTTPException(status_code=500, detail="æ–‡æ¡£å­˜å‚¨æœªåˆå§‹åŒ–")
    store = router.documents_store if not request.doc_store_key else router.documents_store.get(request.doc_store_key, {})
    if request.doc_id not in store:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°")

    doc = store[request.doc_id]

    context = ""
    retrieval_meta = {}
    use_agent = False
    use_memory = _should_use_memory(request)

    # è®°å¿†æ£€ç´¢ï¼šåœ¨æ„å»º system prompt ä¹‹å‰æ‰§è¡Œï¼ˆéœ€æ±‚ 5.1ï¼‰
    memory_context = ""
    if use_memory:
        memory_context = _retrieve_memory_context(
            request.question, api_key=request.api_key, doc_id=request.doc_id
        )

    # æˆªå›¾æ¨¡å¼ï¼šè·³è¿‡å‘é‡æ£€ç´¢ï¼Œä½¿ç”¨ vision ä¸“ç”¨ç²¾ç®€ promptï¼Œè®©æ¨¡å‹ä¸“æ³¨åˆ†æå›¾ç‰‡
    if request.image_base64:
        print(f"[Chat Stream] ğŸ“¸ æˆªå›¾æ¨¡å¼ï¼šè·³è¿‡å‘é‡æ£€ç´¢ï¼Œä½¿ç”¨ vision ä¸“ç”¨ prompt (model={request.model}, provider={request.api_provider})")
        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„PDFæ–‡æ¡£æ™ºèƒ½åŠ©æ‰‹ã€‚ç”¨æˆ·æ­£åœ¨æŸ¥çœ‹æ–‡æ¡£"{doc["filename"]}"ã€‚
ç”¨æˆ·ä»æ–‡æ¡£ä¸­æˆªå–äº†ä¸€å¼ å›¾ç‰‡å¹¶å‘é€ç»™ä½ ã€‚è¯·ä»”ç»†åˆ†æç”¨æˆ·å‘é€çš„å›¾ç‰‡å†…å®¹å¹¶å›ç­”é—®é¢˜ã€‚

å›ç­”è§„åˆ™ï¼š
1. ä»¥ç”¨æˆ·å‘é€çš„å›¾ç‰‡ä¸ºæ ¸å¿ƒä¾æ®è¿›è¡Œå›ç­”ï¼Œä¸è¦å‚è€ƒå…¶ä»–å†…å®¹ã€‚
2. å¦‚æœå›¾ç‰‡åŒ…å«å›¾è¡¨ï¼Œè¯·åˆ†ææ•°æ®å’Œå…³é”®ä¿¡æ¯ã€‚
3. å¦‚æœå›¾ç‰‡åŒ…å«å…¬å¼ï¼Œè¯·ä½¿ç”¨ LaTeX æ ¼å¼ï¼ˆ$å…¬å¼$ï¼‰å±•ç¤ºã€‚
4. å¦‚æœå›¾ç‰‡åŒ…å«è¡¨æ ¼ï¼Œè¯·è½¬æ¢ä¸º Markdown æ ¼å¼ã€‚
5. ç®€æ´æ¸…æ™°ï¼Œå­¦æœ¯å‡†ç¡®ã€‚"""

        # æˆªå›¾æ¨¡å¼ä¹Ÿæ³¨å…¥è®°å¿†ä¸Šä¸‹æ–‡ï¼ˆéœ€æ±‚ 5.2ï¼‰
        system_prompt = _inject_memory_context(system_prompt, memory_context)

        user_content = [
            {"type": "text", "text": request.question or "è¯·åˆ†æè¿™å¼ å›¾ç‰‡"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{request.image_base64}"}}
        ]
    else:
        # éæˆªå›¾æ¨¡å¼ï¼šæ­£å¸¸çš„æ–‡æœ¬æ£€ç´¢æµç¨‹
        # Agent æ¨¡å¼æ ‡å¿—ï¼šåç»­åœ¨ event_generator ä¸­ä½¿ç”¨
        use_agent = request.enable_agent_retrieval and not request.selected_text

        if request.selected_text:
            context = f"ç”¨æˆ·é€‰ä¸­çš„æ–‡æœ¬ï¼š\n{request.selected_text}\n\n"
        elif use_agent:
            # Agent æ¨¡å¼ï¼šä¸Šä¸‹æ–‡ç”± event_generator ä¸­çš„ RetrievalAgent åŠ¨æ€ç”Ÿæˆ
            # æ­¤å¤„ä»…è®¾ç½®å ä½ï¼Œå®é™…ä¸Šä¸‹æ–‡åœ¨æµå¼ç”Ÿæˆå™¨ä¸­å¡«å……
            context = ""
        elif request.enable_vector_search:
            _validate_rerank_request(request)
            
            # æ™ºèƒ½åˆ†ææŸ¥è¯¢ç±»å‹ï¼ŒåŠ¨æ€è°ƒæ•´top_k
            strategy = get_retrieval_strategy(request.question)
            dynamic_top_k = strategy['top_k']
            
            print(f"[Chat Stream] æŸ¥è¯¢ç±»å‹: {strategy['query_type']}, åŠ¨æ€top_k: {dynamic_top_k}, åŸå› : {strategy['reasoning']}")
            
            # vector_context è¿”å›åŒ…å« context å’Œ retrieval_meta çš„å­—å…¸
            context_result = await vector_context(
                request.doc_id,
                request.question,
                vector_store_dir=router.vector_store_dir,
                pages=doc.get("data", {}).get("pages", []),
                api_key=request.api_key,
                top_k=dynamic_top_k,  # ä½¿ç”¨åŠ¨æ€è®¡ç®—çš„top_k
                candidate_k=max(request.candidate_k, dynamic_top_k),
                use_rerank=request.use_rerank,
                reranker_model=request.reranker_model,
                rerank_provider=request.rerank_provider,
                rerank_api_key=request.rerank_api_key,
                rerank_endpoint=request.rerank_endpoint,
                middlewares=[
                    *( [LoggingMiddleware()] if settings.enable_search_logging else [] ),
                    RetryMiddleware(retries=settings.search_retry_retries, delay=settings.search_retry_delay)
                ]
            )
            relevant_text = context_result.get("context", "")
            retrieval_meta = context_result.get("retrieval_meta", {})
            if relevant_text:
                context = f"æ ¹æ®ç”¨æˆ·é—®é¢˜æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼š\n\n{relevant_text}\n\n"
            else:
                context = doc["data"]["full_text"][:8000]
        else:
            context = doc["data"]["full_text"][:8000]

        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„PDFæ–‡æ¡£æ™ºèƒ½åŠ©æ‰‹ã€‚ç”¨æˆ·æ­£åœ¨æŸ¥çœ‹æ–‡æ¡£"{doc["filename"]}"ã€‚
æ–‡æ¡£æ€»é¡µæ•°ï¼š{doc["data"]["total_pages"]}

æ–‡æ¡£å†…å®¹ï¼š
{context}

å›ç­”è§„åˆ™ï¼š
1. åŸºäºæ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”ï¼Œç®€æ´æ¸…æ™°ï¼Œå­¦æœ¯å‡†ç¡®ã€‚
2. ä¸è¦å£°æ˜ä½ æ˜¯å¦å…·å¤‡å¤–éƒ¨å·¥å…·/è”ç½‘ç­‰èƒ½åŠ›ï¼Œä¸è¦è¾“å‡ºä¸å›ç­”æ— å…³çš„å…è´£å£°æ˜ã€‚
3. ä¼˜å…ˆä¾æ®æ–‡æ¡£å†…å®¹å›ç­”ï¼›è‹¥æ–‡æ¡£ä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºå¸¸è¯†ç»™å‡ºæ¦‚è§ˆæ€§è§£ç­”å¹¶æ˜ç¡®ä¸ç¡®å®šä¹‹å¤„ã€‚
4. é‡åˆ°å…¬å¼ã€æ•°æ®ã€å›¾è¡¨ç­‰å…³é”®ä¿¡æ¯æ—¶ï¼Œå¿…é¡»ç›´æ¥å¼•ç”¨åŸæ–‡å±•ç¤ºå®Œæ•´å†…å®¹ï¼Œä¸è¦ä»…æ¦‚æ‹¬æè¿°ã€‚
   - ä¾‹å¦‚ç”¨æˆ·é—®"æœ‰ä»€ä¹ˆå…¬å¼"æ—¶ï¼Œåº”ç›´æ¥å±•ç¤ºå…¬å¼çš„å®Œæ•´è¡¨è¾¾å¼ã€‚
   - å¯¹äºæ•°å­¦å…¬å¼ï¼Œä¼˜å…ˆä½¿ç”¨LaTeXæ ¼å¼å±•ç¤ºï¼ˆ$å…¬å¼$ï¼‰ã€‚
5. ä¸è¦è¯´"æ ¹æ®æ‚¨æä¾›çš„æœ‰é™ç‰‡æ®µ"ã€"åŸºäºç‰‡æ®µ"ç­‰æš—ç¤ºä¿¡æ¯ä¸è¶³çš„æªè¾ï¼Œç›´æ¥å›ç­”é—®é¢˜ã€‚"""

        # æ£€æµ‹ç”Ÿæˆç±»æŸ¥è¯¢ï¼ˆæ€ç»´å¯¼å›¾/æµç¨‹å›¾ï¼‰ï¼Œæ³¨å…¥å¯¹åº”ç³»ç»Ÿæç¤ºè¯
        generation_prompt = get_generation_prompt(request.question)
        if generation_prompt:
            system_prompt += f"\n\n{generation_prompt}"

        # å¼•æ–‡è¿½è¸ªï¼šå¦‚æœ retrieval_meta ä¸­åŒ…å« citationsï¼Œè¿½åŠ å¼•æ–‡æŒ‡ç¤ºæç¤ºè¯
        citations = retrieval_meta.get("citations", [])
        if citations:
            citation_prompt = _context_builder.build_citation_prompt(citations)
            if citation_prompt:
                system_prompt += f"\n\n{citation_prompt}"

        # æ³¨å…¥è®°å¿†ä¸Šä¸‹æ–‡åˆ° system promptï¼ˆéœ€æ±‚ 5.2ï¼‰
        system_prompt = _inject_memory_context(system_prompt, memory_context)

        user_content = request.question

    messages = [
        {"role": "system", "content": system_prompt},
    ]
    # æ’å…¥å¤šè½®å¯¹è¯å†å²ï¼ˆéœ€æ±‚ 3.2ï¼šä½äº system prompt ä¹‹åã€å½“å‰ç”¨æˆ·æ¶ˆæ¯ä¹‹å‰ï¼‰
    if request.chat_history:
        for hist_msg in request.chat_history:
            if isinstance(hist_msg, dict) and hist_msg.get("role") in ("user", "assistant") and hist_msg.get("content"):
                messages.append({"role": hist_msg["role"], "content": hist_msg["content"]})
    messages.append({"role": "user", "content": user_content})

    async def event_generator():
        nonlocal messages, system_prompt, retrieval_meta
        try:
            # ============ Agent å¤šè½®æ£€ç´¢æ¨¡å¼ ============
            if use_agent:
                from services.semantic_group_service import SemanticGroupService
                from services.embedding_service import _get_semantic_groups_dir

                # æ„å»º DocContext
                full_text = doc.get("data", {}).get("full_text", "")
                pages = doc.get("data", {}).get("pages", [])
                chunks = doc.get("data", {}).get("chunks", [])
                if not chunks:
                    # ä» pages æå– chunks
                    chunks = [p.get("content", "") for p in pages if p.get("content")]

                # å°è¯•åŠ è½½è¯­ä¹‰æ„ç¾¤
                groups_dir = _get_semantic_groups_dir()
                group_svc = SemanticGroupService()
                semantic_groups = group_svc.load_groups(request.doc_id, groups_dir) or []

                doc_ctx = DocContext(
                    doc_id=request.doc_id,
                    full_text=full_text,
                    chunks=chunks,
                    pages=pages,
                    semantic_groups=semantic_groups,
                    vector_store_dir=getattr(router, "vector_store_dir", ""),
                    api_key=request.api_key or "",
                )

                agent = RetrievalAgent(
                    api_key=request.api_key or "",
                    model=request.model,
                    provider=request.api_provider,
                    endpoint=PROVIDER_CONFIG.get(request.api_provider, {}).get("endpoint", ""),
                    max_rounds=settings.agent_max_rounds,
                    temperature=settings.agent_planner_temperature,
                )

                agent_context = ""
                async for event in agent.run(
                    question=request.question,
                    doc_ctx=doc_ctx,
                    doc_name=doc.get("filename", ""),
                ):
                    if event["type"] == "retrieval_progress":
                        # å‘å‰ç«¯å‘é€æ£€ç´¢è¿›åº¦
                        yield f"data: {json.dumps(event, ensure_ascii=False)}\n\n"
                    elif event["type"] == "retrieval_complete":
                        agent_context = event.get("context", "")
                        retrieval_meta["agent_search_history"] = event.get("search_history", [])
                        retrieval_meta["agent_detail"] = event.get("detail", [])

                # ç”¨ Agent è·å–çš„ä¸Šä¸‹æ–‡é‡å»º system_prompt
                if agent_context:
                    system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„PDFæ–‡æ¡£æ™ºèƒ½åŠ©æ‰‹ã€‚ç”¨æˆ·æ­£åœ¨æŸ¥çœ‹æ–‡æ¡£"{doc["filename"]}"ã€‚
æ–‡æ¡£æ€»é¡µæ•°ï¼š{doc["data"]["total_pages"]}

æ–‡æ¡£å†…å®¹ï¼š
{agent_context}

å›ç­”è§„åˆ™ï¼š
1. åŸºäºæ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”ï¼Œç®€æ´æ¸…æ™°ï¼Œå­¦æœ¯å‡†ç¡®ã€‚
2. ä¸è¦å£°æ˜ä½ æ˜¯å¦å…·å¤‡å¤–éƒ¨å·¥å…·/è”ç½‘ç­‰èƒ½åŠ›ï¼Œä¸è¦è¾“å‡ºä¸å›ç­”æ— å…³çš„å…è´£å£°æ˜ã€‚
3. ä¼˜å…ˆä¾æ®æ–‡æ¡£å†…å®¹å›ç­”ï¼›è‹¥æ–‡æ¡£ä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºå¸¸è¯†ç»™å‡ºæ¦‚è§ˆæ€§è§£ç­”å¹¶æ˜ç¡®ä¸ç¡®å®šä¹‹å¤„ã€‚
4. é‡åˆ°å…¬å¼ã€æ•°æ®ã€å›¾è¡¨ç­‰å…³é”®ä¿¡æ¯æ—¶ï¼Œå¿…é¡»ç›´æ¥å¼•ç”¨åŸæ–‡å±•ç¤ºå®Œæ•´å†…å®¹ï¼Œä¸è¦ä»…æ¦‚æ‹¬æè¿°ã€‚
   - ä¾‹å¦‚ç”¨æˆ·é—®"æœ‰ä»€ä¹ˆå…¬å¼"æ—¶ï¼Œåº”ç›´æ¥å±•ç¤ºå…¬å¼çš„å®Œæ•´è¡¨è¾¾å¼ã€‚
   - å¯¹äºæ•°å­¦å…¬å¼ï¼Œä¼˜å…ˆä½¿ç”¨LaTeXæ ¼å¼å±•ç¤ºï¼ˆ$å…¬å¼$ï¼‰ã€‚
5. ä¸è¦è¯´"æ ¹æ®æ‚¨æä¾›çš„æœ‰é™ç‰‡æ®µ"ã€"åŸºäºç‰‡æ®µ"ç­‰æš—ç¤ºä¿¡æ¯ä¸è¶³çš„æªè¾ï¼Œç›´æ¥å›ç­”é—®é¢˜ã€‚"""

                    # é‡æ–°æ³¨å…¥è®°å¿†ä¸Šä¸‹æ–‡
                    system_prompt = _inject_memory_context(system_prompt, memory_context)

                    # æ£€æµ‹ç”Ÿæˆç±»æŸ¥è¯¢
                    generation_prompt = get_generation_prompt(request.question)
                    if generation_prompt:
                        system_prompt += f"\n\n{generation_prompt}"

                    # é‡å»º messages
                    messages = [{"role": "system", "content": system_prompt}]
                    if request.chat_history:
                        for hist_msg in request.chat_history:
                            if isinstance(hist_msg, dict) and hist_msg.get("role") in ("user", "assistant") and hist_msg.get("content"):
                                messages.append({"role": hist_msg["role"], "content": hist_msg["content"]})
                    messages.append({"role": "user", "content": request.question})

            # ============ é Agent æ¨¡å¼çš„æ£€ç´¢è¿›åº¦åé¦ˆ ============
            if not use_agent and not request.image_base64:
                yield f"data: {json.dumps({'type': 'retrieval_progress', 'phase': 'complete', 'message': 'æ£€ç´¢å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”...'}, ensure_ascii=False)}\n\n"

            # ============ æµå¼ LLM å›ç­” ============
            middlewares = build_chat_middlewares()
            async for chunk in call_ai_api_stream(
                messages,
                request.api_key,
                request.model,
                request.api_provider,
                endpoint=PROVIDER_CONFIG.get(request.api_provider, {}).get("endpoint", ""),
                middlewares=middlewares,
                enable_thinking=request.enable_thinking,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                custom_params=request.custom_params,
                reasoning_effort=request.reasoning_effort,
            ):
                if chunk.get("error"):
                    yield f"data: {json.dumps({'error': chunk['error']})}\n\n"
                    break
                chunk_data = {
                    'content': chunk.get('content', ''),
                    'reasoning_content': chunk.get('reasoning_content', ''),
                    'done': chunk.get('done', False),
                    'used_provider': chunk.get('used_provider'),
                    'used_model': chunk.get('used_model'),
                    'fallback_used': chunk.get('fallback_used'),
                }
                # åœ¨æœ€åä¸€ä¸ª chunk ä¸­é™„å¸¦ retrieval_metaï¼ˆå« citationsï¼‰
                if chunk.get("done"):
                    chunk_data['retrieval_meta'] = retrieval_meta
                yield f"data: {json.dumps(chunk_data)}\n\n"
                if chunk.get("done"):
                    # å¼‚æ­¥è§¦å‘è®°å¿†å†™å…¥ï¼ˆéœ€æ±‚ 5.3ï¼‰
                    if use_memory:
                        threading.Thread(
                            target=_async_memory_write,
                            args=(memory_service, request),
                            daemon=True,
                        ).start()
                    yield "data: [DONE]\n\n"
                    break

        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")
