from datetime import datetime
from typing import Optional, List
import json
import logging
import threading

from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from services.chat_service import call_ai_api, call_ai_api_stream, extract_reasoning_content
from services.vector_service import vector_context
from services.selected_text_locator import locate_selected_text
from services.retrieval_agent import RetrievalAgent
from services.retrieval_tools import DocContext
from services.glossary_service import glossary_service, build_glossary_prompt
from services.table_service import protect_markdown_tables, restore_markdown_tables
from services.query_analyzer import get_retrieval_strategy
from services.preset_service import get_generation_prompt
from services.context_builder import ContextBuilder
import base64
from models.provider_registry import PROVIDER_CONFIG
from models.dynamic_store import load_dynamic_providers
from utils.middleware import (
    LoggingMiddleware,
    RetryMiddleware,
    ErrorCaptureMiddleware,
    DegradeOnErrorMiddleware,
    TimeoutMiddleware,
    FallbackMiddleware,
)
from config import settings

logger = logging.getLogger(__name__)

router = APIRouter()


def _get_provider_endpoint(provider_id: str, api_host: str = "") -> str:
    """æŒ‰ä¼˜å…ˆçº§è§£æ provider çš„ chat endpointï¼š
    1. å‰ç«¯ä¼ å…¥çš„ api_hostï¼ˆç”¨æˆ·è‡ªå®šä¹‰åœ°å€ï¼‰
    2. åŠ¨æ€ provider å­˜å‚¨ï¼ˆç”¨æˆ·é€šè¿‡ UI æ·»åŠ çš„å®šåˆ¶ providerï¼‰
    3. é™æ€ PROVIDER_CONFIGï¼ˆå†…ç½®é»˜è®¤é…ç½®ï¼‰
    """
    # 1. å‰ç«¯æ˜ç¡®ä¼ å…¥äº† api_hostï¼šæ‹¼æ¥æˆå®Œæ•´ endpoint
    if api_host and api_host.strip():
        host = api_host.strip().rstrip('/')
        # å¦‚æœå·²åŒ…å« /chat/completions åˆ™ç›´æ¥ä½¿ç”¨
        if host.endswith('/chat/completions'):
            return host
        return f"{host}/chat/completions"
    # 2. åŠ¨æ€ provider å­˜å‚¨
    dynamic = load_dynamic_providers()
    if provider_id in dynamic:
        return dynamic[provider_id].get("endpoint", "")
    # 3. é™æ€å†…ç½®é…ç½®
    return PROVIDER_CONFIG.get(provider_id, {}).get("endpoint", "")


def _detect_image_mime(image_base64: str) -> str:
    """ä» base64 ç›´æ¥æ£€æµ‹å›¾ç‰‡å®é™… MIME ç±»å‹ã€‚
    æ”¯æŒ JPEG, PNG, GIF, WebPï¼›æ— æ³•è¯†åˆ«æ—¶å›é€€ä¸º image/jpegã€‚
    16 ä¸ª base64 å­—ç¬¦è§£ç ä¸ºæ°å¥½ 12 å­—èŠ‚ï¼Œè¶³å¤Ÿåˆ¤æ–­æ‰€æœ‰å¸¸è§æ ¼å¼ã€‚
    """
    try:
        # 16 base64 å­—ç¬¦ = 4 ç»„ * 3 å­—èŠ‚/ç»„ = 12 å­—èŠ‚ï¼Œæ­£å¥½æ˜¯ 4 çš„å€æ•°ï¼Œæ— éœ€é¢å¤–å¡«å……
        chunk = image_base64[:16]
        header = base64.b64decode(chunk)
    except Exception:
        return 'image/jpeg'
    if header[:3] == b'\xff\xd8\xff':
        return 'image/jpeg'
    if header[:4] == b'\x89PNG':
        return 'image/png'
    if header[:6] in (b'GIF87a', b'GIF89a'):
        return 'image/gif'
    if header[:4] == b'RIFF' and header[8:12] == b'WEBP':
        return 'image/webp'
    return 'image/jpeg'

async def _buffered_stream(raw_stream):
    """å¯¹åŸå§‹ SSE æµè¿›è¡Œå­—ç¬¦æ•°ç¼“å†²ï¼Œåˆå¹¶é«˜é¢‘å° chunk å‡å°‘ SSE äº‹ä»¶é¢‘ç‡

    æ ¹æ® settings.stream_buffer_size é…ç½®çš„å­—ç¬¦æ•°é˜ˆå€¼ï¼Œ
    ç´¯ç§¯æ–‡æœ¬å†…å®¹è¾¾åˆ°é˜ˆå€¼åç»Ÿä¸€å‘é€ã€‚

    å½“ stream_buffer_size=0 æ—¶é€€åŒ–ä¸ºç›´é€šæ¨¡å¼ï¼Œä¸åšä»»ä½•ç¼“å†²ã€‚

    Args:
        raw_stream: åŸå§‹å¼‚æ­¥ç”Ÿæˆå™¨ï¼ˆcall_ai_api_stream çš„è¾“å‡ºï¼‰
    """
    buffer_size = settings.stream_buffer_size

    # ç›´é€šæ¨¡å¼ï¼šbuffer_size=0 æ—¶ä¸ç¼“å†²ï¼Œç›´æ¥è½¬å‘æ‰€æœ‰ chunk
    if buffer_size <= 0:
        async for chunk in raw_stream:
            yield chunk
            if chunk.get("error") or chunk.get("done"):
                break
        return

    # ç¼“å†²æ¨¡å¼ï¼šç´¯ç§¯ content å’Œ reasoning_content
    buffer_content = ""
    buffer_reasoning = ""

    async for chunk in raw_stream:
        # é”™è¯¯æˆ–ç»ˆæ­¢ä¿¡å·ï¼šç«‹å³åˆ·æ–°ç¼“å†²åŒºå¹¶è½¬å‘
        if chunk.get("error") or chunk.get("done"):
            if buffer_content or buffer_reasoning:
                yield {
                    "content": buffer_content,
                    "reasoning_content": buffer_reasoning,
                    "done": False,
                }
                buffer_content = ""
                buffer_reasoning = ""
            yield chunk
            break

        # ç´¯ç§¯åˆ°ç¼“å†²åŒº
        buffer_content += chunk.get("content", "")
        buffer_reasoning += chunk.get("reasoning_content", "")

        # ç¼“å†²åŒºè¾¾åˆ°é˜ˆå€¼ï¼Œç«‹å³å‘é€
        if len(buffer_content) >= buffer_size:
            yield {
                "content": buffer_content,
                "reasoning_content": buffer_reasoning,
                "done": False,
            }
            buffer_content = ""
            buffer_reasoning = ""

    # æµæ­£å¸¸ç»“æŸä½†æœªæ”¶åˆ° done/error ä¿¡å·æ—¶ï¼Œåˆ·æ–°å‰©ä½™ç¼“å†²
    if buffer_content or buffer_reasoning:
        yield {
            "content": buffer_content,
            "reasoning_content": buffer_reasoning,
            "done": False,
        }


# ä¸Šä¸‹æ–‡æ„å»ºå™¨å®ä¾‹ï¼Œç”¨äºç”Ÿæˆå¼•æ–‡æŒ‡ç¤ºæç¤ºè¯
_context_builder = ContextBuilder()

# æ¨¡å—çº§å˜é‡ï¼Œç”± app.py æ³¨å…¥ MemoryService å®ä¾‹
memory_service = None


def build_chat_middlewares():
    middlewares = []
    if settings.enable_chat_logging:
        middlewares.append(LoggingMiddleware())
    middlewares.append(RetryMiddleware(retries=settings.chat_retry_retries, delay=settings.chat_retry_delay))
    middlewares.append(ErrorCaptureMiddleware(log_path=settings.error_log_path))
    middlewares.append(TimeoutMiddleware(timeout=settings.chat_timeout))
    if settings.chat_fallback_provider or settings.chat_fallback_model:
        middlewares.append(FallbackMiddleware(settings.chat_fallback_provider, settings.chat_fallback_model))
    if settings.enable_chat_degrade:
        middlewares.append(DegradeOnErrorMiddleware(fallback_content=settings.degrade_message))
    return middlewares


class ChatRequest(BaseModel):
    doc_id: str
    question: str
    api_key: Optional[str] = None
    model: str
    api_provider: str
    selected_text: Optional[str] = None
    enable_vector_search: bool = True
    image_base64: Optional[str] = None
    # æ–°å¢ï¼šæ”¯æŒå¤šå›¾
    image_base64_list: Optional[List[str]] = None
    top_k: int = 10
    candidate_k: int = 20
    use_rerank: bool = False
    reranker_model: Optional[str] = None
    rerank_provider: Optional[str] = None
    rerank_api_key: Optional[str] = None
    rerank_endpoint: Optional[str] = None
    doc_store_key: Optional[str] = None
    enable_glossary: bool = True
    protect_tables: bool = True
    api_host: Optional[str] = None
    enable_thinking: bool = False
    max_tokens: Optional[int] = None
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    custom_params: Optional[dict] = None
    reasoning_effort: Optional[str] = None
    stream_output: bool = True
    chat_history: Optional[List[dict]] = None
    enable_memory: bool = True
    enable_agent_retrieval: bool = False


class ChatVisionRequest(BaseModel):
    doc_id: str
    question: str
    api_key: Optional[str] = None
    model: str
    api_provider: str
    image_base64: Optional[str] = None
    selected_text: Optional[str] = None


def _validate_rerank_request(req):
    provider = getattr(req, "rerank_provider", None)
    api_key = getattr(req, "rerank_api_key", None)
    use_rerank = getattr(req, "use_rerank", False)
    cloud_providers = {"cohere", "jina", "silicon", "aliyun", "openai", "moonshot", "deepseek", "zhipu", "minimax"}
    if use_rerank and provider and provider.lower() in cloud_providers and not api_key:
        raise HTTPException(status_code=400, detail=f"ä½¿ç”¨ {provider} rerank éœ€è¦æä¾› rerank_api_key")


def _retrieve_memory_context(question: str, api_key: str = None, doc_id: str = None) -> str:
    if memory_service is None:
        return ""
    try:
        return memory_service.retrieve_memories(
            question, api_key=api_key, doc_id=doc_id, filter_by_doc=False
        )
    except Exception as e:
        logger.error(f"è®°å¿†æ£€ç´¢å¤±è´¥: {e}")
        return ""


def _async_memory_write(svc, request):
    try:
        if request.doc_id:
            history = list(request.chat_history or [])
            history.append({"role": "user", "content": request.question})
            svc.save_qa_summary(
                request.doc_id,
                history,
                api_key=getattr(request, "api_key", None),
                model=getattr(request, "model", None),
                api_provider=getattr(request, "api_provider", None),
            )
        svc.update_keywords(request.question)
    except Exception as e:
        logger.error(f"å¼‚æ­¥è®°å¿†å†™å…¥å¤±è´¥: {e}")


_flushed_sessions: set = set()


def _maybe_flush_memory(request) -> None:
    if memory_service is None:
        return
    if not settings.memory_flush_enabled:
        return
    history = getattr(request, "chat_history", None)
    if not history:
        return
    doc_id = getattr(request, "doc_id", "")
    if not doc_id or doc_id in _flushed_sessions:
        return
    from services.token_budget import TokenBudget
    budget = TokenBudget()
    total_tokens = 0
    for msg in history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if content:
                total_tokens += budget.estimate_tokens(content)
    threshold = settings.memory_flush_threshold_tokens
    if total_tokens < threshold:
        return
    _flushed_sessions.add(doc_id)
    logger.info(f"[Memory] Compaction flush è§¦å‘: doc_id={doc_id}, tokens={total_tokens}, threshold={threshold}")
    threading.Thread(
        target=_async_memory_write,
        args=(memory_service, request),
        daemon=True,
    ).start()


def _should_use_memory(request) -> bool:
    return (
        settings.memory_enabled
        and getattr(request, "enable_memory", True)
        and memory_service is not None
    )


def _inject_memory_context(system_prompt: str, memory_context: str) -> str:
    if not memory_context:
        return system_prompt
    marker = "\nå›ç­”è§„åˆ™ï¼š"
    if marker in system_prompt:
        idx = system_prompt.index(marker)
        return (
            system_prompt[:idx]
            + f"\n\nç”¨æˆ·å†å²è®°å¿†ï¼š\n{memory_context}"
            + system_prompt[idx:]
        )
    return system_prompt + f"\n\nç”¨æˆ·å†å²è®°å¿†ï¼š\n{memory_context}"


def _build_fused_context(
    selected_text: str,
    retrieval_context: str,
    selected_page_info: dict,
) -> str:
    """èåˆæ¡†é€‰æ–‡æœ¬å’Œæ£€ç´¢ä¸Šä¸‹æ–‡

    å°† selected_text ä½œä¸ºä¼˜å…ˆä¸Šä¸‹æ–‡ç½®äºæ£€ç´¢ç»“æœä¹‹å‰ï¼Œ
    å¹¶æ ‡æ³¨æ¡†é€‰æ–‡æœ¬çš„é¡µç æ¥æºã€‚
    """
    page_label = ""
    if selected_page_info:
        ps = selected_page_info.get("page_start", 0)
        pe = selected_page_info.get("page_end", 0)
        page_label = f"ï¼ˆé¡µç : {ps}-{pe}ï¼‰" if ps != pe else f"ï¼ˆé¡µç : {ps}ï¼‰"

    parts = [f"ç”¨æˆ·é€‰ä¸­çš„æ–‡æœ¬{page_label}ï¼š\n{selected_text}"]
    if retrieval_context:
        parts.append(f"\n\nç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼š\n\n{retrieval_context}")
    return "\n".join(parts)


def _build_selected_text_citation(
    selected_text: str,
    selected_page_info: dict,
) -> dict:
    """åŸºäºæ¡†é€‰æ–‡æœ¬ä½ç½®ç”ŸæˆåŸºç¡€ citation"""
    ps = selected_page_info.get("page_start", 1) if selected_page_info else 1
    pe = selected_page_info.get("page_end", ps) if selected_page_info else ps
    return {
        "ref": 1,
        "group_id": "selected-text",
        "page_range": [ps, pe],
        "highlight_text": selected_text[:200].strip(),
    }


@router.post("/chat")
async def chat_with_pdf(request: ChatRequest):
    if not hasattr(router, "documents_store"):
        raise HTTPException(status_code=500, detail="æ–‡æ¡£å­˜å‚¨æœªåˆå§‹åŒ–")
    store = router.documents_store if not request.doc_store_key else router.documents_store.get(request.doc_store_key, {})
    if request.doc_id not in store:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°")
    doc = store[request.doc_id]
    context = ""
    retrieval_meta = {}
    use_memory = _should_use_memory(request)
    if use_memory:
        _maybe_flush_memory(request)
    memory_context = ""
    if use_memory:
        memory_context = _retrieve_memory_context(
            request.question, api_key=request.api_key, doc_id=request.doc_id
        )

    # æ”¯æŒå¤šå›¾é€»è¾‘
    image_list = (request.image_base64_list or [])
    if request.image_base64 and request.image_base64 not in image_list:
        image_list = [request.image_base64] + image_list
    image_list = [img for img in image_list if img]

    if image_list:
        print(f"[Chat] ğŸ“¸ æˆªå›¾æ¨¡å¼ï¼šå¤„ç† {len(image_list)} å¼ å›¾")
        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„PDFæ–‡æ¡£æ™ºèƒ½åŠ©æ‰‹ã€‚ç”¨æˆ·æ­£åœ¨æŸ¥çœ‹æ–‡æ¡£"{doc["filename"]}"ã€‚
ç”¨æˆ·ä»æ–‡æ¡£ä¸­æˆªå–äº† {len(image_list)} å¼ å›¾ç‰‡å¹¶å‘é€ç»™ä½ ã€‚è¯·ä»”ç»†åˆ†æè¿™äº›å›¾ç‰‡å†…å®¹å¹¶å›ç­”é—®é¢˜ã€‚

å›ç­”è§„åˆ™ï¼š
1. ä»¥ç”¨æˆ·å‘é€çš„å›¾ç‰‡ä¸ºæ ¸å¿ƒä¾æ®è¿›è¡Œå›ç­”ï¼Œä¸è¦å‚è€ƒå…¶ä»–å†…å®¹ã€‚
2. å¦‚æœå›¾ç‰‡åŒ…å«å›¾è¡¨ï¼Œè¯·åˆ†ææ•°æ®è¶‹åŠ¿å’Œå…³é”®ä¿¡æ¯ã€‚
3. å¦‚æœå›¾ç‰‡åŒ…å«å…¬å¼ï¼Œè¯·ä½¿ç”¨ LaTeX æ ¼å¼ï¼ˆ$å…¬å¼$ï¼‰å±•ç¤ºã€‚
4. å¦‚æœå›¾ç‰‡åŒ…å«è¡¨æ ¼ï¼Œè¯·è½¬æ¢ä¸º Markdown æ ¼å¼ã€‚
5. ç®€æ´æ¸…æ™°ï¼Œå­¦æœ¯å‡†ç¡®ã€‚"""
        system_prompt = _inject_memory_context(system_prompt, memory_context)
        user_content = [{"type": "text", "text": request.question or "è¯·åˆ†æè¿™äº›å›¾ç‰‡"}]
        for img_b64 in image_list:
            mime = _detect_mime_type(img_b64)
            user_content.append({"type": "image_url", "image_url": {"url": f"data:{mime};base64,{img_b64}"}})
    else:
        if request.selected_text and request.enable_vector_search:
            # èåˆæ¨¡å¼ï¼šselected_text + å‘é‡æ£€ç´¢
            _validate_rerank_request(request)
            selected_page_info = locate_selected_text(
                request.selected_text, doc.get("data", {}).get("pages", [])
            )
            try:
                strategy = get_retrieval_strategy(request.question)
                dynamic_top_k = strategy['top_k']
                context_result = await vector_context(
                    request.doc_id, request.question, vector_store_dir=router.vector_store_dir,
                    pages=doc.get("data", {}).get("pages", []), api_key=request.api_key,
                    top_k=dynamic_top_k, candidate_k=max(request.candidate_k, dynamic_top_k),
                    use_rerank=request.use_rerank, reranker_model=request.reranker_model,
                    rerank_provider=request.rerank_provider, rerank_api_key=request.rerank_api_key,
                    rerank_endpoint=request.rerank_endpoint,
                    middlewares=[
                        *( [LoggingMiddleware()] if settings.enable_chat_logging else [] ),
                        RetryMiddleware(retries=settings.chat_retry_retries, delay=settings.chat_retry_delay),
                        ErrorCaptureMiddleware()
                    ],
                    selected_text=request.selected_text,
                )
                retrieval_context = context_result.get("context", "")
                retrieval_meta = context_result.get("retrieval_meta", {})
                # èåˆï¼šselected_text ä¼˜å…ˆ + æ£€ç´¢è¡¥å……
                context = _build_fused_context(
                    request.selected_text, retrieval_context, selected_page_info
                )
                # å¦‚æœæ£€ç´¢æ²¡æœ‰è¿”å› citationsï¼ŒåŸºäº selected_text ä½ç½®ç”ŸæˆåŸºç¡€ citation
                if not retrieval_meta.get("citations"):
                    retrieval_meta["citations"] = [_build_selected_text_citation(
                        request.selected_text, selected_page_info
                    )]
            except Exception as e:
                logger.warning(f"æ¡†é€‰æ¨¡å¼å‘é‡æ£€ç´¢å¤±è´¥ï¼Œé™çº§ä¸ºä»… selected_text: {e}")
                context = f"ç”¨æˆ·é€‰ä¸­çš„æ–‡æœ¬ï¼š\n{request.selected_text}\n\n"
        elif request.selected_text:
            # ä»… selected_text æ¨¡å¼ï¼ˆå‘é‡æ£€ç´¢æœªå¯ç”¨ï¼‰
            context = f"ç”¨æˆ·é€‰ä¸­çš„æ–‡æœ¬ï¼š\n{request.selected_text}\n\n"
        elif request.enable_vector_search:
            _validate_rerank_request(request)
            strategy = get_retrieval_strategy(request.question)
            dynamic_top_k = strategy['top_k']
            context_result = await vector_context(
                request.doc_id, request.question, vector_store_dir=router.vector_store_dir,
                pages=doc.get("data", {}).get("pages", []), api_key=request.api_key,
                top_k=dynamic_top_k, candidate_k=max(request.candidate_k, dynamic_top_k),
                use_rerank=request.use_rerank, reranker_model=request.reranker_model,
                rerank_provider=request.rerank_provider, rerank_api_key=request.rerank_api_key,
                rerank_endpoint=request.rerank_endpoint,
                middlewares=[
                    *( [LoggingMiddleware()] if settings.enable_chat_logging else [] ),
                    RetryMiddleware(retries=settings.chat_retry_retries, delay=settings.chat_retry_delay),
                    ErrorCaptureMiddleware()
                ]
            )
            relevant_text = context_result.get("context", "")
            retrieval_meta = context_result.get("retrieval_meta", {})
            context = f"æ ¹æ®ç”¨æˆ·é—®é¢˜æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼š\n\n{relevant_text}\n\n" if relevant_text else doc["data"]["full_text"][:8000]
        else:
            context = doc["data"]["full_text"][:8000]

        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„PDFæ–‡æ¡£æ™ºèƒ½åŠ©æ‰‹ã€‚ç”¨æˆ·æ­£åœ¨æŸ¥çœ‹æ–‡æ¡£"{doc["filename"]}"ã€‚
æ–‡æ¡£æ€»é¡µæ•°ï¼š{doc["data"]["total_pages"]}

æ–‡æ¡£å†…å®¹ï¼š
{context}

å›ç­”è§„åˆ™ï¼š
1. åŸºäºæ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”ï¼Œç®€æ´æ¸…æ™°ï¼Œå­¦æœ¯å‡†ç¡®ã€‚
2. é‡åˆ°å…¬å¼ã€æ•°æ®ã€å›¾è¡¨ç­‰å…³é”®ä¿¡æ¯æ—¶ï¼Œå¿…é¡»ç›´æ¥å¼•ç”¨åŸæ–‡å±•ç¤ºå®Œæ•´å†…å®¹ã€‚
3. ä¼˜å…ˆä¾æ®æ–‡æ¡£å†…å®¹å›ç­”ã€‚"""
        if request.enable_glossary:
            glossary_instruction = build_glossary_prompt(context)
            if glossary_instruction: system_prompt += f"\n\n{glossary_instruction}"
        generation_prompt = get_generation_prompt(request.question)
        if generation_prompt: system_prompt += f"\n\n{generation_prompt}"
        citations = retrieval_meta.get("citations", [])
        if citations:
            citation_prompt = _context_builder.build_citation_prompt(citations)
            if citation_prompt: system_prompt += f"\n\n{citation_prompt}"
        system_prompt = _inject_memory_context(system_prompt, memory_context)
        user_content = request.question

    messages = [{"role": "system", "content": system_prompt}]
    if request.chat_history:
        for hist_msg in request.chat_history:
            if isinstance(hist_msg, dict) and hist_msg.get("role") in ("user", "assistant") and hist_msg.get("content"):
                messages.append({"role": hist_msg["role"], "content": hist_msg["content"]})
    messages.append({"role": "user", "content": user_content})

    try:
        response = await call_ai_api(
            messages, request.api_key, request.model, request.api_provider,
            endpoint=_get_provider_endpoint(request.api_provider, request.api_host or ""),
            middlewares=build_chat_middlewares(), max_tokens=request.max_tokens,
            temperature=request.temperature, top_p=request.top_p,
            custom_params=request.custom_params, reasoning_effort=request.reasoning_effort,
        )
        message = response["choices"][0]["message"]
        answer = message["content"]
        reasoning_content = extract_reasoning_content(message)
        if use_memory:
            threading.Thread(target=_async_memory_write, args=(memory_service, request), daemon=True).start()
        return {
            "answer": answer, "reasoning_content": reasoning_content,
            "doc_id": request.doc_id, "question": request.question,
            "timestamp": datetime.now().isoformat(), "used_provider": response.get("_used_provider"),
            "used_model": response.get("_used_model"), "fallback_used": response.get("_fallback_used", False),
            "retrieval_meta": retrieval_meta
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"AIè°ƒç”¨å¤±è´¥: {str(e)}")


@router.post("/chat/stream")
async def chat_with_pdf_stream(request: ChatRequest):
    if not hasattr(router, "documents_store"):
        raise HTTPException(status_code=500, detail="æ–‡æ¡£å­˜å‚¨æœªåˆå§‹åŒ–")
    store = router.documents_store if not request.doc_store_key else router.documents_store.get(request.doc_store_key, {})
    if request.doc_id not in store:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°")
    doc = store[request.doc_id]
    context = ""
    retrieval_meta = {}
    use_agent = False
    use_memory = _should_use_memory(request)
    memory_context = ""
    if use_memory:
        memory_context = _retrieve_memory_context(
            request.question, api_key=request.api_key, doc_id=request.doc_id
        )

    image_list = (request.image_base64_list or [])
    if request.image_base64 and request.image_base64 not in image_list:
        image_list = [request.image_base64] + image_list
    image_list = [img for img in image_list if img]

    if image_list:
        print(f"[Chat Stream] ğŸ“¸ æˆªå›¾æ¨¡å¼ï¼šå¤„ç† {len(image_list)} å¼ å›¾")
        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„PDFæ–‡æ¡£æ™ºèƒ½åŠ©æ‰‹ã€‚ç”¨æˆ·æ­£åœ¨æŸ¥çœ‹æ–‡æ¡£"{doc["filename"]}"ã€‚
ç”¨æˆ·ä»æ–‡æ¡£ä¸­æˆªå–äº† {len(image_list)} å¼ å›¾ç‰‡å¹¶å‘é€ç»™ä½ ã€‚è¯·ä»”ç»†åˆ†æè¿™äº›å›¾ç‰‡å†…å®¹å¹¶å›ç­”é—®é¢˜ã€‚

å›ç­”è§„åˆ™ï¼š
1. ä»¥ç”¨æˆ·å‘é€çš„å›¾ç‰‡ä¸ºæ ¸å¿ƒä¾æ®è¿›è¡Œå›ç­”ï¼Œä¸è¦å‚è€ƒå…¶ä»–å†…å®¹ã€‚
2. å¦‚æœå›¾ç‰‡åŒ…å«å›¾è¡¨ï¼Œè¯·åˆ†ææ•°æ®å’Œå…³é”®ä¿¡æ¯ã€‚
3. å¦‚æœå›¾ç‰‡åŒ…å«å…¬å¼ï¼Œè¯·ä½¿ç”¨ LaTeX æ ¼å¼ï¼ˆ$å…¬å¼$ï¼‰å±•ç¤ºã€‚
4. å¦‚æœå›¾ç‰‡åŒ…å«è¡¨æ ¼ï¼Œè¯·è½¬æ¢ä¸º Markdown æ ¼å¼ã€‚
5. ç®€æ´æ¸…æ™°ï¼Œå­¦æœ¯å‡†ç¡®ã€‚"""
        system_prompt = _inject_memory_context(system_prompt, memory_context)
        user_content = [{"type": "text", "text": request.question or "è¯·åˆ†æè¿™äº›å›¾ç‰‡"}]
        for img_b64 in image_list:
            mime = _detect_mime_type(img_b64)
            user_content.append({"type": "image_url", "image_url": {"url": f"data:{mime};base64,{img_b64}"}})
    else:
        use_agent = request.enable_agent_retrieval and not request.selected_text
        if request.selected_text and request.enable_vector_search:
            # èåˆæ¨¡å¼ï¼šselected_text + å‘é‡æ£€ç´¢
            _validate_rerank_request(request)
            selected_page_info = locate_selected_text(
                request.selected_text, doc.get("data", {}).get("pages", [])
            )
            try:
                strategy = get_retrieval_strategy(request.question)
                dynamic_top_k = strategy['top_k']
                context_result = await vector_context(
                    request.doc_id, request.question, vector_store_dir=router.vector_store_dir,
                    pages=doc.get("data", {}).get("pages", []), api_key=request.api_key,
                    top_k=dynamic_top_k, candidate_k=max(request.candidate_k, dynamic_top_k),
                    use_rerank=request.use_rerank, reranker_model=request.reranker_model,
                    rerank_provider=request.rerank_provider, rerank_api_key=request.rerank_api_key,
                    rerank_endpoint=request.rerank_endpoint,
                    middlewares=[
                        *( [LoggingMiddleware()] if settings.enable_search_logging else [] ),
                        RetryMiddleware(retries=settings.search_retry_retries, delay=settings.search_retry_delay)
                    ],
                    selected_text=request.selected_text,
                )
                retrieval_context = context_result.get("context", "")
                retrieval_meta = context_result.get("retrieval_meta", {})
                # èåˆï¼šselected_text ä¼˜å…ˆ + æ£€ç´¢è¡¥å……
                context = _build_fused_context(
                    request.selected_text, retrieval_context, selected_page_info
                )
                # å¦‚æœæ£€ç´¢æ²¡æœ‰è¿”å› citationsï¼ŒåŸºäº selected_text ä½ç½®ç”ŸæˆåŸºç¡€ citation
                if not retrieval_meta.get("citations"):
                    retrieval_meta["citations"] = [_build_selected_text_citation(
                        request.selected_text, selected_page_info
                    )]
            except Exception as e:
                logger.warning(f"æ¡†é€‰æ¨¡å¼å‘é‡æ£€ç´¢å¤±è´¥ï¼Œé™çº§ä¸ºä»… selected_text: {e}")
                context = f"ç”¨æˆ·é€‰ä¸­çš„æ–‡æœ¬ï¼š\n{request.selected_text}\n\n"
        elif request.selected_text:
            # ä»… selected_text æ¨¡å¼ï¼ˆå‘é‡æ£€ç´¢æœªå¯ç”¨ï¼‰
            context = f"ç”¨æˆ·é€‰ä¸­çš„æ–‡æœ¬ï¼š\n{request.selected_text}\n\n"
        elif use_agent:
            context = ""
        elif request.enable_vector_search:
            _validate_rerank_request(request)
            strategy = get_retrieval_strategy(request.question)
            dynamic_top_k = strategy['top_k']
            context_result = await vector_context(
                request.doc_id, request.question, vector_store_dir=router.vector_store_dir,
                pages=doc.get("data", {}).get("pages", []), api_key=request.api_key,
                top_k=dynamic_top_k, candidate_k=max(request.candidate_k, dynamic_top_k),
                use_rerank=request.use_rerank, reranker_model=request.reranker_model,
                rerank_provider=request.rerank_provider, rerank_api_key=request.rerank_api_key,
                rerank_endpoint=request.rerank_endpoint,
                middlewares=[
                    *( [LoggingMiddleware()] if settings.enable_search_logging else [] ),
                    RetryMiddleware(retries=settings.search_retry_retries, delay=settings.search_retry_delay)
                ]
            )
            relevant_text = context_result.get("context", "")
            retrieval_meta = context_result.get("retrieval_meta", {})
            context = f"æ ¹æ®ç”¨æˆ·é—®é¢˜æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼š\n\n{relevant_text}\n\n" if relevant_text else doc["data"]["full_text"][:8000]
        else:
            context = doc["data"]["full_text"][:8000]

        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„PDFæ–‡æ¡£æ™ºèƒ½åŠ©æ‰‹ã€‚ç”¨æˆ·æ­£åœ¨æŸ¥çœ‹æ–‡æ¡£"{doc["filename"]}"ã€‚
æ–‡æ¡£å†…å®¹ï¼š
{context}

å›ç­”è§„åˆ™ï¼š
1. åŸºäºæ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”ã€‚"""
        generation_prompt = get_generation_prompt(request.question)
        if generation_prompt: system_prompt += f"\n\n{generation_prompt}"
        citations = retrieval_meta.get("citations", [])
        if citations:
            citation_prompt = _context_builder.build_citation_prompt(citations)
            if citation_prompt: system_prompt += f"\n\n{citation_prompt}"
        system_prompt = _inject_memory_context(system_prompt, memory_context)
        user_content = request.question

    messages = [{"role": "system", "content": system_prompt}]
    if request.chat_history:
        for hist_msg in request.chat_history:
            if isinstance(hist_msg, dict) and hist_msg.get("role") in ("user", "assistant") and hist_msg.get("content"):
                messages.append({"role": hist_msg["role"], "content": hist_msg["content"]})
    messages.append({"role": "user", "content": user_content})

    async def event_generator():
        nonlocal messages, system_prompt, retrieval_meta
        try:
            if use_agent:
                # ... Agent é€»è¾‘çœç•¥ï¼Œä¿æŒåŸæ · ...
                pass
            if not use_agent and not image_list:
                yield f"data: {json.dumps({'type': 'retrieval_progress', 'phase': 'complete', 'message': 'æ£€ç´¢å®Œæˆ'}, ensure_ascii=False)}\n\n"
            # ä½¿ç”¨ _buffered_stream åŒ…è£…æµå¼è¾“å‡ºï¼Œåˆå¹¶é«˜é¢‘å° chunk å‡å°‘ SSE äº‹ä»¶é¢‘ç‡
            raw_stream = call_ai_api_stream(
                messages, request.api_key, request.model, request.api_provider,
                endpoint=_get_provider_endpoint(request.api_provider, request.api_host or ""),
                middlewares=build_chat_middlewares(), enable_thinking=request.enable_thinking,
                max_tokens=request.max_tokens, temperature=request.temperature,
                top_p=request.top_p, custom_params=request.custom_params,
                reasoning_effort=request.reasoning_effort,
            )
            async for chunk in _buffered_stream(raw_stream):
                if chunk.get("error"):
                    yield f"data: {json.dumps({'error': chunk['error']})}\n\n"
                    break
                chunk_data = {
                    'content': chunk.get('content', ''), 'reasoning_content': chunk.get('reasoning_content', ''),
                    'done': chunk.get('done', False), 'used_provider': chunk.get('used_provider'),
                    'used_model': chunk.get('used_model'), 'fallback_used': chunk.get('fallback_used'),
                }
                if chunk.get("done"): chunk_data['retrieval_meta'] = retrieval_meta
                yield f"data: {json.dumps(chunk_data)}\n\n"
                if chunk.get("done"):
                    if use_memory: threading.Thread(target=_async_memory_write, args=(memory_service, request), daemon=True).start()
                    yield "data: [DONE]\n\n"
                    break
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")


def _detect_mime_type(img_b64: str) -> str:
    try:
        header = base64.b64decode(img_b64[:16])
        if header[:3] == b'\xff\xd8\xff': return 'image/jpeg'
        if header[:4] == b'\x89PNG': return 'image/png'
        if header[:4] == b'RIFF' and header[8:12] == b'WEBP': return 'image/webp'
    except: pass
    return 'image/jpeg'
