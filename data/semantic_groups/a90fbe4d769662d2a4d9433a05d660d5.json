{
  "schema_version": 1,
  "doc_id": "a90fbe4d769662d2a4d9433a05d660d5",
  "doc_hash": "",
  "created_at": "2026-02-07T09:02:05.704422+00:00",
  "config": {
    "target_chars": 5000,
    "min_chars": 2500,
    "max_chars": 6000
  },
  "groups": [
    {
      "group_id": "group-0",
      "chunk_indices": [
        0,
        1,
        2,
        3
      ],
      "char_count": 2932,
      "summary": "DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection Pr",
      "digest": "DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection Project Page: mettyz.github.io/DiffusionEngine Manlin Zhang1,2*, Jie Wu2*†, Yuxi Ren2*, Ming Li2, Jie Qin2, Xuefeng Xiao2, Wei Liu2, Rui Wang2, Min Zheng2, Andy J. Ma1† 1Sun Yat-sen University 2ByteDance Inc arXiv:2309.03893v1 [cs.CV] 7 Sep 2023 Figure 1: We propose DiffusionEngine to scale up high-quality detection-oriented training pairs. DiffusionEngine is scalable (1st row), diverse (2nd row), and can generalize robustly across domains (3rd row).\nAbstract fusion models with detection-aware signals to make better bounding-box predictions. Additionally, we contribute two Data is the cornerstone of deep learning. This paper reveals datasets, i.e., COCO-DE and VOC-DE, to scale up existthat the recently-developed Diffusion Model is a scalable dataing detection benchmarks for facilitating follow-up research. engine for object detection. Extensive experiments demonstrate that data scaling-up via Existing methods",
      "full_text": "DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection Project Page: mettyz.github.io/DiffusionEngine Manlin Zhang1,2*, Jie Wu2*†, Yuxi Ren2*, Ming Li2, Jie Qin2, Xuefeng Xiao2, Wei Liu2, Rui Wang2, Min Zheng2, Andy J. Ma1† 1Sun Yat-sen University 2ByteDance Inc arXiv:2309.03893v1 [cs.CV] 7 Sep 2023 Figure 1: We propose DiffusionEngine to scale up high-quality detection-oriented training pairs. DiffusionEngine is scalable (1st row), diverse (2nd row), and can generalize robustly across domains (3rd row).\nAbstract fusion models with detection-aware signals to make better bounding-box predictions. Additionally, we contribute two Data is the cornerstone of deep learning. This paper reveals datasets, i.e., COCO-DE and VOC-DE, to scale up existthat the recently-developed Diffusion Model is a scalable dataing detection benchmarks for facilitating follow-up research. engine for object detection. Extensive experiments demonstrate that data scaling-up via Existing methods for scaling up detection-oriented data of-DE can achieve significant improvements in diverse scenarten require manual collection or generative models to obtainios, such as various detection algorithms, self-supervised pretarget images, followed by data augmentation and labeling totraining, data-sparse, label-scarce, cross-domain, and semiproduce training pairs, which are costly, complex, or lack-supervised learning. For example, when using DE with a ing diversity. To address these issues, we present Diffusio-DINO-based\nand semiproduce training pairs, which are costly, complex, or lack-supervised learning. For example, when using DE with a ing diversity. To address these issues, we present Diffusio-DINO-based adapter to scaling-up data, mAP is improved by nEngine (DE), a data scaling-up engine that provides high- 3.1% on COCO, 7.6% on VOC and 11.5% on Clipart. quality detection-oriented training pairs in a single stage.\nIntroductionDE consists of a pre-trained diffusion model and an effective Detection-Adapter, contributing to generating scal- Recent years have witnessed the prevalence of object detec-able, diverse and generalizable detection data in a plug-andtion in extensive vision applications such as scene recogni-play manner. Detection-Adapter is learned to align the imtion and understanding. However, the success of these ap-plicit semantic and location knowledge in off-the-shelf difplications based on object detection heavily relies on high- ⋆Equal contribution. †Corresponding author. quality training data of images with granular box-level annotations. The traditional practice for obtaining such data involves manual annotations for a massive number of images collected from the web, which is expensive, timeconsuming, and expert-involved. Furthermore, the images from real-world scenarios often follow a data-sparse, longtail, or out-of-domain distribution, raising more uncertainty and difficulty",
      "keywords": [],
      "page_range": [
        1,
        1
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T09:02:05.701058+00:00"
      }
    },
    {
      "group_id": "group-1",
      "chunk_indices": [
        4,
        5,
        6,
        7,
        8,
        9
      ],
      "char_count": 5227,
      "summary": "timeconsuming, and expert-involved. Furthermore, the images from real-world scen",
      "digest": "timeconsuming, and expert-involved. Furthermore, the images from real-world scenarios often follow a data-sparse, longtail, or out-of-domain distribution, raising more uncertainty and difficulty in this traditional data collection paradigm.\nRecently, the diffusion model has shown great potential in image generation and stylization, and researchers have explored its use in assisting object detection tasks. For example, DALL-E for detection (Ge et al. 2022) generates the foreground objects and the background context separately, and then employs copy-paste technology to obtain synthetic images. Similarly, X-Paste (Zhao et al. 2023) copies generated foreground objects and pastes them into existing im-Figure 2: Comparing the proposed DiffusionEngine with ages for data expansion. However, these existing solutionsother data collection pipelines for object detection. (a): Trahave several drawbacks: i) Additional expert models are re-ditional pipeline is time-consuming and expert-involved. quir",
      "full_text": "timeconsuming, and expert-involved. Furthermore, the images from real-world scenarios often follow a data-sparse, longtail, or out-of-domain distribution, raising more uncertainty and difficulty in this traditional data collection paradigm.\nRecently, the diffusion model has shown great potential in image generation and stylization, and researchers have explored its use in assisting object detection tasks. For example, DALL-E for detection (Ge et al. 2022) generates the foreground objects and the background context separately, and then employs copy-paste technology to obtain synthetic images. Similarly, X-Paste (Zhao et al. 2023) copies generated foreground objects and pastes them into existing im-Figure 2: Comparing the proposed DiffusionEngine with ages for data expansion. However, these existing solutionsother data collection pipelines for object detection. (a): Trahave several drawbacks: i) Additional expert models are re-ditional pipeline is time-consuming and expert-involved. quired for labeling, increasing the complexity and cost of(b): Existing multi-stage methods contain object-centric imthe data scaling process. ii) These methods naively pasteage generation, segmentation labeling, and copy-paste. It rethe\nand cost of(b): Existing multi-stage methods contain object-centric imthe data scaling process. ii) These methods naively pasteage generation, segmentation labeling, and copy-paste. It rethe generated objects into repeated images, resulting in lim-duces human participation while introducing extra models ited diversity and producing unreasonable images. iii) Imageand producing unconscionable images. (c): Our method genand annotation generation processes are separated, withouterates reliable images and annotations in single stage. fully leveraging the detection-aware concepts of semantics and location learned from the diffusion model. These issues prompt us to raise the question: how to design a more •High Effectiveness: Experiments demonstrate that Difstraightforward, scalable, and effective algorithm for scal-fusionEngine is scalable, diversified, and generalizable, ing up detection data? achieving significant performance improvements under To address this issue, we propose a novel\nalgorithm for scal-fusionEngine is scalable, diversified, and generalizable, ing up detection data? achieving significant performance improvements under To address this issue, we propose a novel tool called Dif-various settings. We also reveal that DiffusionEngine is fusionEngine, comprising a pre-trained diffusion model andsuperior to traditional methods, multi-step approaches, a Detection-Adapter. We reveal that the pre-trained diffu-and Grounding Diffusion Models in data scaling up. sion model has implicitly learned object-level structure and location-aware semantics, which can be explicitly utilized as the backbone of the object detection task. Furthermore, Related Works the Detection-Adapter can be constructed through diverse Object Detection detection frameworks, enabling the acquisition of detection- In recent years, the research area of object detection hasoriented concepts from the frozen diffusion-based backbone to produce precise annotations. Our contributions are sum-seen\nof detection- In recent years, the research area of object detection hasoriented concepts from the frozen diffusion-based backbone to produce precise annotations. Our contributions are sum-seen significant advancements with the advent of convomarized as follows: lutional neural networks (Krizhevsky, Sutskever, and Hin- • New Insight: We propose DiffusionEngine, a simple yetton 2017). Most object detection methods (Girshick 2015; Ren et al. 2015; Cai and Vasconcelos 2019; Redmon et al.effective engine for scaling up object detection data. By 2016; Liu et al. 2016) can be broadly categorized into twoabandoning complex multi-stage processes and instead paradigms: two-stage and one-stage methods. The two-stagedesigning a Detection-Adapter to generate training pairs in a single stage, DiffusionEngine is both efficient andmodels (Girshick 2015; Cai and Vasconcelos 2019; Ren versatile. Moreover, it is orthogonal to most detectionet al. 2015; He et al. 2017) first generate box proposals and\nis both efficient andmodels (Girshick 2015; Cai and Vasconcelos 2019; Ren versatile. Moreover, it is orthogonal to most detectionet al. 2015; He et al. 2017) first generate box proposals and then perform regression and classification. In contrast, one-works and can be used to improve performance further stage models (Redmon et al. 2016; Lin et al. 2017b; Tianin a plug-and-play manner. et al. 2019; Liu et al. 2016) simultaneously predict the posi-•Pioneering and Scalable: Detection-Adapter aligns the implicit knowledge learned by off-the-shelf diffusiontion and class probability of the detection boxes based on the models with task-aware signals, empowering Diffusio-priors of anchors or object centers. Furthermore, the emergence of Transformer (Vaswani et al. 2017) leads to the de-nEngine with excellent labeling ability. Furthermore, velopment of transformer-based detection methods (CarionDiffusionEngine has an infinite capacity for scaling up et al. 2020; Zhu et al. 2021), which aim to",
      "keywords": [],
      "page_range": [
        2,
        2
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T09:02:05.701058+00:00"
      }
    },
    {
      "group_id": "group-2",
      "chunk_indices": [
        10,
        11
      ],
      "char_count": 1513,
      "summary": "excellent labeling ability. Furthermore, velopment of transformer-based detectio",
      "digest": "excellent labeling ability. Furthermore, velopment of transformer-based detection methods (CarionDiffusionEngine has an infinite capacity for scaling up et al. 2020; Zhu et al. 2021), which aim to define the detec-data, with the ability to expand tens of thousands of data. •Novel Dataset: To facilitate further research on ob-tion task as a sequence prediction problem. ject detection, we contribute two scaling-up datasets us- Scaling Up Data for Object Detection ing DiffusionEngine, namely COCO-DE, and VOC-DE.\nThese datasets scale up the original images and anno-Large-scale high-quality training images and annotations are tations, which provides scalable and diverse data forthe keys to advanced detectors. However, real data distrileading-edge research to enable the next generation ofbution faces many challenges, such as few-shot and longstate-of-the-art detection algorithms. tailed. To alleviate this issue, various techniques for scaling up detection data are explored, including data au",
      "full_text": "excellent labeling ability. Furthermore, velopment of transformer-based detection methods (CarionDiffusionEngine has an infinite capacity for scaling up et al. 2020; Zhu et al. 2021), which aim to define the detec-data, with the ability to expand tens of thousands of data. •Novel Dataset: To facilitate further research on ob-tion task as a sequence prediction problem. ject detection, we contribute two scaling-up datasets us- Scaling Up Data for Object Detection ing DiffusionEngine, namely COCO-DE, and VOC-DE.\nThese datasets scale up the original images and anno-Large-scale high-quality training images and annotations are tations, which provides scalable and diverse data forthe keys to advanced detectors. However, real data distrileading-edge research to enable the next generation ofbution faces many challenges, such as few-shot and longstate-of-the-art detection algorithms. tailed. To alleviate this issue, various techniques for scaling up detection data are explored, including data augmen- where α, σ ∈ R+are differentiable functions of t with t t tation (Wang et al. 2019; Zoph et al. 2020; Chen et al. 2021)bounded derivatives, whose choice is determined by the and data synthesis with generative models (Ghiasi et al.noise schedules of the sampler. In the backward diffusion 2021; Zhao et al. 2023; Ge et al. 2022). However, such meth-process, a sequence of denoising steps is performed to proods generate new data using the original data as raw mate-gressively obtain cleaner samples, with the",
      "keywords": [],
      "page_range": [
        2,
        2
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T09:02:05.701058+00:00"
      }
    },
    {
      "group_id": "group-3",
      "chunk_indices": [
        12,
        13,
        14,
        15,
        16,
        17
      ],
      "char_count": 5500,
      "summary": "Ge et al. 2022). However, such meth-process, a sequence of denoising steps is pe",
      "digest": "Ge et al. 2022). However, such meth-process, a sequence of denoising steps is performed to proods generate new data using the original data as raw mate-gressively obtain cleaner samples, with the U-Net estimating rial, which leads to a lack of diversity. On the other hand, the noise added at each time step. Each denoising step can generative models can generate diverse data that never ap-be represented by the following function: peared in the original dataset. (Ge et al. 2022) leverages a powerful text-to-image generative model to generate diverse z = ϵ (z, t, c), (2) t−1 θ t p foreground objects and backgrounds, which are then com- where ϵ refers to the U-Net, and c = τ (P) is the em- θ p θposited to synthesize training data. bedding of the input text prompt. The U-Net blocks con- P sist of a residual block, a self-attention block, and a cross- Generative Models attention block. The text-condition c is injected via each p Generative models, such as generative adversarial\ncon- P sist o",
      "full_text": "Ge et al. 2022). However, such meth-process, a sequence of denoising steps is performed to proods generate new data using the original data as raw mate-gressively obtain cleaner samples, with the U-Net estimating rial, which leads to a lack of diversity. On the other hand, the noise added at each time step. Each denoising step can generative models can generate diverse data that never ap-be represented by the following function: peared in the original dataset. (Ge et al. 2022) leverages a powerful text-to-image generative model to generate diverse z = ϵ (z, t, c), (2) t−1 θ t p foreground objects and backgrounds, which are then com- where ϵ refers to the U-Net, and c = τ (P) is the em- θ p θposited to synthesize training data. bedding of the input text prompt. The U-Net blocks con- P sist of a residual block, a self-attention block, and a cross- Generative Models attention block. The text-condition c is injected via each p Generative models, such as generative adversarial\ncon- P sist of a residual block, a self-attention block, and a cross- Generative Models attention block. The text-condition c is injected via each p Generative models, such as generative adversarial networkscross-attention as both the Key and Value, i.e., (GAN) (Creswell et al. 2018), variational autoencoders (3)Attentioni(Qi, Ki, Vi) = Attention(φi(zt), cp, cp), (VAE) (Kingma and Welling 2013), and flow-based models (Kingma and Dhariwal 2018), have seen significant where φ (z) is the visual representation in the ithU-Net i t advancements in recent years. Recently, Diffusion Proba-block. In the following paper, when referring to extractbilistic Models (DPM) (Ho, Jain, and Abbeel 2020; Sohl-ing intermediate features, we mean extracting the outputs Dickstein et al. 2015) have emerged as a promising re-of each U-Net block unless otherwise stated. search direction, demonstrating their ability to generate high-quality images on diverse datasets (Ho et al. 2022; LDM is Effective Backbone\npromising re-of each U-Net block unless otherwise stated. search direction, demonstrating their ability to generate high-quality images on diverse datasets (Ho et al. 2022; LDM is Effective Backbone for Detection Nichol and Dhariwal 2021; Saharia et al. 2022a). TheseIn this section, we analyze the location and semantic informethods are trained on billions of image-caption pairs formation contained in LDM and reveal that the pre-trained text-to-image generation tasks, such as DALL-E 2 (RameshLDM has implicitly learned detection-oriented signals. et al. 2022), Imagen (Saharia et al. 2022b), and Stable Dif-The location information in LDM is implicitly encoded in fusion (Rombach et al. 2022). Although existing work (Gethe LDM features.To illustrate this more effectively, we viet al. 2022) has explored the use of DPM for detection-sualize the first three primary components of the extracted oriented training data synthesis, it separates the image andfeature maps from different denoising\nhas explored the use of DPM for detection-sualize the first three primary components of the extracted oriented training data synthesis, it separates the image andfeature maps from different denoising stages in Fig. 3(c). label generation stages. This paper takes the first step to gen-At lower resolutions (e.g., 16x16), a coarse layout for oberate high-quality detection training pairs in a single stage.jects in the image can be observed, with pixels in the same category sharing similar colors. As the resolution increases, Method finer-grained location signals become more prominent, allowing for more precise object instance detection.In the following subsections, we first present the preliminary The semantic information in LDM could be investigatedof latent diffusion model (LDM) (Rombach et al. 2022). through the cross-attention between the visual and textualThen, we reveal that LDM is an effective and robust backinformation in the text-conditioned image generation pro-bone for object\nal. 2022). through the cross-attention between the visual and textualThen, we reveal that LDM is an effective and robust backinformation in the text-conditioned image generation pro-bone for object detection and details our novel strategy to cess. We first compute the average cross-attention mapseffectively learn the Detection-Adapter by using existing deacross all the time steps for each object in the text prompt, tection datasets. At last, we present the way to use our DiffusionEngine for scaling up detection data.\nPreliminary We leverage the off-the-shelf pre-trained LDM to generate high-quality images in this work. LDM is a conditional image generator that includes an autoencoder for perceptual compression and a diffusion probabilistic model in the latent space. It is built with the U-Net backbone modulated via a cross-attention mechanism for text-guided image generation. The process of image-guided text-to-image generation consists of four stages: i) encoding the image to the latent space z = E(x); ii) obtaining a noisy sample by for- 0 ward diffusion; iii) getting a clean sample via backward dif-Figure 3: (a)(b): Comparing our DE to cross-attention for defusion; and iv) decoding the latent vector back to the imagetection, we find DE produces more accurate and less noisy. As shown in (Ho, Jain, and Abbeel 2020; Lux = D(z) results. (c): Different U-Net decoding stages capture coarse-0 et al. 2022), the forward diffusion has a closed-form solu-to-fine object features. (d): Simply binarizing the",
      "keywords": [],
      "page_range": [
        3,
        3
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T09:02:05.701058+00:00"
      }
    },
    {
      "group_id": "group-4",
      "chunk_indices": [
        18
      ],
      "char_count": 411,
      "summary": "Abbeel 2020; Lux = D(z) results. (c): Different U-Net decoding stages capture co",
      "digest": "Abbeel 2020; Lux = D(z) results. (c): Different U-Net decoding stages capture coarse-0 et al. 2022), the forward diffusion has a closed-form solu-to-fine object features. (d): Simply binarizing the crosstion to obtain the noise sample in any time step t, attention for each object highlights the semantic relative areas but is not sufficient for finer object detection. z = α z + σ ϵ, ϵ ∼N (ϵ|0, I), (1) t t 0 t",
      "full_text": "Abbeel 2020; Lux = D(z) results. (c): Different U-Net decoding stages capture coarse-0 et al. 2022), the forward diffusion has a closed-form solu-to-fine object features. (d): Simply binarizing the crosstion to obtain the noise sample in any time step t, attention for each object highlights the semantic relative areas but is not sufficient for finer object detection. z = α z + σ ϵ, ϵ ∼N (ϵ|0, I), (1) t t 0 t",
      "keywords": [],
      "page_range": [
        3,
        3
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T09:02:05.701058+00:00"
      }
    },
    {
      "group_id": "group-5",
      "chunk_indices": [
        19,
        20,
        21,
        22,
        23,
        24
      ],
      "char_count": 5107,
      "summary": "Figure 4: Overview of the proposed DiffusionEngine. The upper figure shows the t",
      "digest": "Figure 4: Overview of the proposed DiffusionEngine. The upper figure shows the training procedure of DiffusionEngine. Each image undergoes a 1-step noise adding and then denoising to simulate the last image generation step in the LDM. The detection adapter learns to leverage the extracted pyramid features from the U-Net for detection. The figure below shows how we use the trained DiffusionEngine for data scaling-up. A reference image undergoes a random number (k) of noise-adding steps and then denoising with text guidance. Finally, low-confidence detections are filtered out. then binarize them with the OTSU’s auto-thresholdingDINO (Zhang et al. 2022) in this paper. Specifically, the feamethod (Otsu 1979). Results in Fig. 3(d) demonstrate thatture maps are extracted from each U-Net block, and groups the binarized cross-attention maps highlight the related ob-of feature maps with the same resolution are concatenated ject regions, indicating that the semantic guidance providedto form a\nan",
      "full_text": "Figure 4: Overview of the proposed DiffusionEngine. The upper figure shows the training procedure of DiffusionEngine. Each image undergoes a 1-step noise adding and then denoising to simulate the last image generation step in the LDM. The detection adapter learns to leverage the extracted pyramid features from the U-Net for detection. The figure below shows how we use the trained DiffusionEngine for data scaling-up. A reference image undergoes a random number (k) of noise-adding steps and then denoising with text guidance. Finally, low-confidence detections are filtered out. then binarize them with the OTSU’s auto-thresholdingDINO (Zhang et al. 2022) in this paper. Specifically, the feamethod (Otsu 1979). Results in Fig. 3(d) demonstrate thatture maps are extracted from each U-Net block, and groups the binarized cross-attention maps highlight the related ob-of feature maps with the same resolution are concatenated ject regions, indicating that the semantic guidance providedto form a\nand groups the binarized cross-attention maps highlight the related ob-of feature maps with the same resolution are concatenated ject regions, indicating that the semantic guidance providedto form a pyramid. The detection adapter then utilizes the in the text prompt is well-reflected in LDM features.pyramid feature for predicting the bounding box ˆy.\nIs Cross-Attention Sufficient for Detection? BinarizingAdapter Optimization. In order to optimize the detecthe cross-attention (BCA) seems like a direct and explain-tion adapter, we require pairs of aligned LDM features and able approach to generating bounding boxes for object de-ground-truth detection results. While a naive approach to tection. Figs. 3(a)(b) compare the detection results of ourcollect such data would be to generate a new synthetic DiffusionEngine to the BCA. As shown in Fig. 3, the BCAdataset and extract features during the image generation prosuffers from four important limitations: i) Instance Merging.cess, this method is impractical due to the lack of ground- BCA reflects patch-wise text-visual similarity rather thantruth detection results and the burden of annotation. To cirinstance understanding, leading to instance merging whencumvent this issue, we propose to leverage existing object detection benchmarks for adapter learning, where the LDMthere are overlapping\nTo cirinstance understanding, leading to instance merging whencumvent this issue, we propose to leverage existing object detection benchmarks for adapter learning, where the LDMthere are overlapping instances of the same category (e.g., the two cats). ii) Instance Chunking. BCA chunks an in-features are obtained by simulating the last denoising step stance into multiple parts when partially obscured by otherwith real images. Our training procedure is illustrated at the objects (e.g., the couch). iii) Forcing Detect. BCA producestop of Fig. 4. Given an image x ∈ RH×W ×3with its groundinterpretable results only when the object is actually present truth annotations, the encoder first encodes into the y E x latent representation, where Rh×w×c within the image. When an object in the text prompt fails to be z = E(x) z ∈0 0 generated, BCA detection produces unexpected noise results h = H/8 and w = W/8. Then, a single forward diffusion (e.g., the remote). iv) Missing Detect., there could be\nfails to be z = E(x) z ∈0 0 generated, BCA detection produces unexpected noise results h = H/8 and w = W/8. Then, a single forward diffusion (e.g., the remote). iv) Missing Detect., there could be unex-step is performed to obtain the penultimate noisy sample z1 pected objects in the image that are not present in the text using Eq. 1 for t = 1. Finally, by feeding z and the time 1 prompt, causing missing detections (e.g., the potted plant is step into the U-Net for one-step denoising, we can extract t detected by DE (a) but missing in BCA (b)). intermediate features that approximate the features from the To address these issues, we propose to learn a detectionlast image generation step: adapter that aligns the semantic and location information in (4)LDM with detection-aware signals for improved detection. zˆ0 = ϵθ(z1, 1, c∅), where ∅ is equivalent to the unconditional signal. c∅ = τθ() Learning Detection-Adapter The chosen detection framework determines the training objective and can\ndetection. zˆ0 = ϵθ(z1, 1, c∅), where ∅ is equivalent to the unconditional signal. c∅ = τθ() Learning Detection-Adapter The chosen detection framework determines the training objective and can be simplified to: Adapter Architecture. As shown at the top of Fig. 4, the proposed DiffusionEngine is comprised of a frozen diffu- (5)LDE = LDet(y, ˆy). sion model and a detection adapter that is designed to produce accurate detection bounding boxes. It is worth notingWe have empirically found that whether or not using an that any detection framework can be employed as the detec-image-aligned text prompt during training has little effect tion adapter. We use the state-of-the-art detection frameworkon the training procedure (c in Eq. 2 versus cin Eq. 4), as p ∅",
      "keywords": [],
      "page_range": [
        4,
        4
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T09:02:05.701058+00:00"
      }
    },
    {
      "group_id": "group-6",
      "chunk_indices": [
        25,
        26,
        27,
        28,
        29,
        30
      ],
      "char_count": 5014,
      "summary": "the conditioning signal has a negligible impact on the gener- Image Size. The re",
      "digest": "the conditioning signal has a negligible impact on the gener- Image Size. The reference image is resized to 512 × 1024 ation results in only one step of denoising. with the original aspect ratio then random crop within 7682. Discussions. This one-step training procedure offers three Image Diversity. For each image, we randomly choose a main advantages: i) It only requires image-detection pairsseed and an encoding ratio between 0.3 and 1.0 to ensure for training, which allows for the use of datasets withoutgenerative diversity. When the encoding ratio is set to 1.0, corresponding image descriptions; ii) The layout and com-the image is converted to Gaussian noise, and the generation ponents of the original image are well-preserved after theprocess is collapsed to the text-to-image generation process. inversion, which ensures the credibility of ground-truth an-Annotation Diversity. We establish an annotation lower notations. iii) Existing labeled detection benchmarks can bebound and\nproce",
      "full_text": "the conditioning signal has a negligible impact on the gener- Image Size. The reference image is resized to 512 × 1024 ation results in only one step of denoising. with the original aspect ratio then random crop within 7682. Discussions. This one-step training procedure offers three Image Diversity. For each image, we randomly choose a main advantages: i) It only requires image-detection pairsseed and an encoding ratio between 0.3 and 1.0 to ensure for training, which allows for the use of datasets withoutgenerative diversity. When the encoding ratio is set to 1.0, corresponding image descriptions; ii) The layout and com-the image is converted to Gaussian noise, and the generation ponents of the original image are well-preserved after theprocess is collapsed to the text-to-image generation process. inversion, which ensures the credibility of ground-truth an-Annotation Diversity. We establish an annotation lower notations. iii) Existing labeled detection benchmarks can bebound and\nprocess. inversion, which ensures the credibility of ground-truth an-Annotation Diversity. We establish an annotation lower notations. iii) Existing labeled detection benchmarks can bebound and record the number of generated annotations for directly used to learn the detection adapter, without addi-each category during the scaling-up procedure. This process tional data collection and labeling efforts. ends when all categories exceed the target lower bound.\nScaling Up Data with DiffusionEngine Table 1: Dataset Statistics of COCO-DE and VOC-DE.\nOur DiffusionEngine, equipped with the learned detection adapter, can effectively scale up data in a single stage. Dataset #Images #Scale #Instances #Scale Image. By learning a detection adapter without modifying COCO 117,266 - 849,949 -the LDM, the image generation process remains identical COCO-DE 205,287 1.7 1,281,418 1.5 to the original process in LDM. As shown at the bottom of × × Fig. 4, the reference image is first encoded and forwarded til VOC 16,551 - 47,223 - VOC-DE 64,934 3.9× 168,141 3.6×a random noise-adding step k using Eq. 1. Then, the noisy sample z is denoised for k steps to generate the image k guided by the text embedding. Since we no longer need to maintain the layout as in the training stage, we can fully uti- Experiments lize all image-generation capabilities inherited from LDM.\nImplementation DetailsLabel. Consistent with the training process, we extract features from the last denoising step, feed them to the adapter, We freeze the pre-trained Stable Diffusion v2 and optimize and obtain detection results for the generated image. Follow-the detection-adapter solely on COCO (Lin et al. 2014) withing the empirical practice in detector inference, we filter outout additional data. The adapter is trained for 90k iterations low-confidence predictions with a threshold, keep-with a global batch size of 64. AdamW (Loshchilov and Hut- δ = 0.3 ter 2019) is employed, with the lr starting at 2e-4 and de-ing the rest as generated annotations.\nDiversity. By modifying the seed, encode ratio, guidancecreases to 2e-5 at the 80k iteration. For data scaling-up, we scale, and conditional text prompt, our DE can scale up theuse DPM-Solver++ as the sampler, with default inference reference dataset with labeled generated images that havesteps of 30 and a classifier-free guidance scale of 7.5. various degrees of discrepancy to the reference images. The COCO Detection Evaluationsecond row in Fig.1 provides an example of data scaling-up using different encode ratios. As the noise-adding step in-We evaluate the effectiveness of scaling up data using Difcreases, slight distortion accumulates, resulting in more di-fusionEngine (DE) on the widely-used COCO object detecverse reconstructed images compared to the original input. tion benchmark. For fair comparison, we maintain identical Our DE generates labeled data well for multi-object tasksbatch sizes, apply the same data augmentations, and conwith different sizes and is not limited to the\nfair comparison, we maintain identical Our DE generates labeled data well for multi-object tasksbatch sizes, apply the same data augmentations, and conwith different sizes and is not limited to the original layout.duct an equal number of training iterations in each experi- Prompts. For datasets that have off-the-shelf captions for ment group. We used the default settings for each algorithm each image, we directly use these captions as input textas in (Chen et al. 2019; Wu et al. 2019). To simplify the prompts for image generation. For those without captions, exposition, we utilize “DE” to denote data scaling-up via we use a generic text prompt, ’A [domain], with [cls-a], [cls-DiffusionEngine in subsequent sections. The outcomes are b],... in the [domain].’, where [cls-i] represents the objectdetailed in Table 2, which indicates that DE is orthogonal to names appearing in each image and the [domain] tag is cu-existing works in the following aspects: rated respect to the data, e.g.,",
      "keywords": [],
      "page_range": [
        5,
        5
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T09:02:05.701058+00:00"
      }
    },
    {
      "group_id": "group-7",
      "chunk_indices": [
        31,
        32
      ],
      "char_count": 1699,
      "summary": "objectdetailed in Table 2, which indicates that DE is orthogonal to names appear",
      "digest": "objectdetailed in Table 2, which indicates that DE is orthogonal to names appearing in each image and the [domain] tag is cu-existing works in the following aspects: rated respect to the data, e.g., photo, clipart. Detection Algorithms. We adopt various detection algorithms, including the anchor-based one-stage algorithm Reti- DiffusionEngine Detection Dataset naNet (Lin et al. 2017a), anchor-based two-stage algorithm In this section, we detail the construction of our two scaling- Faster-RCNN (Ren et al. 2015), and anchor-free algorithm up datasets, termed COCO-DE and VOC-DE. The statistics DINO (Zhang et al. 2022). The results show that incorof the two datasets are summarized in Tab. 1. porating data generated via DE outperforms the baseline Reference Images & Text Prompts. We employ anby e.g. 3.3% and 3.1% mAP with RetinaNet and DINO image-guided text-to-image generation process to scaling-(ResNet50), respectively. This demonstrates that combinup datasets. For COCO-DE, we adopt the\ne",
      "full_text": "objectdetailed in Table 2, which indicates that DE is orthogonal to names appearing in each image and the [domain] tag is cu-existing works in the following aspects: rated respect to the data, e.g., photo, clipart. Detection Algorithms. We adopt various detection algorithms, including the anchor-based one-stage algorithm Reti- DiffusionEngine Detection Dataset naNet (Lin et al. 2017a), anchor-based two-stage algorithm In this section, we detail the construction of our two scaling- Faster-RCNN (Ren et al. 2015), and anchor-free algorithm up datasets, termed COCO-DE and VOC-DE. The statistics DINO (Zhang et al. 2022). The results show that incorof the two datasets are summarized in Tab. 1. porating data generated via DE outperforms the baseline Reference Images & Text Prompts. We employ anby e.g. 3.3% and 3.1% mAP with RetinaNet and DINO image-guided text-to-image generation process to scaling-(ResNet50), respectively. This demonstrates that combinup datasets. For COCO-DE, we adopt the\ne.g. 3.3% and 3.1% mAP with RetinaNet and DINO image-guided text-to-image generation process to scaling-(ResNet50), respectively. This demonstrates that combinup datasets. For COCO-DE, we adopt the images froming DE-generated data with different detection algorithms COCO train2017 as references and their corresponding cap-achieves consistent performance gains. tions as text prompts. For VOC-DE, the images are from theBackbone Pre-training Algorithm. In addition to fully- Pascal VOC trainval0712 split, and we use the generic textsupervised pre-training, we use self-supervised pre-training prompt as described in the former section. backbone (Grill et al. 2020) as initialization to validate the",
      "keywords": [],
      "page_range": [
        5,
        5
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T09:02:05.701058+00:00"
      }
    },
    {
      "group_id": "group-8",
      "chunk_indices": [
        33,
        34,
        35,
        36,
        37,
        38,
        39
      ],
      "char_count": 5244,
      "summary": "Table 2: Effectiveness of DE on COCO. We combine the DE-generated training data ",
      "digest": "Table 2: Effectiveness of DE on COCO. We combine the DE-generated training data with different Frameworks, Backbones, Pre-train Dataset. Consistent improvement demonstrates the proposed DE is effective and orthogonal to existing methods.\nFramework Backbone Pre-train Schedule mAP mAP50 mAP75 mAPs mAPm mAPl RetinaNet 38.0 57.0 40.6 22.2 41.2 49.3R50 IN-1k 6×w/ DE 41.3 (3.3↑) 60.4 (3.4↑) 44.2 (3.6↑) 24.1 (1.9↑) 45.9 (4.7↑) 53.8 (4.5↑) Faster-RCNN 39.0 59.4 42.4 23.2 41.7 51.3 w/ BYOL (Grill et al. 2020) R50 IN-1k 9× 40.4 60.6 44.0 24.4 43.5 51.8 w/ DE + BYOL 43.8 (4.8) 63.7 (4.3) 47.4 (5.0) 25.8 (2.6) 47.8 (6.1) 57.3 (6.0)↑ ↑ ↑ ↑ ↑ ↑ DINO 49.2 67.0 53.6 32.9 52.3 62.8R50 IN-1k 6×w/ DE 52.3 (3.1↑) 70.0 (3.0↑) 57.1 (3.5↑) 35.3 (2.4↑) 56.0 (3.7↑) 66.3 (3.5↑) DINO Swin-L IN-22k 9 57.1 76.1 62.2 39.0 61.3 72.9 w/ DE × 58.8 (1.7) 77.1 (1.0) 64.3 (2.1) 41.8 (2.8) 62.6 (1.3) 75.0 (2.1)↑ ↑ ↑ ↑ ↑ ↑ could already surpass the baseline that training on the real, manually labeled dataset. By combining ",
      "full_text": "Table 2: Effectiveness of DE on COCO. We combine the DE-generated training data with different Frameworks, Backbones, Pre-train Dataset. Consistent improvement demonstrates the proposed DE is effective and orthogonal to existing methods.\nFramework Backbone Pre-train Schedule mAP mAP50 mAP75 mAPs mAPm mAPl RetinaNet 38.0 57.0 40.6 22.2 41.2 49.3R50 IN-1k 6×w/ DE 41.3 (3.3↑) 60.4 (3.4↑) 44.2 (3.6↑) 24.1 (1.9↑) 45.9 (4.7↑) 53.8 (4.5↑) Faster-RCNN 39.0 59.4 42.4 23.2 41.7 51.3 w/ BYOL (Grill et al. 2020) R50 IN-1k 9× 40.4 60.6 44.0 24.4 43.5 51.8 w/ DE + BYOL 43.8 (4.8) 63.7 (4.3) 47.4 (5.0) 25.8 (2.6) 47.8 (6.1) 57.3 (6.0)↑ ↑ ↑ ↑ ↑ ↑ DINO 49.2 67.0 53.6 32.9 52.3 62.8R50 IN-1k 6×w/ DE 52.3 (3.1↑) 70.0 (3.0↑) 57.1 (3.5↑) 35.3 (2.4↑) 56.0 (3.7↑) 66.3 (3.5↑) DINO Swin-L IN-22k 9 57.1 76.1 62.2 39.0 61.3 72.9 w/ DE × 58.8 (1.7) 77.1 (1.0) 64.3 (2.1) 41.8 (2.8) 62.6 (1.3) 75.0 (2.1)↑ ↑ ↑ ↑ ↑ ↑ could already surpass the baseline that training on the real, manually labeled dataset. By combining the generated dataset with the real labeled data, we achieve further improvement, which indicates that the DE-generated data is an effective supplement to the real dataset.\nTable 3: DE on VOC-0712. The backbone is ResNet50. † indicates annotations of real images are not used for training.\nMethod #Images mAP mAP mAP50 75 Faster-RCNN 16551 (1) 50.7 80.2 55.0 Figure 5: Performance with increasing schedule on COCO × 5 † 52.5 (1.8) 77.2 58.1w/ DE × ↑ v.s COCO w/DE. 6 58.3 (7.6) 82.7 64.7× ↑ robustness of DE. The second block of Tab. 2 shows that it leads to a 1.4% mAP improvement over the fully-supervisedComparison with SOTAs baseline. Moreover, combining DE with the self-supervisedThis section compares DE with some state-of-the-art data pre-trained backbone further boosts mAP to 43.8%, which isscaling-up techniques, such as Copy-Paste (Ghiasi et al. an additional gain of 3.4% compared to the previous result.2021) and DALL-E for Detection (Ge et al. 2022). Follow- Backbone and Pre-training Dataset. In the last two blocks, ing (Ge et al. 2022), we use Faster-RCNN with ResNet-50 as we conduct experiments on the DINO framework with twothe backbone and experiment on the VOC2012 segmentation backbones: ResNet50 (He et al. 2016) and Swin-L (Liu et al.set. As shown in Tab. 4, the\nas we conduct experiments on the DINO framework with twothe backbone and experiment on the VOC2012 segmentation backbones: ResNet50 (He et al. 2016) and Swin-L (Liu et al.set. As shown in Tab. 4, the relative gain of DE surpasses 2022) Transformer. DE provides a 3.1% mAP gain with thethat of DALL-E by adding only twice the amount of original ResNet50 backbone and +1.7% with the Swin-L. Even start-data, even surpassing the strong baseline Copy-Paste (Ghiasi ing with a strong baseline with large backbone architectureet al. 2021), demonstrating that DE helps provide higher- (Swin-L) and pre-trained on a bigger dataset (ImageNet-quality pairs. Although copy-paste infinitely scales up the 22k), DE can further boost the performance. amount of training data through random combination, its di- Performance with Schedule Scaling. Fig. 5 depicts the per-versity is limited by the original instances, while DE does formance curves for training with or without the generatednot. We can see that the\nwith Schedule Scaling. Fig. 5 depicts the per-versity is limited by the original instances, while DE does formance curves for training with or without the generatednot. We can see that the performance continues to increase COCO-DE for various schedules. The X-axis displays theas more generated data is added, indicating that DE is an efrelative iterations w.r.t. the declined learning rate. Here wefective solution for large-scale data expansion. We also comhave three observations: i) validation performance using thepare with X-Paste (Zhao et al. 2023) under the same setting 3 schedule gradually improves as expected. ii) withoutthat the baseline CenterNet2 (Zhou, Koltun, and Kr¨ahenb¨uhl× generated data, increasing schedule leads to a decrease in2021) is trained with the Swin-L backbone on COCO. Reboth validation mAP and the training loss, i.e., overfittingsults show that the relative mAP improvement by the prooccurs. iii) scaling data with DE further improves the per-posed DE without\nCOCO. Reboth validation mAP and the training loss, i.e., overfittingsults show that the relative mAP improvement by the prooccurs. iii) scaling data with DE further improves the per-posed DE without using the mask for training is 2.0%, which formance, indicating that DE is an effective data scaling-upis higher than 1.5% improvement achieved by X-Paste. technology rather than a simple data replay. We observe the same tendency of overfitting for all baselines in Tab. 2, evenCross Domain Data Scaling-up for DINO with strong default data augmentation (see supp.).To assess the robustness of DiffusionEngine in out-of- Generalization. We also experiment on the VOC-0712domain scenarios, we conducted experiments on the Clipartdataset (Tab. 3) to verify the generalization of Diffusio-1k dataset (Inoue et al. 2018), which comprises 500 Clipart nEngine. When training only with the generated data, wedomain images. As indicated in the first block of Tab. 5, we",
      "keywords": [],
      "page_range": [
        6,
        6
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T09:02:05.701058+00:00"
      }
    },
    {
      "group_id": "group-9",
      "chunk_indices": [
        40,
        41,
        42,
        43,
        44
      ],
      "char_count": 3631,
      "summary": "Figure 6: Comparison with Grounded Diffusion Model (GDM). Scaling up data with G",
      "digest": "Figure 6: Comparison with Grounded Diffusion Model (GDM). Scaling up data with GLIGEN (Li et al. 2023) and our DE follows a distinct paradigm. While GDM specifies the layout and explicitly controls the image generation, our DE predicts the layout concurrently with the generation process. As depicted, GDM may generate unexpected objects in unspecified areas, leading to missed annotation①, whereas the annotations of specified areas may be wrong due to mistaken generation②.\nTable 4: Compare with SOTAs on VOC-12. We only use theTable 5: The results of cross-domain object detection on the Segmentation Set for experiments following DALL-E, butClipart1k test set for VOC-12 →Clipart-1k adaptation. † the seg. masks are NOT used in our experiments. ∗denotesindicates annotations of real images are not used for training. our reproduced result in the same setting.\nLabeled Unlabeled mAP50 Method #Images mAP mAP mAP50 75 VOC - 28.8 Faster-RCNN 1464 (1) 17.0 45.5 - Sup Clipart - 45.0× w/ DALL-E 41× 25",
      "full_text": "Figure 6: Comparison with Grounded Diffusion Model (GDM). Scaling up data with GLIGEN (Li et al. 2023) and our DE follows a distinct paradigm. While GDM specifies the layout and explicitly controls the image generation, our DE predicts the layout concurrently with the generation process. As depicted, GDM may generate unexpected objects in unspecified areas, leading to missed annotation①, whereas the annotations of specified areas may be wrong due to mistaken generation②.\nTable 4: Compare with SOTAs on VOC-12. We only use theTable 5: The results of cross-domain object detection on the Segmentation Set for experiments following DALL-E, butClipart1k test set for VOC-12 →Clipart-1k adaptation. † the seg. masks are NOT used in our experiments. ∗denotesindicates annotations of real images are not used for training. our reproduced result in the same setting.\nLabeled Unlabeled mAP50 Method #Images mAP mAP mAP50 75 VOC - 28.8 Faster-RCNN 1464 (1) 17.0 45.5 - Sup Clipart - 45.0× w/ DALL-E 41× 25.9 (8.9↑) 51.8 - DE† - 56.5 (11.5↑) Faster-RCNN∗ 1464 (1×) 18.1 44.9 9.8 VOC Clipart 49.3 Semi-Supw/ Copy-Paste - 24.5 (6.5↑) 54.9 17.2 VOC Clipart + DE† 52.9 (7.9↑) (AT (Li et al. 2022))2× 26.1 (8.1↑) 57.9 18.9 VOC+DE† Clipart 63.4 (14.1↑) 3× 30.0 (11.9↑) 63.1 23.3w/ DE 5 34.2 (16.1) 67.4 29.4× ↑ 9 39.0 (20.9) 71.6 37.9× ↑ Condition: GDMs necessitate category lists, prompts, and additional bounding boxes, DiffusionEngine only require simple text prompts and optional reference images. directly trained the model using the DE-generated dataset.\nPerformance: As shown in Fig. 6, DiffusionEngine effec-The results indicate that DE significantly outperforms the tively unifies the processes of image generation and labeling, model trained on Clipart (+11.5%), thus highlighting the efthereby enabling the provision of a wide variety of imagesficacy of DiffusionEngine in scaling up cross-domain data. with detailed annotations. In contrast, GDM is limited byIn addition, we leveraged Adaptive Teacher (AT (Li et al. the conditions of the box and leads to missed annotations,2022)) to perform cross-domain semi-supervised experimistaken image generation, and simplistic layouts.ments. As demonstrated in the second block of Tab. 5, incorporating DE-generated images as either unlabeled or labeled data yielded gains of 7.9% and 14.1%, respectively. Limitations and Further Work These findings validate the robustness of DE to generate images and labels for semi-supervised learning. All-in-One Model. DiffusionEngine can be easily extended to other\nLimitations and Further Work These findings validate the robustness of DE to generate images and labels for semi-supervised learning. All-in-One Model. DiffusionEngine can be easily extended to other tasks via task-specific adaptors.\nDiscussion with Grounded Diffusion Models ChatGPT. Textual guidance prompts are not available in We also investigate recent grounded diffusion modelsmany scenarios. It would be interesting to introduce Chat- (GDMs) such as ReCo (Yang et al. 2023) and GLIGEN (LiGPT with in-context learning to generate guidance prompts.\nRLHF. Integrating task-aware human feedback may furtheret al. 2023), and compare with DiffusionEngine: Paradigm: GDMs are primarily designed to generate con-improve the alignment and quality of detection pairs. trollable results based on detection boxes, whereas Diffu-We hope that our work will inspire more researchers to insionEngine strives to generate diverse images with accuratevestigate data scaling-up using the diffusion model and proannotations via a single-step inference. vide valuable insights for future research.",
      "keywords": [],
      "page_range": [
        7,
        7
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T09:02:05.701058+00:00"
      }
    },
    {
      "group_id": "group-10",
      "chunk_indices": [
        45,
        46,
        47
      ],
      "char_count": 2816,
      "summary": "Conclusion Ho, J.; Saharia, C.; Chan, W.; Fleet, D. J.; Norouzi, M.; and Saliman",
      "digest": "Conclusion Ho, J.; Saharia, C.; Chan, W.; Fleet, D. J.; Norouzi, M.; and Salimans, T. 2022. Cascaded Diffusion Models for High Fidelity ImageWe introduce the DiffusionEngine (DE), a scalable and effi- Generation. J. Mach. Learn. Res., 23(47): 1–33. cient data engine for object detection that generates high- Inoue, N.; Furuta, R.; Yamasaki, T.; and Aizawa, K. 2018. Cross-quality detection-oriented training pairs in a single stage. domain weakly-supervised object detection through progressiveThe detection-adapter aligns the implicit detection-oriented domain adaptation. In Proceedings of the IEEE Conference onknowledge in off-the-shelf diffusion models to generate ac- Computer Vision and Pattern Recognition, 5001–5009. curate annotations. Additionally, we contribute two datasets, Kingma, D. P.; and Dhariwal, P. 2018. Glow: Generative flow withCOCO-DE and VOC-DE, which are intended to scale up exinvertible 1x1 convolutions. Advances in neural information proisting detection benchmarks.\nP.",
      "full_text": "Conclusion Ho, J.; Saharia, C.; Chan, W.; Fleet, D. J.; Norouzi, M.; and Salimans, T. 2022. Cascaded Diffusion Models for High Fidelity ImageWe introduce the DiffusionEngine (DE), a scalable and effi- Generation. J. Mach. Learn. Res., 23(47): 1–33. cient data engine for object detection that generates high- Inoue, N.; Furuta, R.; Yamasaki, T.; and Aizawa, K. 2018. Cross-quality detection-oriented training pairs in a single stage. domain weakly-supervised object detection through progressiveThe detection-adapter aligns the implicit detection-oriented domain adaptation. In Proceedings of the IEEE Conference onknowledge in off-the-shelf diffusion models to generate ac- Computer Vision and Pattern Recognition, 5001–5009. curate annotations. Additionally, we contribute two datasets, Kingma, D. P.; and Dhariwal, P. 2018. Glow: Generative flow withCOCO-DE and VOC-DE, which are intended to scale up exinvertible 1x1 convolutions. Advances in neural information proisting detection benchmarks.\nP.; and Dhariwal, P. 2018. Glow: Generative flow withCOCO-DE and VOC-DE, which are intended to scale up exinvertible 1x1 convolutions. Advances in neural information proisting detection benchmarks. Our experiments demonstrate cessing systems, 31. that DE enables the generation of scalable, diverse, and gen-Kingma, D. P.; and Welling, M. 2013. Auto-encoding variational eralizable data, and incorporating data scaling up via DE bayes. arXiv preprint arXiv:1312.6114. through a plug-and-play manner can achieve significant im-Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2017. Imagenet provements in various scenarios. classification with deep convolutional neural networks. Communications of the ACM, 60(6): 84–90. References Li, Y.; Liu, H.; Wu, Q.; Mu, F.; Yang, J.; Gao, J.; Li, C.; and Lee, Cai, Z.; and Vasconcelos, N. 2019. Cascade R-CNN: high qualityY. J. 2023. Gligen: Open-set grounded text-to-image generation. object detection and instance segmentation. IEEE transactions on In\nLee, Cai, Z.; and Vasconcelos, N. 2019. Cascade R-CNN: high qualityY. J. 2023. Gligen: Open-set grounded text-to-image generation. object detection and instance segmentation. IEEE transactions on In Proceedings of the IEEE/CVF Conference on Computer Vision pattern analysis and machine intelligence, 43(5): 1483–1498. and Pattern Recognition, 22511–22521. Carion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov, A.; andLi, Y.-J.; Dai, X.; Ma, C.-Y.; Liu, Y.-C.; Chen, K.; Wu, B.; He, Z.; Zagoruyko, S. 2020. End-to-end object detection with transform-Kitani, K.; and Vajda, P. 2022. Cross-Domain Adaptive Teacher ers. In Computer Vision–ECCV 2020: 16th European Conference, for Object Detection. In IEEE Conference on Computer Vision and Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16, 213– Pattern Recognition.",
      "keywords": [],
      "page_range": [
        8,
        8
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T09:02:05.701058+00:00"
      }
    },
    {
      "group_id": "group-11",
      "chunk_indices": [
        48,
        49,
        50
      ],
      "char_count": 2472,
      "summary": "229. Springer. Lin, T.; Goyal, P.; Girshick, R. B.; He, K.; and Doll´ar, P. 2017",
      "digest": "229. Springer. Lin, T.; Goyal, P.; Girshick, R. B.; He, K.; and Doll´ar, P. 2017a. Fo- Chen, K.; Wang, J.; Pang, J.; Cao, Y.; Xiong, Y.; Li, X.; Sun, S.; cal Loss for Dense Object Detection. In IEEE International Con- Feng, W.; Liu, Z.; Xu, J.; Zhang, Z.; Cheng, D.; Zhu, C.; Cheng, ference on Computer Vision, 2999–3007. IEEE Computer Society.\nT.; Zhao, Q.; Li, B.; Lu, X.; Zhu, R.; Wu, Y.; Dai, J.; Wang, J.; Lin, T.-Y.; Goyal, P.; Girshick, R.; He, K.; and Doll´ar, P. 2017b.\nShi, J.; Ouyang, W.; Loy, C. C.; and Lin, D. 2019. MMDetection: Focal loss for dense object detection. In Proceedings of the IEEE Open MMLab Detection Toolbox and Benchmark. arXiv preprintinternational conference on computer vision, 2980–2988. arXiv:1906.07155. Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, Chen, Y.; Li, Y.; Kong, T.; Qi, L.; Chu, R.; Li, L.; and Jia, J. 2021. D.; Doll´ar, P.; and Zitnick, C. L. 2014. Microsoft coco: Common Scale-aware automatic augmentation for object detection",
      "full_text": "229. Springer. Lin, T.; Goyal, P.; Girshick, R. B.; He, K.; and Doll´ar, P. 2017a. Fo- Chen, K.; Wang, J.; Pang, J.; Cao, Y.; Xiong, Y.; Li, X.; Sun, S.; cal Loss for Dense Object Detection. In IEEE International Con- Feng, W.; Liu, Z.; Xu, J.; Zhang, Z.; Cheng, D.; Zhu, C.; Cheng, ference on Computer Vision, 2999–3007. IEEE Computer Society.\nT.; Zhao, Q.; Li, B.; Lu, X.; Zhu, R.; Wu, Y.; Dai, J.; Wang, J.; Lin, T.-Y.; Goyal, P.; Girshick, R.; He, K.; and Doll´ar, P. 2017b.\nShi, J.; Ouyang, W.; Loy, C. C.; and Lin, D. 2019. MMDetection: Focal loss for dense object detection. In Proceedings of the IEEE Open MMLab Detection Toolbox and Benchmark. arXiv preprintinternational conference on computer vision, 2980–2988. arXiv:1906.07155. Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, Chen, Y.; Li, Y.; Kong, T.; Qi, L.; Chu, R.; Li, L.; and Jia, J. 2021. D.; Doll´ar, P.; and Zitnick, C. L. 2014. Microsoft coco: Common Scale-aware automatic augmentation for object detection. In Pro- objects in context. In Computer Vision–ECCV 2014: 13th Euroceedings of the IEEE/CVF Conference on Computer Vision andpean Conference, Zurich, Switzerland, September 6-12, 2014, Pro- Pattern Recognition, 9563–9572. ceedings, Part V 13, 740–755. Creswell, A.; White, T.; Dumoulin, V.; Arulkumaran, K.; Sengupta, Liu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.- B.; and Bharath, A. A. 2018. Generative adversarial networks: AnY.; and Berg, A. C. 2016.\nT.; Dumoulin, V.; Arulkumaran, K.; Sengupta, Liu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.- B.; and Bharath, A. A. 2018. Generative adversarial networks: AnY.; and Berg, A. C. 2016. Ssd: Single shot multibox detector. In overview. IEEE signal processing magazine, 35(1): 53–65. Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, PartGe, Y.; Xu, J.; Zhao, B. N.; Itti, L.; and Vineet, V. 2022. Dall-e I 14, 21–37. Springer.for detection: Language-driven context image synthesis for object detection. arXiv preprint arXiv:2206.09592. Liu, Z.; Hu, H.; Lin, Y.; Yao, Z.; Xie, Z.; Wei, Y.; Ning, J.; Cao, Y.; Ghiasi, G.; Cui, Y.; Srinivas, A.; Qian, R.; Lin, T.-Y.; Cubuk, E. D.; Zhang, Z.; Dong, L.; Wei, F.; and Guo, B. 2022. Swin Transformer Le, Q. V.; and Zoph, B. 2021. Simple copy-paste is a strong dataV2: Scaling Up Capacity and Resolution. In CVPR, 11999–12009. augmentation method for instance segmentation. In",
      "keywords": [],
      "page_range": [
        8,
        8
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T09:02:05.701058+00:00"
      }
    },
    {
      "group_id": "group-12",
      "chunk_indices": [
        51,
        52,
        53
      ],
      "char_count": 2711,
      "summary": "2022. Swin Transformer Le, Q. V.; and Zoph, B. 2021. Simple copy-paste is a stro",
      "digest": "2022. Swin Transformer Le, Q. V.; and Zoph, B. 2021. Simple copy-paste is a strong dataV2: Scaling Up Capacity and Resolution. In CVPR, 11999–12009. augmentation method for instance segmentation. In Proceedings of IEEE. the IEEE/CVF conference on computer vision and pattern recogni-Loshchilov, I.; and Hutter, F. 2019. Decoupled Weight Decay Regtion, 2918–2928. ularization. In Proceedings of the International Conference on Girshick, R. 2015. Fast r-cnn. In Proceedings of the IEEE interna-Learning Representations. tional conference on computer vision, 1440–1448. Lu, C.; Zhou, Y.; Bao, F.; Chen, J.; Li, C.; and Zhu, J. 2022. DPM- Solver++: Fast Solver for Guided Sampling of Diffusion Proba-Grill, J.; Strub, F.; Altch´e, F.; Tallec, C.; Richemond, P. H.; Buchatskaya, E.; Doersch, C.; Pires, B. ´ bilistic Models. arXiv preprint arXiv:2211.01095. A.; Guo, Z.; Azar, M. G.; Piot, B.; Kavukcuoglu, K.; Munos, R.; and Valko, M. 2020. Boot-Nichol, A. Q.; and Dhariwal, P. 2021. Improved denoising\n´",
      "full_text": "2022. Swin Transformer Le, Q. V.; and Zoph, B. 2021. Simple copy-paste is a strong dataV2: Scaling Up Capacity and Resolution. In CVPR, 11999–12009. augmentation method for instance segmentation. In Proceedings of IEEE. the IEEE/CVF conference on computer vision and pattern recogni-Loshchilov, I.; and Hutter, F. 2019. Decoupled Weight Decay Regtion, 2918–2928. ularization. In Proceedings of the International Conference on Girshick, R. 2015. Fast r-cnn. In Proceedings of the IEEE interna-Learning Representations. tional conference on computer vision, 1440–1448. Lu, C.; Zhou, Y.; Bao, F.; Chen, J.; Li, C.; and Zhu, J. 2022. DPM- Solver++: Fast Solver for Guided Sampling of Diffusion Proba-Grill, J.; Strub, F.; Altch´e, F.; Tallec, C.; Richemond, P. H.; Buchatskaya, E.; Doersch, C.; Pires, B. ´ bilistic Models. arXiv preprint arXiv:2211.01095. A.; Guo, Z.; Azar, M. G.; Piot, B.; Kavukcuoglu, K.; Munos, R.; and Valko, M. 2020. Boot-Nichol, A. Q.; and Dhariwal, P. 2021. Improved denoising\n´ bilistic Models. arXiv preprint arXiv:2211.01095. A.; Guo, Z.; Azar, M. G.; Piot, B.; Kavukcuoglu, K.; Munos, R.; and Valko, M. 2020. Boot-Nichol, A. Q.; and Dhariwal, P. 2021. Improved denoising diffustrap Your Own Latent - A New Approach to Self-Supervisedsion probabilistic models. In International Conference on Machine Learning. In Advances in neural information processing systems, Learning, 8162–8171. PMLR. volume 33, 21271–21284. Otsu, N. 1979. A threshold selection method from gray-level his- He, K.; Gkioxari, G.; Doll´ar, P.; and Girshick, R. 2017. Mask r-cnn. tograms.IEEE transactions on systems, man, and cybernetics, In Proceedings of the IEEE international conference on computer 9(1): 62–66. vision, 2961–2969. Ramesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen, M. 2022. He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual Learn-Hierarchical text-conditional image generation with clip latents. ing for Image Recognition. In CVPR, 770–778. IEEE ComputerarXiv\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual Learn-Hierarchical text-conditional image generation with clip latents. ing for Image Recognition. In CVPR, 770–778. IEEE ComputerarXiv preprint arXiv:2204.06125. Society. Redmon, J.; Divvala, S.; Girshick, R.; and Farhadi, A. 2016. You Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion proba-only look once: Unified, real-time object detection. In Proceedings bilistic models. Advances in Neural Information Processing Sys-of the IEEE conference on computer vision and pattern recognition, tems, 33: 6840–6851. 779–788. Ren, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-cnn: Towards real-time object detection with region proposal networks.",
      "keywords": [],
      "page_range": [
        8,
        8
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T09:02:05.701058+00:00"
      }
    },
    {
      "group_id": "group-13",
      "chunk_indices": [
        54
      ],
      "char_count": 119,
      "summary": "Advances in neural information processing systems, 28. Rombach, R.; Blattmann, A",
      "digest": "Advances in neural information processing systems, 28. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B.",
      "full_text": "Advances in neural information processing systems, 28. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B.",
      "keywords": [],
      "page_range": [
        9,
        9
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T09:02:05.701058+00:00"
      }
    },
    {
      "group_id": "group-14",
      "chunk_indices": [
        55,
        56
      ],
      "char_count": 1459,
      "summary": "2022. High-resolution image synthesis with latent diffusion models. In Proceedin",
      "digest": "2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 10684–10695. Saharia, C.; Chan, W.; Chang, H.; Lee, C.; Ho, J.; Salimans, T.; Fleet, D.; and Norouzi, M. 2022a. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, 1–10. Saharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton, E. L.; Ghasemipour, K.; Gontijo Lopes, R.; Karagol Ayan, B.; Salimans, T.; et al. 2022b. Photorealistic text-to-image diffusion models with deep language understanding.Advances in Neural Information Processing Systems, 35: 36479–36494. Sohl-Dickstein, J.; Weiss, E.; Maheswaranathan, N.; and Ganguli, S. 2015. Deep Unsupervised Learning using Nonequilibrium Thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, 2256–2265. Tian, Z.; Shen, C.; Chen, H.; and He, T. 2019. Fcos: Fully convolutional one-stage object detection. In Proceedings of\nPr",
      "full_text": "2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 10684–10695. Saharia, C.; Chan, W.; Chang, H.; Lee, C.; Ho, J.; Salimans, T.; Fleet, D.; and Norouzi, M. 2022a. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, 1–10. Saharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton, E. L.; Ghasemipour, K.; Gontijo Lopes, R.; Karagol Ayan, B.; Salimans, T.; et al. 2022b. Photorealistic text-to-image diffusion models with deep language understanding.Advances in Neural Information Processing Systems, 35: 36479–36494. Sohl-Dickstein, J.; Weiss, E.; Maheswaranathan, N.; and Ganguli, S. 2015. Deep Unsupervised Learning using Nonequilibrium Thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, 2256–2265. Tian, Z.; Shen, C.; Chen, H.; and He, T. 2019. Fcos: Fully convolutional one-stage object detection. In Proceedings of\nProceedings of the 32nd International Conference on Machine Learning, 2256–2265. Tian, Z.; Shen, C.; Chen, H.; and He, T. 2019. Fcos: Fully convolutional one-stage object detection. In Proceedings of the IEEE/CVF international conference on computer vision, 9627–9636. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems,",
      "keywords": [],
      "page_range": [
        9,
        9
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T09:02:05.701058+00:00"
      }
    },
    {
      "group_id": "group-15",
      "chunk_indices": [
        57,
        58
      ],
      "char_count": 1635,
      "summary": "30. Wang, H.; Wang, Q.; Yang, F.; Zhang, W.; and Zuo, W. 2019. Data augmentation",
      "digest": "30. Wang, H.; Wang, Q.; Yang, F.; Zhang, W.; and Zuo, W. 2019. Data augmentation for object detection via progressive and selective instance-switching. arXiv preprint arXiv:1906.00358. Wu, Y.; Kirillov, A.; Massa, F.; Lo, W.-Y.; and Girshick, R. 2019. Detectron2. https://github.com/facebookresearch/detectron2. Yang, Z.; Wang, J.; Gan, Z.; Li, L.; Lin, K.; Wu, C.; Duan, N.; Liu, Z.; Liu, C.; Zeng, M.; et al. 2023. Reco: Region-controlled text-toimage generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 14246–14255. Zhang, H.; Li, F.; Liu, S.; Zhang, L.; Su, H.; Zhu, J.; Ni, L.; and Shum, H. 2022. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. In International Conference on Learning Representations.\nZhao, H.; Sheng, D.; Bao, J.; Chen, D.; Chen, D.; Wen, F.; Yuan, L.; Liu, C.; Zhou, W.; Chu, Q.; et al. 2023. X-Paste: Revisit Copy- Paste at Scale with CLIP and StableDiffusion. In Proceedings of the International",
      "full_text": "30. Wang, H.; Wang, Q.; Yang, F.; Zhang, W.; and Zuo, W. 2019. Data augmentation for object detection via progressive and selective instance-switching. arXiv preprint arXiv:1906.00358. Wu, Y.; Kirillov, A.; Massa, F.; Lo, W.-Y.; and Girshick, R. 2019. Detectron2. https://github.com/facebookresearch/detectron2. Yang, Z.; Wang, J.; Gan, Z.; Li, L.; Lin, K.; Wu, C.; Duan, N.; Liu, Z.; Liu, C.; Zeng, M.; et al. 2023. Reco: Region-controlled text-toimage generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 14246–14255. Zhang, H.; Li, F.; Liu, S.; Zhang, L.; Su, H.; Zhu, J.; Ni, L.; and Shum, H. 2022. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. In International Conference on Learning Representations.\nZhao, H.; Sheng, D.; Bao, J.; Chen, D.; Chen, D.; Wen, F.; Yuan, L.; Liu, C.; Zhou, W.; Chu, Q.; et al. 2023. X-Paste: Revisit Copy- Paste at Scale with CLIP and StableDiffusion. In Proceedings of the International Conference on Machine Learning.\nZhou, X.; Koltun, V.; and Kr¨ahenb¨uhl, P. 2021. Probabilistic twostage detection. arXiv preprint arXiv:2103.07461. Zhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2021. Deformable DETR: Deformable Transformers for End-to-End Object Detection. In Proceedings of the International Conference on Learning Representations.\nZoph, B.; Cubuk, E. D.; Ghiasi, G.; Lin, T.-Y.; Shlens, J.; and Le, Q. V. 2020. Learning data augmentation strategies for object detection. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVII 16, 566–583. Springer.",
      "keywords": [],
      "page_range": [
        9,
        9
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T09:02:05.701058+00:00"
      }
    },
    {
      "group_id": "group-16",
      "chunk_indices": [
        59,
        60,
        61,
        62
      ],
      "char_count": 2988,
      "summary": "Supplementary Implementation Details Performance Gain Analysis Prompts for Scali",
      "digest": "Supplementary Implementation Details Performance Gain Analysis Prompts for Scaling-up Data To investigate the performance gain of DE, we further ana- We use the following additional prompts for improvinglyze the improvement in category based on Faster-RCNN. In Figure 8, we sort the categories according to the number ofphoto generation quality: annotations in COCO. Each bar represents the precision gainPositive Prompt: elegant, meticulous, magnificent, maxiover the baseline for a specific category. It can be observedmum details, extremely hyper aesthetic, highly detailed.\nNegative Prompt: naked, deformed, bad anatomy, out ofthat the mAP gain mainly comes from classes with fewer focus, disfigured, bad image, poorly drawn face, mutation, annotations in the original dataset (less than 10k), indicating that DE helps to alleviate the lack-of-sample problem.mutated, extra limb, ugly, disgusting, poorly drawn hands, missing limb, floating limbs, disconnected limbs, blurry, mutated hands and fi",
      "full_text": "Supplementary Implementation Details Performance Gain Analysis Prompts for Scaling-up Data To investigate the performance gain of DE, we further ana- We use the following additional prompts for improvinglyze the improvement in category based on Faster-RCNN. In Figure 8, we sort the categories according to the number ofphoto generation quality: annotations in COCO. Each bar represents the precision gainPositive Prompt: elegant, meticulous, magnificent, maxiover the baseline for a specific category. It can be observedmum details, extremely hyper aesthetic, highly detailed.\nNegative Prompt: naked, deformed, bad anatomy, out ofthat the mAP gain mainly comes from classes with fewer focus, disfigured, bad image, poorly drawn face, mutation, annotations in the original dataset (less than 10k), indicating that DE helps to alleviate the lack-of-sample problem.mutated, extra limb, ugly, disgusting, poorly drawn hands, missing limb, floating limbs, disconnected limbs, blurry, mutated hands and fingers, watermark, oversaturated, distorted hands.\nThese prompts tend to generate realistic photos, so they are not used when generating clipart.\nTraining Schedule Following the common experiment setup, we refer 90k iterations with batch size 16 to a ”1× schedule”, and the final number of schedules is based on the total learning samples.\nTraining Schedule Following the common experiment setup, we refer 90k iterations with batch size 16 to a ”1× schedule”, and the final number of schedules is based on the total learning samples.\nNote that the only difference between ours and the baseline is the addition of annotated training data produced by the proposed DiffusionEngine (DE), while maintaining equal total iterations. The training setup is detailed as follows: Table 6: Details for Schedule in Table 2 (manuscript).\nModel Batchsize LR Total Iter. Schedule RetinaNet-R50 32 0.02 270k 6× Faster-RCNN-R50 48 0.04 270k 9× DINO-R50 32 2e-4 270k 6× DINO-Swin-L 48 2e-4 270k 9× Figure 8: Analyze Performance Gain by Category.\nPerformance with Schedule Scaling As shown in Figure 7, the tendency of overfitting also occurred for DINO with strong default data augmentation, More Qualitative Results while combining COCO-DE alleviates the issue. Here we provide more visualization results of data scaling up for photo (Figure 9, 10), and clipart (Figure 11). We also show that DiffusionEngine generalizes well across domains by simply modifying the prompt (Figure 12). The groundtruth (GT) annotations for the reference images are shown but not used in our generation process.\nFigure 7: Performance with increasing schedule on COCO v.s COCO w/DE.\nFigure 9: Visualization on Scalable and Diverse generation of photo with DiffusionEngine.\nFigure 10: Visualization on Scalable and Diverse generation of photo with DiffusionEngine.\nFigure 11: Visualization on Scalable and Diverse generation of clipart with DiffusionEngine.\nFigure 12: Visualization on generalization ability of DiffusionEngine.",
      "keywords": [],
      "page_range": [
        10,
        10
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T09:02:05.701058+00:00"
      }
    }
  ]
}