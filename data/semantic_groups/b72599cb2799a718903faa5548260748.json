{
  "schema_version": 1,
  "doc_id": "b72599cb2799a718903faa5548260748",
  "doc_hash": "",
  "created_at": "2026-02-07T08:48:44.962094+00:00",
  "config": {
    "target_chars": 5000,
    "min_chars": 2500,
    "max_chars": 6000
  },
  "groups": [
    {
      "group_id": "group-0",
      "chunk_indices": [
        0,
        1,
        2,
        3,
        4
      ],
      "char_count": 3879,
      "summary": "D URL_BERT: P - BERT- MOM S RETRAINED BASEDODEL FOR M D URL DALICIOUSOMAINS AND ",
      "digest": "D URL_BERT: P - BERT- MOM S RETRAINED BASEDODEL FOR M D URL DALICIOUSOMAINS AND SETECTION AND CLASSIFICATION Abdelkader El Mahdaouy1, Salima Lamsiyah2, Meryem Janati Idrissi1, Hamza Alami3, Zakaria Yartaoui4,5, and arXiv:2409.09143v1 [cs.CR] 13 Sep 2024 Ismail Berrada1 1College of Computing, Mohammed VI Polytechnic University, Ben Guerir, Morocco 2Department of Computer Science, Faculty of Science, Technology and Medicine, University of Luxembourg, Luxembourg 3LISAC Laboratory, Faculty of Sciences Dhar El Mehraz, USMBA, Fez, Morocco 4Vanguard Center, Mohammed VI Polytechnic University, Ben Guerir, Morocco 5National Moroccan Computer Emergency and Response Team (maCert), Morocco, firstname.lastname@{um6p.ma1,4|uni.lu2|usmba.ac.ma3} September 17, 2024 ABSTRACT Detecting and classifying suspicious or malicious domain names and URLs is fundamental task in cybersecurity. To leverage such indicators of compromise, cybersecurity vendors and practitioners often maintain and update blacklists\no",
      "full_text": "D URL_BERT: P - BERT- MOM S RETRAINED BASEDODEL FOR M D URL DALICIOUSOMAINS AND SETECTION AND CLASSIFICATION Abdelkader El Mahdaouy1, Salima Lamsiyah2, Meryem Janati Idrissi1, Hamza Alami3, Zakaria Yartaoui4,5, and arXiv:2409.09143v1 [cs.CR] 13 Sep 2024 Ismail Berrada1 1College of Computing, Mohammed VI Polytechnic University, Ben Guerir, Morocco 2Department of Computer Science, Faculty of Science, Technology and Medicine, University of Luxembourg, Luxembourg 3LISAC Laboratory, Faculty of Sciences Dhar El Mehraz, USMBA, Fez, Morocco 4Vanguard Center, Mohammed VI Polytechnic University, Ben Guerir, Morocco 5National Moroccan Computer Emergency and Response Team (maCert), Morocco, firstname.lastname@{um6p.ma1,4|uni.lu2|usmba.ac.ma3} September 17, 2024 ABSTRACT Detecting and classifying suspicious or malicious domain names and URLs is fundamental task in cybersecurity. To leverage such indicators of compromise, cybersecurity vendors and practitioners often maintain and update blacklists\nor malicious domain names and URLs is fundamental task in cybersecurity. To leverage such indicators of compromise, cybersecurity vendors and practitioners often maintain and update blacklists of known malicious domains and URLs. However, blacklists frequently fail to identify emerging and obfuscated threats. Over the past few decades, there has been significant interest in developing machine learning models that automatically detect malicious domains and URLs, addressing the limitations of blacklists maintenance and updates. In this paper, we introduce DomURLs_BERT, a pre-trained BERT-based encoder adapted for detecting and classifying suspicious/malicious domains and URLs. DomURLs_BERT is pre-trained using the Masked Language Modeling (MLM) objective on a large multilingual corpus of URLs, domain names, and Domain Generation Algorithms (DGA) dataset. In order to assess the performance of DomURLs_BERT, we have conducted experiments on several binary and multi-class classification\nURLs, domain names, and Domain Generation Algorithms (DGA) dataset. In order to assess the performance of DomURLs_BERT, we have conducted experiments on several binary and multi-class classification tasks involving domain names and URLs, covering phishing, malware, DGA, and DNS tunneling.\nThe evaluations results show that the proposed encoder outperforms state-of-the-art character-based deep learning models and cybersecurity-focused BERT models across multiple tasks and datasets.\nThe pre-training dataset1, the pre-trained DomURLs_BERT2encoder, and the experiments source code3are publicly available. 1Introduction Domain names and Uniform Resource Locators (URLs) are fundamental components in navigating and identifying resources on the Internet. Nevertheless, they are frequently exploited for various malicious activities in cyberspace, such as phishing campaigns, malware distribution, spam dissemination, and Command and Control (C&C) server operations, among others [1, 2, 3, 4, 5].Thus, detecting and flagging malicious domains and URLs is crucial for network security.\nTraditionally, cybersecurity vendors and practitioners rely on blacklists and heuristic methods to identify malicious domain names and URLs [6, 7, 8, 9]. While blacklists are essential for blocking known threats, they are reactive by 1https://hf.co/datasets/amahdaouy/Web_DomURLs 2https://hf.co/amahdaouy/DomURLs_BERT 3https://github.com/AbdelkaderMH/DomURLs_BERT\nA PREPRINT - SEPTEMBER 17, 2024 nature, posing challenges in maintenance and being vulnerable to evasion techniques. On the other hand, heuristic methods, which use patterns and behavioral analysis to identify potential threats, offer a more proactive approach to detection [7, 10, 11]. However, they are prone to false positives and require continuous updates to remain effective against evolving obfuscation tactics [4, 5, 10, 12, 13].",
      "keywords": [],
      "page_range": [
        1,
        1
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-1",
      "chunk_indices": [
        5,
        6,
        7,
        8,
        9,
        10,
        11
      ],
      "char_count": 5247,
      "summary": "To overcome the limitations of blacklisting and heuristic-based methods, a growi",
      "digest": "To overcome the limitations of blacklisting and heuristic-based methods, a growing body of research has focused on developing Machine Learning (ML) techniques for detecting malicious URLs and domain names [2, 8, 12]. The goal of these approaches is to automatically train models that can distinguish between legitimate and malicious threats based on data. Traditional ML-based techniques rely heavily on hand-engineered features, where the learning process involves identifying patterns in the data to guide the model’s decision-making. Consequently, numerous studies have proposed various feature sets for ML-based classification of malicious domains and URLs [2, 4, 9, 11]. Although ML-based methods have demonstrated promising results across different domain name and URL classification tasks, the manual feature engineering process is both costly and time-consuming [6, 7].\nRecently, a considerable amount of literature has been published on the use of Deep Learning (DL) for detecting and classi",
      "full_text": "To overcome the limitations of blacklisting and heuristic-based methods, a growing body of research has focused on developing Machine Learning (ML) techniques for detecting malicious URLs and domain names [2, 8, 12]. The goal of these approaches is to automatically train models that can distinguish between legitimate and malicious threats based on data. Traditional ML-based techniques rely heavily on hand-engineered features, where the learning process involves identifying patterns in the data to guide the model’s decision-making. Consequently, numerous studies have proposed various feature sets for ML-based classification of malicious domains and URLs [2, 4, 9, 11]. Although ML-based methods have demonstrated promising results across different domain name and URL classification tasks, the manual feature engineering process is both costly and time-consuming [6, 7].\nRecently, a considerable amount of literature has been published on the use of Deep Learning (DL) for detecting and classifying malicious domain names and URLs [5, 6, 7, 13]. These studies leverage the representation learning capabilities of deep neural networks, which can automatically learn hierarchical features at different levels of abstraction from raw input data [14]. As a result, various neural network architectures have been explored. Typically, these architectures either use hand-engineered features or learn representations of characters, n-grams, and sub-words for classifying malicious domain names and URLs [6, 13, 15, 16, 17].\nThe introduction of the transformer architecture [18] has resulted in significant breakthroughs and advancements in Artificial Intelligence. Beyond natural language processing, transformers have been employed in various fields such as computer vision, data science, robotics, and cybersecurity [19, 20, 21, 22]. Particularly, self-supervised pre-training of stacked transformer blocks—whether in encoder, decoder, or encoder-decoder configurations—has demonstrated state-of-the-art performance when fine-tuned for downstream tasks [23, 24]. In line with the pretrain-finetune paradigm, researchers have proposed fine-tuning or adapting pre-trained Bidirectional Encoder Representations from Transformers (BERT) [25] for cybersecurity tasks [5, 21, 22, 26, 27, 28, 29, 30, 31, 32]. Following the domain-adaptive pre-training approach, several BERT-based encoders have been pre-trained using the Masked Language Modeling (MLM) objective on domain-specific corpora for the classification of malicious\npre-training approach, several BERT-based encoders have been pre-trained using the Masked Language Modeling (MLM) objective on domain-specific corpora for the classification of malicious and phishing URLs [22, 29, 33, 31]. However, these models have not been explicitly pre-trained on both domain names and URLs, and much of the existing research has focused primarily on phishing URLs.\nIn this paper, we introduce DomURLs_BERT, a BERT-based encoder pre-trained on a large-scale corpus using the MLM objective. The pre-training corpus includes multilingual URLs, domain names, and Domain Generation Algorithms (DGA) datasets. Additionally, we propose a lightweight preprocessing method for the input data and train our model’s tokenizer from scratch using SentencePiece tokenization. To evaluate the performance of DomURLs_BERT in detecting malicious URLs and domain names, we conducted a comprehensive evaluation on a diverse set of datasets covering DGA, DNS tunneling techniques, malware classification, and phishing/malicious URL classification. The overall results show that our model outperforms six character-based deep learning models and four BERT-based models on multiple classification tasks. To summarize, the main contributions of this paper are as follows: • We introduce DomURLs_BERT, a specialized BERT-based encoder pre-trained on a large-scale multilingual corpus of\ntasks. To summarize, the main contributions of this paper are as follows: • We introduce DomURLs_BERT, a specialized BERT-based encoder pre-trained on a large-scale multilingual corpus of URLs, domain names, and DGA datasets. • We propose a light preprocessing tailored to the characteristics of URLs and domain names, and train domainspecific tokenizer. • We evaluate DomURLs_BERT on various malicious URLs and domain names classification tasks, including DGA, DNS tunneling, malware classification, and phishing. • We conduct our experiments on both binary and multi-class classification tasks. • We compare our model with several state-of-the-art deep learning models, including character-based models and pre-trained cybersecurity BERT models.\nThe rest of the paper is organized as follows: Section 2 reviews related work in the field of malicious domain and URL detection. In Section 3, we describe the proposed method for DomURLs_BERT pre-training. Sections 4 presents the experimental results. Finally, Section 5 concludes the paper and outlines potential directions for future research. 2Related Work The field of natural language processing is currently undergoing a revolutionary transformation, driven by the advent of large pre-trained language models (PLMs) based on the groundbreaking Transformer architecture [18]. However, 2",
      "keywords": [],
      "page_range": [
        2,
        2
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-2",
      "chunk_indices": [
        12,
        13,
        14,
        15,
        16,
        17,
        18
      ],
      "char_count": 5949,
      "summary": "A PREPRINT - SEPTEMBER 17, 2024 applying these models to domain-specific tasks p",
      "digest": "A PREPRINT - SEPTEMBER 17, 2024 applying these models to domain-specific tasks poses challenges, as general models often fail to represent domainspecific terms and contexts not covered in their training data. To address this issue, domain-specific PLMs have been developed, such as BioBERT [34] for biomedical text and SciBERT [35] for scientific literature. Similarly, in the cybersecurity domain, several models based on the BERT architecture [25] have been created to capture domain-specific language and improve performance on cybersecurity-related tasks [22].\nFor instance, CyBERT [36] is a domain-specific variant of BERT pre-trained on a large cybersecurity corpus using MLM. It focuses on generating contextualized embeddings specifically designed for cybersecurity tasks like cyber threat intelligence and malware detection. In the same context, CySecBERT [21] is a domain-adapted version of BERT pre-trained on large cybersecurity corpora. It is designed to improve performance across multi",
      "full_text": "A PREPRINT - SEPTEMBER 17, 2024 applying these models to domain-specific tasks poses challenges, as general models often fail to represent domainspecific terms and contexts not covered in their training data. To address this issue, domain-specific PLMs have been developed, such as BioBERT [34] for biomedical text and SciBERT [35] for scientific literature. Similarly, in the cybersecurity domain, several models based on the BERT architecture [25] have been created to capture domain-specific language and improve performance on cybersecurity-related tasks [22].\nFor instance, CyBERT [36] is a domain-specific variant of BERT pre-trained on a large cybersecurity corpus using MLM. It focuses on generating contextualized embeddings specifically designed for cybersecurity tasks like cyber threat intelligence and malware detection. In the same context, CySecBERT [21] is a domain-adapted version of BERT pre-trained on large cybersecurity corpora. It is designed to improve performance across multiple cybersecurity tasks, including classification and named entity recognition while addressing challenges like catastrophic forgetting during domain adaptation. CySecBERT has demonstrated superior performance compared to both BERT and CyBERT in several cybersecurity tasks. Additionally, SecureBERT [20], based on the RoBERTa architecture, incorporates continual pre-training with a specialized tokenizer and fine-tuned pre-trained weights to capture both general and cybersecurity-specific language. Evaluated on MLM and NER tasks, SecureBERT has shown promising\nwith a specialized tokenizer and fine-tuned pre-trained weights to capture both general and cybersecurity-specific language. Evaluated on MLM and NER tasks, SecureBERT has shown promising results in comprehending cybersecurity text. On the other hand, SecBERT [19], developed from scratch, is trained on various cybersecurity corpora, such as \"APTnotes\" and \"CASIE,\" and targets a broad range of cybersecurity data.\nMore recently, several models have focused on specific tasks within the cybersecurity domain. For example, MalBERT [37], a BERT-based model, is specialized in detecting malicious software. Similarly, Li et al. [29] introduced URLBERT, the first pre-trained model specifically designed for URL classification and detection tasks. URLBERT incorporates novel pre-training techniques, such as self-supervised contrastive learning and virtual adversarial training, to enhance its understanding of URL structures and robustness, achieving state-of-the-art results in phishing detection and web page classification. Motivated by the success of PLMs in cybersecurity tasks, we propose DomURLs_BERT, a specialized BERT-based encoder pre-trained on a large multilingual corpus of URLs, domain names, and DGA datasets. This paper contextualizes DomURLs_BERT by reviewing recent studies on the classification of malicious domain names and URLs. For a detailed review of existing large language models in\nDGA datasets. This paper contextualizes DomURLs_BERT by reviewing recent studies on the classification of malicious domain names and URLs. For a detailed review of existing large language models in cybersecurity, readers can refer to the recent study by Xu et al. [22]. 2.1Malicious domain names classification Detecting malicious domains, especially those generated by domain generation algorithms, is a crucial task in cybersecurity. Early work by Yadav et al. [1] laid the foundation by focusing on detecting algorithmically generated malicious domain names through linguistic analysis. Building on this, Cucchiarelli et al. [9] proposed using n-gram features, enhancing the ability to capture linguistic patterns in DGA-generated domains. Liew and Law [17] further advanced the field by introducing subword tokenization techniques for DGA classification, a method that allows more granular token analysis, improving model robustness against unseen domain variations. Shi et al. [8] explored\nsubword tokenization techniques for DGA classification, a method that allows more granular token analysis, improving model robustness against unseen domain variations. Shi et al. [8] explored machine learning techniques, particularly extreme machine learning, for detecting malicious domain names. This approach demonstrates the effectiveness of using machine learning models to identify domains that exhibit abnormal patterns. Tian et al. [32] introduced Dom-bert, a pre-trained model designed to detect malicious domains, leveraging contextual information embedded in domain names to enhance detection performance. In the broader context, Kang [3] reviewed various malicious domain detection techniques, while Hamroun et al. [11] focused specifically on lexical-based methods, emphasizing the importance of features derived from the domain names themselves. Together, these works underscore the importance of both lexical features and advanced machine-learning techniques in detecting\nthe importance of features derived from the domain names themselves. Together, these works underscore the importance of both lexical features and advanced machine-learning techniques in detecting DGA-generated and malicious domains. 2.2Malicious URLs classification In the field of malicious URLs detection, machine learning and deep learning approaches have been extensively studied and developed, with significant advancements in recent years. These approaches can be broadly categorized into traditional machine learning methods, neural network-based methods, and transformer-based models. Traditional machine learning methods, which rely on manually engineered features, were initially prominent in malicious URL detection. Sahoo et al. [2] provided a comprehensive survey of these early efforts, highlighting how machine learning techniques such as support vector machines, decision trees, naive Bayes, and random forests were applied to extract statistical and lexical features from URLs.",
      "keywords": [],
      "page_range": [
        3,
        3
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-3",
      "chunk_indices": [
        19,
        20
      ],
      "char_count": 1062,
      "summary": "highlighting how machine learning techniques such as support vector machines, de",
      "digest": "highlighting how machine learning techniques such as support vector machines, decision trees, naive Bayes, and random forests were applied to extract statistical and lexical features from URLs. Similarly, Aljabri et al. [4] reviewed more recent methods and highlighted the shift toward deep learning techniques due to their ability to automate feature extraction and improve detection performance.\nThe shift towards deep learning led to the development of several promising models. In this context, Le et al. [6] proposed URLNet, a deep learning-based method that captures both character- and word-level representations of URLs 3\nA PREPRINT - SEPTEMBER 17, 2024 to improve classification accuracy. Vazhayil et al. [15] performed a comparative study between shallow and deep networks, concluding that deep networks outperform traditional machine learning models by capturing more complex patterns in URLs. Afzal et al. [16] took this further by introducing Urldeepdetect, a deep learning model that in",
      "full_text": "highlighting how machine learning techniques such as support vector machines, decision trees, naive Bayes, and random forests were applied to extract statistical and lexical features from URLs. Similarly, Aljabri et al. [4] reviewed more recent methods and highlighted the shift toward deep learning techniques due to their ability to automate feature extraction and improve detection performance.\nThe shift towards deep learning led to the development of several promising models. In this context, Le et al. [6] proposed URLNet, a deep learning-based method that captures both character- and word-level representations of URLs 3\nA PREPRINT - SEPTEMBER 17, 2024 to improve classification accuracy. Vazhayil et al. [15] performed a comparative study between shallow and deep networks, concluding that deep networks outperform traditional machine learning models by capturing more complex patterns in URLs. Afzal et al. [16] took this further by introducing Urldeepdetect, a deep learning model that integrates semantic vector models to enhance URL representation.",
      "keywords": [],
      "page_range": [
        3,
        3
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-4",
      "chunk_indices": [
        21,
        22,
        23,
        24,
        25
      ],
      "char_count": 3751,
      "summary": "More recently, transformer-based models have emerged as a dominant approach, dri",
      "digest": "More recently, transformer-based models have emerged as a dominant approach, driven by their capacity to understand the semantic and contextual information of URLs. As previously mentioned, the BERT model and its variants have been particularly influential in this area. Chang et al. [26] and Otieno et al. [27] explored the application of BERT for URL detection, demonstrating that transformer models outperform traditional methods in terms of both accuracy and robustness. Building on these efforts, Su et al. [28] and Yu et al. [5] proposed modified BERT variants that further enhance semantic understanding for malicious URL detection. The introduction of URLBERT by Li et al. [29], a contrastive and adversarial pre-trained model, continues this trend, pushing the boundaries of transformer-based URL classification. Several other transformer-based models have also been proposed, focusing on improving phishing URL detection. For example, URLTran [33] applies transformers specifically to\nURL c",
      "full_text": "More recently, transformer-based models have emerged as a dominant approach, driven by their capacity to understand the semantic and contextual information of URLs. As previously mentioned, the BERT model and its variants have been particularly influential in this area. Chang et al. [26] and Otieno et al. [27] explored the application of BERT for URL detection, demonstrating that transformer models outperform traditional methods in terms of both accuracy and robustness. Building on these efforts, Su et al. [28] and Yu et al. [5] proposed modified BERT variants that further enhance semantic understanding for malicious URL detection. The introduction of URLBERT by Li et al. [29], a contrastive and adversarial pre-trained model, continues this trend, pushing the boundaries of transformer-based URL classification. Several other transformer-based models have also been proposed, focusing on improving phishing URL detection. For example, URLTran [33] applies transformers specifically to\nURL classification. Several other transformer-based models have also been proposed, focusing on improving phishing URL detection. For example, URLTran [33] applies transformers specifically to phishing detection, while Bozkir et al. [13] introduced GramBeddings, a neural network that utilizes n-gram embeddings to enhance the identification of phishing URLs. This direction was further extended by Liu et al. [30], who combined a pre-trained language model with multi-level feature attention for improved detection accuracy.\nOverall, the progression from traditional machine learning approaches to advanced deep learning and transformer-based models has significantly improved the ability to classify malicious URLs. The integration of semantic understanding, n-gram embeddings, and pre-trained models has pushed the state-of-the-art, enabling more accurate and robust detection of malicious URLs across different attack types. 3Methodology This section presents our methodology for pre-training the DomURLs_BERT encoder, focusing on the collection of pre-training data, preprocessing of domain names and URLs, tokenizer training, and domain-adaptive pre-training.\nTable 1: Pre-training Dataset Training Development domain names 19,941,474 1,049,558 URLs 355,116,387 18,690,330 Total 375,057,861 19,739,888 3.1Pre-training data We have collected a large-scale pre-training corpus of domain names and URLs from the following datasets: • mC4: The multilingual colossal Common Crawl Corpus4. This is a cleaned version of the Common Crawl’s web corpus, curated by the Allen Institute for Artificial Intelligence [38], containing approximately 170 million URLs. • falcon-refinedweb: An English large-scale dataset curated for large language model pre-training. This dataset is compiled from CommonCrawl, using strict filtering and extensive deduplication [39], and contains around 128 million URLs5. • CBA Web tracking datasets: A dataset compiled by the Broadband Communications Systems and Architectures Research Group6, containing 76M URLs and 1.5M domain names. • Tranco top 1M: is a dataset of top 1M domain names compiled and ranked by Tranco7[40]. • UTL_DGA22: A\nSystems and Architectures Research Group6, containing 76M URLs and 1.5M domain names. • Tranco top 1M: is a dataset of top 1M domain names compiled and ranked by Tranco7[40]. • UTL_DGA22: A Domain Generation Algorithm botnet dataset, containing 4.3 million entries from 76 DGA families [12]. • UMUDGA: A dataset for profiling DGA-based botnets, consisting of 30 million manually labeled DGA entries [41]. 4https://hf.co/datasets/legacy-datasets/mc4 5https://hf.co/datasets/tiiuae/falcon-refinedweb 6https://cba.upc.edu/downloads/category/29-web-tracking-datasets# 7https://tranco-list.eu/ 4",
      "keywords": [],
      "page_range": [
        4,
        4
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-5",
      "chunk_indices": [
        26,
        27,
        28
      ],
      "char_count": 2723,
      "summary": "A PREPRINT - SEPTEMBER 17, 2024 Since the pre-training dataset is curated from m",
      "digest": "A PREPRINT - SEPTEMBER 17, 2024 Since the pre-training dataset is curated from multiple sources, the data cleaning process includes deduplication based on exact matching. The final pre-training dataset contains 375,057,861 samples for model training and 19,739,888 samples for development. Table 1 provides details on the collected dataset, which is publicly available on Hugging Face Datasets8. 3.2Pre-training Procedure 3.2.1Preprocessing Figure 1: Overall URL structure A URL consists of several components, which can be grouped into three main parts: the scheme (protocol), the domain name, and the path. Figure 1 illustrates the overall structure of a URL (source9). Our proposed input preprocessing method involves removing the protocol identifier and splitting the URL into two parts: the domain name and the path.\nThese two parts are delimited by special tokens, [DOMAIN] and [PATH], indicating the start of the domain name and the URL path, respectively. Additionally, if the input URL conta",
      "full_text": "A PREPRINT - SEPTEMBER 17, 2024 Since the pre-training dataset is curated from multiple sources, the data cleaning process includes deduplication based on exact matching. The final pre-training dataset contains 375,057,861 samples for model training and 19,739,888 samples for development. Table 1 provides details on the collected dataset, which is publicly available on Hugging Face Datasets8. 3.2Pre-training Procedure 3.2.1Preprocessing Figure 1: Overall URL structure A URL consists of several components, which can be grouped into three main parts: the scheme (protocol), the domain name, and the path. Figure 1 illustrates the overall structure of a URL (source9). Our proposed input preprocessing method involves removing the protocol identifier and splitting the URL into two parts: the domain name and the path.\nThese two parts are delimited by special tokens, [DOMAIN] and [PATH], indicating the start of the domain name and the URL path, respectively. Additionally, if the input URL contains an IP address instead of a domain name, we use the [IP] and [IPv6] special tokens in place of [DOMAIN] for IPv4 and IPv6 addresses, respectively. Finally, the [CLS] and [SEP] tokens are appended to the start and end of the input URL or domain, as follows: Figure 2: A sample of preprocessed domain names and URLs 3.2.2Tokenizer training After cleaning and preprocessing the data, we trained our tokenizer from scratch using the SentencePiece tokenization method, which employs the Byte Pair Encoding (BPE) algorithm [42]. SentencePiece is language-agnostic and does not require any pre-tokenization, as it processes input as a sequence of Unicode characters. For tokenizer training, we utilized the HuggingFace tokenizers library10. The vocabulary size was set to 32,000. 3.2.3Domain-adaptive pre-training\nprocesses input as a sequence of Unicode characters. For tokenizer training, we utilized the HuggingFace tokenizers library10. The vocabulary size was set to 32,000. 3.2.3Domain-adaptive pre-training Domain-adaptive pre-training has been shown to enhance the contextualized word embeddings of existing domaingeneric Pre-trained Language Models (PLMs) [43]. This improvement has also been demonstrated in cybersecurity applications, where several domain-adapted models have been proposed [20, 21, 29, 33, 31, 22]. Following this trend, we continued the pre-training of the BERT-base encoder introduced in [25]. The model consists of approximately 110 million parameters, with 12 transformer layers, a hidden dimension size of 768, and 12 attention heads. 8https://hf.co/datasets/amahdaouy/Web_DomURLs 9https://www.seoforgooglenews.com/p/everything-urls-news-publishers 10https://github.com/huggingface/tokenizers 5",
      "keywords": [],
      "page_range": [
        5,
        5
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-6",
      "chunk_indices": [
        29,
        30,
        31,
        32,
        33,
        34
      ],
      "char_count": 4428,
      "summary": "A PREPRINT - SEPTEMBER 17, 2024 Pre-training is performed using the MLM objectiv",
      "digest": "A PREPRINT - SEPTEMBER 17, 2024 Pre-training is performed using the MLM objective on our dataset, following the guidelines of Devlin et al. (2019) [25], where 15% of the input sequence’s subwords are randomly selected for masking. The model is trained to minimize the cross-entropy loss between the predicted sequence and the original sequence. We use the HuggingFace transformers11 library for training on a server equipped with 4 NVIDIA A100 GPUs, each with 80GB of RAM. The maximum sequence length, per-device batch size, and learning rate are set to 128, 768, and −4, respectively. The model is 1 × 10 trained for 260,000 steps. Our pre-trained model is publicly available on HuggingFace Models12. 4Experiments and Results In this section, we present the evaluation datasets, the deep learning models used for comparison, the experimental settings, and the evaluation metrics. We then discuss and analyze the obtained results. 4.1Evaluation Datasets To evaluate the effectiveness of our model,\nmo",
      "full_text": "A PREPRINT - SEPTEMBER 17, 2024 Pre-training is performed using the MLM objective on our dataset, following the guidelines of Devlin et al. (2019) [25], where 15% of the input sequence’s subwords are randomly selected for masking. The model is trained to minimize the cross-entropy loss between the predicted sequence and the original sequence. We use the HuggingFace transformers11 library for training on a server equipped with 4 NVIDIA A100 GPUs, each with 80GB of RAM. The maximum sequence length, per-device batch size, and learning rate are set to 128, 768, and −4, respectively. The model is 1 × 10 trained for 260,000 steps. Our pre-trained model is publicly available on HuggingFace Models12. 4Experiments and Results In this section, we present the evaluation datasets, the deep learning models used for comparison, the experimental settings, and the evaluation metrics. We then discuss and analyze the obtained results. 4.1Evaluation Datasets To evaluate the effectiveness of our model,\nmodels used for comparison, the experimental settings, and the evaluation metrics. We then discuss and analyze the obtained results. 4.1Evaluation Datasets To evaluate the effectiveness of our model, we employed several domain name and URL classification datasets. For malicious domain name classification, we used the DNS Tunneling dataset [44], UMUDGA [41], and UTL_DGA22 [12]. Additionally, we collected a malware domain names dataset, ThreatFox_MalDom, from the ThreatFox13database in June 2024. For legitimate domain names, we used the Tranco list.\nMendeley AK Singh Kaggle MaliciousFor malicious URL classification, we utilized several datasets, including [45], URLs [46], Grambedding [47], LNU_Phish [48], PhiUSIIL [49], and PhishCrawl [50]. We also curated a malware URL dataset, ThreatFox_MalURLs, from the ThreatFox database in June 2024. For legitimate URLs, we employed the benign URLs from the Kaggle Malicious URLs dataset [46].\nAll the used datasets have been divided into 60%, 20%, and 20% for training, validation, and testing, respectively. Table 2 summarizes the characteristics of the evaluation datasets.\nTable 2: Evaluation datasets Dataset Type Year Task Num Classes Size Training Validation Test DNS Tunneling [44] domain names 2019DNS Tunneling 5 96,063 57,637 19,213 19,213 UMUDGA [41] domain names 2020DGA botnet 51 3,098,6261,859,175 619,725 619,726 UTL_DGA22 [12] domain names 2022DGA botnet 77 4,297,9162,578,749 859,583 859,584 ThreatFox_MalDom (ours) domain names 2024 Malware 65 176,065 105,639 35,213 35,213 Mendely AK Singh [45] URLs 2020 Malicious 2 1,530,687 812,253 359,217 359,217 Kaggle malicious URLs [46] URLs 2021 Malicious 4 641,126 384,673 128,225 128,228 Grambedding [47] URLs 2023 Phishing 2 800,003 480,008 159,997 159,998 LNU_Phish [48] URLs 2022 Phishing 2 22,501 13,501 4,500 4,500 PhiUSIIL [49] URLs 2024 Phishing 2 235,370 141,222 47,074 47,074 PhishCrawl [50] URLs 2024 Phishing 2 101,827 61,095 20,366 20,366 ThreatFox_MalURLs (ours) URLs 2024 Malware 58 682,003 409,201 136,401 136,401 4.2Comparison methods We compared our model with several state-of-the-art deep\nPhishing 2 101,827 61,095 20,366 20,366 ThreatFox_MalURLs (ours) URLs 2024 Malware 58 682,003 409,201 136,401 136,401 4.2Comparison methods We compared our model with several state-of-the-art deep learning models, including six character-based RNN and CNN models, as described below: • CharCNN: This model employs an embedding layer followed by three one-dimensional convolutional layers with kernel sizes of 3, 4, and 5, respectively. The final convolutional layer is followed by a dropout layer and a classification layer. • CharGRU: This model uses an embedding layer and multiple GRU layers. The last GRU layer is followed by a dropout layer and a classification layer. • CharLSTM: This model utilizes an embedding layer and multiple LSTM layers. The final LSTM layer is followed by a dropout layer and a classification layer. • CharBiGRU: This model uses an embedding layer and multiple bidirectional GRU layers. The last BiGRU layer is followed by a dropout layer and a classification layer.\nand a classification layer. • CharBiGRU: This model uses an embedding layer and multiple bidirectional GRU layers. The last BiGRU layer is followed by a dropout layer and a classification layer. 11https://github.com/huggingface/transformers 12https://hf.co/amahdaouy/DomURLs_BERT 13https://threatfox.abuse.ch/ 6",
      "keywords": [],
      "page_range": [
        6,
        6
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-7",
      "chunk_indices": [
        35,
        36,
        37,
        38,
        39
      ],
      "char_count": 4041,
      "summary": "A PREPRINT - SEPTEMBER 17, 2024 CharBiLSTM•: This model employs an embedding lay",
      "digest": "A PREPRINT - SEPTEMBER 17, 2024 CharBiLSTM•: This model employs an embedding layer and multiple bidirectional LSTM layers. The final BiLSTM layer is followed by a dropout layer and a classification layer. • CharCNNBiLSTM: This model uses CNN layers to extract local features from character embeddings, which are then passed into a BiLSTM layer to capture contextual dependencies.\nMoreover, we compared our model with five state-of-the-art domain-generic and domain-specific BERT-based PLMs, including BERT [25], SecBERT [19], SecureBERT [20], CySecBERT [21], and URLBERT [29]. 4.3Experiments settings We implemented our model and the other state-of-the-art models using Pytorch14 deep learning framework, Lightning15, and HuggingFace transformers16 library. All our experiments have been conducted on a Dell PowerEdge XE8545 server, having 4 NVIDIA A100-SXM4-80GB GPUs, 1000 GiB RAM, and 2 AMD EPYC 7713 64-Core Processor 1.9GHz.\nAll models are trained using AdamW optimizer [51]. We used a batch siz",
      "full_text": "A PREPRINT - SEPTEMBER 17, 2024 CharBiLSTM•: This model employs an embedding layer and multiple bidirectional LSTM layers. The final BiLSTM layer is followed by a dropout layer and a classification layer. • CharCNNBiLSTM: This model uses CNN layers to extract local features from character embeddings, which are then passed into a BiLSTM layer to capture contextual dependencies.\nMoreover, we compared our model with five state-of-the-art domain-generic and domain-specific BERT-based PLMs, including BERT [25], SecBERT [19], SecureBERT [20], CySecBERT [21], and URLBERT [29]. 4.3Experiments settings We implemented our model and the other state-of-the-art models using Pytorch14 deep learning framework, Lightning15, and HuggingFace transformers16 library. All our experiments have been conducted on a Dell PowerEdge XE8545 server, having 4 NVIDIA A100-SXM4-80GB GPUs, 1000 GiB RAM, and 2 AMD EPYC 7713 64-Core Processor 1.9GHz.\nAll models are trained using AdamW optimizer [51]. We used a batch size of 128 and the maximum sequence length is fixed to 128 and 64 for URLs and domain names, respectively. For character-based deep learning models, the number of epochs, the learning rate, the weight decay, the number of RNN layers, the hidden dimensions size are fixed to, 20 1−3, 1−3, 3, 128, respectively. For BERT-based models, the number of epochs, the learning rate, and the weight decay are fixed to 10, 1−5, 1−3, respectively. For all models, weight decay is applied to all the layers weights except biases and Layer Normalization. For all models and dataset, we utilized the following performance measures: • Accuracy: Accuracy = TP+TN is The proportion of all correct predictions (both true positives and true TP FP TN FN negatives) out of all predictions.+ + + • True Positive Rate (TPR) / Recall / Sensitivity/ Detection Rate: TPR TP is the proportion of actual = TP+FN positives that are correctly identified by the\nFN negatives) out of all predictions.+ + + • True Positive Rate (TPR) / Recall / Sensitivity/ Detection Rate: TPR TP is the proportion of actual = TP+FN positives that are correctly identified by the model. • Specificity (SPC) / True Negative Rate: SPC = TN is the proportion of actual negatives that are correctly FP+TN identified by the model. • Positive Predictive Value (PPV) / Precision: PPV TP is the proportion of predicted positives that are = TP FP+actually positive.\nTN• Negative Predictive Value (NPV): NPV = is the proportion of predicted negatives that are actually TN+FN negative. • F1 score: F1 2·(PPV×TPR)is the harmonic mean of precision and recall. = PPV+TPR P • Weighted F1 Score (F1_wted): F1 = n (w · F1) where w is the proportion of the total numberweighted i=1 i i i of samples belonging to class i, and F1 is the F1 score for class i. This score considers each class’s importance i by weighting their respective F1 scores according to their frequency. • Micro F1 Score (F1_mic): 2·T P aggregates the contributions of all classes to F1micro = micro 2·T Pmicro+F Pmicro+F Nmicrocompute precision and recall, treating all instances equally, regardless of the class. • Diagnostic Efficiency (DE): DE = TPR × SPC is the product of sensitivity (TPR) and specificity (SPC).\nIndicates the overall diagnostic ability of the test. Higher values indicate better performance. • False Positive Rate (FPR): FPR = FP is the proportion of actual negatives that are incorrectly identified FP TN as positive by the model. Lower values indicate better performance.+ • False Discovery Rate (FDR): FDR FP is the proportion of predicted positives that are actually negative. = FP+TP Lower values indicate better performance. • False Negative Rate (FNR): FNR = FN is the proportion of actual positives that are incorrectly identified FN+TP as negative by the model. Lower values indicate better performance.\nFor all evaluation measures, we report the macro-average performances (except F1_wted, F1_mic, and Accuracy). 14https://pytorch.org/ 15https://lightning.ai/ 16https://github.com/huggingface/transformers 7",
      "keywords": [],
      "page_range": [
        7,
        7
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-8",
      "chunk_indices": [
        40
      ],
      "char_count": 998,
      "summary": "A PREPRINT - SEPTEMBER 17, 2024 Table 3: Malicious domain names detection. For e",
      "digest": "A PREPRINT - SEPTEMBER 17, 2024 Table 3: Malicious domain names detection. For each dataset, the best performances are highlighted in bold font. All performance measures are presented as percentages. dataset Name AccuracyF1_macro F1_wtedF1_micro TPR PPV NPV SPC DE FDR FPR FNR CharBiGRU 85.26 84.58 85.02 85.26 83.96 86.24 86.24 83.96 69.58 13.764 16.040 16.040 CharBiLSTM 85.70 85.24 85.60 85.70 84.86 85.94 85.94 84.86 71.63 14.061 15.139 15.139 ThreatFox_MalDomains CharCNN 85.03 84.46 84.87 85.03 83.99 85.48 85.48 83.99 69.97 14.516 16.006 16.006 CharCNNBiLSTM 84.94 84.33 84.75 84.94 83.82 85.53 85.53 83.82 69.58 14.466 16.181 16.181 CharGRU 85.09 84.61 84.98 85.09 84.25 85.26 85.26 84.25 70.60 14.736 15.750 15.750 CharLSTM 85.03 84.50 84.89 85.03 84.06 85.39 85.39 84.06 70.15 14.608 15.938 15.938 CySecBERT 87.04 86.57 86.91 87.04 86.08 87.57 87.57 86.08 73.60 12.426 13.917 13.917 SecBERT 84.97 84.63 84.94 84.97 84.50 84.79 84.79 84.50 71.29 15.213 15.497 15.497 SecureBERT 86.55 86.04",
      "full_text": "A PREPRINT - SEPTEMBER 17, 2024 Table 3: Malicious domain names detection. For each dataset, the best performances are highlighted in bold font. All performance measures are presented as percentages. dataset Name AccuracyF1_macro F1_wtedF1_micro TPR PPV NPV SPC DE FDR FPR FNR CharBiGRU 85.26 84.58 85.02 85.26 83.96 86.24 86.24 83.96 69.58 13.764 16.040 16.040 CharBiLSTM 85.70 85.24 85.60 85.70 84.86 85.94 85.94 84.86 71.63 14.061 15.139 15.139 ThreatFox_MalDomains CharCNN 85.03 84.46 84.87 85.03 83.99 85.48 85.48 83.99 69.97 14.516 16.006 16.006 CharCNNBiLSTM 84.94 84.33 84.75 84.94 83.82 85.53 85.53 83.82 69.58 14.466 16.181 16.181 CharGRU 85.09 84.61 84.98 85.09 84.25 85.26 85.26 84.25 70.60 14.736 15.750 15.750 CharLSTM 85.03 84.50 84.89 85.03 84.06 85.39 85.39 84.06 70.15 14.608 15.938 15.938 CySecBERT 87.04 86.57 86.91 87.04 86.08 87.57 87.57 86.08 73.60 12.426 13.917 13.917 SecBERT 84.97 84.63 84.94 84.97 84.50 84.79 84.79 84.50 71.29 15.213 15.497 15.497 SecureBERT 86.55 86.04",
      "keywords": [],
      "page_range": [
        8,
        8
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-9",
      "chunk_indices": [
        41
      ],
      "char_count": 996,
      "summary": "15.938 CySecBERT 87.04 86.57 86.91 87.04 86.08 87.57 87.57 86.08 73.60 12.426 13",
      "digest": "15.938 CySecBERT 87.04 86.57 86.91 87.04 86.08 87.57 87.57 86.08 73.60 12.426 13.917 13.917 SecBERT 84.97 84.63 84.94 84.97 84.50 84.79 84.79 84.50 71.29 15.213 15.497 15.497 SecureBERT 86.55 86.04 86.41 86.55 85.54 87.11 87.11 85.54 72.62 12.886 14.458 14.458 BERT 86.90 86.46 86.79 86.90 86.03 87.25 87.25 86.03 73.62 12.746 13.966 13.966 URLBERT 85.10 84.59 84.97 85.10 84.17 85.42 85.42 84.17 70.37 14.584 15.831 15.831 88.73 88.33 88.62 88.73 87.83 89.30 89.30 87.83 76.72 10.705 12.165 12.165DomURLs_BERT CharBiGRU 97.52 97.16 97.52 97.52 97.23 97.10 97.10 97.23 94.52 2.901 2.775 2.775 CharBiLSTM 98.54 98.33 98.54 98.54 98.27 98.40 98.40 98.27 96.56 1.600 1.734 1.734 CharCNN 96.86 96.41 96.86 96.86 96.35 96.46 96.46 96.35 92.82 3.539 3.646 3.646 CharCNNBiLSTM 97.76 97.44 97.76 97.76 97.46 97.42 97.42 97.46 94.97 2.581 2.544 2.544 CharGRU 97.52 97.16 97.52 97.52 97.15 97.17 97.17 97.15 94.36 2.831 2.853 2.853UMUDGA CharLSTM 98.35 98.11 98.34 98.35 98.05 98.16 98.16 98.05 96.13 1.836",
      "full_text": "15.938 CySecBERT 87.04 86.57 86.91 87.04 86.08 87.57 87.57 86.08 73.60 12.426 13.917 13.917 SecBERT 84.97 84.63 84.94 84.97 84.50 84.79 84.79 84.50 71.29 15.213 15.497 15.497 SecureBERT 86.55 86.04 86.41 86.55 85.54 87.11 87.11 85.54 72.62 12.886 14.458 14.458 BERT 86.90 86.46 86.79 86.90 86.03 87.25 87.25 86.03 73.62 12.746 13.966 13.966 URLBERT 85.10 84.59 84.97 85.10 84.17 85.42 85.42 84.17 70.37 14.584 15.831 15.831 88.73 88.33 88.62 88.73 87.83 89.30 89.30 87.83 76.72 10.705 12.165 12.165DomURLs_BERT CharBiGRU 97.52 97.16 97.52 97.52 97.23 97.10 97.10 97.23 94.52 2.901 2.775 2.775 CharBiLSTM 98.54 98.33 98.54 98.54 98.27 98.40 98.40 98.27 96.56 1.600 1.734 1.734 CharCNN 96.86 96.41 96.86 96.86 96.35 96.46 96.46 96.35 92.82 3.539 3.646 3.646 CharCNNBiLSTM 97.76 97.44 97.76 97.76 97.46 97.42 97.42 97.46 94.97 2.581 2.544 2.544 CharGRU 97.52 97.16 97.52 97.52 97.15 97.17 97.17 97.15 94.36 2.831 2.853 2.853UMUDGA CharLSTM 98.35 98.11 98.34 98.35 98.05 98.16 98.16 98.05 96.13 1.836",
      "keywords": [],
      "page_range": [
        8,
        8
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-10",
      "chunk_indices": [
        42
      ],
      "char_count": 996,
      "summary": "97.42 97.42 97.46 94.97 2.581 2.544 2.544 CharGRU 97.52 97.16 97.52 97.52 97.15 ",
      "digest": "97.42 97.42 97.46 94.97 2.581 2.544 2.544 CharGRU 97.52 97.16 97.52 97.52 97.15 97.17 97.17 97.15 94.36 2.831 2.853 2.853UMUDGA CharLSTM 98.35 98.11 98.34 98.35 98.05 98.16 98.16 98.05 96.13 1.836 1.951 1.951 CySecBERT 98.74 98.55 98.74 98.74 98.49 98.61 98.61 98.49 97.00 1.386 1.507 1.507 SecBERT 97.95 97.65 97.95 97.95 97.47 97.83 97.83 97.47 94.98 2.167 2.535 2.535 SecureBERT 98.81 98.64 98.81 98.81 98.59 98.69 98.69 98.59 97.19 1.306 1.414 1.414 BERT 98.76 98.58 98.76 98.76 98.59 98.58 98.58 98.59 97.20 1.424 1.407 1.407 URLBERT 97.58 97.24 97.58 97.58 97.35 97.13 97.13 97.35 94.76 2.866 2.652 2.652 DomURLs_BERT 99.11 98.98 99.11 99.11 98.98 98.99 98.99 98.98 97.96 1.012 1.024 1.024 CharBiGRU 97.15 96.03 97.16 97.15 96.27 95.79 95.79 96.27 92.66 4.206 3.726 3.726 CharBiLSTM 98.26 97.55 98.25 98.26 97.50 97.61 97.61 97.50 95.03 2.386 2.504 2.504 CharCNN 96.58 95.18 96.57 96.58 94.90 95.46 95.46 94.90 89.97 4.537 5.098 5.098 CharCNNBiLSTM 97.45 96.44 97.45 97.45 96.48 96.39 96.39",
      "full_text": "97.42 97.42 97.46 94.97 2.581 2.544 2.544 CharGRU 97.52 97.16 97.52 97.52 97.15 97.17 97.17 97.15 94.36 2.831 2.853 2.853UMUDGA CharLSTM 98.35 98.11 98.34 98.35 98.05 98.16 98.16 98.05 96.13 1.836 1.951 1.951 CySecBERT 98.74 98.55 98.74 98.74 98.49 98.61 98.61 98.49 97.00 1.386 1.507 1.507 SecBERT 97.95 97.65 97.95 97.95 97.47 97.83 97.83 97.47 94.98 2.167 2.535 2.535 SecureBERT 98.81 98.64 98.81 98.81 98.59 98.69 98.69 98.59 97.19 1.306 1.414 1.414 BERT 98.76 98.58 98.76 98.76 98.59 98.58 98.58 98.59 97.20 1.424 1.407 1.407 URLBERT 97.58 97.24 97.58 97.58 97.35 97.13 97.13 97.35 94.76 2.866 2.652 2.652 DomURLs_BERT 99.11 98.98 99.11 99.11 98.98 98.99 98.99 98.98 97.96 1.012 1.024 1.024 CharBiGRU 97.15 96.03 97.16 97.15 96.27 95.79 95.79 96.27 92.66 4.206 3.726 3.726 CharBiLSTM 98.26 97.55 98.25 98.26 97.50 97.61 97.61 97.50 95.03 2.386 2.504 2.504 CharCNN 96.58 95.18 96.57 96.58 94.90 95.46 95.46 94.90 89.97 4.537 5.098 5.098 CharCNNBiLSTM 97.45 96.44 97.45 97.45 96.48 96.39 96.39",
      "keywords": [],
      "page_range": [
        8,
        8
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-11",
      "chunk_indices": [
        43,
        44
      ],
      "char_count": 1986,
      "summary": "98.25 98.26 97.50 97.61 97.61 97.50 95.03 2.386 2.504 2.504 CharCNN 96.58 95.18 ",
      "digest": "98.25 98.26 97.50 97.61 97.61 97.50 95.03 2.386 2.504 2.504 CharCNN 96.58 95.18 96.57 96.58 94.90 95.46 95.46 94.90 89.97 4.537 5.098 5.098 CharCNNBiLSTM 97.45 96.44 97.45 97.45 96.48 96.39 96.39 96.48 93.05 3.608 3.521 3.521 UTL_DGA22 CharGRU 97.20 96.10 97.21 97.20 96.33 95.87 95.87 96.33 92.77 4.127 3.667 3.667 CharLSTM 98.30 97.61 98.29 98.30 97.54 97.68 97.68 97.54 95.12 2.320 2.459 2.459 CySecBERT 98.62 98.06 98.62 98.62 97.91 98.22 98.22 97.91 95.84 1.776 2.093 2.093 SecBERT 97.88 97.03 97.88 97.88 96.97 97.10 97.10 96.97 93.99 2.903 3.034 3.034 SecureBERT 98.64 98.09 98.64 98.64 98.02 98.17 98.17 98.02 96.07 1.834 1.978 1.978 BERT 98.58 98.00 98.58 98.58 97.84 98.17 98.17 97.84 95.71 1.831 2.160 2.160 URLBERT 97.55 96.56 97.55 97.55 96.55 96.58 96.58 96.55 93.18 3.420 3.451 3.451 DomURLs_BERT 98.80 98.32 98.80 98.80 98.34 98.31 98.31 98.34 96.70 1.695 1.661 1.661 CharBiGRU 99.97 99.90 99.97 99.97 99.93 99.88 99.88 99.93 99.86 0.123 0.070 0.070 CharBiLSTM 99.96 99.89 99.96\nDomUR",
      "full_text": "98.25 98.26 97.50 97.61 97.61 97.50 95.03 2.386 2.504 2.504 CharCNN 96.58 95.18 96.57 96.58 94.90 95.46 95.46 94.90 89.97 4.537 5.098 5.098 CharCNNBiLSTM 97.45 96.44 97.45 97.45 96.48 96.39 96.39 96.48 93.05 3.608 3.521 3.521 UTL_DGA22 CharGRU 97.20 96.10 97.21 97.20 96.33 95.87 95.87 96.33 92.77 4.127 3.667 3.667 CharLSTM 98.30 97.61 98.29 98.30 97.54 97.68 97.68 97.54 95.12 2.320 2.459 2.459 CySecBERT 98.62 98.06 98.62 98.62 97.91 98.22 98.22 97.91 95.84 1.776 2.093 2.093 SecBERT 97.88 97.03 97.88 97.88 96.97 97.10 97.10 96.97 93.99 2.903 3.034 3.034 SecureBERT 98.64 98.09 98.64 98.64 98.02 98.17 98.17 98.02 96.07 1.834 1.978 1.978 BERT 98.58 98.00 98.58 98.58 97.84 98.17 98.17 97.84 95.71 1.831 2.160 2.160 URLBERT 97.55 96.56 97.55 97.55 96.55 96.58 96.58 96.55 93.18 3.420 3.451 3.451 DomURLs_BERT 98.80 98.32 98.80 98.80 98.34 98.31 98.31 98.34 96.70 1.695 1.661 1.661 CharBiGRU 99.97 99.90 99.97 99.97 99.93 99.88 99.88 99.93 99.86 0.123 0.070 0.070 CharBiLSTM 99.96 99.89 99.96\nDomURLs_BERT 98.80 98.32 98.80 98.80 98.34 98.31 98.31 98.34 96.70 1.695 1.661 1.661 CharBiGRU 99.97 99.90 99.97 99.97 99.93 99.88 99.88 99.93 99.86 0.123 0.070 0.070 CharBiLSTM 99.96 99.89 99.96 99.96 99.93 99.85 99.85 99.93 99.85 0.152 0.073 0.073 CharCNN 99.98 99.95 99.98 99.98 99.99 99.91 99.91 99.99 99.98 0.088 0.009 0.009 CharCNNBiLSTM 99.97 99.92 99.97 99.97 99.93 99.91 99.91 99.93 99.87 0.093 0.067 0.067DNS Tunneling CharGRU 99.97 99.90 99.97 99.97 99.90 99.90 99.90 99.90 99.81 0.096 0.096 0.096 CharLSTM 99.97 99.90 99.97 99.97 99.93 99.88 99.88 99.93 99.86 0.123 0.070 0.070 CySecBERT 99.98 99.94 99.98 99.98 99.94 99.94 99.94 99.94 99.87 0.064 0.064 0.064 SecBERT 99.98 99.94 99.98 99.98 99.94 99.94 99.94 99.94 99.87 0.064 0.064 0.064 SecureBERT 99.98 99.94 99.98 99.98 99.94 99.94 99.94 99.94 99.87 0.064 0.064 0.064 BERT 99.98 99.94 99.98 99.98 99.94 99.94 99.94 99.94 99.87 0.064 0.064 0.064 URLBERT 99.97 99.92 99.97 99.97 99.91 99.93 99.93 99.91 99.81 0.067 0.093 0.093",
      "keywords": [],
      "page_range": [
        8,
        8
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-12",
      "chunk_indices": [
        45,
        46,
        47
      ],
      "char_count": 2112,
      "summary": "99.94 99.94 99.94 99.87 0.064 0.064 0.064 BERT 99.98 99.94 99.98 99.98 99.94 99.",
      "digest": "99.94 99.94 99.94 99.87 0.064 0.064 0.064 BERT 99.98 99.94 99.98 99.98 99.94 99.94 99.94 99.94 99.87 0.064 0.064 0.064 URLBERT 99.97 99.92 99.97 99.97 99.91 99.93 99.93 99.91 99.81 0.067 0.093 0.093 DomURLs_BERT 99.98 99.94 99.98 99.98 99.94 99.94 99.94 99.94 99.87 0.064 0.064 0.064 4.4 Results In this section, we present the results of our model alongside state-of-the-art character-based and BERT-based models.\nAll evaluated models are compared on both binary and multi-class classification tasks for domain names and URLs. 4.4.1Domains names classification Tasks Table 3 summarizes the obtained results for binary classification of domain names. The aim of this task is to detect malicious domain names and DNS tunneling. The overall results show that DomURLs_BERT achieves the best performance across all datasets for most evaluation metrics. However, on the DNS tunneling dataset, the CharCNN model outperforms in Specificity (SPC) and diagnostic efficiency (DE). Additionally, the results ind",
      "full_text": "99.94 99.94 99.94 99.87 0.064 0.064 0.064 BERT 99.98 99.94 99.98 99.98 99.94 99.94 99.94 99.94 99.87 0.064 0.064 0.064 URLBERT 99.97 99.92 99.97 99.97 99.91 99.93 99.93 99.91 99.81 0.067 0.093 0.093 DomURLs_BERT 99.98 99.94 99.98 99.98 99.94 99.94 99.94 99.94 99.87 0.064 0.064 0.064 4.4 Results In this section, we present the results of our model alongside state-of-the-art character-based and BERT-based models.\nAll evaluated models are compared on both binary and multi-class classification tasks for domain names and URLs. 4.4.1Domains names classification Tasks Table 3 summarizes the obtained results for binary classification of domain names. The aim of this task is to detect malicious domain names and DNS tunneling. The overall results show that DomURLs_BERT achieves the best performance across all datasets for most evaluation metrics. However, on the DNS tunneling dataset, the CharCNN model outperforms in Specificity (SPC) and diagnostic efficiency (DE). Additionally, the results indicate that finetuning BERT and cybersecurity-specific BERT-based models (BERT, SecureBERT, and CySecBERT) outperforms character-based deep learning models in most datasets and metrics.\nTable 4 presents the obtained results for domain names multi-class classification tasks. The aim of these tasks is to classify domain names into a set of predefined class labels. The overall results show that DomURLs_BERT model outperform the other state-of-the-art models domain generation algorithm classification datasets on most evaluation 8\nA PREPRINT - SEPTEMBER 17, 2024 measures. Nevertheless, the CharBiLSTM achieves better F1_macro and precision (PPV) on the UTL_DGA22 dataset.\nFor malware domain names classification (ThreathFox_MalDomains dataset), DomURLs_BERT model yields better Accuracy, F1_wted, F1_mic, NPV, specificity (SPC), and false positive rate (FPR). However, the best F1_macro, precision (PPV), diagnostic efficiency (DE), recall (TPR) and false negative rate (FNR) are achieved by CharCNN, CharCNN, BERT, and CharBiGRU, respectively. For DNS tunneling, all models achieve nearly similar performances.",
      "keywords": [],
      "page_range": [
        8,
        8
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-13",
      "chunk_indices": [
        48
      ],
      "char_count": 205,
      "summary": "However, the best results are obtained using CharBiGRU, CharGRU, and SecureBERT ",
      "digest": "However, the best results are obtained using CharBiGRU, CharGRU, and SecureBERT models.\nTable 4: Malicious domain names classification. For each dataset, the best performances are highlighted in bold font.",
      "full_text": "However, the best results are obtained using CharBiGRU, CharGRU, and SecureBERT models.\nTable 4: Malicious domain names classification. For each dataset, the best performances are highlighted in bold font.",
      "keywords": [],
      "page_range": [
        9,
        9
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-14",
      "chunk_indices": [
        49
      ],
      "char_count": 999,
      "summary": "All performance measures are presented as percentages. dataset Name AccuracyF1_m",
      "digest": "All performance measures are presented as percentages. dataset Name AccuracyF1_macro F1_wtedF1_micro TPR PPV NPV SPC DE FDR FPR FNR CharBiGRU 74.87 35.97 72.80 74.87 34.31 46.60 99.50 99.39 33.85 34.941 0.611 65.686 CharBiLSTM 74.48 32.00 71.90 74.48 29.99 42.87 99.50 99.36 29.49 38.664 0.640 70.012 ThreatFox_MalDomains CharCNN 73.94 37.43 71.01 73.94 32.66 54.71 99.51 99.32 32.11 32.985 0.681 67.341 CharCNNBiLSTM 68.67 10.30 64.41 68.67 11.97 9.76 99.39 99.27 11.43 10.243 0.731 88.029 CharGRU 74.55 35.29 71.95 74.55 33.62 48.85 99.51 99.36 33.12 35.768 0.635 66.384 CharLSTM 73.53 29.64 71.21 73.53 27.99 37.41 99.47 99.36 27.52 30.281 0.636 72.007 CySecBERT 75.55 30.02 73.69 75.55 28.67 39.41 99.50 99.43 28.25 23.667 0.565 71.331 SecBERT 74.44 28.14 72.16 74.44 26.60 41.25 99.49 99.38 26.13 27.979 0.616 73.400 SecureBERT 75.88 31.97 73.85 75.88 30.88 39.66 99.52 99.43 30.44 23.416 0.573 69.123 BERT 75.36 25.66 72.83 75.36 25.21 35.01 99.52 99.41 24.74 17.295 0.587 74.794 URLBERT 72.75",
      "full_text": "All performance measures are presented as percentages. dataset Name AccuracyF1_macro F1_wtedF1_micro TPR PPV NPV SPC DE FDR FPR FNR CharBiGRU 74.87 35.97 72.80 74.87 34.31 46.60 99.50 99.39 33.85 34.941 0.611 65.686 CharBiLSTM 74.48 32.00 71.90 74.48 29.99 42.87 99.50 99.36 29.49 38.664 0.640 70.012 ThreatFox_MalDomains CharCNN 73.94 37.43 71.01 73.94 32.66 54.71 99.51 99.32 32.11 32.985 0.681 67.341 CharCNNBiLSTM 68.67 10.30 64.41 68.67 11.97 9.76 99.39 99.27 11.43 10.243 0.731 88.029 CharGRU 74.55 35.29 71.95 74.55 33.62 48.85 99.51 99.36 33.12 35.768 0.635 66.384 CharLSTM 73.53 29.64 71.21 73.53 27.99 37.41 99.47 99.36 27.52 30.281 0.636 72.007 CySecBERT 75.55 30.02 73.69 75.55 28.67 39.41 99.50 99.43 28.25 23.667 0.565 71.331 SecBERT 74.44 28.14 72.16 74.44 26.60 41.25 99.49 99.38 26.13 27.979 0.616 73.400 SecureBERT 75.88 31.97 73.85 75.88 30.88 39.66 99.52 99.43 30.44 23.416 0.573 69.123 BERT 75.36 25.66 72.83 75.36 25.21 35.01 99.52 99.41 24.74 17.295 0.587 74.794 URLBERT 72.75",
      "keywords": [],
      "page_range": [
        8,
        8
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-15",
      "chunk_indices": [
        50
      ],
      "char_count": 996,
      "summary": "27.979 0.616 73.400 SecureBERT 75.88 31.97 73.85 75.88 30.88 39.66 99.52 99.43 3",
      "digest": "27.979 0.616 73.400 SecureBERT 75.88 31.97 73.85 75.88 30.88 39.66 99.52 99.43 30.44 23.416 0.573 69.123 BERT 75.36 25.66 72.83 75.36 25.21 35.01 99.52 99.41 24.74 17.295 0.587 74.794 URLBERT 72.75 22.91 69.00 72.75 21.38 30.98 99.50 99.29 20.81 29.021 0.709 78.624 DomURLs_BERT 76.64 26.45 74.53 76.64 25.19 33.20 99.53 99.47 24.78 17.565 0.534 74.814 CharBiGRU 88.95 83.95 88.26 88.95 84.87 85.47 99.77 99.77 84.68 12.569 0.231 15.134 CharBiLSTM 90.68 86.25 90.23 90.68 86.62 87.59 99.81 99.81 86.47 10.445 0.192 13.380 CharCNN 85.83 80.68 85.50 85.83 80.64 82.67 99.71 99.70 80.43 15.367 0.296 19.365 CharCNNBiLSTM 87.11 81.38 86.43 87.11 82.05 83.51 99.73 99.73 81.84 14.528 0.270 17.947 CharGRU 89.26 84.42 88.54 89.26 85.25 86.16 99.78 99.78 85.07 11.882 0.224 14.746UMUDGA CharLSTM 90.48 85.95 89.91 90.48 86.52 87.69 99.80 99.80 86.36 10.354 0.196 13.483 CySecBERT 90.40 86.07 90.19 90.40 86.23 87.36 99.80 99.80 86.09 10.681 0.195 13.767 SecBERT 89.48 85.27 89.29 89.48 85.59 86.33 99.78",
      "full_text": "27.979 0.616 73.400 SecureBERT 75.88 31.97 73.85 75.88 30.88 39.66 99.52 99.43 30.44 23.416 0.573 69.123 BERT 75.36 25.66 72.83 75.36 25.21 35.01 99.52 99.41 24.74 17.295 0.587 74.794 URLBERT 72.75 22.91 69.00 72.75 21.38 30.98 99.50 99.29 20.81 29.021 0.709 78.624 DomURLs_BERT 76.64 26.45 74.53 76.64 25.19 33.20 99.53 99.47 24.78 17.565 0.534 74.814 CharBiGRU 88.95 83.95 88.26 88.95 84.87 85.47 99.77 99.77 84.68 12.569 0.231 15.134 CharBiLSTM 90.68 86.25 90.23 90.68 86.62 87.59 99.81 99.81 86.47 10.445 0.192 13.380 CharCNN 85.83 80.68 85.50 85.83 80.64 82.67 99.71 99.70 80.43 15.367 0.296 19.365 CharCNNBiLSTM 87.11 81.38 86.43 87.11 82.05 83.51 99.73 99.73 81.84 14.528 0.270 17.947 CharGRU 89.26 84.42 88.54 89.26 85.25 86.16 99.78 99.78 85.07 11.882 0.224 14.746UMUDGA CharLSTM 90.48 85.95 89.91 90.48 86.52 87.69 99.80 99.80 86.36 10.354 0.196 13.483 CySecBERT 90.40 86.07 90.19 90.40 86.23 87.36 99.80 99.80 86.09 10.681 0.195 13.767 SecBERT 89.48 85.27 89.29 89.48 85.59 86.33 99.78",
      "keywords": [],
      "page_range": [
        9,
        9
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-16",
      "chunk_indices": [
        51
      ],
      "char_count": 991,
      "summary": "89.91 90.48 86.52 87.69 99.80 99.80 86.36 10.354 0.196 13.483 CySecBERT 90.40 86",
      "digest": "89.91 90.48 86.52 87.69 99.80 99.80 86.36 10.354 0.196 13.483 CySecBERT 90.40 86.07 90.19 90.40 86.23 87.36 99.80 99.80 86.09 10.681 0.195 13.767 SecBERT 89.48 85.27 89.29 89.48 85.59 86.33 99.78 99.78 85.44 11.711 0.216 14.407 SecureBERT 90.54 86.22 90.33 90.54 86.60 87.09 99.81 99.81 86.46 10.945 0.192 13.402 BERT 90.40 85.99 90.10 90.40 86.22 87.51 99.80 99.80 86.07 10.533 0.196 13.781 URLBERT 89.32 85.07 88.98 89.32 85.41 86.33 99.78 99.78 85.24 11.705 0.221 14.587 DomURLs_BERT 90.86 86.26 90.46 90.86 86.66 87.78 99.81 99.81 86.52 10.258 0.185 13.337 CharBiGRU 86.16 82.50 85.12 86.16 83.56 85.02 99.81 99.81 83.41 14.978 0.188 16.443 CharBiLSTM 88.36 85.23 87.60 88.36 86.07 87.05 99.84 99.84 85.95 12.952 0.156 13.933 CharCNN 83.13 79.78 82.69 83.13 80.05 82.29 99.77 99.77 79.90 17.715 0.228 19.955 CharCNNBiLSTM 84.46 80.96 83.84 84.46 81.43 83.49 99.79 99.79 81.28 16.510 0.210 18.571 UTL_DGA22 CharGRU 86.48 83.42 85.86 86.48 84.15 84.90 99.82 99.82 84.02 13.805 0.183 15.850",
      "full_text": "89.91 90.48 86.52 87.69 99.80 99.80 86.36 10.354 0.196 13.483 CySecBERT 90.40 86.07 90.19 90.40 86.23 87.36 99.80 99.80 86.09 10.681 0.195 13.767 SecBERT 89.48 85.27 89.29 89.48 85.59 86.33 99.78 99.78 85.44 11.711 0.216 14.407 SecureBERT 90.54 86.22 90.33 90.54 86.60 87.09 99.81 99.81 86.46 10.945 0.192 13.402 BERT 90.40 85.99 90.10 90.40 86.22 87.51 99.80 99.80 86.07 10.533 0.196 13.781 URLBERT 89.32 85.07 88.98 89.32 85.41 86.33 99.78 99.78 85.24 11.705 0.221 14.587 DomURLs_BERT 90.86 86.26 90.46 90.86 86.66 87.78 99.81 99.81 86.52 10.258 0.185 13.337 CharBiGRU 86.16 82.50 85.12 86.16 83.56 85.02 99.81 99.81 83.41 14.978 0.188 16.443 CharBiLSTM 88.36 85.23 87.60 88.36 86.07 87.05 99.84 99.84 85.95 12.952 0.156 13.933 CharCNN 83.13 79.78 82.69 83.13 80.05 82.29 99.77 99.77 79.90 17.715 0.228 19.955 CharCNNBiLSTM 84.46 80.96 83.84 84.46 81.43 83.49 99.79 99.79 81.28 16.510 0.210 18.571 UTL_DGA22 CharGRU 86.48 83.42 85.86 86.48 84.15 84.90 99.82 99.82 84.02 13.805 0.183 15.850",
      "keywords": [],
      "page_range": [
        9,
        9
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-17",
      "chunk_indices": [
        52
      ],
      "char_count": 998,
      "summary": "17.715 0.228 19.955 CharCNNBiLSTM 84.46 80.96 83.84 84.46 81.43 83.49 99.79 99.7",
      "digest": "17.715 0.228 19.955 CharCNNBiLSTM 84.46 80.96 83.84 84.46 81.43 83.49 99.79 99.79 81.28 16.510 0.210 18.571 UTL_DGA22 CharGRU 86.48 83.42 85.86 86.48 84.15 84.90 99.82 99.82 84.02 13.805 0.183 15.850 CharLSTM 87.96 84.32 86.75 87.96 85.52 86.73 99.84 99.84 85.39 13.269 0.162 14.484 CySecBERT 87.97 84.76 87.35 87.97 85.35 86.90 99.84 99.84 85.23 13.095 0.160 14.653 SecBERT 86.94 84.16 86.60 86.94 84.72 85.76 99.82 99.83 84.60 14.235 0.174 15.281 SecureBERT 88.17 84.76 87.33 88.17 85.71 87.49 99.84 99.84 85.60 12.506 0.157 14.290 BERT 87.88 84.51 87.14 87.88 85.24 87.21 99.84 99.84 85.12 12.792 0.162 14.764 URLBERT 87.17 84.04 86.46 87.17 84.79 86.24 99.83 99.83 84.67 13.764 0.172 15.214 88.50 87.68 88.50 86.08 99.85 99.85 85.97 0.153 13.918DomURLs_BERT 85.09 86.08 12.624 CharBiGRU 99.98 99.94 99.98 99.98 99.94 99.93 100.00 100.00 99.94 0.067 0.004 0.056 CharBiLSTM 96.92 88.47 96.34 96.92 87.01 96.45 99.37 99.28 86.30 3.550 0.718 12.986 CharCNN 99.97 99.94 99.97 99.97 99.93 99.94 99.99",
      "full_text": "17.715 0.228 19.955 CharCNNBiLSTM 84.46 80.96 83.84 84.46 81.43 83.49 99.79 99.79 81.28 16.510 0.210 18.571 UTL_DGA22 CharGRU 86.48 83.42 85.86 86.48 84.15 84.90 99.82 99.82 84.02 13.805 0.183 15.850 CharLSTM 87.96 84.32 86.75 87.96 85.52 86.73 99.84 99.84 85.39 13.269 0.162 14.484 CySecBERT 87.97 84.76 87.35 87.97 85.35 86.90 99.84 99.84 85.23 13.095 0.160 14.653 SecBERT 86.94 84.16 86.60 86.94 84.72 85.76 99.82 99.83 84.60 14.235 0.174 15.281 SecureBERT 88.17 84.76 87.33 88.17 85.71 87.49 99.84 99.84 85.60 12.506 0.157 14.290 BERT 87.88 84.51 87.14 87.88 85.24 87.21 99.84 99.84 85.12 12.792 0.162 14.764 URLBERT 87.17 84.04 86.46 87.17 84.79 86.24 99.83 99.83 84.67 13.764 0.172 15.214 88.50 87.68 88.50 86.08 99.85 99.85 85.97 0.153 13.918DomURLs_BERT 85.09 86.08 12.624 CharBiGRU 99.98 99.94 99.98 99.98 99.94 99.93 100.00 100.00 99.94 0.067 0.004 0.056 CharBiLSTM 96.92 88.47 96.34 96.92 87.01 96.45 99.37 99.28 86.30 3.550 0.718 12.986 CharCNN 99.97 99.94 99.97 99.97 99.93 99.94 99.99",
      "keywords": [],
      "page_range": [
        9,
        9
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-18",
      "chunk_indices": [
        53
      ],
      "char_count": 988,
      "summary": "99.98 99.98 99.94 99.93 100.00 100.00 99.94 0.067 0.004 0.056 CharBiLSTM 96.92 8",
      "digest": "99.98 99.98 99.94 99.93 100.00 100.00 99.94 0.067 0.004 0.056 CharBiLSTM 96.92 88.47 96.34 96.92 87.01 96.45 99.37 99.28 86.30 3.550 0.718 12.986 CharCNN 99.97 99.94 99.97 99.97 99.93 99.94 99.99 99.99 99.93 0.060 0.006 0.068 DNS Tunneling CharCNNBiLSTM 99.97 99.93 99.97 99.97 99.93 99.93 99.99 99.99 99.93 0.069 0.006 0.068 CharGRU 99.98 99.94 99.98 99.98 99.94 99.94 100.00 100.00 99.94 0.062 0.004 0.056 CharLSTM 96.55 87.65 96.00 96.55 85.84 96.09 99.29 99.19 85.03 3.912 0.805 14.163 CySecBERT 99.97 99.94 99.97 99.97 99.92 99.95 99.99 99.99 99.91 0.047 0.009 0.080 SecBERT 99.96 99.92 99.96 99.96 99.90 99.93 99.99 99.99 99.90 0.069 0.008 0.096 SecureBERT 99.98 99.95 99.98 99.98 99.93 99.96 100.00 100.00 99.93 0.042 0.005 0.065 BERT 99.96 99.91 99.96 99.96 99.90 99.92 99.99 99.99 99.89 0.081 0.010 0.096 URLBERT 99.93 99.81 99.93 99.93 99.84 99.78 99.98 99.99 99.83 0.217 0.015 0.158 DomURLs_BERT 99.97 99.93 99.97 99.97 99.93 99.94 99.99 99.99 99.92 0.063 0.006 0.072 4.4.2URLs",
      "full_text": "99.98 99.98 99.94 99.93 100.00 100.00 99.94 0.067 0.004 0.056 CharBiLSTM 96.92 88.47 96.34 96.92 87.01 96.45 99.37 99.28 86.30 3.550 0.718 12.986 CharCNN 99.97 99.94 99.97 99.97 99.93 99.94 99.99 99.99 99.93 0.060 0.006 0.068 DNS Tunneling CharCNNBiLSTM 99.97 99.93 99.97 99.97 99.93 99.93 99.99 99.99 99.93 0.069 0.006 0.068 CharGRU 99.98 99.94 99.98 99.98 99.94 99.94 100.00 100.00 99.94 0.062 0.004 0.056 CharLSTM 96.55 87.65 96.00 96.55 85.84 96.09 99.29 99.19 85.03 3.912 0.805 14.163 CySecBERT 99.97 99.94 99.97 99.97 99.92 99.95 99.99 99.99 99.91 0.047 0.009 0.080 SecBERT 99.96 99.92 99.96 99.96 99.90 99.93 99.99 99.99 99.90 0.069 0.008 0.096 SecureBERT 99.98 99.95 99.98 99.98 99.93 99.96 100.00 100.00 99.93 0.042 0.005 0.065 BERT 99.96 99.91 99.96 99.96 99.90 99.92 99.99 99.99 99.89 0.081 0.010 0.096 URLBERT 99.93 99.81 99.93 99.93 99.84 99.78 99.98 99.99 99.83 0.217 0.015 0.158 DomURLs_BERT 99.97 99.93 99.97 99.97 99.93 99.94 99.99 99.99 99.92 0.063 0.006 0.072 4.4.2URLs",
      "keywords": [],
      "page_range": [
        9,
        9
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-19",
      "chunk_indices": [
        54,
        55
      ],
      "char_count": 1650,
      "summary": "99.89 0.081 0.010 0.096 URLBERT 99.93 99.81 99.93 99.93 99.84 99.78 99.98 99.99 ",
      "digest": "99.89 0.081 0.010 0.096 URLBERT 99.93 99.81 99.93 99.93 99.84 99.78 99.98 99.99 99.83 0.217 0.015 0.158 DomURLs_BERT 99.97 99.93 99.97 99.97 99.93 99.94 99.99 99.99 99.92 0.063 0.006 0.072 4.4.2URLs classification Tasks Tables 5 and 6 summarizes the obtained results for URLs binary classification tasks (malicious URLs detection). The overall obtained results show that DomURLs_BERT outperforms the state-of-the-art character-based models and the evaluated BERT-based models on Grambedding, PhishCrawl, and kaggle malicious urls datasets. Besides, it achieves comparable or nearly similar performances on LNU_Phish, Mendely AK Singh, and PhiUSIIL datasets. For LNU_Phish, the top performances are achieved by CharGRU, CySecBERT, SecBERT, SecureBERT, BERT, and DomUrlsBERT models. For Mendely AK Singh dataset, DomURLs_BERT yields the best accuracy, F1_macro, F1_wted, and F1_micro.\nWhereas, SecureBERT obtains better recall (TPR), specificity (SPC), diagnostic efficiency (DE), false positive rate (",
      "full_text": "99.89 0.081 0.010 0.096 URLBERT 99.93 99.81 99.93 99.93 99.84 99.78 99.98 99.99 99.83 0.217 0.015 0.158 DomURLs_BERT 99.97 99.93 99.97 99.97 99.93 99.94 99.99 99.99 99.92 0.063 0.006 0.072 4.4.2URLs classification Tasks Tables 5 and 6 summarizes the obtained results for URLs binary classification tasks (malicious URLs detection). The overall obtained results show that DomURLs_BERT outperforms the state-of-the-art character-based models and the evaluated BERT-based models on Grambedding, PhishCrawl, and kaggle malicious urls datasets. Besides, it achieves comparable or nearly similar performances on LNU_Phish, Mendely AK Singh, and PhiUSIIL datasets. For LNU_Phish, the top performances are achieved by CharGRU, CySecBERT, SecBERT, SecureBERT, BERT, and DomUrlsBERT models. For Mendely AK Singh dataset, DomURLs_BERT yields the best accuracy, F1_macro, F1_wted, and F1_micro.\nWhereas, SecureBERT obtains better recall (TPR), specificity (SPC), diagnostic efficiency (DE), false positive rate (FPR) and false negative rate (FNR) performances. Additionally, the CharCNN model yields the best precision (PPV), 9\nA PREPRINT - SEPTEMBER 17, 2024 NPV, and false detection rate (FDR). For PhiUSIIL dataset, CySecBERT achieves the best overall performances, while the other models, including DomURLs_BERT, obtain comparable results. For ThreatFox_MalURLs dataset, all models yield nearly perfect performances, while the top results are achieved by CySecBERT, SecureBERT, and BERT models.\nIn accordance with domain names classification tasks, BERT model and the other cybersecurity BERT-based models obtain state-of-the-art perfornaces on all datasets.",
      "keywords": [],
      "page_range": [
        9,
        9
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-20",
      "chunk_indices": [
        56
      ],
      "char_count": 999,
      "summary": "Table 5: Malicious URLs detection (part 1). For each dataset, the best performan",
      "digest": "Table 5: Malicious URLs detection (part 1). For each dataset, the best performances are highlighted in bold font. All performance measures are presented as percentages. dataset Name AccuracyF1_macro F1_wtedF1_micro TPR PPV NPV SPC DE FDR FPR FNR CharBiGRU 97.50 97.50 97.50 97.50 97.50 97.51 97.51 97.50 95.06 2.489 2.497 2.497 CharBiLSTM 97.53 97.53 97.53 97.53 97.53 97.54 97.54 97.53 95.11 2.455 2.472 2.472 CharCNN 97.02 97.02 97.02 97.02 97.02 97.02 97.02 97.02 94.13 2.975 2.977 2.977 CharCNNBiLSTM 96.95 96.95 96.95 96.95 96.95 96.96 96.96 96.95 93.99 3.038 3.049 3.049 Grambedding CharGRU 97.52 97.52 97.52 97.52 97.52 97.53 97.53 97.52 95.10 2.475 2.480 2.480 CharLSTM 97.45 97.45 97.45 97.45 97.45 97.46 97.46 97.45 94.97 2.542 2.546 2.546 CySecBERT 98.40 98.40 98.40 98.40 98.40 98.41 98.41 98.40 96.81 1.593 1.603 1.603 SecBERT 97.91 97.91 97.91 97.91 97.91 97.91 97.91 97.91 95.85 2.087 2.094 2.094 SecureBERT 98.42 98.42 98.42 98.42 98.42 98.42 98.42 98.42 96.85 1.581 1.584 1.584 BERT",
      "full_text": "Table 5: Malicious URLs detection (part 1). For each dataset, the best performances are highlighted in bold font. All performance measures are presented as percentages. dataset Name AccuracyF1_macro F1_wtedF1_micro TPR PPV NPV SPC DE FDR FPR FNR CharBiGRU 97.50 97.50 97.50 97.50 97.50 97.51 97.51 97.50 95.06 2.489 2.497 2.497 CharBiLSTM 97.53 97.53 97.53 97.53 97.53 97.54 97.54 97.53 95.11 2.455 2.472 2.472 CharCNN 97.02 97.02 97.02 97.02 97.02 97.02 97.02 97.02 94.13 2.975 2.977 2.977 CharCNNBiLSTM 96.95 96.95 96.95 96.95 96.95 96.96 96.96 96.95 93.99 3.038 3.049 3.049 Grambedding CharGRU 97.52 97.52 97.52 97.52 97.52 97.53 97.53 97.52 95.10 2.475 2.480 2.480 CharLSTM 97.45 97.45 97.45 97.45 97.45 97.46 97.46 97.45 94.97 2.542 2.546 2.546 CySecBERT 98.40 98.40 98.40 98.40 98.40 98.41 98.41 98.40 96.81 1.593 1.603 1.603 SecBERT 97.91 97.91 97.91 97.91 97.91 97.91 97.91 97.91 95.85 2.087 2.094 2.094 SecureBERT 98.42 98.42 98.42 98.42 98.42 98.42 98.42 98.42 96.85 1.581 1.584 1.584 BERT",
      "keywords": [],
      "page_range": [
        10,
        10
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-21",
      "chunk_indices": [
        57
      ],
      "char_count": 997,
      "summary": "98.40 96.81 1.593 1.603 1.603 SecBERT 97.91 97.91 97.91 97.91 97.91 97.91 97.91 ",
      "digest": "98.40 96.81 1.593 1.603 1.603 SecBERT 97.91 97.91 97.91 97.91 97.91 97.91 97.91 97.91 95.85 2.087 2.094 2.094 SecureBERT 98.42 98.42 98.42 98.42 98.42 98.42 98.42 98.42 96.85 1.581 1.584 1.584 BERT 98.26 98.26 98.26 98.26 98.26 98.26 98.26 98.26 96.55 1.737 1.739 1.739 URLBERT 96.87 96.87 96.87 96.87 96.87 96.88 96.88 96.87 93.84 3.120 3.128 3.128 DomURLs_BERT 98.51 98.51 98.51 98.51 98.51 98.51 98.51 98.51 97.05 1.488 1.488 1.488 CharBiGRU 99.98 99.97 99.98 99.98 99.98 99.96 99.96 99.98 99.97 0.036 0.016 0.016 CharBiLSTM 99.98 99.97 99.98 99.98 99.98 99.96 99.96 99.98 99.97 0.036 0.016 0.016 CharCNN 99.98 99.97 99.98 99.98 99.98 99.96 99.96 99.98 99.97 0.036 0.016 0.016 CharCNNBiLSTM 99.98 99.97 99.98 99.98 99.98 99.96 99.96 99.98 99.97 0.036 0.016 0.016 LNU_Phish CharGRU 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 0.000 0.000 0.000 CharLSTM 99.98 99.97 99.98 99.98 99.98 99.96 99.96 99.98 99.97 0.036 0.016 0.016 CySecBERT 100.00 100.00 100.00 100.00 100.00 100.00",
      "full_text": "98.40 96.81 1.593 1.603 1.603 SecBERT 97.91 97.91 97.91 97.91 97.91 97.91 97.91 97.91 95.85 2.087 2.094 2.094 SecureBERT 98.42 98.42 98.42 98.42 98.42 98.42 98.42 98.42 96.85 1.581 1.584 1.584 BERT 98.26 98.26 98.26 98.26 98.26 98.26 98.26 98.26 96.55 1.737 1.739 1.739 URLBERT 96.87 96.87 96.87 96.87 96.87 96.88 96.88 96.87 93.84 3.120 3.128 3.128 DomURLs_BERT 98.51 98.51 98.51 98.51 98.51 98.51 98.51 98.51 97.05 1.488 1.488 1.488 CharBiGRU 99.98 99.97 99.98 99.98 99.98 99.96 99.96 99.98 99.97 0.036 0.016 0.016 CharBiLSTM 99.98 99.97 99.98 99.98 99.98 99.96 99.96 99.98 99.97 0.036 0.016 0.016 CharCNN 99.98 99.97 99.98 99.98 99.98 99.96 99.96 99.98 99.97 0.036 0.016 0.016 CharCNNBiLSTM 99.98 99.97 99.98 99.98 99.98 99.96 99.96 99.98 99.97 0.036 0.016 0.016 LNU_Phish CharGRU 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 0.000 0.000 0.000 CharLSTM 99.98 99.97 99.98 99.98 99.98 99.96 99.96 99.98 99.97 0.036 0.016 0.016 CySecBERT 100.00 100.00 100.00 100.00 100.00 100.00",
      "keywords": [],
      "page_range": [
        10,
        10
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-22",
      "chunk_indices": [
        58
      ],
      "char_count": 997,
      "summary": "100.00 100.00 100.00 100.00 100.00 100.00 100.00 0.000 0.000 0.000 CharLSTM 99.9",
      "digest": "100.00 100.00 100.00 100.00 100.00 100.00 100.00 0.000 0.000 0.000 CharLSTM 99.98 99.97 99.98 99.98 99.98 99.96 99.96 99.98 99.97 0.036 0.016 0.016 CySecBERT 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 0.000 0.000 0.000 SecBERT 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 0.000 0.000 0.000 SecureBERT 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 0.000 0.000 0.000 BERT 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 0.000 0.000 0.000 URLBERT 99.98 99.97 99.98 99.98 99.98 99.96 99.96 99.98 99.97 0.036 0.016 0.016 DomURLs_BERT 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 0.000 0.000 0.000 CharBiGRU 98.82 83.53 98.69 98.82 77.42 93.41 93.41 77.42 54.91 6.589 22.584 22.584 CharBiLSTM 98.88 83.99 98.74 98.88 77.18 95.71 95.71 77.18 54.40 4.286 22.821 22.821 CharCNN 98.80 82.20 98.62 98.80 74.96 95.88 95.88 74.96 49.96 4.124 25.041 25.041Mendeley AK Singh CharCNNBiLSTM 98.76 81.46 98.57 98.76 74.15 95.69",
      "full_text": "100.00 100.00 100.00 100.00 100.00 100.00 100.00 0.000 0.000 0.000 CharLSTM 99.98 99.97 99.98 99.98 99.98 99.96 99.96 99.98 99.97 0.036 0.016 0.016 CySecBERT 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 0.000 0.000 0.000 SecBERT 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 0.000 0.000 0.000 SecureBERT 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 0.000 0.000 0.000 BERT 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 0.000 0.000 0.000 URLBERT 99.98 99.97 99.98 99.98 99.98 99.96 99.96 99.98 99.97 0.036 0.016 0.016 DomURLs_BERT 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 0.000 0.000 0.000 CharBiGRU 98.82 83.53 98.69 98.82 77.42 93.41 93.41 77.42 54.91 6.589 22.584 22.584 CharBiLSTM 98.88 83.99 98.74 98.88 77.18 95.71 95.71 77.18 54.40 4.286 22.821 22.821 CharCNN 98.80 82.20 98.62 98.80 74.96 95.88 95.88 74.96 49.96 4.124 25.041 25.041Mendeley AK Singh CharCNNBiLSTM 98.76 81.46 98.57 98.76 74.15 95.69",
      "keywords": [],
      "page_range": [
        10,
        10
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-23",
      "chunk_indices": [
        59
      ],
      "char_count": 998,
      "summary": "77.18 95.71 95.71 77.18 54.40 4.286 22.821 22.821 CharCNN 98.80 82.20 98.62 98.8",
      "digest": "77.18 95.71 95.71 77.18 54.40 4.286 22.821 22.821 CharCNN 98.80 82.20 98.62 98.80 74.96 95.88 95.88 74.96 49.96 4.124 25.041 25.041Mendeley AK Singh CharCNNBiLSTM 98.76 81.46 98.57 98.76 74.15 95.69 95.69 74.15 48.35 4.308 25.850 25.850 CharGRU 98.85 83.49 98.70 98.85 76.74 95.15 95.15 76.74 53.54 4.851 23.257 23.257 CharLSTM 98.83 83.12 98.68 98.83 76.30 95.14 95.14 76.30 52.65 4.859 23.704 23.704 CySecBERT 98.97 86.40 98.89 98.97 81.39 93.44 93.44 81.39 62.85 6.563 18.611 18.611 SecBERT 98.81 83.83 98.69 98.81 78.60 91.59 91.59 78.60 57.30 8.410 21.401 21.401 SecureBERT 98.89 85.64 98.82 98.89 81.58 90.98 90.98 81.58 63.27 9.017 18.418 18.418 BERT 98.97 86.30 98.88 98.97 81.33 93.27 93.27 81.33 62.74 6.725 18.670 18.670 URLBERT 98.68 80.38 98.48 98.68 73.49 93.51 93.51 73.49 47.06 6.489 26.506 26.506 99.00 86.70 98.92 99.00DomURLs_BERT 81.46 94.16 94.16 81.46 62.99 5.841 18.535 18.535 CharBiGRU 99.80 99.79 99.80 99.80 99.76 99.82 99.82 99.76 99.53 0.179 0.237 0.237 CharBiLSTM 99.78",
      "full_text": "77.18 95.71 95.71 77.18 54.40 4.286 22.821 22.821 CharCNN 98.80 82.20 98.62 98.80 74.96 95.88 95.88 74.96 49.96 4.124 25.041 25.041Mendeley AK Singh CharCNNBiLSTM 98.76 81.46 98.57 98.76 74.15 95.69 95.69 74.15 48.35 4.308 25.850 25.850 CharGRU 98.85 83.49 98.70 98.85 76.74 95.15 95.15 76.74 53.54 4.851 23.257 23.257 CharLSTM 98.83 83.12 98.68 98.83 76.30 95.14 95.14 76.30 52.65 4.859 23.704 23.704 CySecBERT 98.97 86.40 98.89 98.97 81.39 93.44 93.44 81.39 62.85 6.563 18.611 18.611 SecBERT 98.81 83.83 98.69 98.81 78.60 91.59 91.59 78.60 57.30 8.410 21.401 21.401 SecureBERT 98.89 85.64 98.82 98.89 81.58 90.98 90.98 81.58 63.27 9.017 18.418 18.418 BERT 98.97 86.30 98.88 98.97 81.33 93.27 93.27 81.33 62.74 6.725 18.670 18.670 URLBERT 98.68 80.38 98.48 98.68 73.49 93.51 93.51 73.49 47.06 6.489 26.506 26.506 99.00 86.70 98.92 99.00DomURLs_BERT 81.46 94.16 94.16 81.46 62.99 5.841 18.535 18.535 CharBiGRU 99.80 99.79 99.80 99.80 99.76 99.82 99.82 99.76 99.53 0.179 0.237 0.237 CharBiLSTM 99.78",
      "keywords": [],
      "page_range": [
        10,
        10
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-24",
      "chunk_indices": [
        60
      ],
      "char_count": 995,
      "summary": "26.506 26.506 99.00 86.70 98.92 99.00DomURLs_BERT 81.46 94.16 94.16 81.46 62.99 ",
      "digest": "26.506 26.506 99.00 86.70 98.92 99.00DomURLs_BERT 81.46 94.16 94.16 81.46 62.99 5.841 18.535 18.535 CharBiGRU 99.80 99.79 99.80 99.80 99.76 99.82 99.82 99.76 99.53 0.179 0.237 0.237 CharBiLSTM 99.78 99.78 99.78 99.78 99.75 99.80 99.80 99.75 99.51 0.196 0.247 0.247 CharCNN 99.79 99.79 99.79 99.79 99.76 99.82 99.82 99.76 99.52 0.182 0.239 0.239 CharCNNBiLSTM 99.81 99.81 99.81 99.81 99.78 99.83 99.83 99.78 99.56 0.167 0.219 0.219 CharGRU 99.79 99.78 99.79 99.79 99.76 99.82 99.82 99.76 99.51 0.185 0.244 0.244PhiUSIIL CharLSTM 99.78 99.78 99.78 99.78 99.75 99.81 99.81 99.75 99.50 0.192 0.251 0.251 CySecBERT 99.82 99.81 99.82 99.82 99.78 99.84 99.84 99.78 99.57 0.161 0.216 0.216 SecBERT 99.80 99.79 99.80 99.80 99.76 99.82 99.82 99.76 99.53 0.180 0.236 0.236 SecureBERT 99.81 99.80 99.81 99.81 99.78 99.83 99.83 99.78 99.56 0.173 0.222 0.222 BERT 99.80 99.80 99.80 99.80 99.78 99.82 99.82 99.78 99.55 0.180 0.223 0.223 URLBERT 99.79 99.78 99.79 99.79 99.76 99.81 99.81 99.76 99.51 0.191 0.243",
      "full_text": "26.506 26.506 99.00 86.70 98.92 99.00DomURLs_BERT 81.46 94.16 94.16 81.46 62.99 5.841 18.535 18.535 CharBiGRU 99.80 99.79 99.80 99.80 99.76 99.82 99.82 99.76 99.53 0.179 0.237 0.237 CharBiLSTM 99.78 99.78 99.78 99.78 99.75 99.80 99.80 99.75 99.51 0.196 0.247 0.247 CharCNN 99.79 99.79 99.79 99.79 99.76 99.82 99.82 99.76 99.52 0.182 0.239 0.239 CharCNNBiLSTM 99.81 99.81 99.81 99.81 99.78 99.83 99.83 99.78 99.56 0.167 0.219 0.219 CharGRU 99.79 99.78 99.79 99.79 99.76 99.82 99.82 99.76 99.51 0.185 0.244 0.244PhiUSIIL CharLSTM 99.78 99.78 99.78 99.78 99.75 99.81 99.81 99.75 99.50 0.192 0.251 0.251 CySecBERT 99.82 99.81 99.82 99.82 99.78 99.84 99.84 99.78 99.57 0.161 0.216 0.216 SecBERT 99.80 99.79 99.80 99.80 99.76 99.82 99.82 99.76 99.53 0.180 0.236 0.236 SecureBERT 99.81 99.80 99.81 99.81 99.78 99.83 99.83 99.78 99.56 0.173 0.222 0.222 BERT 99.80 99.80 99.80 99.80 99.78 99.82 99.82 99.78 99.55 0.180 0.223 0.223 URLBERT 99.79 99.78 99.79 99.79 99.76 99.81 99.81 99.76 99.51 0.191 0.243",
      "keywords": [],
      "page_range": [
        10,
        10
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-25",
      "chunk_indices": [
        61,
        62
      ],
      "char_count": 1423,
      "summary": "99.78 99.83 99.83 99.78 99.56 0.173 0.222 0.222 BERT 99.80 99.80 99.80 99.80 99.",
      "digest": "99.78 99.83 99.83 99.78 99.56 0.173 0.222 0.222 BERT 99.80 99.80 99.80 99.80 99.78 99.82 99.82 99.78 99.55 0.180 0.223 0.223 URLBERT 99.79 99.78 99.79 99.79 99.76 99.81 99.81 99.76 99.51 0.191 0.243 0.243 DomURLs_BERT 99.80 99.80 99.80 99.80 99.77 99.83 99.83 99.77 99.54 0.170 0.229 0.229 Table 7 summarizes the obtained results for malicious URLs multi-class classification. The overall obtained results show that DomURLs_BERT achieve comparable results to the best performing models (SecureBERT and CySecBERT). For kaggle malicious urls dataset, DomURLs_BERT yields slightly better Accuracy, F1_wted, F1_micro, NPV, specificity (SPC), and false positive rate (FPR) performances, while SecureBERT obtains slightly better F1_macro, recall (TPR), precison, diagnostic efficiency (DE), false detection rate (FDR), and false negative rate (FNR). Although CySecBERT outperforms all evaluated models on the ThreatFox_MalURLs dataset, DomURLs_BERT, BERT, SecureBERT also demonstrate performances that are\n",
      "full_text": "99.78 99.83 99.83 99.78 99.56 0.173 0.222 0.222 BERT 99.80 99.80 99.80 99.80 99.78 99.82 99.82 99.78 99.55 0.180 0.223 0.223 URLBERT 99.79 99.78 99.79 99.79 99.76 99.81 99.81 99.76 99.51 0.191 0.243 0.243 DomURLs_BERT 99.80 99.80 99.80 99.80 99.77 99.83 99.83 99.77 99.54 0.170 0.229 0.229 Table 7 summarizes the obtained results for malicious URLs multi-class classification. The overall obtained results show that DomURLs_BERT achieve comparable results to the best performing models (SecureBERT and CySecBERT). For kaggle malicious urls dataset, DomURLs_BERT yields slightly better Accuracy, F1_wted, F1_micro, NPV, specificity (SPC), and false positive rate (FPR) performances, while SecureBERT obtains slightly better F1_macro, recall (TPR), precison, diagnostic efficiency (DE), false detection rate (FDR), and false negative rate (FNR). Although CySecBERT outperforms all evaluated models on the ThreatFox_MalURLs dataset, DomURLs_BERT, BERT, SecureBERT also demonstrate performances that are\nrate (FDR), and false negative rate (FNR). Although CySecBERT outperforms all evaluated models on the ThreatFox_MalURLs dataset, DomURLs_BERT, BERT, SecureBERT also demonstrate performances that are closely comparable. In accordance with the previously reported results, most BERTbased models, especially those adapted to cybersecurity domain, yields state-of-the-art performances on malicious URLs classification tasks. 10",
      "keywords": [],
      "page_range": [
        10,
        10
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-26",
      "chunk_indices": [
        63
      ],
      "char_count": 995,
      "summary": "A PREPRINT - SEPTEMBER 17, 2024 Table 6: Malicious URLs detection (part 2). For ",
      "digest": "A PREPRINT - SEPTEMBER 17, 2024 Table 6: Malicious URLs detection (part 2). For each dataset, the best performances are highlighted in bold font. All performance measures are presented as percentages. dataset Name AccuracyF1_macro F1_wtedF1_micro TPR PPV NPV SPC DE FDR FPR FNR CharBiGRU 96.80 96.78 96.80 96.80 96.81 96.75 96.75 96.81 93.71 3.251 3.195 3.195 CharBiLSTM 96.88 96.84 96.87 96.88 96.71 97.04 97.04 96.71 93.48 2.964 3.295 3.295 CharCNN 97.13 97.10 97.13 97.13 97.04 97.17 97.17 97.04 94.16 2.828 2.959 2.959 CharCNNBiLSTM 96.97 96.94 96.97 96.97 96.83 97.08 97.08 96.83 93.74 2.921 3.167 3.167 PhishCrawl CharGRU 96.94 96.90 96.93 96.94 96.78 97.07 97.07 96.78 93.64 2.932 3.218 3.218 CharLSTM 97.09 97.06 97.08 97.09 96.96 97.19 97.19 96.96 93.98 2.807 3.044 3.044 CySecBERT 98.15 98.14 98.15 98.15 98.06 98.22 98.22 98.06 96.15 1.775 1.937 1.937 SecBERT 97.36 97.34 97.36 97.36 97.27 97.42 97.42 97.27 94.61 2.583 2.727 2.727 SecureBERT 98.23 98.22 98.23 98.23 98.16 98.28 98.28",
      "full_text": "A PREPRINT - SEPTEMBER 17, 2024 Table 6: Malicious URLs detection (part 2). For each dataset, the best performances are highlighted in bold font. All performance measures are presented as percentages. dataset Name AccuracyF1_macro F1_wtedF1_micro TPR PPV NPV SPC DE FDR FPR FNR CharBiGRU 96.80 96.78 96.80 96.80 96.81 96.75 96.75 96.81 93.71 3.251 3.195 3.195 CharBiLSTM 96.88 96.84 96.87 96.88 96.71 97.04 97.04 96.71 93.48 2.964 3.295 3.295 CharCNN 97.13 97.10 97.13 97.13 97.04 97.17 97.17 97.04 94.16 2.828 2.959 2.959 CharCNNBiLSTM 96.97 96.94 96.97 96.97 96.83 97.08 97.08 96.83 93.74 2.921 3.167 3.167 PhishCrawl CharGRU 96.94 96.90 96.93 96.94 96.78 97.07 97.07 96.78 93.64 2.932 3.218 3.218 CharLSTM 97.09 97.06 97.08 97.09 96.96 97.19 97.19 96.96 93.98 2.807 3.044 3.044 CySecBERT 98.15 98.14 98.15 98.15 98.06 98.22 98.22 98.06 96.15 1.775 1.937 1.937 SecBERT 97.36 97.34 97.36 97.36 97.27 97.42 97.42 97.27 94.61 2.583 2.727 2.727 SecureBERT 98.23 98.22 98.23 98.23 98.16 98.28 98.28",
      "keywords": [],
      "page_range": [
        11,
        11
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-27",
      "chunk_indices": [
        64
      ],
      "char_count": 996,
      "summary": "98.14 98.15 98.15 98.06 98.22 98.22 98.06 96.15 1.775 1.937 1.937 SecBERT 97.36 ",
      "digest": "98.14 98.15 98.15 98.06 98.22 98.22 98.06 96.15 1.775 1.937 1.937 SecBERT 97.36 97.34 97.36 97.36 97.27 97.42 97.42 97.27 94.61 2.583 2.727 2.727 SecureBERT 98.23 98.22 98.23 98.23 98.16 98.28 98.28 98.16 96.35 1.722 1.837 1.837 BERT 98.14 98.13 98.14 98.14 98.06 98.21 98.21 98.06 96.14 1.792 1.942 1.942 URLBERT 96.70 96.66 96.69 96.70 96.57 96.78 96.78 96.57 93.25 3.224 3.426 3.426 DomURLs_BERT 98.38 98.37 98.38 98.38 98.38 98.36 98.36 98.38 96.79 1.636 1.619 1.619 CharBiGRU 99.98 99.98 99.98 99.98 99.98 99.98 99.98 99.98 99.96 0.017 0.019 0.019 CharBiLSTM 99.95 99.95 99.95 99.95 99.95 99.96 99.96 99.95 99.89 0.044 0.053 0.053 CharCNN 99.97 99.97 99.97 99.97 99.97 99.96 99.96 99.97 99.94 0.038 0.030 0.030ThreatFox_MalURLs CharCNNBiLSTM 99.97 99.97 99.97 99.97 99.97 99.97 99.97 99.97 99.93 0.033 0.033 0.033 CharGRU 99.97 99.96 99.97 99.97 99.96 99.97 99.97 99.96 99.92 0.030 0.040 0.040 CharLSTM 99.96 99.96 99.96 99.96 99.96 99.96 99.96 99.96 99.91 0.037 0.045 0.045 CySecBERT 100.00",
      "full_text": "98.14 98.15 98.15 98.06 98.22 98.22 98.06 96.15 1.775 1.937 1.937 SecBERT 97.36 97.34 97.36 97.36 97.27 97.42 97.42 97.27 94.61 2.583 2.727 2.727 SecureBERT 98.23 98.22 98.23 98.23 98.16 98.28 98.28 98.16 96.35 1.722 1.837 1.837 BERT 98.14 98.13 98.14 98.14 98.06 98.21 98.21 98.06 96.14 1.792 1.942 1.942 URLBERT 96.70 96.66 96.69 96.70 96.57 96.78 96.78 96.57 93.25 3.224 3.426 3.426 DomURLs_BERT 98.38 98.37 98.38 98.38 98.38 98.36 98.36 98.38 96.79 1.636 1.619 1.619 CharBiGRU 99.98 99.98 99.98 99.98 99.98 99.98 99.98 99.98 99.96 0.017 0.019 0.019 CharBiLSTM 99.95 99.95 99.95 99.95 99.95 99.96 99.96 99.95 99.89 0.044 0.053 0.053 CharCNN 99.97 99.97 99.97 99.97 99.97 99.96 99.96 99.97 99.94 0.038 0.030 0.030ThreatFox_MalURLs CharCNNBiLSTM 99.97 99.97 99.97 99.97 99.97 99.97 99.97 99.97 99.93 0.033 0.033 0.033 CharGRU 99.97 99.96 99.97 99.97 99.96 99.97 99.97 99.96 99.92 0.030 0.040 0.040 CharLSTM 99.96 99.96 99.96 99.96 99.96 99.96 99.96 99.96 99.91 0.037 0.045 0.045 CySecBERT 100.00",
      "keywords": [],
      "page_range": [
        11,
        11
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-28",
      "chunk_indices": [
        65
      ],
      "char_count": 995,
      "summary": "0.033 0.033 0.033 CharGRU 99.97 99.96 99.97 99.97 99.96 99.97 99.97 99.96 99.92 ",
      "digest": "0.033 0.033 0.033 CharGRU 99.97 99.96 99.97 99.97 99.96 99.97 99.97 99.96 99.92 0.030 0.040 0.040 CharLSTM 99.96 99.96 99.96 99.96 99.96 99.96 99.96 99.96 99.91 0.037 0.045 0.045 CySecBERT 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 99.99 0.004 0.004 0.004 SecBERT 99.99 99.99 99.99 99.99 100.00 99.99 99.99 100.00 99.99 0.006 0.005 0.005 SecureBERT 100.00 100.00 100.00 100.00 99.99 100.00 100.00 99.99 99.99 0.004 0.005 0.005 BERT 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 0.001 0.002 0.002 URLBERT 99.99 99.99 99.99 99.99 99.99 99.99 99.99 99.99 99.98 0.009 0.010 0.010 DomURLs_BERT 99.99 99.99 99.99 99.99 99.99 99.99 99.99 99.99 99.99 0.007 0.007 0.007 CharBiGRU 98.68 98.51 98.68 98.68 98.28 98.74 98.74 98.28 96.58 1.263 1.716 1.716 CharBiLSTM 98.63 98.45 98.63 98.63 98.26 98.65 98.65 98.26 96.54 1.354 1.738 1.738 Kaggle malicious URLs CharCNN 98.39 98.18 98.39 98.39 98.05 98.31 98.31 98.05 96.14 1.687 1.945 1.945 CharCNNBiLSTM 98.42 98.20 98.41 98.42",
      "full_text": "0.033 0.033 0.033 CharGRU 99.97 99.96 99.97 99.97 99.96 99.97 99.97 99.96 99.92 0.030 0.040 0.040 CharLSTM 99.96 99.96 99.96 99.96 99.96 99.96 99.96 99.96 99.91 0.037 0.045 0.045 CySecBERT 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 99.99 0.004 0.004 0.004 SecBERT 99.99 99.99 99.99 99.99 100.00 99.99 99.99 100.00 99.99 0.006 0.005 0.005 SecureBERT 100.00 100.00 100.00 100.00 99.99 100.00 100.00 99.99 99.99 0.004 0.005 0.005 BERT 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 0.001 0.002 0.002 URLBERT 99.99 99.99 99.99 99.99 99.99 99.99 99.99 99.99 99.98 0.009 0.010 0.010 DomURLs_BERT 99.99 99.99 99.99 99.99 99.99 99.99 99.99 99.99 99.99 0.007 0.007 0.007 CharBiGRU 98.68 98.51 98.68 98.68 98.28 98.74 98.74 98.28 96.58 1.263 1.716 1.716 CharBiLSTM 98.63 98.45 98.63 98.63 98.26 98.65 98.65 98.26 96.54 1.354 1.738 1.738 Kaggle malicious URLs CharCNN 98.39 98.18 98.39 98.39 98.05 98.31 98.31 98.05 96.14 1.687 1.945 1.945 CharCNNBiLSTM 98.42 98.20 98.41 98.42",
      "keywords": [],
      "page_range": [
        11,
        11
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-29",
      "chunk_indices": [
        66
      ],
      "char_count": 999,
      "summary": "98.63 98.63 98.26 98.65 98.65 98.26 96.54 1.354 1.738 1.738 Kaggle malicious URL",
      "digest": "98.63 98.63 98.26 98.65 98.65 98.26 96.54 1.354 1.738 1.738 Kaggle malicious URLs CharCNN 98.39 98.18 98.39 98.39 98.05 98.31 98.31 98.05 96.14 1.687 1.945 1.945 CharCNNBiLSTM 98.42 98.20 98.41 98.42 97.90 98.53 98.53 97.90 95.81 1.468 2.105 2.105 CharGRU 98.65 98.48 98.65 98.65 98.38 98.58 98.58 98.38 96.77 1.415 1.624 1.624 CharLSTM 98.58 98.40 98.58 98.58 98.29 98.51 98.51 98.29 96.60 1.493 1.712 1.712 CySecBERT 99.03 98.91 99.03 99.03 98.89 98.93 98.93 98.89 97.78 1.067 1.113 1.113 SecBERT 98.84 98.69 98.84 98.84 98.72 98.67 98.67 98.72 97.45 1.329 1.283 1.283 SecureBERT 99.00 98.88 99.00 99.00 98.86 98.90 98.90 98.86 97.73 1.103 1.140 1.140 BERT 98.95 98.82 98.95 98.95 98.90 98.74 98.74 98.90 97.81 1.261 1.101 1.101 URLBERT 98.60 98.41 98.59 98.60 98.21 98.62 98.62 98.21 96.44 1.379 1.789 1.789 DomURLs_BERT 99.13 99.02 99.13 99.13 98.93 99.11 99.11 98.93 97.88 0.889 1.066 1.066 Table 7: Malicious URLs classification. For each dataset, the best performances are highlighted in bold",
      "full_text": "98.63 98.63 98.26 98.65 98.65 98.26 96.54 1.354 1.738 1.738 Kaggle malicious URLs CharCNN 98.39 98.18 98.39 98.39 98.05 98.31 98.31 98.05 96.14 1.687 1.945 1.945 CharCNNBiLSTM 98.42 98.20 98.41 98.42 97.90 98.53 98.53 97.90 95.81 1.468 2.105 2.105 CharGRU 98.65 98.48 98.65 98.65 98.38 98.58 98.58 98.38 96.77 1.415 1.624 1.624 CharLSTM 98.58 98.40 98.58 98.58 98.29 98.51 98.51 98.29 96.60 1.493 1.712 1.712 CySecBERT 99.03 98.91 99.03 99.03 98.89 98.93 98.93 98.89 97.78 1.067 1.113 1.113 SecBERT 98.84 98.69 98.84 98.84 98.72 98.67 98.67 98.72 97.45 1.329 1.283 1.283 SecureBERT 99.00 98.88 99.00 99.00 98.86 98.90 98.90 98.86 97.73 1.103 1.140 1.140 BERT 98.95 98.82 98.95 98.95 98.90 98.74 98.74 98.90 97.81 1.261 1.101 1.101 URLBERT 98.60 98.41 98.59 98.60 98.21 98.62 98.62 98.21 96.44 1.379 1.789 1.789 DomURLs_BERT 99.13 99.02 99.13 99.13 98.93 99.11 99.11 98.93 97.88 0.889 1.066 1.066 Table 7: Malicious URLs classification. For each dataset, the best performances are highlighted in bold",
      "keywords": [],
      "page_range": [
        11,
        11
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-30",
      "chunk_indices": [
        67
      ],
      "char_count": 996,
      "summary": "1.789 DomURLs_BERT 99.13 99.02 99.13 99.13 98.93 99.11 99.11 98.93 97.88 0.889 1",
      "digest": "1.789 DomURLs_BERT 99.13 99.02 99.13 99.13 98.93 99.11 99.11 98.93 97.88 0.889 1.066 1.066 Table 7: Malicious URLs classification. For each dataset, the best performances are highlighted in bold font. All performance measures are presented as percentages. dataset Name AccuracyF1_macro F1_wtedF1_micro TPR PPV NPV SPC DE FDR FPR FNR CharBiGRU 98.36 97.45 98.35 98.36 96.86 98.06 99.29 99.17 96.05 1.938 0.832 3.136 CharBiLSTM 98.35 97.38 98.34 98.35 96.87 97.93 99.28 99.16 96.04 2.074 0.841 3.135 Kaggle malicious URLs CharCNN 97.90 96.85 97.90 97.90 96.48 97.25 99.03 98.95 95.45 2.754 1.051 3.525 CharCNNBiLSTM 98.13 97.01 98.13 98.13 96.76 97.27 99.15 99.07 95.85 2.729 0.929 3.242 CharGRU 98.20 97.28 98.19 98.20 96.52 98.07 99.26 99.03 95.57 1.926 0.972 3.483 CharLSTM 98.39 97.39 98.38 98.39 96.69 98.13 99.36 99.12 95.83 1.872 0.876 3.310 CySecBERT 98.66 97.90 98.66 98.66 97.63 98.19 99.36 99.37 97.01 1.809 0.628 2.371 SecBERT 98.44 97.58 98.43 98.44 97.03 98.15 99.32 99.20 96.25 1.849",
      "full_text": "1.789 DomURLs_BERT 99.13 99.02 99.13 99.13 98.93 99.11 99.11 98.93 97.88 0.889 1.066 1.066 Table 7: Malicious URLs classification. For each dataset, the best performances are highlighted in bold font. All performance measures are presented as percentages. dataset Name AccuracyF1_macro F1_wtedF1_micro TPR PPV NPV SPC DE FDR FPR FNR CharBiGRU 98.36 97.45 98.35 98.36 96.86 98.06 99.29 99.17 96.05 1.938 0.832 3.136 CharBiLSTM 98.35 97.38 98.34 98.35 96.87 97.93 99.28 99.16 96.04 2.074 0.841 3.135 Kaggle malicious URLs CharCNN 97.90 96.85 97.90 97.90 96.48 97.25 99.03 98.95 95.45 2.754 1.051 3.525 CharCNNBiLSTM 98.13 97.01 98.13 98.13 96.76 97.27 99.15 99.07 95.85 2.729 0.929 3.242 CharGRU 98.20 97.28 98.19 98.20 96.52 98.07 99.26 99.03 95.57 1.926 0.972 3.483 CharLSTM 98.39 97.39 98.38 98.39 96.69 98.13 99.36 99.12 95.83 1.872 0.876 3.310 CySecBERT 98.66 97.90 98.66 98.66 97.63 98.19 99.36 99.37 97.01 1.809 0.628 2.371 SecBERT 98.44 97.58 98.43 98.44 97.03 98.15 99.32 99.20 96.25 1.849",
      "keywords": [],
      "page_range": [
        11,
        11
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-31",
      "chunk_indices": [
        68,
        69
      ],
      "char_count": 1753,
      "summary": "96.69 98.13 99.36 99.12 95.83 1.872 0.876 3.310 CySecBERT 98.66 97.90 98.66 98.6",
      "digest": "96.69 98.13 99.36 99.12 95.83 1.872 0.876 3.310 CySecBERT 98.66 97.90 98.66 98.66 97.63 98.19 99.36 99.37 97.01 1.809 0.628 2.371 SecBERT 98.44 97.58 98.43 98.44 97.03 98.15 99.32 99.20 96.25 1.849 0.801 2.969 98.14 97.76 98.52 97.17 1.477 2.238SecureBERT 98.81 98.81 98.81 99.47 99.40 0.598 BERT 98.65 97.84 98.65 98.65 97.64 98.04 99.35 99.39 97.04 1.963 0.612 2.356 URLBERT 98.30 97.32 98.30 98.30 96.87 97.78 99.23 99.16 96.04 2.216 0.841 3.133 DomURLs_BERT 98.82 98.04 98.82 98.82 97.64 98.46 99.50 99.42 97.07 1.543 0.578 2.359 CharBiGRU 98.86 80.15 98.81 98.86 77.81 84.97 99.98 99.98 77.80 13.310 0.021 22.186 CharBiLSTM 98.89 79.43 98.86 98.89 77.40 84.48 99.98 99.98 77.38 12.075 0.020 22.600 CharCNN 98.64 79.46 98.60 98.64 76.75 84.24 99.98 99.97 76.73 15.763 0.026 23.252ThreatFox_MalURLs CharCNNBiLSTM 98.24 67.66 98.09 98.24 64.90 75.86 99.97 99.97 64.87 20.696 0.033 35.098 CharGRU 98.77 79.24 98.74 98.77 78.14 82.57 99.98 99.98 78.12 17.429 0.022 21.858 CharLSTM 98.69 76.37 98.64\nC",
      "full_text": "96.69 98.13 99.36 99.12 95.83 1.872 0.876 3.310 CySecBERT 98.66 97.90 98.66 98.66 97.63 98.19 99.36 99.37 97.01 1.809 0.628 2.371 SecBERT 98.44 97.58 98.43 98.44 97.03 98.15 99.32 99.20 96.25 1.849 0.801 2.969 98.14 97.76 98.52 97.17 1.477 2.238SecureBERT 98.81 98.81 98.81 99.47 99.40 0.598 BERT 98.65 97.84 98.65 98.65 97.64 98.04 99.35 99.39 97.04 1.963 0.612 2.356 URLBERT 98.30 97.32 98.30 98.30 96.87 97.78 99.23 99.16 96.04 2.216 0.841 3.133 DomURLs_BERT 98.82 98.04 98.82 98.82 97.64 98.46 99.50 99.42 97.07 1.543 0.578 2.359 CharBiGRU 98.86 80.15 98.81 98.86 77.81 84.97 99.98 99.98 77.80 13.310 0.021 22.186 CharBiLSTM 98.89 79.43 98.86 98.89 77.40 84.48 99.98 99.98 77.38 12.075 0.020 22.600 CharCNN 98.64 79.46 98.60 98.64 76.75 84.24 99.98 99.97 76.73 15.763 0.026 23.252ThreatFox_MalURLs CharCNNBiLSTM 98.24 67.66 98.09 98.24 64.90 75.86 99.97 99.97 64.87 20.696 0.033 35.098 CharGRU 98.77 79.24 98.74 98.77 78.14 82.57 99.98 99.98 78.12 17.429 0.022 21.858 CharLSTM 98.69 76.37 98.64\nCharCNNBiLSTM 98.24 67.66 98.09 98.24 64.90 75.86 99.97 99.97 64.87 20.696 0.033 35.098 CharGRU 98.77 79.24 98.74 98.77 78.14 82.57 99.98 99.98 78.12 17.429 0.022 21.858 CharLSTM 98.69 76.37 98.64 98.69 75.09 81.70 99.98 99.98 75.07 16.580 0.024 24.907 CySecBERT 99.17 84.40 99.15 99.17 83.56 87.80 99.99 99.98 83.55 12.201 0.015 16.437 SecBERT 99.00 81.40 98.95 99.00 79.58 86.18 99.98 99.98 79.57 12.093 0.018 20.418 SecureBERT 99.11 83.54 99.10 99.11 81.62 87.68 99.98 99.98 81.61 10.599 0.016 18.380 BERT 99.13 83.06 99.11 99.13 82.08 86.05 99.98 99.98 82.06 13.954 0.016 17.924 URLBERT 98.51 75.07 98.47 98.51 74.00 79.99 99.97 99.97 73.98 18.289 0.027 25.996 DomURLs_BERT 99.08 81.32 99.04 99.08 79.95 84.78 99.98 99.98 79.93 13.500 0.017 20.053 11",
      "keywords": [],
      "page_range": [
        11,
        11
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-32",
      "chunk_indices": [
        70,
        71,
        72,
        73,
        74,
        75
      ],
      "char_count": 4993,
      "summary": "A PREPRINT - SEPTEMBER 17, 2024 5Conclusion In this work, we introduced DomURLs_",
      "digest": "A PREPRINT - SEPTEMBER 17, 2024 5Conclusion In this work, we introduced DomURLs_BERT, a novel state-of-the-art pre-trained language model for detecting and classifying malicious or suspicious domain names and URLs. DomURLs_BERT is pre-trained using the masked language modeling objective on a large-scale, multilingual corpus comprising URLs, domain names, and domain generation algorithm datasets. We presented a detailed methodology for data collection, preprocessing, and domainadaptive pre-training.\nTo evaluate the performance of our model, we conducted experiments on several binary and multi-class classification tasks related to domain names and URLs, including phishing and malware detection, DGA identification, and DNS tunneling. Our results demonstrate that DomURLs_BERT achieves state-of-the-art performance across multiple datasets. Furthermore, the findings highlight that character-based deep learning models, such as RNNs, CNNs, and their combinations, serve as strong end-to-end bas",
      "full_text": "A PREPRINT - SEPTEMBER 17, 2024 5Conclusion In this work, we introduced DomURLs_BERT, a novel state-of-the-art pre-trained language model for detecting and classifying malicious or suspicious domain names and URLs. DomURLs_BERT is pre-trained using the masked language modeling objective on a large-scale, multilingual corpus comprising URLs, domain names, and domain generation algorithm datasets. We presented a detailed methodology for data collection, preprocessing, and domainadaptive pre-training.\nTo evaluate the performance of our model, we conducted experiments on several binary and multi-class classification tasks related to domain names and URLs, including phishing and malware detection, DGA identification, and DNS tunneling. Our results demonstrate that DomURLs_BERT achieves state-of-the-art performance across multiple datasets. Furthermore, the findings highlight that character-based deep learning models, such as RNNs, CNNs, and their combinations, serve as strong end-to-end baselines for URL and domain name classification. In comparison, fine-tuning a domain-generic pre-trained BERT model and adapting BERT-based models to the cybersecurity domain consistently outperforms the baseline character-based models on most benchmark datasets. Future work includes evaluating the model performance on other domain names and URLs classification tasks and exploring robust fine-tuning approaches for dealing with adversarial attacks.\nReferences [1] Sandeep Yadav, Ashwath Kumar Krishna Reddy, AL Narasimha Reddy, and Supranamaya Ranjan. Detecting algorithmically generated malicious domain names. In Proceedings of the 10th ACM SIGCOMM conference on Internet measurement, pages 48–61, 2010. [2] Doyen Sahoo, Chenghao Liu, and Steven CH Hoi. Malicious url detection using machine learning: A survey. arXiv preprint arXiv:1701.07179, 2017. [3] Kang Li, Xiangzhan Yu, and Jiujin Wang. A review: How to detect malicious domains. In Xingming Sun, Xiaorui Zhang, Zhihua Xia, and Elisa Bertino, editors, Advances in Artificial Intelligence and Security, pages 152–162, Cham, 2021. Springer International Publishing. [4] Malak Aljabri, Hanan S Altamimi, Shahd A Albelali, Maimunah Al-Harbi, Haya T Alhuraib, Najd K Alotaibi, Amal A Alahmadi, Fahd Alhaidari, Rami Mustafa A Mohammad, and Khaled Salah. Detecting malicious urls using machine learning techniques: review and research directions. IEEE Access, 10:121395–121417, 2022. [5] Boyang\nFahd Alhaidari, Rami Mustafa A Mohammad, and Khaled Salah. Detecting malicious urls using machine learning techniques: review and research directions. IEEE Access, 10:121395–121417, 2022. [5] Boyang Yu, Fei Tang, Daji Ergu, Rui Zeng, Bo Ma, and Fangyao Liu. Efficient classification of malicious urls: M-bert-a modified bert variant for enhanced semantic understanding. IEEE Access, 2024. [6] Hung Le, Quang Pham, Doyen Sahoo, and Steven CH Hoi. Urlnet: Learning a url representation with deep learning for malicious url detection. arXiv preprint arXiv:1802.03162, 2018. [7] Cagatay Catal, Görkem Giray, Bedir Tekinerdogan, Sandeep Kumar, and Suyash Shukla. Applications of deep learning for phishing detection: a systematic literature review. Knowledge and Information Systems, 64(6):1457– 1500, 2022. [8] Yong Shi, Gong Chen, and Juntao Li. Malicious domain name detection based on extreme machine learning.\nNeural Processing Letters, 48(3):1347–1357, 2018. [9] Alessandro Cucchiarelli, Christian Morbidoni, Luca Spalazzi, and Marco Baldi. Algorithmically generated malicious domain names detection based on n-grams features. Expert Systems with Applications, 170:114551, 2021. [10] Carlo Marcelo Revoredo da Silva, Eduardo Luzeiro Feitosa, and Vinicius Cardoso Garcia. Heuristic-based strategy for phishing prediction: A survey of url-based approach. Computers & Security, 88:101613, 2020. [11] Cherifa Hamroun, Ahmed Amamou, Kamel Haddadou, Hayat Haroun, and Guy Pujolle. A review on lexical based malicious domain name detection methods. Annals of Telecommunications, pages 1–17, 2024. [12] Tong Anh Tuan, Nguyen Viet Anh, Tran Thi Luong, and Hoang Viet Long. Utl_dga22-a dataset for dga botnet detection and classification. Computer Networks, 221:109508, 2023. [13] Ahmet Selman Bozkir, Firat Coskun Dalgic, and Murat Aydos. Grambeddings: a new neural network for url based identification of phishing\nand classification. Computer Networks, 221:109508, 2023. [13] Ahmet Selman Bozkir, Firat Coskun Dalgic, and Murat Aydos. Grambeddings: a new neural network for url based identification of phishing web pages through n-gram embeddings. Computers & Security, 124:102964, 2023. [14] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015. [15] Anu Vazhayil, R Vinayakumar, and KP Soman. Comparative study of the detection of malicious urls using shallow and deep networks. In 2018 9th International Conference on Computing, Communication and Networking Technologies (ICCCNT), pages 1–6. IEEE, 2018. 12",
      "keywords": [],
      "page_range": [
        12,
        12
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-33",
      "chunk_indices": [
        76,
        77,
        78,
        79,
        80
      ],
      "char_count": 4621,
      "summary": "A PREPRINT - SEPTEMBER 17, 2024 [16] Sara Afzal, Muhammad Asim, Abdul Rehman Jav",
      "digest": "A PREPRINT - SEPTEMBER 17, 2024 [16] Sara Afzal, Muhammad Asim, Abdul Rehman Javed, Mirza Omer Beg, and Thar Baker. Urldeepdetect: A deep learning approach for detecting malicious urls using semantic vector models. Journal of Network and Systems Management, 29:1–27, 2021. [17] Sea Ran Cleon Liew and Ngai-Fong Law. Use of subword tokenization for domain generation algorithm classification. Cybersecur., 6(1):49, 2023. [18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.\nWallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998–6008, 2017. [19] Jackaduma. Secbert: Pretrained bert model for cyber security text, learned cybersecurity knowledge. A",
      "full_text": "A PREPRINT - SEPTEMBER 17, 2024 [16] Sara Afzal, Muhammad Asim, Abdul Rehman Javed, Mirza Omer Beg, and Thar Baker. Urldeepdetect: A deep learning approach for detecting malicious urls using semantic vector models. Journal of Network and Systems Management, 29:1–27, 2021. [17] Sea Ran Cleon Liew and Ngai-Fong Law. Use of subword tokenization for domain generation algorithm classification. Cybersecur., 6(1):49, 2023. [18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.\nWallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998–6008, 2017. [19] Jackaduma. Secbert: Pretrained bert model for cyber security text, learned cybersecurity knowledge. Available online: https://github.com/jackaduma/SecBERT, 2022. Accessed: Aug. 02, 2024. [20] Ehsan Aghaei, Xi Niu, Waseem Shadid, and Ehab Al-Shaer. Securebert: A domain-specific language model for cybersecurity. In Security and Privacy in Communication Networks: 18th EAI International Conference, SecureComm 2022, Virtual Event, October 2022, Proceedings, pages 39–56. Springer, 2023. [21] Markus Bayer, Philipp Kuehn, Ramin Shanehsaz, and Christian Reuter. Cysecbert: A domain-adapted language model for the cybersecurity domain. ACM Trans. Priv. Secur., 27(2):18, 2024. [22] HanXiang Xu, ShenAo Wang, Ningke Li, Yanjie Zhao, Kai Chen,\nChristian Reuter. Cysecbert: A domain-adapted language model for the cybersecurity domain. ACM Trans. Priv. Secur., 27(2):18, 2024. [22] HanXiang Xu, ShenAo Wang, Ningke Li, Yanjie Zhao, Kai Chen, Kailong Wang, Yang Liu, Ting Yu, and HaoYu Wang. Large language models for cyber security: A systematic literature review. arXiv preprint arXiv:2405.04760, 2024. [23] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. [24] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. Large language models: A survey. arXiv preprint arXiv:2402.06196, 2024. [25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:\nSocher, Xavier Amatriain, and Jianfeng Gao. Large language models: A survey. arXiv preprint arXiv:2402.06196, 2024. [25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics, 2019. [26] Weiling Chang, Fei Du, and Yijing Wang. Research on malicious url detection technology based on bert model. In 2021 IEEE 9th International Conference on Information, Communication and Networks (ICICN), pages 340–345. IEEE, 2021. [27] Denish Omondi Otieno, Faranak Abri, Akbar Siami Namin, and Keith S Jones. Detecting phishing urls using the bert transformer model. In 2023 IEEE International Conference on Big Data (BigData), pages 2483–2492. IEEE, 2023. [28] Ming-Yang Su and Kuan-Lin Su. Bert-based approaches to identifying malicious urls. Sensors, 23(20):8499, 2023. [29] Yujie Li, Yanbin Wang, Haitao Xu, Zhenhao Guo, Zheng Cao, and Lun\nIEEE, 2023. [28] Ming-Yang Su and Kuan-Lin Su. Bert-based approaches to identifying malicious urls. Sensors, 23(20):8499, 2023. [29] Yujie Li, Yanbin Wang, Haitao Xu, Zhenhao Guo, Zheng Cao, and Lun Zhang. Urlbert: A contrastive and adversarial pre-trained model for url classification. arXiv preprint arXiv:2402.11495, 2024. [30] Ruitong Liu, Yanbin Wang, Haitao Xu, Zhan Qin, Yiwei Liu, and Zheng Cao. Malicious url detection via pretrained language model guided multi-level feature attention network. arXiv preprint arXiv:2311.12372, 2023. [31] Yanbin Wang, Weifan Zhu, Haitao Xu, Zhan Qin, Kui Ren, and Wenrui Ma. A large-scale pretrained deep model for phishing url detection. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1–5, 2023. [32] Yu Tian and Zhenyu Li. Dom-bert: Detecting malicious domains with pre-training model. In International Conference on Passive and Active Network Measurement, pages 133–158. Springer, 2024.",
      "keywords": [],
      "page_range": [
        13,
        13
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-34",
      "chunk_indices": [
        81,
        82
      ],
      "char_count": 703,
      "summary": "2023. [32] Yu Tian and Zhenyu Li. Dom-bert: Detecting malicious domains with pre",
      "digest": "2023. [32] Yu Tian and Zhenyu Li. Dom-bert: Detecting malicious domains with pre-training model. In International Conference on Passive and Active Network Measurement, pages 133–158. Springer, 2024. [33] Pranav Maneriker, Jack W. Stokes, Edir Garcia Lazo, Diana Carutasu, Farid Tajaddodianfar, and Arun Gururajan. Urltran: Improving phishing url detection using transformers. In MILCOM 2021 - 2021 IEEE Military Communications Conference (MILCOM), pages 197–204, 2021. [34] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\nBiobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234–1240, 2020. 13",
      "full_text": "2023. [32] Yu Tian and Zhenyu Li. Dom-bert: Detecting malicious domains with pre-training model. In International Conference on Passive and Active Network Measurement, pages 133–158. Springer, 2024. [33] Pranav Maneriker, Jack W. Stokes, Edir Garcia Lazo, Diana Carutasu, Farid Tajaddodianfar, and Arun Gururajan. Urltran: Improving phishing url detection using transformers. In MILCOM 2021 - 2021 IEEE Military Communications Conference (MILCOM), pages 197–204, 2021. [34] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\nBiobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234–1240, 2020. 13",
      "keywords": [],
      "page_range": [
        13,
        13
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    },
    {
      "group_id": "group-35",
      "chunk_indices": [
        83,
        84,
        85,
        86,
        87
      ],
      "char_count": 3858,
      "summary": "A PREPRINT - SEPTEMBER 17, 2024 [35] Iz Beltagy, Kyle Lo, and Arman Cohan. SciBE",
      "digest": "A PREPRINT - SEPTEMBER 17, 2024 [35] Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific text. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615–3620, Hong Kong, China, November 2019. Association for Computational Linguistics. [36] Priyanka Ranade, Aritran Piplai, Anupam Joshi, and Tim Finin. Cybert: Contextualized embeddings for the cybersecurity domain. In 2021 IEEE International Conference on Big Data (Big Data), pages 3334–3342. IEEE, 2021. [37] Abir Rahali and Moulay A Akhloufi. Malbert: Using transformers for cybersecurity and malicious software detection. arXiv preprint arXiv:2103.03806, 2021. [38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the\ndete",
      "full_text": "A PREPRINT - SEPTEMBER 17, 2024 [35] Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific text. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615–3620, Hong Kong, China, November 2019. Association for Computational Linguistics. [36] Priyanka Ranade, Aritran Piplai, Anupam Joshi, and Tim Finin. Cybert: Contextualized embeddings for the cybersecurity domain. In 2021 IEEE International Conference on Big Data (Big Data), pages 3334–3342. IEEE, 2021. [37] Abir Rahali and Moulay A Akhloufi. Malbert: Using transformers for cybersecurity and malicious software detection. arXiv preprint arXiv:2103.03806, 2021. [38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the\ndetection. arXiv preprint arXiv:2103.03806, 2021. [38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. [39] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. [40] Victor Le Pochat, Tom Van Goethem, Samaneh Tajalizadehkhoob, Maciej Korczy´nski, and Wouter Joosen. Tranco: A research-oriented top sites ranking hardened against manipulation. arXiv preprint arXiv:1806.01156, 2018. [41] Mattia Zago, Manuel Gil Pérez, and Gregorio Martínez Pérez. UMUDGA: A dataset for profiling dga-based botnet. Computers & Security, 92:101719, 2020. [42]\narXiv preprint arXiv:1806.01156, 2018. [41] Mattia Zago, Manuel Gil Pérez, and Gregorio Martínez Pérez. UMUDGA: A dataset for profiling dga-based botnet. Computers & Security, 92:101719, 2020. [42] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018. [43] Suchin Gururangan, Ana Marasovi´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A.\nSmith. Don’t stop pretraining: Adapt language models to domains and tasks, 2020. [44] Yakov Bubnov. DNS tunneling queries for binary classification, 2019. [45] Singh A.K. Dataset of malicious and benign webpages. Mendeley Data, 2020. [46] Manu Siddhartha. Malicious urls dataset. Kaggle, 2021. Accessed: 05-May-2024. [47] Ahmet Selman Bozkir, Firat Coskun Dalgic, and Murat Aydos. Grambeddings: A new neural network for url based identification of phishing web pages through n-gram embeddings. Computers & Security, 124:102964, 2023. [48] Giovanni Apruzzese and V. S. Subrahmanian. Mitigating adversarial gray-box attacks against phishing detectors.\nIEEE Transactions on Dependable and Secure Computing, pages 1–19, 2022. [49] Arvind Prasad and Shalini Chandra. Phiusiil: A diverse security profile empowered phishing url detection framework based on similarity index and incremental learning. Computers & Security, 136:103545, 2024. [50] Nguyet Quang Do, Ali Selamat, Hamido Fujita, and Ondrej Krejcar. An integrated model based on deep learning classifiers and pre-trained transformer for phishing url detection. Future Generation Computer Systems, 161:269–285, 2024. [51] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. 14",
      "keywords": [],
      "page_range": [
        14,
        14
      ],
      "summary_status": "failed",
      "llm_meta": {
        "model": "gpt-4o-mini",
        "temperature": 0.3,
        "prompt_version": "v1",
        "created_at": "2026-02-07T08:48:44.958382+00:00"
      }
    }
  ]
}