{
  "filename": "CONTRASTIVE CONDITIONALâ€“UNCONDITIONAL.pdf",
  "upload_time": "2025-12-11T14:00:39.260978",
  "data": {
    "full_text": "CONTRASTIVE CONDITIONALâ€“UNCONDITIONAL\nALIGNMENT FOR LONG-TAILED DIFFUSION MODEL\nFangChen1 AlexVilla1 GongboLiang2\nfchen20@ucmerced.edu avilla49@ucmerced.edu gliang@tamusa.edu\nXiaoyiLu1 MengTang1\nxiaoyi.lu@ucmerced.edu mtang4@ucmerced.edu\n1University of California Merced\n2Texas A&M University-San Antonio\nABSTRACT\nTraining data for class-conditional image synthesis often exhibit a long-tailed\ndistributionwithlimitedimagesfortailclasses. Suchanimbalancecausesmode\ncollapseandreducesthediversityofsynthesizedimagesfortailclasses. Forclass-\nconditionaldiffusionmodelstrainedonimbalanceddata,weaimtoimprovethe\ndiversityandfidelityoftailclassimageswithoutcompromisingthequalityofhead\nclassimages. Weachievethisbyintroducingtwosimplebuthighlyeffectiveloss\nfunctions. Firstly,weemployanUnsupervisedContrastiveLoss(UCL)utilizing\nnegativesamplestoincreasethedistance/dissimilarityamongsyntheticimages.\nSuchregularizationiscoupledwithastandardtrickofbatchresamplingtofurther\ndiversifytail-classimages. OursecondlossisanAlignmentLoss(AL)thataligns\nclass-conditionalgenerationwithunconditionalgenerationatlargetimesteps. This\nsecondlossmakesthedenoisingprocessinsensitivetoclassconditionsfortheinitial\nsteps,whichenrichestailclassesthroughknowledgesharingfromheadclasses.We\nsuccessfullyleveragecontrastivelearningandconditional-unconditionalalignment\nforclass-imbalanceddiffusionmodels. Ourframeworkiseasytoimplementas\ndemonstratedonbothU-NetbasedarchitectureandDiffusionTransformer. Our\nmethodoutperformsvanilladenoisingdiffusionprobabilisticmodels,score-based\ndiffusionmodel,andalternativemethodsforclass-imbalancedimagegeneration\nacrossvariousdatasets,inparticularImageNet-LTwith256Ã—256resolution.\n1 INTRODUCTION\nRecentadvancesindiffusionmodels(Hoetal.,2020;Songetal.,2020)haveledtobreakthroughin\nvariousgenerationtaskssuchasimagegeneration(Zhangetal.,2023;Rombachetal.,2022;Ruiz\net al., 2023), video generation (Ho et al., 2022a;b), image editing (Couairon et al., 2022; Kawar\netal.,2023),3Dgeneration(Pooleetal.,2022),etc. Thesediffusion-basedgenerativemodelsrelyon\nlarge-scaledatasetsfortraining,whichoftenfollowalong-taileddistributionwithdominantdatafor\nheadclassesandlimiteddatafortailclasses. Similartotheclass-imbalancedrecognitionmodel(Liu\netal.,2022)andtheclass-imbalancedgenerativeadversarialnetwork(Tanetal.,2020;Rangwani\netal.,2022;2023;Khorrametal.,2024),diffusionmodelisnotabletogeneratehigh-qualityimages\nfortailclassesduetothescarcityoftrainingdata. AsshowninFig.1,theoriginaldiffusionmodel\nwithtransformerbackbone (Maetal.,2024)generatesinferiorimagesforatailclass(redwine)\nwhentrainedusingalong-tailedversionofImageNet. Weaimtoincreasethefidelityanddiversityof\ntailclassimageswhilemaintainingthequalityofheadclassimagesfordiffusionmodels. Ourwork\nhasapositivesocietalimpactoncontentgenerationofunderrepresentedgroups.\nWe look into a highly related and crucial problem of long-tailed image recognition, for which\ncontrastivelearningisaneffectivemethod(Jiangetal.,2021;Lietal.,2022). Contrastivelearning\n1\n5202\ntcO\n42\n]VC.sc[\n2v25090.7052:viXra\n\n(a)SiT(Maetal.,2024) (b)SiTwithCCUA(ours)\nFigure1: Syntheticimagesforatailclass(â€˜redwineâ€™)by(a)standardSiTand(b)proposedCCUA\nframework.Botharetrainedonlong-tailedImageNetdatasetfor160epochswith256x256resolution.\nis a general learning paradigm of minimizing the distance of positive pairs while maximizing\nthe distance of negative pairs, which has attracted lots of research interest for self-supervised\nrepresentationlearning(Chenetal.,2020). Ontheotherhand,diffusionmodelhasalsoemergedasan\neffectivemodelforrepresentationlearning(Fuestetal.,2024;Baranchuketal.,2021). Weexplicitly\nincorporatecontrastivelearningregularizationtodiffusionmodelstofurtherenhancerepresentation\nlearningtowardsbetterimagegeneration.\nOurfirstlossisanunsupervisedcontrastivelosswithnegativesamplesonly,servingasregularization\nfordenoisingdiffusionprobabilisticmodels(DDPM)andscore-baseddiffusionmodels(SBDM).\nGiventhatmodecollapseduringinferencemanifestsasgeneratedsamplesbeingoverlysimilarfor\ntailclasses,weintroduceunsupervisedcontrastivelearningwithnegativesamplestomaximizethe\ndistancesbetweengeneratedimages. Unlikesupervisedcontrastivelearning(Khoslaetal.,2020),\nunsupervisedcontrastivelearningdistinguishesbetweenrepresentationsofimagesregardlessoftheir\nclass,henceincreasingintra-classandinter-classimagediversity.\nOur second loss is an alignment loss designed to align estimated noises from conditional and\nunconditionalgeneration,whicheffectivelyminimizestheKLdivergencebetweenlatentdistributions\nforconditionalandunconditionalgeneration. Whilesuchconditional-unconditionalalignmentseems\nundesirablewithlesscontrollability,acriticalaspectofourapproachisthatouralignmentlossis\nweightedmoreforlargertimestepscorrespondingtotheinitialstageofthereverseprocess. Giventhe\nknownoverfittinginconditionalgenerationfortailclassesforbothGAN,wearemotivatedbythe\nsuccessofconditional-unconditionalalignmentforlong-tailedGAN(Khorrametal.,2024;Shahbazi\netal.,2022),whichfacilitatesknowledgesharingbetweenheadclassesandtailclasses. Notably,\nunconditionalGANgenerationhasbeenobservedtoachievesuperiorFIDthanconditionalgeneration\nunderlimiteddata(Shahbazietal.,2022). Weadaptconditional-unconditionalalignmentfromGAN\ntodiffusionmodels. UnliketheGAN-basedmethod(Khorrametal.,2024)whichalignsconditional\ngeneration and unconditional generation for low-resolution representations of images exhibiting\nintra-classsimilarity,weleverageobservedimagesimilarityduringtheinitialdenoisingstepsfortail\nclassesandheadclasses(Sietal.,2024). Hence,weproposetomatchconditionalgenerationwith\nunconditionalgenerationforlargetimesteps.\nWhilecontrastivelearninghasbeenproposedfordiffusionmodelstoimproveadversarialrobust-\nness (Ouyang et al., 2023), find semantically meaningful directions (Dalva & Yanardag, 2024),\nacceleratetraining(Yuetal.,2024),andregularizerepresentation(Wang&He,2025),weeffectively\nleveragedcontrastivelearningforclass-imbalanceddiffusionmodelsanddemonstratethesuperior\nperformanceofourmethodonlong-tailedimagegenerationviacomprehensiveexperiments.\nThemaincontributionsofthisworkareasfollows:\nâ€¢ WeproposeContrastiveConditional-UnconditionalAlignmentforDiffusionModel(CCUA)\nwithimbalanceddata. Ourproposedcontrastivelosswithconditional-unconditionalalign-\nmentareeasytoimplementwithstandardDDPMandSBDMpipelineusingbothUNet-based\narchitectureandDiffusionTransformer(DiT).\n2\n\nâ€¢ Ourfirstloss, UnsupervisedContrastiveLoss(UCL),employsunsupervisedcontrastive\nlearninglosswithnegativesamplesonly,enhancingwithin-classdiversity.\nâ€¢ OursecondAlignmentLoss(AL)alignsunconditionalgenerationandconditionalgeneration\nfortheinitialstepsinthedenoisingprocess,facilitatingknowledgesharingbetweenhead\nandtailclasses.\nâ€¢ Weimprovedthediversityandfidelityoftailclassimagesforconditionalgenerationwhile\nmaintainingthequalityofheadclassimagesformultipledatasetsandvariousresolutions,\ninparticularImageNet-LTwith256x256resolution.\n2 RELATED WORK\nClass-imbalanced Image Generation Generative models such as VAE (Kingma et al., 2019),\nGAN(Karrasetal.,2019;Brocketal.,2019),anddiffusionmodels(Hoetal.,2020;Rombachetal.,\n2022)generateinferiorimagesfortailclasseswhentrainedwithreal-worlddatawithalong-tailed\ndistribution. Ithasattractedalotofresearchinterest(Aietal.,2023;Tanetal.,2020;Khorrametal.,\n2024;Qinetal.,2023;Zhangetal.,2024)toaddressthisissueforvarioustypesofmodels. Many\nmethodsaddresstheproblemofclassimbalancebyaugmentingtrainingdataforthetailclasses.\nA VAE is fine-tuned on tailclasses under amajority-based prior(Ai et al.,2023). It isobserved\nthatGAN(Tanetal.,2020)canamplifybiases,leadingtotailclassestobebarelygeneratedduring\ninference,highlightingthatthefairnessinGANneedsimprovement. Khorrametal.(2024)proposea\nGAN-basedlong-tailedgenerationmethod,namedUTLO,whichsharesthelatentrepresentations\nofconditionalGANwithunconditionalGANandimplicitlysharesknowledgebetweenthehead\nclassandtailclass. ThemotivationwithUTLOisthatlow-resolutionrepresentationsofimagesfrom\nGANaresimilarforheadclassesandtailclasses. Weobserveasimilarphenomenonfordenoised\nimagesintheinitialstepsofthedenoisingprocess,andfurtherproposeaconditional-unconditional\nalignmentlossdesignedfordiffusionmodels. Recentworkaddressesclass-imbalanceddiffusion\nmodels(Qinetal.,2023;Yanetal.,2024;Zhangetal.,2024)byregularizationlossestoalignor\nseparatethedistributionsofsyntheticimagesandtheircorrespondinglatentrepresentationsacross\ndifferentclasses. Forexample,CBDM(Qinetal.,2023)lossminimizesthedistanceofestimated\nnoiseitemsbetweenthesetwomodels,theoriginalDDPMmodelandasecondmodeltrainedwith\npseudolabelswhichformauniformdistribution. DiffROP(Yanetal.,2024)attemptstocombine\ncontrastivelearningwithdiffusionmodelbymaximizingthedistanceofdistributionsbetweenclasses.\nHowever, DiffROP only considers pairs of images of different classes as negative pairs without\nregularizingimagesofthesameclass. Conceptually,thefirstpartofourmethod,theunsupervised\ncontrastivelossdesignedtodisperseunconditionallatentspace,generalizesDiffROPbyincluding\nextranegativepairsofimageswithineachclass,hencepromotingintra-classdiversity.\nContrastiveLearningforRepresentationLearning Bothunsupervisedcontrastivelearning(Chen\netal.,2020;Heetal.,2020)andsupervisedcontrastivelearning(Khoslaetal.,2020;Lietal.,2022)\nareapproachesusedinrepresentationlearningtoproducefeatureembeddingsbymaximizingthe\nsimilaritybetweenpositivepairsandminimizingthesimilaritybetweennegativepairs. Unsupervised\ncontrastivelearningdoesnotrequirelabelsandtypicallyaugmentsthesamedatatoformpositive\npairs(Chenetal.,2020;Heetal.,2020). Supervisedcontrastivelearningincorporatesclasslabels\nand pulls together all samples from the same class while pushing apart samples from different\nclasses. Contrastivelearningisaneffectivemethodforself-supervisedlearning(Heetal.,2020)\nandlong-tailedrecognition(Jiangetal.,2021;Lietal.,2022). Weeffectivelyutilizedcontrastive\nlearningforclass-imbalanceddiffusionmodels. Ourworkisdifferentfromthecontrastive-guided\ndiffusion process (Ouyang et al., 2023), which aims to improve adversarial robustness. Another\ndifferenceisthatwefocusoncontrastivelearningduringtraining,whilecontrastive-guideddiffusion\nprocess(Ouyangetal.,2023)isaboutguidanceforinference. Whilethediffusionmodeloriginally\nforgenerationtasksbecomesanemergingtechniqueforrepresentationlearning(Fuestetal.,2024;\nBaranchuketal.,2021),wedirectlyintegratecontrastivelearningregularizationtodiffusionmodels\nfor better generation on imbalanced data. Notably, unsupervised contrastive loss with negative\nsamplesexclusivelyfocusesonseparatingdissimilarinstancesintheembeddingspace,whichwe\nleveragetoaddressmodecollapse,asdetailedinSec.3.1. Inthecontextofgenerativemodeling,\nREPA(Yuetal.,2024)alignsdiffusionmodelfeatureswithfeaturesfromafrozenvisionencoderto\nacceleratetraining. UnlikeREPA,wedirectlyregularizediffusionmodelfeatureswithoutexternal\nmodels. Mostrelevantisconcurrentworkofdispersiveloss(Wang&He,2025),whichissimilarto\n3\n\nNoise Estimation Network ðœ–0\nmini-batch ð“› ð“›\nð’–ð’„ð’ ð’‚ð’\nð‘¥ !# â„Ž $# %&\nð‘’(âˆ—;âˆ…) â„Ž â€™\" $( ðœ–.\" $((ð‘¥ !\";ð‘¡,âˆ…)\nð‘¥\"\n!\nð‘‘(âˆ—)\nð‘’(âˆ—;ð‘) ðœ–\" (ð‘¥\";ð‘¡,ð‘\")\n(,$- !\nð‘\"\nclass\nembedding share weights\nFigure2: OverviewoftheproposedCCUAframeworkofcontrastivelearningfordiffusionmodel.\nThenoiseestimationnetworkisdividedintotwoparts,alatentencodenetworke(âˆ—)andadecode\nnetworkd(âˆ—).e(âˆ—)encodesanimagewithnoisex toalow-dimensionallatenth,whichisdecodedto\nt\nnoiseÏµbyd(âˆ—). Specifically,thelatentencodenetworke(âˆ—)anddecodenetworkd(âˆ—)forunet-based\nmodelanddiffusiontransformerareshowninFig.5intheappendix.A.1. Weincreasethedistanceof\nnegativepairsofunconditionalencodedlatentsfordifferentsamplesbyanunsupervisedcontrastive\nlossL withnegativesamplesonly,andalignunconditionalandconditionalgenerationforthesame\nucl\nsamplexi andutilizeanalignmentlossL tominimizetheirdistancesatinitialtimesteps.\nt al\nourunsupervisedcontrastivelosswithnegativepairsonly. Dispersivelossisdirectlyandexplicitly\napplied to conditional training, as designed for balanced diffusion models. By comparison, our\nunsupervisedcontrastivelossisformulatedintheunconditionallatentspaceandisimplicitlyextended\ntoconditionaltrainingthroughourconditionalâ€“unconditionalalignmentloss. Thiscarefullydesigned\ncontrastiveconditional-unconditionalalignmentframeworkforlong-taileddiffusionmodelsachieves\nbettergenerationasshowninexperiments.\n3 METHOD\nWeproposeContrastiveConditional-UnconditionalAlignment(CCUA)frameworkforDiffusion\nModel. Fig.2givesanoverviewofourframework,whileFig.5intheappendixA.1detailshowwe\napplyCCUAframeworkintoUNet-basedmodelandDiffusionTransformer. Ourframeworkinvolves\naunsupervisedcontrastivelosswithnegativepairsonly,andaconditional-unconditionalalignment\nloss,asoutlinedinSec.3.1andSec.3.2,respectively. Sec.3.3summarizesouroverallframework.\n3.1 UNSUPERVISEDCONTRASTIVELOSS\nWeobservedthesynthesizedimagesofDDPMforatailclassconcentratedaroundthelimitedtraining\nimagesinthelatentspace,leadingtotheissueofmodecollapse,asshownintheFig.9,Fig.10inthe\nappendixA.3.3,andFig.8intheappendixA.3.2withvisualizingthedistributionofsyntheticimages\nforthetailclass. Byapplyingthecontrastivelosswithnegativesamplesonly,weintendtoincrease\nthedistanceofeachsynthesizedimagefromotherimagesinthelatentspace,henceincreasingthe\ndiversityofsyntheticimagesinparticularfortailclasses.\nWeconsidercontrastivelosswithnegativesamplesonly,wherethenegativesamplesarebasedon\nnoisyimagesfromamini-batch. Specifically,wedivideanoiseestimationnetworkintotwoparts,\nalatentencodenetworke(âˆ—),anddecodenetworkd(âˆ—). e(âˆ—)encodesanimagewithnoisex toa\nt\nlow-dimensionallatenth,whichisdecodedtotheestimatenoiseitemÏµbyd(âˆ—). Ourcontrastiveloss\nisdefinedforthelatentshwitheachimageinamini-batchtreatedasananchor. Allotherimagesare\ntreatedasnegativesamples. Theanchorsamplexi andnegativesamplesxj arefedintotheencoder\nt t\ne(âˆ—)togetthelatentshi =e(xi;âˆ…)andhj =e(xj;âˆ…). OurunsupervisedcontrastivelossL\nanc t neg t ucl\nisdefinedasfollows:\nL =âˆ’ 1 (cid:88) log Ï€ ai nc ,withÏ€i =exp(hi ancÂ·hi pos),Ï€j =exp(hi ancÂ·hj neg),\nucl |B| Ï€i +(cid:80) Ï€j anc Ï„ neg Ï„\niâˆˆB anc jâˆˆB,jÌ¸=i neg\n(1)\n4\n\nwhereBdenotesamini-batch,andÏ„ isatemperatureforsoftmaxwhichwekeep0.1asthedefault\nsetting. Note that it is common to augment an anchor image xi to be a positive sample in many\nt\nunsupervisedcontrastivelearningmethods. Here,wedonâ€™tapplyanydataaugmentationanddirectly\ntreattheanchorlatenthi itselfasthepositivevectorinthecontrastiveloss,i.e.,hi :=hi .\nanc pos anc\nWhycontrastivelossencouragesdiversityanddiscouragesmodecollapse? Intheworstcase,that\nallembeddingscollapsetoaconstantvector,thecontrastivelossbecomeslogN forabatchwithN\nsamples. Suchconstantembeddingsyieldthemaximumpossiblelossandarethereforediscouraged.\nIntuitively,contrastivelossintroducesarepulsiveforcebetweendifferentsamples. Thenumerator\nremainsconstant,asself-similarityisalwaysmaximal,whilethedenominatoraggregatespairwise\nsimilarities across the batch. Minimizing the loss requires reducing similarities between distinct\nsamples,effectivelypushingtheirembeddingsapartinfeaturespace. Inthisway,themodelavoids\ncollapsebymaximizinginter-sampledistances. Geometrically,theoptimalsolution(intheabsence\nofaugmentation)correspondstoembeddingsbeinguniformlydistributedonahypersphere.\nBatchResampleisasimplestrategyforlong-tailedrecognitionandgeneration(Zhangetal.,2024).\nHowever,itmayleadtomodecollapseduetorepetitiveimagesfortailclasses.Sinceourunsupervised\ncontrastivelossreliesonpairsofimages,batchresamplingincreasesthechanceofimagesfromthe\nsametailclassappearinginthesamebatch,whichfurtherdiversifytailclassimages. Wechoose\nbatchresamplingasanoptionalstrategyforourframeworkandprovideanablationstudyinSec.4.3.\n3.2 CONDITIONAL-UNCONDITIONALALIGNMENTLOSS\nConditinal-unconditinalAlignmentFacilitatesKnowledgeSharingforClass-imbalancedGAN\nWe take inspiration from UTLO (Khorram et al., 2024), which addresses long-tailed generation\nwith GANs, and observes that the similarity between head class and tail class images increases\nat lower resolution representations. To share knowledge between the head class and tail class,\nUTLO(Khorrametal.,2024)proposesunconditionalGANobjectivesforlower-resolutionrepresen-\ntationsandconditionalGANobjectivesforsubsequenthigher-resolutionimages. Morespecifically,\nthelower-resolutionrepresentationsbecomeslesssensitivetoclasslabels,andthesecondpartofthe\ngeneratorgeneratesthefinedetailsofimages. Utilizingunconditionalgenerationforlowerresolution\niseffectiveinincreasingthediversityandqualityoftailclassimages. Inadifferentsettingoftraining\nGANwithlimiteddata,Transitional-GAN(Shahbazietal.,2022)leveragesunconditionalgeneration\ninatwo-stagedtrainingscheme,asunconditionalgenerationcangiveabetterFIDthanconditional\ngenerationwhendataislimited.\nConditional-unconditinalAlignmentforClass-imbalancedDiffusionModel Weaimtoadapt\ntheconditional-unconditionalalignmentapproachusedinGANs(Khorrametal.,2024;Shahbazi\net al., 2022) to diffusion models. Our key insight is that the diffusion model denoises images\nrecursively,andthesimilaritybetweenheadclassimagesandtailclassimagesishigherduringthe\ninitialtimestepsofthedenoisingprocess. ThisissimilarbutdifferentfromUTLO(Khorrametal.,\n2024),whichproposesunconditionalgenerationforlower-resolutionrepresentations. Wevisualize\nthereverseprocessingofunconditionalgenerationandconditionalgenerationwithdifferentclass\nlabelsoftheoriginalDDPM,startingfromthesameGaussiannoise,asshowninFig.3andmore\nexamplesinappendixA.3.4. Imagesfromunconditionalgenerationandconditionalgenerationshare\nsimilarlow-frequencycomponents,e.g.,thecoarseshapeofsynthesizedobjectsandbackground\nregion,especiallyattheinitialtimesteps(Sietal.,2024).\nMotivated by this observation, we propose to match unconditional generation and conditional\ngeneration at the initial time steps, to enable knowledge sharing between tail classes and head\nclasses with abundant data. Another motivation is that class conditions are not necessary for all\ntimesteps. Infact,usingadaptiveguidanceweightwithasmallweightintheinitialstepscanyielda\nbetterFIDcomparedtousingconstantguidanceweight(Wangetal.,2024).\nFromConditional-unconditionalDistributionMatchingtoAlignmentLoss WepenalizeKL\ndivergence between the conditional distribution p (x |xi,ci) and unconditional distribution\nÎ¸ tâˆ’1 t\np (x |xi):\nÎ¸ tâˆ’1 t\nLi,t =D [p (x |xi,ci)||p (x |xi)]. (2)\nal KL Î¸ tâˆ’1 t Î¸ tâˆ’1 t\n5\n\nLow Frequency High Frequency\nFigure 3: The leftmost column shows synthetic images with different classes but with the same\ninitialnoiseorrandomseed. Wevisualizethelow-frequencycomponentandthehigh-frequency\ncomponentfromthereverseprocessing. Itshowsthatlow-frequencycomponentsaresimilarduring\ninitialtimestepsfordifferentclasseswiththesameinitialnoise(Sietal.,2024). Moreexamplesare\ninappendixA.3.4.\nSuppose p (x |x ,c) = N(x ;Âµ (x ,t,c),Ïƒ2I),p (x |x ) = N(x ;Âµ (x ,t),Ïƒ2I),\nÎ¸ tâˆ’1 t tâˆ’1 Î¸ t t Î¸ xâˆ’t t tâˆ’1 Î¸ t t\nthenwehave:\n1\nLi a, lt =E[ 2Ïƒ2||Âµ Î¸(xi t,t,ci)âˆ’Âµ Î¸(xi t,t)||2]+C, (3)\nt\nwhereCisconstant,and\n1 Î² 1 Î²\nÂµ Î¸(xi t,t,ci)= âˆš\nÎ±\ntxi tâˆ’ âˆš\nÎ±\ntâˆš 1t\nâˆ’Î±Â¯\ntÏµ Î¸(xi t,t,ci), Âµ Î¸(xi t,t)= âˆš\nÎ±\ntxi tâˆ’ âˆš\nÎ±\ntâˆš 1t\nâˆ’Î±Â¯\ntÏµ Î¸(xi t,t),\n(4)\nwhereÎ± =1âˆ’Î² ,Î±Â¯\n=(cid:81)t\n(1âˆ’Î² ),{Î² } isthevarianceschedule.\nt t t i=1 i t 1:T\nThen,L simplifiesto:\nal\nLi,t âˆ||Ïµ (xi,t,ci)âˆ’Ïµ (xi,t)||2. (5)\nal Î¸ t Î¸ t\nAsshowninFig.2,theanchorvectorhi isfedintodecodenetworkd(âˆ—)togetunconditionalnoise\nanc\nestimationÏµi :=Ïµ (xi;t,âˆ…). Meanwhile,theanchorimagexi isalsoincorporatedwiththeclass\nunc Î¸ t t\nlabelconditioncifedintothenetworktogettheconditionalnoiseestimationÏµi :=Ïµ (xi;t,ci).\ncond Î¸ t\nThealignmentlossisdefinedbetweentheunconditionalandconditionalnoiseestimation:\nL al = |B1 |(cid:88) E t,x0[ Tt âˆ¥Ïµ Î¸(xi t;t,ci)âˆ’Ïµ Î¸(xi t;t,âˆ…)âˆ¥2]. (6)\niâˆˆB\nThelossisweightedlinearlybytimestepst,sothattheinitialstepswithlargetareweightedmore.\nInotherwords,wealignconditionalgenerationandunconditionalgenerationfortheinitialsteps.\nAlgorithm1TrainingalgorithmofCCUA.\n3.3 OVERALLFRAMEWORK\nSetL ,L ,L =0\nddpm ucl al\nOur final loss function L is the\nforeachimage-classpair(xi 0,ci)inthisbatchBdo\nsum of the standard DDPM (Ho Samp âˆšleÏµi âˆ¼N( âˆš0,I),tâˆ¼U({0,1,...,T})\netal.,2020)lossL andourcon- xi = Î±Â¯ xi + 1âˆ’Î±Â¯ Ïµi\nddpm t t 0 t\ntrastive conditional-unconditional Ïµi =Ïµ (xi;t,âˆ…),Ïµi =Ïµ (xi;t,ci)\nunc Î¸ t cond Î¸ t\nalignmentlossL ccua: hi =e(xi)\nanc t\nL ccua =Î±Â·L ucl+Î³Â·L al, (7) Ï€ anc =exp(hi ancÂ·hi anc/Ï„),SetÏ€ neg =0\nwhere Î± and Î³ are the hyper- forxj t inthisbatchwithiÌ¸=j do\nparameters for our unsupervised hj =e(xj)\nneg t\ncontrastivelossandalignmentloss. Ï€ =Ï€ +exp(hi Â·hj /Ï„)\nneg neg anc neg\nendfor\nTheoverallalgorithmframeworkis\nshownasAlgorithm1. Inclassifier- L ucl =L ucl+(âˆ’log Ï€anÏ€ ca +n Ï€c neg)\nfree guidance (CFG), the class la- ifUnconditionalTrainingofCFGthen\nbels c are randomly dropped by L ddpm =L ddpm+âˆ¥Ïµiâˆ’Ïµi uncâˆ¥2\na specific probability p . In elseifConditionalTrainingofCFGthen\nuncond\noursetting,wekeepthep uncond = L ddpm =L ddpm+âˆ¥Ïµiâˆ’Ïµi condâˆ¥2\n10%,sameas(Qinetal.,2023).The L\nal\n=L al+ Ttâˆ¥Ïµi uncâˆ’Ïµi condâˆ¥2\nparametersofthenoiseestimation endif\nnetworkÏµ areupdatedbythegradi- endfor\nÎ¸\nentofthefinallossâˆ‡ LasinEq.7. Computethegradientwiththeloss\nÎ¸\nL= 1 (L +Î±Â·L +Î³Â·L )\n|B| ddpm ucl al\n6\n\nTable1: ComparisononImageNet-LT256Ã—256withSiTpipeline. WeuseBlueâ€˜()â€™tohighlightthe\nimprovementofourmethodoverSiTbaselineonFIDandRecall,whichdenotestheoverallquality\nanddiversity,respectively.\nEpochs Method ISâ†‘ FIDâ†“ sFIDâ†“ Prec.(%)â†‘ Recall(%)â†‘\nSiT 53.93 33.87 22.66 54.56 19.17\n40 +DispersiveLoss 53.87 34.00 22.68 54.87 19.84\n+CCUA(ours) 73.60 25.89(-7.98) 16.54 58.62 23.05(+4.08)\nSiT 78.88 25.72 21.95 64.35 18.58\n80 +DispersiveLoss 83.80 25.22 21.76 65.50 17.30\n+CCUA(ours) 103.66 19.99(-5.73) 14.88 65.80 21.31(+2.73)\nSiT 103.11 21.20 20.17 69.34 18.25\n120 +DispersiveLoss 104.02 21.32 20.46 68.07 18.56\n+CCUA(ours) 119.11 17.55(-3.65) 13.89 68.38 21.20(+2.95)\nSiT 111.70 19.95 20.12 70.34 18.61\n160 +DispersiveLoss 115.64 19.76 20.18 69.98 18.96\n+CCUA(ours) 124.87 16.41(-3.54) 13.17 69.84 21.34(+2.73)\n4 EXPERIMENTS\n4.1 EXPERIMENTALSETUP\nWereportresultsonmultipledatasets,includingthelong-tailedversionsofImageNet,TinyImageNet,\nandPlacesdatasets,namelyImageNet-LT,TinyImageNet-LT,andPlaces-LT,whicharecommonly\nuseddatasetsinlong-tailedimagegenerationandrecognition.Wealsoreportresultsandconductmore\nanalysisonCIFAR10-LT/CIFAR100-LTdatasets,whichareshownintheappendixA.2. Wemeasure\nISScore,FID,spatialFID(Dhariwal&Nichol,2021),KernelInceptionDistance(KID)(BinÂ´kowski\netal.,2018)astheoverallqualitymetrics. WealsoreportFIDoftailclasses(FID ),inclinedto\ntail\nmeasurequalityfortailclasses. PrecisionandRecallarereportedtorespectivelymeasurefidelity\nand diversity, which are widely used in evaluating generative models on long-tailed scenes (Qin\netal.,2023;Zhangetal.,2024;Khorrametal.,2024). Moreimplementationdetailsareprovidedin\ntheappendixA.1. Wecompareourproposedmethodswithmultiplebaselines,coveringdiffusion\ntransformerandunet-basedarchitecture. Fordiffusiontransformer,wecompareCCUAtoSiT(Ma\netal.,2024)andDispersiveLoss(Wang&He,2025). Forunet-basedarchitecture,wecomparetothe\noriginalDDPM(Hoetal.,2020),CBDM(Qinetal.,2023),andOCLT(Zhangetal.,2024).\n4.2 QUANTITATIVERESULTS\nClass-imbalancedGenerationforDiffusionTransformer WeapplytheproposedCCUAlossinto\nthestandarddiffusiontransformertrainingpipeline,SiT(Maetal.,2024),onImageNet-LTdataset,\nandcompareittoDispersiveLoss(Wang&He,2025),aconcurrentworkofapplyingcontrastive\nlearningforimprovingSiTonbalanceddataset. AllmodelsaresampledwithODEflow50stepsfor\nconditionalgenerationwithCFGstrength7.5. AsshowninTable1,ourmethodachievesremarkable\nimprovementcomparedtoSiTandDispersiveLossonvarioustrainingepochs. Ourmethodachieves\nabove10%improvementonRecall,whilekeepthesimilarPrecisionasSiT,illustratingthehigher\ndiversityoftheproposedCCUAlosswithouttradingofffidelity.\nClass-imbalanced Generation for DDPM For training unet-based architecture DDPM on\nTinyImageNet-LTandPlaces-LTdatasets,ourmethodalsoachievesthebestperformance,asshown\ninTab.2. AllmodelsaresampledwithDDIM100stepsforconditionalgenerationwithoptimalCFG\nstrength. ForTinyImageNet-LTdatasets,weprovidethemetricsoftheDDPMmodeltrainedonthe\nbalancedversion,i.e.,theoriginalTinyImageNetdatasets,denotedbyDDPMâˆ— ,asthetheoretical\nbal\noptimalreference. ThebestFIDandKIDscoreofourmethodillustratesthehighqualityofimages\nsynthesizedbyourmethod1.\n1TheFIDscoreofourreproducedDDPM,CBDM(Qinetal.,2023),andOCLT(Zhangetal.,2024)are\nbetterthanthenumbersreportedin(Qinetal.,2023;Zhangetal.,2024). Wetrainallmethodsfromscratch\nwithoutfine-tuningandfindtheoptimalguidancestrengthÏ‰foreachmethod.Suchimplementationisdifferent\nfromtheoneusedin(Qinetal.,2023;Zhangetal.,2024)butisconsideredmorefair.\n7\n\nTable2: ComparisononTinyImageNet-LT64Ã—64andPlaces-LT64Ã—64withDDPMpipeline.\nBlueâ€˜()â€™showsimprovementofourmethodoverDDPMbaseline. Greenâ€˜()â€™showsimprovementof\nDDPMtrainedonbalancedversionovertrainedonlong-tailedversion.\nDataset Method FIDâ†“ FID â†“ KID â†“\ntail Ã—1k\nDDPMâˆ— (Hoetal.,2020) 15.73(-2.94) 25.62(-12.26) 3.19(-3.12)\nbal\nDDPM(Hoetal.,2020) 18.67 40.12 6.31\nTinyImageNet-LT CBDM(Qinetal.,2023) 20.90 48.07 6.55\nOCLT(Zhangetal.,2024) 17.72 39.67 5.61\nCCUA(ours) 15.24(-3.43) 30.39(-9.73) 3.83(-2.48)\nDDPM(Hoetal.,2020) 13.89 23.74 5.26\nCBDM(Qinetal.,2023) 15.15 26.07 5.64\nPlaces-LT\nOCLT(Zhangetal.,2024) 13.04 22.84 4.17\nCCUA(ours) 11.99(-1.90) 20.84(-2.90) 3.63(-1.63)\nTable3: FIDscoreforthreeâ€˜super-categoriesâ€™: â€˜Headâ€™,â€˜Bodyâ€™,andâ€˜Tailâ€™. Allmodelsaremeasured\nwithDDIM(Songetal.,2020)100samplingstepsforconditionalgenerationwithCFG.\nFIDâ†“ P\ncategory Head Body Tail\nDataset All\nâˆ¼80% âˆ¼17% âˆ¼3%\nMethod\nDDPM(Hoetal.,2020) 21.27 33.25 40.12 18.67\nCBDM(Qinetal.,2023) 24.12 35.02 48.07 20.90\nTinyImageNet-LT\nOCLT(Zhangetal.,2024) 20.64 30.62 39.67 17.72\nCCUA(ours) 21.32 27.98 30.39 15.24\nDDPM(Hoetal.,2020) 19.26 20.31 23.74 13.89\nCBDM(Qinetal.,2023) 21.26 21.68 26.07 15.15\nPlaces-LT\nOCLT(Zhangetal.,2024) 19.16 19.43 22.84 13.04\nCCUA(ours) 18.15 19.27 20.84 11.99\nResultsAcrossVariousCategoryIntervals Toseetheeffectivenessofourmethodontailclasses,\nwedividethePlaces-LTandTinyImageNet-LTdatasetsintothreesupercategories: â€˜Headâ€™,â€˜Bodyâ€™,\nand â€˜Tailâ€™, with classes sorted by the number of training images. For each dataset, the top 33%\nclasseswereallocatedtotheâ€˜Headâ€™category,thenext34%classestotheâ€˜Bodyâ€™category,andthe\nlast33%classestotheâ€˜Tailâ€™category. ThepercentageP oftrainingimagesbelongingto\ncategory\neachcategoryisshowninthefirstrowofTab.3. OurmethodachievesthebestFIDscoreacross\nallthreecategoriesonPlaces-LTandTinyImageNet-LTdatasets. Fortheâ€˜Tailâ€™classes,whichare\nonlyâˆ¼3%,ourmethodimprovestheirFIDfrom23.74to20.84onPlaces-LTand40.12to30.39on\nTinyImageNet-LT,respectively. Additionally,bothâ€˜Headâ€™andâ€˜Bodyâ€™classesshowimprovedFID\nscoreswithourmethod,asdemonstratedinTab.3.\n4.3 QUALITATIVERESULTSANDABLATIONSTUDY\nVisualization of Synthetic Images Fig. 4 shows synthetic images of SiT and with CCUA for\nImageNet-LTtailclassesâ€˜espressoâ€™andâ€˜windowshadeâ€™. ComparedtoSiT,CCUAachievesvisually\nhigher fidelity and diversity. More qualitative results on ImageNet-LT, TinyImageNet-LT, and\nCIFAR100-LTareshowninappendixA.3.1andA.3.3.\nAblationStudyofLossFunction Table4showsanablation Table4: LossFunctionAnalysis.\nstudyoftheproposedlossesincludingL andL ,whichis\nucl al\nconductedonTinyImageNet-LT.Forthelossfunctionablation\nLossFunction FIDâ†“\nstudy,wedisablebatchresamplestrategywhichisseparately\nL 18.67\nablated. OurmethodachieveslowerFIDwhenapplyingonly ddpm\nL w/L (Eq.1) 18.05\ntheL loss(Eq.1)oronlyL loss(Eq.6)comparedtothe ddpm ucl\nucl al L w/L (Eq.6) 17.84\nddpm al\nstandard L loss only. The best result is achieved with\nddpm L w/L (Eq.7) 17.16\nddpm ccua\nbothlosses,i.e.,L (Eq.7).\nccua\nAblationStudyofBatchResampleStrategy AsshowninTable5,wemeasuretheperformanceof\nSiTandCCUA,withorwithoutapplyingbatchresamplingstrategy. ComparedtoSiT,theproposed\n8\n\n(a)SyntheticimagesfromSiT(Maetal.,2024).\n(b)SyntheticimagesfromCCUA(ours).\nFigure 4: Synthetic images of SiT and it with CCUA for ImageNet-LT tail classes â€˜espressoâ€™\nand â€˜window shadeâ€™. All methods start the denoising process from the same Gaussian noise at\ncorrespondinggridcells. CCUAshowsconsistentlyhigherdiversityandfidelitycomparedtoSiT.\nTable5: BatchResampleStrategyAnalysisonImageNet-LT.Allmodelsaretrainedfromscratchto\n40epochs. Blueâ€˜()â€™highlightstheimprovementofeachmethodcomparedtotheSiTbaseline,while\nRedâ€˜()â€™highlightsthedeclinecomparedtoSiTbaseline.\nMethod ISâ†‘ FIDâ†“ sFIDâ†“ Prec.(%)â†‘ Recall(%)â†‘\nSiT(baseline) 53.93 33.87 22.66 54.56 19.17\n+BatchResample 71.39(+17.46) 28.05(-5.82) 21.57(-1.09) 57.61(+3.05) 17.26(-1.91)\nL (Eq.7) 57.79(+3.86) 32.25(-1.62) 20.71(-1.95) 54.47(-0.09) 20.66(+1.49)\nccua\n+BatchResample 73.60(+19.67) 25.89(-7.98) 16.54(-6.12) 58.62(+4.06) 23.05(+3.88)\nCCUAlossachievesbetterIS,FID,sFIDandRecallwithonly0.09Precisionâ€™sdecline,illustratesour\nmethodonimprovingdiversitywithouttradingofffidelity. Withapplyingbatchresamplestrategy,\ntheCCUAfurtherimprovesallmetrics. NotesthattheCCUAincreasesRecallfrom20.66to23.05,\nwhileSiTsuffersadeclineinRecallfrom19.17to17.26whenapplyingbatchresampling. Aswe\ndiscussinSection.3.1,theL couldbenefitfrombatchresamplestrategyduetotheincreasing\nucl\ninstancesoftailclasses,leadingtomorediversedistributioninthelatentspace.\n5 CONCLUSION\nReal-worlddatafortrainingimagegenerationmodelsoftenexhibitlong-taileddistributions. Similar\nto class-imbalanced GANs (Khorram et al., 2024), class-imbalanced diffusion models generates\ninferior tail class images due to data scarcity. We propose a framework with two losses. Firstly,\nweemployanunsupervisedcontrastivelosswithnegativesamplesonlytocontrastthelatentsof\ndifferentsyntheticimagesateverydenoisingstep,promotingintra-classdiversityinparticularfor\ntailclasses. Oursecondlossalignsclass-conditionalgenerationwithunconditionalgenerationfor\nlargetimesteps. Thisencouragestheinitialdenoisingstepstobeclass-agnostic,therebyenrichingtail\nclassesthroughknowledgesharingfromheadclassesâ€“aprincipledemonstratedtoenhancelong-tailed\nGANperformance(Shahbazietal.,2022;Khorrametal.,2024),whichwesuccessfullyadaptto\ndiffusion models. With the two losses, our framework of contrastive conditional-unconditional\nalignment boosts the performane of DDPM (Ho et al., 2020) and SiT (Ma et al., 2024) for long-\ntailedimagegenerationandoutperformsalternativemethodsincludingCBDM(Qinetal.,2023),\nOCLT(Zhangetal.,2024), andDispersiveLoss(Wang&He,2025). Extensiveexperimentson\nmultipledatasetsinparticularImageNet-LTwith256x256resolutiondemonstratedtheeffectiveness\nofourmethodonbothUNet-basedarchitectureandDiffusionTransformer.\n9\n\nREFERENCES\nQingzhongAi,PengyunWang,LirongHe,LiangjianWen,LujiaPan,andZenglinXu. Generative\noversampling for imbalanced data via majority-guided vae. In International Conference on\nArtificialIntelligenceandStatistics,pp.3315â€“3330.PMLR,2023.\nDmitryBaranchuk,IvanRubachev,AndreyVoynov,ValentinKhrulkov,andArtemBabenko. Label-\nefficientsemanticsegmentationwithdiffusionmodels. arXivpreprintarXiv:2112.03126,2021.\nMikoÅ‚ajBinÂ´kowski,DanicaJSutherland,MichaelArbel,andArthurGretton. Demystifyingmmd\ngans. arXivpreprintarXiv:1801.01401,2018.\nAndrewBrock,JeffDonahue,andKarenSimonyan. Largescalegantrainingforhighfidelitynatural\nimagesynthesis. InICLR,2019.\nKaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced\ndatasetswithlabel-distribution-awaremarginloss. Advancesinneuralinformationprocessing\nsystems,32,2019.\nTingChen,SimonKornblith,MohammadNorouzi,andGeoffreyHinton. Asimpleframeworkfor\ncontrastivelearningofvisualrepresentations. 2020.\nGuillaumeCouairon,JakobVerbeek,HolgerSchwenk,andMatthieuCord. Diffedit: Diffusion-based\nsemanticimageeditingwithmaskguidance. arXivpreprintarXiv:2210.11427,2022.\nYusuf Dalva and Pinar Yanardag. Noiseclr: A contrastive learning approach for unsupervised\ndiscovery of interpretable directions in diffusion models. In Proceedings of the IEEE/CVF\nConferenceonComputerVisionandPatternRecognition,pp.24209â€“24218,2024.\nPrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. Advances\ninneuralinformationprocessingsystems,34:8780â€“8794,2021.\nMichaelFuest,PingchuanMa,MingGui,JohannesSFischer,VincentTaoHu,andBjornOmmer.\nDiffusionmodelsandrepresentationlearning: Asurvey. arXivpreprintarXiv:2407.00783,2024.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervisedvisualrepresentationlearning. InCVPR,2020.\nJonathanHoandTimSalimans. Classifier-freediffusionguidance. arXivpreprintarXiv:2207.12598,\n2022.\nJonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin\nneuralinformationprocessingsystems,33:6840â€“6851,2020.\nJonathanHo,WilliamChan,ChitwanSaharia,JayWhang,RuiqiGao,AlexeyGritsenko,DiederikP\nKingma,BenPoole,MohammadNorouzi,DavidJFleet,etal. Imagenvideo: Highdefinition\nvideogenerationwithdiffusionmodels. arXivpreprintarXiv:2210.02303,2022a.\nJonathanHo,TimSalimans,AlexeyGritsenko,WilliamChan,MohammadNorouzi,andDavidJ\nFleet.Videodiffusionmodels.AdvancesinNeuralInformationProcessingSystems,35:8633â€“8646,\n2022b.\nZiyuJiang,TianlongChen,BobakJMortazavi,andZhangyangWang. Self-damagingcontrastive\nlearning. 2021.\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarialnetworks. InCVPR,2019.\nTeroKarras,MiikaAittala,JanneHellsten,SamuliLaine,JaakkoLehtinen,andTimoAila. Training\ngenerativeadversarialnetworkswithlimiteddata. Advancesinneuralinformationprocessing\nsystems,33:12104â€“12114,2020.\nBahjatKawar,ShiranZada,OranLang,OmerTov,HuiwenChang,TaliDekel,InbarMosseri,and\nMichalIrani. Imagic: Text-basedrealimageeditingwithdiffusionmodels. InProceedingsofthe\nIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.6007â€“6017,2023.\n10\n\nSaeedKhorram,MingqiJiang,MohamadShahbazi,MohamadHDanesh,andLiFuxin. Tamingthe\ntailinclass-conditionalgans: Knowledgesharingviaunconditionaltrainingatlowerresolutions.\nInProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.\n7580â€“7590,2024.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron\nMaschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural\ninformationprocessingsystems,33:18661â€“18673,2020.\nDiederikPKingma,MaxWelling,etal. Anintroductiontovariationalautoencoders. Foundations\nandTrendsÂ®inMachineLearning,12(4):307â€“392,2019.\nTianhongLi,PengCao,YuanYuan,LijieFan,YuzheYang,RogerioSFeris,PiotrIndyk,andDina\nKatabi. Targetedsupervisedcontrastivelearningforlong-tailedrecognition. InProceedingsofthe\nIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.6918â€“6928,2022.\nZiwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and X Yu Stella. Open\nlong-tailedrecognitioninadynamicworld. IEEETransactionsonPatternAnalysisandMachine\nIntelligence,2022.\nNanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and\nSainingXie. Sit: Exploringflowanddiffusion-basedgenerativemodelswithscalableinterpolant\ntransformers. InEuropeanConferenceonComputerVision,pp.23â€“40.Springer,2024.\nYidong Ouyang, Liyan Xie, and Guang Cheng. Improving adversarial robustness through the\ncontrastive-guided diffusion process. In International Conference on Machine Learning, pp.\n26699â€“26723.PMLR,2023.\nBenPoole,AjayJain,JonathanT.Barron,andBenMildenhall. Dreamfusion: Text-to-3dusing2d\ndiffusion. arXiv,2022.\nYiming Qin, Huangjie Zheng, Jiangchao Yao, Mingyuan Zhou, and Ya Zhang. Class-balancing\ndiffusionmodels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern\nRecognition,pp.18434â€“18443,2023.\nHarshRangwani,NamanJaswani,TejanKarmali,VarunJampani,andRVenkateshBabu. Improving\ngans for long-tailed data through group spectral regularization. In European Conference on\nComputerVision,pp.426â€“442.Springer,2022.\nHarshRangwani,LavishBansal,KartikSharma,TejanKarmali,VarunJampani,andRVenkatesh\nBabu. Noisytwins: Class-consistentanddiverseimagegenerationthroughstylegans. InProceed-\ningsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.5987â€“5996,\n2023.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjoÂ¨rn Ommer. High-\nresolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-\nenceoncomputervisionandpatternrecognition,pp.10684â€“10695,2022.\nNatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfirAberman.\nDreambooth:Finetuningtext-to-imagediffusionmodelsforsubject-drivengeneration. InProceed-\ningsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pp.22500â€“22510,\n2023.\nMohamadShahbazi,MartinDanelljan,DandaPaniPaudel,andLucVanGool. Collapsebycondition-\ning: Trainingclass-conditionalGANswithlimiteddata. InInternationalConferenceonLearning\nRepresentations,2022. URLhttps://openreview.net/forum?id=7TZeCsNOUB_.\nChenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. Freeu: Free lunch in diffusion u-net.\nInProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.\n4733â€“4743,2024.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprintarXiv:2010.02502,2020.\n11\n\nChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonShlens,andZbigniewWojna. Rethinking\ntheinceptionarchitectureforcomputervision. InProceedingsoftheIEEEconferenceoncomputer\nvisionandpatternrecognition,pp.2818â€“2826,2016.\nShuhanTan,YujunShen,andBoleiZhou. Improvingthefairnessofdeepgenerativemodelswithout\nretraining. arXivpreprintarXiv:2012.04842,2020.\nLaurensVanderMaatenandGeoffreyHinton. Visualizingdatausingt-sne. Journalofmachine\nlearningresearch,9(11),2008.\nRunqian Wang and Kaiming He. Diffuse and disperse: Image generation with representation\nregularization. arXivpreprintarXiv:2506.09027,June2025.\nXiWang,NicolasDufour,NefeliAndreou,Marie-PauleCani,VictoriaFernaÂ´ndezAbrevaya,David\nPicard, and Vicky Kalogeiton. Analysis of classifier-free guidance weight schedulers. arXiv\npreprintarXiv:2404.13040,2024.\nDivinYan,LuQi,VincentTaoHu,Ming-HsuanYang,andMengTang. Trainingclass-imbalanced\ndiffusionmodelviaoverlapoptimization. arXivpreprintarXiv:2402.10821,2024.\nSihyunYu,SangkyungKwak,HuiwonJang,JongheonJeong,JonathanHuang,JinwooShin,and\nSainingXie. Representationalignmentforgeneration: Trainingdiffusiontransformersiseasier\nthanyouthink. arXivpreprintarXiv:2410.06940,2024.\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image\ndiffusionmodels. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,\npp.3836â€“3847,2023.\nTianjiaoZhang,HuangjieZheng,JiangchaoYao,XiangfengWang,MingyuanZhou,YaZhang,and\nYanfengWang.Long-taileddiffusionmodelswithorientedcalibration.InTheTwelfthInternational\nConferenceonLearningRepresentations,2024.\n12\n\nshare weights\nkcolB\nseR\nnwoD elpmaS seR klB ...\nkcolB\nseR\nnwoD elpmaS seR klB ...\nseR klB pU\nelpmaS kcolB\nseR\n...\n(a)w/UNet-basedModel\nTiD/TiS kcolB TiD/TiS kcolB TiD/TiS kcolB ...\nTiD/TiS kcolB TiD/TiS kcolB TiD/TiS kcolB ...\nraeniL NLada\nshare weights\n(b)w/DiffusionTransformer\nFigure5: ModeldetailsofCCUAframeworkwithUNet-basedModelandDiffusionTransformer. As\nshowninFig.2,thenoiseestimationnetworkisdividedintotwoparts,alatentencodednetworke(âˆ—)\nandadecodednetworkd(âˆ—). (a)ForUNet-basedarchitecture,e(âˆ—)isdefinedastheUNetencoder,\nwhiled(âˆ—)isdefinedastheUNetdecoder. (b)ForDiffusionTransformer,e(âˆ—)isdefinedasallthe\nSiT/DiTblocks,whiled(âˆ—)isdefinedasthefinallinearandadaLNprojectionlayer.\nTable 6: Our method outperforms other baselines on all datasets. We also provide the results of\nDDPM trained on balanced datasets, which show the upper bound of performance. All models\naremeasuredwithDDIM(Songetal.,2020)100stepsforconditionalgenerationwithCFG.Blue\nâ€˜()â€™ shows improvement of our method over DDPM baseline (Ho et al., 2020). Green â€˜()â€™ shows\nimprovementofDDPMtrainedonbalancedversionovertrainedonlong-tailedversion.\nDataset Method FIDâ†“ FID â†“ KID â†“\ntail Ã—1k\nDDPMâˆ— (Hoetal.,2020) 4.90(-1.03) 6.27(-5.98) 1.32(-0.32)\nbal\nDDPM(Hoetal.,2020) 5.93 12.25 1.64\nCIFAR10-LT CBDM(Qinetal.,2023) 5.81 10.01 1.58\nOCLT(Zhangetal.,2024) 6.10 11.13 1.58\nCCUA(ours) 5.56(-0.37) 10.03(-2.22) 1.27(-0.37)\nDDPMâˆ— (Hoetal.,2020) 5.15(-1.80) 8.97(-8.48) 1.05(-0.66)\nbal\nDDPM(Hoetal.,2020) 6.95 17.45 1.71\nCIFAR100-LT CBDM(Qinetal.,2023) 6.50 17.36 1.41\nOCLT(Zhangetal.,2024) 6.45 17.22 1.42\nCCUA(ours) 6.24(-0.71) 16.35(-1.10) 1.36(-0.35)\nA TECHNICAL APPENDICES AND SUPPLEMENTARY MATERIAL\nA.1 IMPLEMENTATIONDETAILS\nDatasetDetails Wekeeptheoriginal32Ã—32resolutionforCIFAR10-LT/CIFAR100-LT,andresize\nimagesto64Ã—64forTinyImageNet-LTandPlaces-LT,while256Ã—256forImageNet-LT.Same\nas(Qinetal.,2023;Yanetal.,2024;Zhangetal.,2024),weadoptthesamemethodologypresented\nin(Caoetal.,2019)toconstructlong-tailedversiondatasetswithanimbalancefactorof0.01.\nModelArchitectureDetails AsdescribedinSec.3,theproposedCCUAframeworkcanbeapplied\nintoUNet-basedarchitectureandDiffusionTransformer. InFig.5,wedisplayourUNet-basedmodel\nandDiffusionTransformerindetails. AsshowninFig.2,thenoiseestimationnetworkisdividedinto\ntwoparts,alatentencodednetworke(âˆ—)andadecodednetworkd(âˆ—). Inoursetting,forUNet-based\nmodel,e(âˆ—)isdefinedastheUNetencoder,whiled(âˆ—)isdefinedastheUNetdecoder,asshownin\nFig.5(a). ForDiffusionTransformer,e(âˆ—)isdefinedasalltheSiT/DiTblocks,whiled(âˆ—)isdefined\nasthefinallinearandadaLNprojectionlayer,asshowninFig.5(b).\nTrainingDetails ForSiTpipeline,westrictlyfollowthesametraininghyper-parametersettings\nasDispersiveLoss(Wang&He,2025). Weuse4A6000GPUstotrainallSiTbasedmodelson\nImageNet-LTdatasetswithbatchsize48. Duringthetrainingprocess,allmethodsaretrainedfrom\nscratch. Wereport40epochs,80epochs,120epochs,and160epochsresultsinTable1. ForDDPM\npipeline,wefollowthesametrainingconfigurationsofthebaselinemodels(Hoetal.,2020;Qin\net al., 2023; Zhang et al., 2024). We use one RTX 4090 GPU to train each model on CIFAR10-\n13\n\nTable7: WecompareourmethodwithotherbaselinesonCIFAR10-LT/CIFAR100-LTdatasets,with\nDDPM1000stepsforconditionalgenerationwithCFG.Wealsoreportthedataaugmentationmethod\nADA(Karrasetal.,2020)andÏ‰-scheduler(Wangetal.,2024),whichareorthogonaltoourmethod.\nCIFAR10-LT CIFAR100-LT\nMethod\nFIDâ†“ ISâ†‘ FIDâ†“ ISâ†‘\nDDPMâˆ— (Hoetal.,2020) 4.87 9.35 5.20 13.29\nbal\nDDPM(Hoetal.,2020) 5.81 9.36 7.09 12.64\n+ADA(Karrasetal.,2020) - - 6.69 12.87\n+Ï‰-Scheduler(Wangetal.,2024) 5.87 9.22 6.60 12.10\nCBDM(Qinetal.,2023) 5.92 9.38 6.52 12.79\nOCLT(Zhangetal.,2024) 5.69 9.42 6.23 13.18\nCCUA(ours) 5.57 9.42 5.99 13.01\nLT/CIFAR100-LTdatasetswithbatchsize64whileusing2A100GPUsforTinyImageNet-LTand\nPlaces-LT,withbatchsize128. Duringthetrainingprocess,allmethodsaretrainedfromscratchfor\n200kiterationsonCIFAR10-LT/CIFAR100-LTdatasets,100kiterationsonTinyImageNet-LTand\nPlaces-LTdatasets,whiletheyfollowtheclassifier-freeguidance(CFG)algorithm(Ho&Salimans,\n2022),whichrandomlydropslabelswithaprobabilityof10%. OnImageNet-LTandTinyImageNet-\nLTdatasets,weapplybatchresamplestrategywiththere-balancedfactor0.1,whilewedonâ€™tapply\nbatchresamplestrategyonPlaces-LTandCIFAR10-LT/CIFAR100-LTdatasets. ForTinyImageNet-\nLTandPlaces-LTdatasets,wealsoapplytimestepadaptiveweightt/T tounsupervisedcontrastive\nlosssincewefindtheunsupervisedcontrastivelosscouldalsobenefitfromsuchanadaptiveweight,\nlikealignmentloss.\nEvaluation Details In Table 2, FID denotes the FID score for synthetic images of the last\ntail\n30%classesineachdataset. Specifically,wecategorizetheâ€˜Tailâ€™classesasthelast66classesfor\nTinyImageLT(200classes),thelast121classesforPlaces-LT(365classes),respectively. InTable1,\ntheevaluationmetricsarebasedon50ksyntheticimagesgeneratedbyeachmethodwithaCFG\nstrength7.5. InTable2,theevaluationmetricsarebasedon10ksyntheticimagesgeneratedbyeach\nmethod. Duringinference,weperformagridsearchalgorithmforeachmethodtodeterminethe\noptimalguidancestrengthÏ‰ofCFG,ensuringeachmodelachievesitsbestperformance.\nA.2 MOREQUANTITATIVERESULTS\nClass-imbalancedGenerationonCIFAR10-LT/CIFAR100-LTdatasets Weconductmoreex-\nperimentsandanalysisonCIFAR10-LT/CIFAR100-LTdatasets,asshowninTable6andTable7.\nWe provide the metrics of the DDPM model trained on the balanced version, i.e., the original\nCIFAR10/CIFAR100 datasets, denoted by DDPMâˆ— , as the theoretical optimal reference. On\nbal\nTable8: WecompareourmethodwithotherbaselinesonCIFAR10-LT/CIFAR100-LTdatasetswith\nDDIM10samplingstepsforconditionalgenerationwithCFG.Blueâ€˜()â€™showsimprovementofour\nmethodoverDDPMbaseline(Hoetal.,2020). Greenâ€˜()â€™showsimprovementofDDPMtrainedon\nbalancedversionovertrainedonlong-tailedversion.\nDataset Method FIDâ†“ FID â†“ KID â†“\ntail Ã—1k\nDDPMâˆ— 13.28(-1.44) 13.26(-5.01) 6.06(-0.98)\nbal\nDDPM 14.72 18.27 7.04\nCIFAR10-LT CBDM 13.54 16.90 6.52\nOCLT 15.48 20.73 7.39\nCCUA(ours) 13.16(-1.56) 16.83(-1.44) 6.04(-1.00)\nDDPMâˆ— 13.34(-0.75) 18.27(-6.80) 5.56(-0.57)\nbal\nDDPM 14.09 25.07 6.13\nCIFAR100-LT CBDM 13.37 23.97 5.83\nOCLT 13.70 24.48 5.73\nCCUA(ours) 12.90(-1.19) 23.17(-1.90) 5.63(-0.50)\n14\n\nCIFAR10-LT/CIFAR100-LT,ourmethodachievesthelowestFIDandKIDcomparedtobaseline\nmethods. NotethattheFIDgapbetweenDDPMandDDPMâˆ— is1.03onCIFAR10-LTand1.8on\nbal\nCIFAR100-LT,whileourmethodimprovesFID0.37over1.03onCIFAR10-LTand0.71over1.8on\nCIFAR100-LT,achieving>35%performanceimprovement. Toinvestigatetheconsistencyofsuch\nimprovements,wecompareourmethodandallbaselinemethodsonCIFAR10-LTandCIFAR100-LT\nwithDDPM1000steps. AsshowninTab.7,ourmethodachievesconsistentimprovementsofFIDfor\nfull1000samplingsteps. Wealsocomparetoawidelyuseddataaugmentationtechnique,Adaptive\nDiscriminatorAdaption(ADA)(Karrasetal.,2020)forgenerativemodelsonthefullDDPM1000\nsamplingsteps. Besides,weapplytheCFGguidancestrengthschedulerÏ‰-cos(Wangetal.,2024)on\nDDPM,whichgraduallyincreasestheguidancestrengthduringsamplingtimestepsdecreasingto\nforcethemodeltransferfromunconditionaltoconditionalgeneration.\nConsistentImprovementforFewerSamplingSteps Toinvestigatethemodelâ€™sperformanceon\nextremelyfewsamplingsteps,WeevaluateourmethodandallbaselinesforDDIM10stepswithCFG\nconditionalgenerationonCIFAR10-LT/CIFAR100-LT.AsshowninTab.8,ourmethodachievesthe\nbestFID,FID andKIDscores.Ourmethodachievesevenbetterresultsthanthetheoreticaloptimal\ntail\nmodelDDPMâˆ— undersuchanextremeexperimentalsetting. Suchanimprovementdemonstrates\nbal\ntheeffectivenessofourmethodfortraininglong-tailedimagegenerationdiffusionmodel.\nClass-imbalancedUnconditionalGeneration Wealso Table 9: Unconditional generation w/\nevaluate all the models for class-imbalanced uncondi- DDIM100steps.\ntional generation. As shown in Tab. 9, the proposed\nmethodreducesFIDfrom27.52to24.16forCIFAR10-\nCIFAR10-LT CIFAR100-LT\nLTandfrom18.53to15.97forCIFAR100-LT.Suchan Method\nFIDâ†“ ISâ†‘ FIDâ†“ ISâ†‘\nimprovementinunconditionalgenerationhighlightsthe\nDDPM 27.52 6.65 18.53 8.68\neffectiveness of the proposed contrastive learning loss,\nCBDM 25.60 6.70 17.06 9.00\nparticularlytheunsupervisedcontrastivelossL .\nucl OCLT 31.38 6.34 18.97 8.73\nOurs 24.16 6.80 15.97 9.23\nStatistical Significance Analysis\nToinspecttherobustnessofthepro- Table10: StatisticalAnalysisforrandomseedonCIFAR100-\nposed method, we conduct statisti- LT,samplingwithDDIM100steps.\ncalsignificanceanalysisontheddpm\nandourmethod,asshowninTab.10.\nWithapplyingdifferentrandomseed, Seed Method FIDâ†“ Method FIDâ†“\ntheFIDdeviationofDDPMisabout 0 DDPM 7.02 Ours 6.26\n0.04whileourmethodisabout0.01, 42 DDPM 6.95 Ours 6.24\n2025 DDPM 7.04 Ours 6.27\nillustrating the robustness of our\nmethodfordifferentrandomseeds.\nLimitationofourmethod OurMSElossinvolvesbothconditionalgenerationandunconditional\ngeneration,whichrequirestwopassesofthedenosingnetworkduringtrainingandincreasestraining\ntime. Inpractice,withapplyingourmethod,thetrainingtimeforoneepochis1.6xofthatofDDPM,\nand1.48xofthatofSiT.However,ourmethoddoesnâ€™tincreaseinferencelatency.\nA.3 MOREQUALITATIVERESULTS\nA.3.1 MOREVISUALIZATIONOFSYNTHETICIMAGESONIMAGENET-LTAND\nTINYIMAGENET-LT\nFig.6showssyntheticimagesofSiTandwiththeproposedCCUAforImageNet-LTtailclasses\nâ€˜bubbleâ€™,â€˜redwineâ€™,â€˜comicbookâ€™andâ€˜yawlâ€™. Fig.7showssyntheticimagesofDDPMandwiththe\nproposedCCUAforTinyImageNet-LTtailclassesâ€˜teapotâ€™,â€˜watertowerâ€™,â€˜pretzelâ€™,â€˜mushroomâ€™,\nâ€˜orangeâ€™,andâ€˜pizzaâ€™. ThesemethodsstartthedenoisingprocessfromthesameGaussiannoiseat\ncorrespondinggridcells. AsshowninFig.6andFig.7,syntheticimagesofCCUAshowconsistently\nhigherdiversityandfidelitycomparedtoSiT.\n15\n\n(a)SyntheticimagesfromSiT(Maetal.,2024).\n(b)SyntheticimagesfromCCUA(ours).\nFigure6: SyntheticimagesofSiTandCCUAforImageNet-LTtailclasses(fromtop-lefttoright-\nbottom: â€˜bubbleâ€™,â€˜redwineâ€™,â€˜comicbookâ€™andâ€˜yawlâ€™). Allmethodsstartthedenoisingprocessfrom\nthesameGaussiannoiseatcorrespondinggridcells. CCUAshowsconsistentlyhigherdiversityand\nfidelitycomparedtoSiT.\n16\n\n(a)SyntheticimagesfromDDPM(Hoetal.,2020).\n(b)SyntheticimagesfromCCUA(ours).\nFigure7: MoresyntheticresultsforTinyImageNet-LTtailclasses(fromtop-lefttoright-bottom:\nâ€˜teapotâ€™,â€˜watertowerâ€™,â€˜pretzelâ€™,â€˜mushroomâ€™,â€˜orangeâ€™,andâ€˜pizzaâ€™). Imagesincorrespondinggrid\ncellforDDPMandourmethodareinitializedfromthesameGaussiannoise. Ourmethodachieves\nmorediverseimageswithhigherfidelityfortailclasses. Notethatforâ€˜pretzelâ€™andâ€˜orangeâ€™classes,\nDDPMfailstosynthesizeimagescorrelatedtotheclasswhileCCUAsynthesizesdiverseimages\nwithhighquality.\n17\n\n(a)Imagesinlow-dimensionalembeddings. (b)Densitydistributionofsyntheticimages.\nFigure8: (a)Visualizationoflow-dimensionalembeddingsoffivetrainingimagesxforatailclass\n(â€˜wormâ€™ in CIFAR100-LT) and synthetic images generated by DDPM (Ho et al., 2020) and our\nmethod. (b)Wealsoshowthedistributionsofrealimagesandsyntheticimagesgeneratedbyour\nmethodandtheoriginalDDPM.\nA.3.2 DISTRIBUTIONOFSYNTHETICIMAGESFORTAILCLASS\nWevisualizesyntheticimagesinthefeaturespaceandtheirdensityfunctionforourmethodand\ntheoriginalDDPM.Specifically,weusetheInception-V3(Szegedyetal.,2016)modeltoextract\n2048dimensionalfeaturesofeachimage,andthenapplyt-SNE(VanderMaaten&Hinton,2008)\ntoproject these featuresinto 2 dimensions. As shownin Fig.8, theoriginalDDPM overfitsand\ngenerateshighlysimilarimages,whilesyntheticimagesbasedonourmethodhavemorediversity.\nWevisualizetheimagedistributionmorespecificallybyusingkerneldensityestimationinthebottom.\nThegreyregionrepresentsthedistributionofrealâ€˜wormâ€™imagesfromtheclass-balancedCIFAR100\ndataset,whileredâ€˜xâ€™pointsdenoteall5â€˜wormâ€™imagesfromtheclass-imbalancedCIFAR100-LT\ndataset. Theblueregionrepresentsthedistributionofimagessynthesizedbyourmethod,whilethe\ngreen region is for the original DDPM. Areas with higher color saturation (dark, green, or blue)\nindicateregionsofhigherdensity,whichcorrespondtomodesofdistribution. Syntheticimagesfrom\nDDPMmostlyconcentratesaroundthetrainingimages,leadingtomodecollapse. Syntheticimages\nfromourmethodspansthespaceenclosedbyalltrainingimages,thedistributionofwhichisshown\ninbluein(b).\nA.3.3 MODECOLLAPSEISSUEONTAILCLASS\nFig.9showssyntheticimagesofbaselinemethodsandourmethodforCIFAR100-LTtailclassesâ€˜roseâ€™\nandâ€˜tableâ€™. AllmethodsstartthedenoisingprocessfromthesameGaussiannoiseatcorresponding\ngrid cells. Red dashed ellipses highlight the mode collapse issues observed in DDPM. With the\nsameinitialnoise,ourmethodgivessyntheticimageswithhigherdiversityandfidelitycomparedto\nDDPM.Forexample,DDPMalwaysgeneratesroseimagescontainingonlyonerose. Ourmethod\ncangenerateanimagecontainingtworoses(seethesixthcolumnofthelastrow). Tofurtherclarify,\nintheâ€˜tableâ€™class,DDPMrepeatedlyproducesnear-identicalimagesashighlightedinredellipses.\nIncontrast,ourmethodproducesamorevariedsetoftableimages. Thissuggeststhatourmethodcan\nbettercapturegreaterdiversityinimagegenerationwhencomparedtoDDPM(Hoetal.,2020). To\nfurtherillustratehowtheproposedmethodmitigatestheissueofoverfitting,wevisualizethetop-10\nnearestneighborsamong1000syntheticimagestoananchorimageinthetrainingsetfortheoriginal\nDDPMandourmethod. AsshowninFig.10,DDPMshowsthemodecollapsetothetrainingimage\nwhileourmethodâ€™sgeneratedimagesshowmuchbetterdiversity. Forexample,inthefirsttworows,\nDDPMgeneratesrepeatedverticalwormswhileourmethodgenerateswormswithdiversedirections.\nInthe9th-10throws,DDPMgeneratestableswithrepeatedmodeswhileourmethodgeneratestables\nwithdifferentstylesandcolors.\nA.3.4 MOREVISUALIZATIONOFREVERSEPROCESSOFDDPMFORDIFFERENTCLASSES\nToillustrateourobservationinSection3.2moreclearly,wedecomposex intoacombinationof\nt\nlow-frequencyimagesandhigh-frequencyimages,asshowninFig11. Thelow-frequencyimages\nwiththesameinitialnoiseareverysimilarfordifferentclassesfortheinitialsteps,whichisalso\nobservedinpriorwork(Sietal.,2024).\n18\n\n(a)SyntheticimagesfromDDPM(Hoetal.,2020).Redellipsesshowsimilarimagesgenerated.\n(b)SyntheticimagesfromCCUA(ours)havemorediversityandarelessrepetitive.\nFigure 9: The synthesized results for CIFAR100-LT tail classes â€˜roseâ€™ and â€˜tableâ€™ are shown for\nourmethodandbaselinemethods. AllmethodsinitiatereverseprocessingfromthesameGaussian\nnoiseforimagesatcorrespondinggridcells. Reddashedellipseshighlightmodecollapseissues\nobserved in DDPM. Overall, our method demonstrates higher diversity and fidelity compared to\nDDPMbaseline.\nFigure10: Toseeoverfittingontailclasses,wefindTop-10nearestneighbors(3rdto12thcolumns\nsorted by distances) among 1000 synthetic images to an anchor image (Leftmost column) in the\ntrainingset. KNNisbasedonL distancesofInceptionV3embeddings. Foreachexample,thetop\n2\nrowistheresultsofDDPM,andthebottomrowshowsours. ThenearestneighborsfromDDPM\nshowthemodecollapsetothetrainingimage,whileourmethodgeneratedmorediverseimages.\n19\n\nFigure11: Top: reverseprocessstartingfromthesameinitialGaussiannoisebutwithdifferentclass\nconditions(2nd-4throws)orwithoutcondition(1strow). Middle: low-frequencycomponentsof\neachnoisyimage. Bottom: high-frequencycomponentsofeachnoisyimage.\n20\n\nB THE USE OF LARGE LANGUAGE MODELS (LLMS)\nWeusedalargelanguagemodeltorefineourwritingandcorrectgrammar,butnottogeneratethe\ninitialdraft. TheLLMservedonlyasasupportivetooltoimprovepresentation,notasasourceof\ncontent.\n21\n\n",
    "total_pages": 21,
    "pages": [
      {
        "page": 1,
        "content": "CONTRASTIVE CONDITIONALâ€“UNCONDITIONAL\nALIGNMENT FOR LONG-TAILED DIFFUSION MODEL\nFangChen1 AlexVilla1 GongboLiang2\nfchen20@ucmerced.edu avilla49@ucmerced.edu gliang@tamusa.edu\nXiaoyiLu1 MengTang1\nxiaoyi.lu@ucmerced.edu mtang4@ucmerced.edu\n1University of California Merced\n2Texas A&M University-San Antonio\nABSTRACT\nTraining data for class-conditional image synthesis often exhibit a long-tailed\ndistributionwithlimitedimagesfortailclasses. Suchanimbalancecausesmode\ncollapseandreducesthediversityofsynthesizedimagesfortailclasses. Forclass-\nconditionaldiffusionmodelstrainedonimbalanceddata,weaimtoimprovethe\ndiversityandfidelityoftailclassimageswithoutcompromisingthequalityofhead\nclassimages. Weachievethisbyintroducingtwosimplebuthighlyeffectiveloss\nfunctions. Firstly,weemployanUnsupervisedContrastiveLoss(UCL)utilizing\nnegativesamplestoincreasethedistance/dissimilarityamongsyntheticimages.\nSuchregularizationiscoupledwithastandardtrickofbatchresamplingtofurther\ndiversifytail-classimages. OursecondlossisanAlignmentLoss(AL)thataligns\nclass-conditionalgenerationwithunconditionalgenerationatlargetimesteps. This\nsecondlossmakesthedenoisingprocessinsensitivetoclassconditionsfortheinitial\nsteps,whichenrichestailclassesthroughknowledgesharingfromheadclasses.We\nsuccessfullyleveragecontrastivelearningandconditional-unconditionalalignment\nforclass-imbalanceddiffusionmodels. Ourframeworkiseasytoimplementas\ndemonstratedonbothU-NetbasedarchitectureandDiffusionTransformer. Our\nmethodoutperformsvanilladenoisingdiffusionprobabilisticmodels,score-based\ndiffusionmodel,andalternativemethodsforclass-imbalancedimagegeneration\nacrossvariousdatasets,inparticularImageNet-LTwith256Ã—256resolution.\n1 INTRODUCTION\nRecentadvancesindiffusionmodels(Hoetal.,2020;Songetal.,2020)haveledtobreakthroughin\nvariousgenerationtaskssuchasimagegeneration(Zhangetal.,2023;Rombachetal.,2022;Ruiz\net al., 2023), video generation (Ho et al., 2022a;b), image editing (Couairon et al., 2022; Kawar\netal.,2023),3Dgeneration(Pooleetal.,2022),etc. Thesediffusion-basedgenerativemodelsrelyon\nlarge-scaledatasetsfortraining,whichoftenfollowalong-taileddistributionwithdominantdatafor\nheadclassesandlimiteddatafortailclasses. Similartotheclass-imbalancedrecognitionmodel(Liu\netal.,2022)andtheclass-imbalancedgenerativeadversarialnetwork(Tanetal.,2020;Rangwani\netal.,2022;2023;Khorrametal.,2024),diffusionmodelisnotabletogeneratehigh-qualityimages\nfortailclassesduetothescarcityoftrainingdata. AsshowninFig.1,theoriginaldiffusionmodel\nwithtransformerbackbone (Maetal.,2024)generatesinferiorimagesforatailclass(redwine)\nwhentrainedusingalong-tailedversionofImageNet. Weaimtoincreasethefidelityanddiversityof\ntailclassimageswhilemaintainingthequalityofheadclassimagesfordiffusionmodels. Ourwork\nhasapositivesocietalimpactoncontentgenerationofunderrepresentedgroups.\nWe look into a highly related and crucial problem of long-tailed image recognition, for which\ncontrastivelearningisaneffectivemethod(Jiangetal.,2021;Lietal.,2022). Contrastivelearning\n1\n5202\ntcO\n42\n]VC.sc[\n2v25090.7052:viXra"
      },
      {
        "page": 2,
        "content": "(a)SiT(Maetal.,2024) (b)SiTwithCCUA(ours)\nFigure1: Syntheticimagesforatailclass(â€˜redwineâ€™)by(a)standardSiTand(b)proposedCCUA\nframework.Botharetrainedonlong-tailedImageNetdatasetfor160epochswith256x256resolution.\nis a general learning paradigm of minimizing the distance of positive pairs while maximizing\nthe distance of negative pairs, which has attracted lots of research interest for self-supervised\nrepresentationlearning(Chenetal.,2020). Ontheotherhand,diffusionmodelhasalsoemergedasan\neffectivemodelforrepresentationlearning(Fuestetal.,2024;Baranchuketal.,2021). Weexplicitly\nincorporatecontrastivelearningregularizationtodiffusionmodelstofurtherenhancerepresentation\nlearningtowardsbetterimagegeneration.\nOurfirstlossisanunsupervisedcontrastivelosswithnegativesamplesonly,servingasregularization\nfordenoisingdiffusionprobabilisticmodels(DDPM)andscore-baseddiffusionmodels(SBDM).\nGiventhatmodecollapseduringinferencemanifestsasgeneratedsamplesbeingoverlysimilarfor\ntailclasses,weintroduceunsupervisedcontrastivelearningwithnegativesamplestomaximizethe\ndistancesbetweengeneratedimages. Unlikesupervisedcontrastivelearning(Khoslaetal.,2020),\nunsupervisedcontrastivelearningdistinguishesbetweenrepresentationsofimagesregardlessoftheir\nclass,henceincreasingintra-classandinter-classimagediversity.\nOur second loss is an alignment loss designed to align estimated noises from conditional and\nunconditionalgeneration,whicheffectivelyminimizestheKLdivergencebetweenlatentdistributions\nforconditionalandunconditionalgeneration. Whilesuchconditional-unconditionalalignmentseems\nundesirablewithlesscontrollability,acriticalaspectofourapproachisthatouralignmentlossis\nweightedmoreforlargertimestepscorrespondingtotheinitialstageofthereverseprocess. Giventhe\nknownoverfittinginconditionalgenerationfortailclassesforbothGAN,wearemotivatedbythe\nsuccessofconditional-unconditionalalignmentforlong-tailedGAN(Khorrametal.,2024;Shahbazi\netal.,2022),whichfacilitatesknowledgesharingbetweenheadclassesandtailclasses. Notably,\nunconditionalGANgenerationhasbeenobservedtoachievesuperiorFIDthanconditionalgeneration\nunderlimiteddata(Shahbazietal.,2022). Weadaptconditional-unconditionalalignmentfromGAN\ntodiffusionmodels. UnliketheGAN-basedmethod(Khorrametal.,2024)whichalignsconditional\ngeneration and unconditional generation for low-resolution representations of images exhibiting\nintra-classsimilarity,weleverageobservedimagesimilarityduringtheinitialdenoisingstepsfortail\nclassesandheadclasses(Sietal.,2024). Hence,weproposetomatchconditionalgenerationwith\nunconditionalgenerationforlargetimesteps.\nWhilecontrastivelearninghasbeenproposedfordiffusionmodelstoimproveadversarialrobust-\nness (Ouyang et al., 2023), find semantically meaningful directions (Dalva & Yanardag, 2024),\nacceleratetraining(Yuetal.,2024),andregularizerepresentation(Wang&He,2025),weeffectively\nleveragedcontrastivelearningforclass-imbalanceddiffusionmodelsanddemonstratethesuperior\nperformanceofourmethodonlong-tailedimagegenerationviacomprehensiveexperiments.\nThemaincontributionsofthisworkareasfollows:\nâ€¢ WeproposeContrastiveConditional-UnconditionalAlignmentforDiffusionModel(CCUA)\nwithimbalanceddata. Ourproposedcontrastivelosswithconditional-unconditionalalign-\nmentareeasytoimplementwithstandardDDPMandSBDMpipelineusingbothUNet-based\narchitectureandDiffusionTransformer(DiT).\n2"
      },
      {
        "page": 3,
        "content": "â€¢ Ourfirstloss, UnsupervisedContrastiveLoss(UCL),employsunsupervisedcontrastive\nlearninglosswithnegativesamplesonly,enhancingwithin-classdiversity.\nâ€¢ OursecondAlignmentLoss(AL)alignsunconditionalgenerationandconditionalgeneration\nfortheinitialstepsinthedenoisingprocess,facilitatingknowledgesharingbetweenhead\nandtailclasses.\nâ€¢ Weimprovedthediversityandfidelityoftailclassimagesforconditionalgenerationwhile\nmaintainingthequalityofheadclassimagesformultipledatasetsandvariousresolutions,\ninparticularImageNet-LTwith256x256resolution.\n2 RELATED WORK\nClass-imbalanced Image Generation Generative models such as VAE (Kingma et al., 2019),\nGAN(Karrasetal.,2019;Brocketal.,2019),anddiffusionmodels(Hoetal.,2020;Rombachetal.,\n2022)generateinferiorimagesfortailclasseswhentrainedwithreal-worlddatawithalong-tailed\ndistribution. Ithasattractedalotofresearchinterest(Aietal.,2023;Tanetal.,2020;Khorrametal.,\n2024;Qinetal.,2023;Zhangetal.,2024)toaddressthisissueforvarioustypesofmodels. Many\nmethodsaddresstheproblemofclassimbalancebyaugmentingtrainingdataforthetailclasses.\nA VAE is fine-tuned on tailclasses under amajority-based prior(Ai et al.,2023). It isobserved\nthatGAN(Tanetal.,2020)canamplifybiases,leadingtotailclassestobebarelygeneratedduring\ninference,highlightingthatthefairnessinGANneedsimprovement. Khorrametal.(2024)proposea\nGAN-basedlong-tailedgenerationmethod,namedUTLO,whichsharesthelatentrepresentations\nofconditionalGANwithunconditionalGANandimplicitlysharesknowledgebetweenthehead\nclassandtailclass. ThemotivationwithUTLOisthatlow-resolutionrepresentationsofimagesfrom\nGANaresimilarforheadclassesandtailclasses. Weobserveasimilarphenomenonfordenoised\nimagesintheinitialstepsofthedenoisingprocess,andfurtherproposeaconditional-unconditional\nalignmentlossdesignedfordiffusionmodels. Recentworkaddressesclass-imbalanceddiffusion\nmodels(Qinetal.,2023;Yanetal.,2024;Zhangetal.,2024)byregularizationlossestoalignor\nseparatethedistributionsofsyntheticimagesandtheircorrespondinglatentrepresentationsacross\ndifferentclasses. Forexample,CBDM(Qinetal.,2023)lossminimizesthedistanceofestimated\nnoiseitemsbetweenthesetwomodels,theoriginalDDPMmodelandasecondmodeltrainedwith\npseudolabelswhichformauniformdistribution. DiffROP(Yanetal.,2024)attemptstocombine\ncontrastivelearningwithdiffusionmodelbymaximizingthedistanceofdistributionsbetweenclasses.\nHowever, DiffROP only considers pairs of images of different classes as negative pairs without\nregularizingimagesofthesameclass. Conceptually,thefirstpartofourmethod,theunsupervised\ncontrastivelossdesignedtodisperseunconditionallatentspace,generalizesDiffROPbyincluding\nextranegativepairsofimageswithineachclass,hencepromotingintra-classdiversity.\nContrastiveLearningforRepresentationLearning Bothunsupervisedcontrastivelearning(Chen\netal.,2020;Heetal.,2020)andsupervisedcontrastivelearning(Khoslaetal.,2020;Lietal.,2022)\nareapproachesusedinrepresentationlearningtoproducefeatureembeddingsbymaximizingthe\nsimilaritybetweenpositivepairsandminimizingthesimilaritybetweennegativepairs. Unsupervised\ncontrastivelearningdoesnotrequirelabelsandtypicallyaugmentsthesamedatatoformpositive\npairs(Chenetal.,2020;Heetal.,2020). Supervisedcontrastivelearningincorporatesclasslabels\nand pulls together all samples from the same class while pushing apart samples from different\nclasses. Contrastivelearningisaneffectivemethodforself-supervisedlearning(Heetal.,2020)\nandlong-tailedrecognition(Jiangetal.,2021;Lietal.,2022). Weeffectivelyutilizedcontrastive\nlearningforclass-imbalanceddiffusionmodels. Ourworkisdifferentfromthecontrastive-guided\ndiffusion process (Ouyang et al., 2023), which aims to improve adversarial robustness. Another\ndifferenceisthatwefocusoncontrastivelearningduringtraining,whilecontrastive-guideddiffusion\nprocess(Ouyangetal.,2023)isaboutguidanceforinference. Whilethediffusionmodeloriginally\nforgenerationtasksbecomesanemergingtechniqueforrepresentationlearning(Fuestetal.,2024;\nBaranchuketal.,2021),wedirectlyintegratecontrastivelearningregularizationtodiffusionmodels\nfor better generation on imbalanced data. Notably, unsupervised contrastive loss with negative\nsamplesexclusivelyfocusesonseparatingdissimilarinstancesintheembeddingspace,whichwe\nleveragetoaddressmodecollapse,asdetailedinSec.3.1. Inthecontextofgenerativemodeling,\nREPA(Yuetal.,2024)alignsdiffusionmodelfeatureswithfeaturesfromafrozenvisionencoderto\nacceleratetraining. UnlikeREPA,wedirectlyregularizediffusionmodelfeatureswithoutexternal\nmodels. Mostrelevantisconcurrentworkofdispersiveloss(Wang&He,2025),whichissimilarto\n3"
      },
      {
        "page": 4,
        "content": "Noise Estimation Network ðœ–0\nmini-batch ð“› ð“›\nð’–ð’„ð’ ð’‚ð’\nð‘¥ !# â„Ž $# %&\nð‘’(âˆ—;âˆ…) â„Ž â€™\" $( ðœ–.\" $((ð‘¥ !\";ð‘¡,âˆ…)\nð‘¥\"\n!\nð‘‘(âˆ—)\nð‘’(âˆ—;ð‘) ðœ–\" (ð‘¥\";ð‘¡,ð‘\")\n(,$- !\nð‘\"\nclass\nembedding share weights\nFigure2: OverviewoftheproposedCCUAframeworkofcontrastivelearningfordiffusionmodel.\nThenoiseestimationnetworkisdividedintotwoparts,alatentencodenetworke(âˆ—)andadecode\nnetworkd(âˆ—).e(âˆ—)encodesanimagewithnoisex toalow-dimensionallatenth,whichisdecodedto\nt\nnoiseÏµbyd(âˆ—). Specifically,thelatentencodenetworke(âˆ—)anddecodenetworkd(âˆ—)forunet-based\nmodelanddiffusiontransformerareshowninFig.5intheappendix.A.1. Weincreasethedistanceof\nnegativepairsofunconditionalencodedlatentsfordifferentsamplesbyanunsupervisedcontrastive\nlossL withnegativesamplesonly,andalignunconditionalandconditionalgenerationforthesame\nucl\nsamplexi andutilizeanalignmentlossL tominimizetheirdistancesatinitialtimesteps.\nt al\nourunsupervisedcontrastivelosswithnegativepairsonly. Dispersivelossisdirectlyandexplicitly\napplied to conditional training, as designed for balanced diffusion models. By comparison, our\nunsupervisedcontrastivelossisformulatedintheunconditionallatentspaceandisimplicitlyextended\ntoconditionaltrainingthroughourconditionalâ€“unconditionalalignmentloss. Thiscarefullydesigned\ncontrastiveconditional-unconditionalalignmentframeworkforlong-taileddiffusionmodelsachieves\nbettergenerationasshowninexperiments.\n3 METHOD\nWeproposeContrastiveConditional-UnconditionalAlignment(CCUA)frameworkforDiffusion\nModel. Fig.2givesanoverviewofourframework,whileFig.5intheappendixA.1detailshowwe\napplyCCUAframeworkintoUNet-basedmodelandDiffusionTransformer. Ourframeworkinvolves\naunsupervisedcontrastivelosswithnegativepairsonly,andaconditional-unconditionalalignment\nloss,asoutlinedinSec.3.1andSec.3.2,respectively. Sec.3.3summarizesouroverallframework.\n3.1 UNSUPERVISEDCONTRASTIVELOSS\nWeobservedthesynthesizedimagesofDDPMforatailclassconcentratedaroundthelimitedtraining\nimagesinthelatentspace,leadingtotheissueofmodecollapse,asshownintheFig.9,Fig.10inthe\nappendixA.3.3,andFig.8intheappendixA.3.2withvisualizingthedistributionofsyntheticimages\nforthetailclass. Byapplyingthecontrastivelosswithnegativesamplesonly,weintendtoincrease\nthedistanceofeachsynthesizedimagefromotherimagesinthelatentspace,henceincreasingthe\ndiversityofsyntheticimagesinparticularfortailclasses.\nWeconsidercontrastivelosswithnegativesamplesonly,wherethenegativesamplesarebasedon\nnoisyimagesfromamini-batch. Specifically,wedivideanoiseestimationnetworkintotwoparts,\nalatentencodenetworke(âˆ—),anddecodenetworkd(âˆ—). e(âˆ—)encodesanimagewithnoisex toa\nt\nlow-dimensionallatenth,whichisdecodedtotheestimatenoiseitemÏµbyd(âˆ—). Ourcontrastiveloss\nisdefinedforthelatentshwitheachimageinamini-batchtreatedasananchor. Allotherimagesare\ntreatedasnegativesamples. Theanchorsamplexi andnegativesamplesxj arefedintotheencoder\nt t\ne(âˆ—)togetthelatentshi =e(xi;âˆ…)andhj =e(xj;âˆ…). OurunsupervisedcontrastivelossL\nanc t neg t ucl\nisdefinedasfollows:\nL =âˆ’ 1 (cid:88) log Ï€ ai nc ,withÏ€i =exp(hi ancÂ·hi pos),Ï€j =exp(hi ancÂ·hj neg),\nucl |B| Ï€i +(cid:80) Ï€j anc Ï„ neg Ï„\niâˆˆB anc jâˆˆB,jÌ¸=i neg\n(1)\n4"
      },
      {
        "page": 5,
        "content": "whereBdenotesamini-batch,andÏ„ isatemperatureforsoftmaxwhichwekeep0.1asthedefault\nsetting. Note that it is common to augment an anchor image xi to be a positive sample in many\nt\nunsupervisedcontrastivelearningmethods. Here,wedonâ€™tapplyanydataaugmentationanddirectly\ntreattheanchorlatenthi itselfasthepositivevectorinthecontrastiveloss,i.e.,hi :=hi .\nanc pos anc\nWhycontrastivelossencouragesdiversityanddiscouragesmodecollapse? Intheworstcase,that\nallembeddingscollapsetoaconstantvector,thecontrastivelossbecomeslogN forabatchwithN\nsamples. Suchconstantembeddingsyieldthemaximumpossiblelossandarethereforediscouraged.\nIntuitively,contrastivelossintroducesarepulsiveforcebetweendifferentsamples. Thenumerator\nremainsconstant,asself-similarityisalwaysmaximal,whilethedenominatoraggregatespairwise\nsimilarities across the batch. Minimizing the loss requires reducing similarities between distinct\nsamples,effectivelypushingtheirembeddingsapartinfeaturespace. Inthisway,themodelavoids\ncollapsebymaximizinginter-sampledistances. Geometrically,theoptimalsolution(intheabsence\nofaugmentation)correspondstoembeddingsbeinguniformlydistributedonahypersphere.\nBatchResampleisasimplestrategyforlong-tailedrecognitionandgeneration(Zhangetal.,2024).\nHowever,itmayleadtomodecollapseduetorepetitiveimagesfortailclasses.Sinceourunsupervised\ncontrastivelossreliesonpairsofimages,batchresamplingincreasesthechanceofimagesfromthe\nsametailclassappearinginthesamebatch,whichfurtherdiversifytailclassimages. Wechoose\nbatchresamplingasanoptionalstrategyforourframeworkandprovideanablationstudyinSec.4.3.\n3.2 CONDITIONAL-UNCONDITIONALALIGNMENTLOSS\nConditinal-unconditinalAlignmentFacilitatesKnowledgeSharingforClass-imbalancedGAN\nWe take inspiration from UTLO (Khorram et al., 2024), which addresses long-tailed generation\nwith GANs, and observes that the similarity between head class and tail class images increases\nat lower resolution representations. To share knowledge between the head class and tail class,\nUTLO(Khorrametal.,2024)proposesunconditionalGANobjectivesforlower-resolutionrepresen-\ntationsandconditionalGANobjectivesforsubsequenthigher-resolutionimages. Morespecifically,\nthelower-resolutionrepresentationsbecomeslesssensitivetoclasslabels,andthesecondpartofthe\ngeneratorgeneratesthefinedetailsofimages. Utilizingunconditionalgenerationforlowerresolution\niseffectiveinincreasingthediversityandqualityoftailclassimages. Inadifferentsettingoftraining\nGANwithlimiteddata,Transitional-GAN(Shahbazietal.,2022)leveragesunconditionalgeneration\ninatwo-stagedtrainingscheme,asunconditionalgenerationcangiveabetterFIDthanconditional\ngenerationwhendataislimited.\nConditional-unconditinalAlignmentforClass-imbalancedDiffusionModel Weaimtoadapt\ntheconditional-unconditionalalignmentapproachusedinGANs(Khorrametal.,2024;Shahbazi\net al., 2022) to diffusion models. Our key insight is that the diffusion model denoises images\nrecursively,andthesimilaritybetweenheadclassimagesandtailclassimagesishigherduringthe\ninitialtimestepsofthedenoisingprocess. ThisissimilarbutdifferentfromUTLO(Khorrametal.,\n2024),whichproposesunconditionalgenerationforlower-resolutionrepresentations. Wevisualize\nthereverseprocessingofunconditionalgenerationandconditionalgenerationwithdifferentclass\nlabelsoftheoriginalDDPM,startingfromthesameGaussiannoise,asshowninFig.3andmore\nexamplesinappendixA.3.4. Imagesfromunconditionalgenerationandconditionalgenerationshare\nsimilarlow-frequencycomponents,e.g.,thecoarseshapeofsynthesizedobjectsandbackground\nregion,especiallyattheinitialtimesteps(Sietal.,2024).\nMotivated by this observation, we propose to match unconditional generation and conditional\ngeneration at the initial time steps, to enable knowledge sharing between tail classes and head\nclasses with abundant data. Another motivation is that class conditions are not necessary for all\ntimesteps. Infact,usingadaptiveguidanceweightwithasmallweightintheinitialstepscanyielda\nbetterFIDcomparedtousingconstantguidanceweight(Wangetal.,2024).\nFromConditional-unconditionalDistributionMatchingtoAlignmentLoss WepenalizeKL\ndivergence between the conditional distribution p (x |xi,ci) and unconditional distribution\nÎ¸ tâˆ’1 t\np (x |xi):\nÎ¸ tâˆ’1 t\nLi,t =D [p (x |xi,ci)||p (x |xi)]. (2)\nal KL Î¸ tâˆ’1 t Î¸ tâˆ’1 t\n5"
      },
      {
        "page": 6,
        "content": "Low Frequency High Frequency\nFigure 3: The leftmost column shows synthetic images with different classes but with the same\ninitialnoiseorrandomseed. Wevisualizethelow-frequencycomponentandthehigh-frequency\ncomponentfromthereverseprocessing. Itshowsthatlow-frequencycomponentsaresimilarduring\ninitialtimestepsfordifferentclasseswiththesameinitialnoise(Sietal.,2024). Moreexamplesare\ninappendixA.3.4.\nSuppose p (x |x ,c) = N(x ;Âµ (x ,t,c),Ïƒ2I),p (x |x ) = N(x ;Âµ (x ,t),Ïƒ2I),\nÎ¸ tâˆ’1 t tâˆ’1 Î¸ t t Î¸ xâˆ’t t tâˆ’1 Î¸ t t\nthenwehave:\n1\nLi a, lt =E[ 2Ïƒ2||Âµ Î¸(xi t,t,ci)âˆ’Âµ Î¸(xi t,t)||2]+C, (3)\nt\nwhereCisconstant,and\n1 Î² 1 Î²\nÂµ Î¸(xi t,t,ci)= âˆš\nÎ±\ntxi tâˆ’ âˆš\nÎ±\ntâˆš 1t\nâˆ’Î±Â¯\ntÏµ Î¸(xi t,t,ci), Âµ Î¸(xi t,t)= âˆš\nÎ±\ntxi tâˆ’ âˆš\nÎ±\ntâˆš 1t\nâˆ’Î±Â¯\ntÏµ Î¸(xi t,t),\n(4)\nwhereÎ± =1âˆ’Î² ,Î±Â¯\n=(cid:81)t\n(1âˆ’Î² ),{Î² } isthevarianceschedule.\nt t t i=1 i t 1:T\nThen,L simplifiesto:\nal\nLi,t âˆ||Ïµ (xi,t,ci)âˆ’Ïµ (xi,t)||2. (5)\nal Î¸ t Î¸ t\nAsshowninFig.2,theanchorvectorhi isfedintodecodenetworkd(âˆ—)togetunconditionalnoise\nanc\nestimationÏµi :=Ïµ (xi;t,âˆ…). Meanwhile,theanchorimagexi isalsoincorporatedwiththeclass\nunc Î¸ t t\nlabelconditioncifedintothenetworktogettheconditionalnoiseestimationÏµi :=Ïµ (xi;t,ci).\ncond Î¸ t\nThealignmentlossisdefinedbetweentheunconditionalandconditionalnoiseestimation:\nL al = |B1 |(cid:88) E t,x0[ Tt âˆ¥Ïµ Î¸(xi t;t,ci)âˆ’Ïµ Î¸(xi t;t,âˆ…)âˆ¥2]. (6)\niâˆˆB\nThelossisweightedlinearlybytimestepst,sothattheinitialstepswithlargetareweightedmore.\nInotherwords,wealignconditionalgenerationandunconditionalgenerationfortheinitialsteps.\nAlgorithm1TrainingalgorithmofCCUA.\n3.3 OVERALLFRAMEWORK\nSetL ,L ,L =0\nddpm ucl al\nOur final loss function L is the\nforeachimage-classpair(xi 0,ci)inthisbatchBdo\nsum of the standard DDPM (Ho Samp âˆšleÏµi âˆ¼N( âˆš0,I),tâˆ¼U({0,1,...,T})\netal.,2020)lossL andourcon- xi = Î±Â¯ xi + 1âˆ’Î±Â¯ Ïµi\nddpm t t 0 t\ntrastive conditional-unconditional Ïµi =Ïµ (xi;t,âˆ…),Ïµi =Ïµ (xi;t,ci)\nunc Î¸ t cond Î¸ t\nalignmentlossL ccua: hi =e(xi)\nanc t\nL ccua =Î±Â·L ucl+Î³Â·L al, (7) Ï€ anc =exp(hi ancÂ·hi anc/Ï„),SetÏ€ neg =0\nwhere Î± and Î³ are the hyper- forxj t inthisbatchwithiÌ¸=j do\nparameters for our unsupervised hj =e(xj)\nneg t\ncontrastivelossandalignmentloss. Ï€ =Ï€ +exp(hi Â·hj /Ï„)\nneg neg anc neg\nendfor\nTheoverallalgorithmframeworkis\nshownasAlgorithm1. Inclassifier- L ucl =L ucl+(âˆ’log Ï€anÏ€ ca +n Ï€c neg)\nfree guidance (CFG), the class la- ifUnconditionalTrainingofCFGthen\nbels c are randomly dropped by L ddpm =L ddpm+âˆ¥Ïµiâˆ’Ïµi uncâˆ¥2\na specific probability p . In elseifConditionalTrainingofCFGthen\nuncond\noursetting,wekeepthep uncond = L ddpm =L ddpm+âˆ¥Ïµiâˆ’Ïµi condâˆ¥2\n10%,sameas(Qinetal.,2023).The L\nal\n=L al+ Ttâˆ¥Ïµi uncâˆ’Ïµi condâˆ¥2\nparametersofthenoiseestimation endif\nnetworkÏµ areupdatedbythegradi- endfor\nÎ¸\nentofthefinallossâˆ‡ LasinEq.7. Computethegradientwiththeloss\nÎ¸\nL= 1 (L +Î±Â·L +Î³Â·L )\n|B| ddpm ucl al\n6"
      },
      {
        "page": 7,
        "content": "Table1: ComparisononImageNet-LT256Ã—256withSiTpipeline. WeuseBlueâ€˜()â€™tohighlightthe\nimprovementofourmethodoverSiTbaselineonFIDandRecall,whichdenotestheoverallquality\nanddiversity,respectively.\nEpochs Method ISâ†‘ FIDâ†“ sFIDâ†“ Prec.(%)â†‘ Recall(%)â†‘\nSiT 53.93 33.87 22.66 54.56 19.17\n40 +DispersiveLoss 53.87 34.00 22.68 54.87 19.84\n+CCUA(ours) 73.60 25.89(-7.98) 16.54 58.62 23.05(+4.08)\nSiT 78.88 25.72 21.95 64.35 18.58\n80 +DispersiveLoss 83.80 25.22 21.76 65.50 17.30\n+CCUA(ours) 103.66 19.99(-5.73) 14.88 65.80 21.31(+2.73)\nSiT 103.11 21.20 20.17 69.34 18.25\n120 +DispersiveLoss 104.02 21.32 20.46 68.07 18.56\n+CCUA(ours) 119.11 17.55(-3.65) 13.89 68.38 21.20(+2.95)\nSiT 111.70 19.95 20.12 70.34 18.61\n160 +DispersiveLoss 115.64 19.76 20.18 69.98 18.96\n+CCUA(ours) 124.87 16.41(-3.54) 13.17 69.84 21.34(+2.73)\n4 EXPERIMENTS\n4.1 EXPERIMENTALSETUP\nWereportresultsonmultipledatasets,includingthelong-tailedversionsofImageNet,TinyImageNet,\nandPlacesdatasets,namelyImageNet-LT,TinyImageNet-LT,andPlaces-LT,whicharecommonly\nuseddatasetsinlong-tailedimagegenerationandrecognition.Wealsoreportresultsandconductmore\nanalysisonCIFAR10-LT/CIFAR100-LTdatasets,whichareshownintheappendixA.2. Wemeasure\nISScore,FID,spatialFID(Dhariwal&Nichol,2021),KernelInceptionDistance(KID)(BinÂ´kowski\netal.,2018)astheoverallqualitymetrics. WealsoreportFIDoftailclasses(FID ),inclinedto\ntail\nmeasurequalityfortailclasses. PrecisionandRecallarereportedtorespectivelymeasurefidelity\nand diversity, which are widely used in evaluating generative models on long-tailed scenes (Qin\netal.,2023;Zhangetal.,2024;Khorrametal.,2024). Moreimplementationdetailsareprovidedin\ntheappendixA.1. Wecompareourproposedmethodswithmultiplebaselines,coveringdiffusion\ntransformerandunet-basedarchitecture. Fordiffusiontransformer,wecompareCCUAtoSiT(Ma\netal.,2024)andDispersiveLoss(Wang&He,2025). Forunet-basedarchitecture,wecomparetothe\noriginalDDPM(Hoetal.,2020),CBDM(Qinetal.,2023),andOCLT(Zhangetal.,2024).\n4.2 QUANTITATIVERESULTS\nClass-imbalancedGenerationforDiffusionTransformer WeapplytheproposedCCUAlossinto\nthestandarddiffusiontransformertrainingpipeline,SiT(Maetal.,2024),onImageNet-LTdataset,\nandcompareittoDispersiveLoss(Wang&He,2025),aconcurrentworkofapplyingcontrastive\nlearningforimprovingSiTonbalanceddataset. AllmodelsaresampledwithODEflow50stepsfor\nconditionalgenerationwithCFGstrength7.5. AsshowninTable1,ourmethodachievesremarkable\nimprovementcomparedtoSiTandDispersiveLossonvarioustrainingepochs. Ourmethodachieves\nabove10%improvementonRecall,whilekeepthesimilarPrecisionasSiT,illustratingthehigher\ndiversityoftheproposedCCUAlosswithouttradingofffidelity.\nClass-imbalanced Generation for DDPM For training unet-based architecture DDPM on\nTinyImageNet-LTandPlaces-LTdatasets,ourmethodalsoachievesthebestperformance,asshown\ninTab.2. AllmodelsaresampledwithDDIM100stepsforconditionalgenerationwithoptimalCFG\nstrength. ForTinyImageNet-LTdatasets,weprovidethemetricsoftheDDPMmodeltrainedonthe\nbalancedversion,i.e.,theoriginalTinyImageNetdatasets,denotedbyDDPMâˆ— ,asthetheoretical\nbal\noptimalreference. ThebestFIDandKIDscoreofourmethodillustratesthehighqualityofimages\nsynthesizedbyourmethod1.\n1TheFIDscoreofourreproducedDDPM,CBDM(Qinetal.,2023),andOCLT(Zhangetal.,2024)are\nbetterthanthenumbersreportedin(Qinetal.,2023;Zhangetal.,2024). Wetrainallmethodsfromscratch\nwithoutfine-tuningandfindtheoptimalguidancestrengthÏ‰foreachmethod.Suchimplementationisdifferent\nfromtheoneusedin(Qinetal.,2023;Zhangetal.,2024)butisconsideredmorefair.\n7"
      },
      {
        "page": 8,
        "content": "Table2: ComparisononTinyImageNet-LT64Ã—64andPlaces-LT64Ã—64withDDPMpipeline.\nBlueâ€˜()â€™showsimprovementofourmethodoverDDPMbaseline. Greenâ€˜()â€™showsimprovementof\nDDPMtrainedonbalancedversionovertrainedonlong-tailedversion.\nDataset Method FIDâ†“ FID â†“ KID â†“\ntail Ã—1k\nDDPMâˆ— (Hoetal.,2020) 15.73(-2.94) 25.62(-12.26) 3.19(-3.12)\nbal\nDDPM(Hoetal.,2020) 18.67 40.12 6.31\nTinyImageNet-LT CBDM(Qinetal.,2023) 20.90 48.07 6.55\nOCLT(Zhangetal.,2024) 17.72 39.67 5.61\nCCUA(ours) 15.24(-3.43) 30.39(-9.73) 3.83(-2.48)\nDDPM(Hoetal.,2020) 13.89 23.74 5.26\nCBDM(Qinetal.,2023) 15.15 26.07 5.64\nPlaces-LT\nOCLT(Zhangetal.,2024) 13.04 22.84 4.17\nCCUA(ours) 11.99(-1.90) 20.84(-2.90) 3.63(-1.63)\nTable3: FIDscoreforthreeâ€˜super-categoriesâ€™: â€˜Headâ€™,â€˜Bodyâ€™,andâ€˜Tailâ€™. Allmodelsaremeasured\nwithDDIM(Songetal.,2020)100samplingstepsforconditionalgenerationwithCFG.\nFIDâ†“ P\ncategory Head Body Tail\nDataset All\nâˆ¼80% âˆ¼17% âˆ¼3%\nMethod\nDDPM(Hoetal.,2020) 21.27 33.25 40.12 18.67\nCBDM(Qinetal.,2023) 24.12 35.02 48.07 20.90\nTinyImageNet-LT\nOCLT(Zhangetal.,2024) 20.64 30.62 39.67 17.72\nCCUA(ours) 21.32 27.98 30.39 15.24\nDDPM(Hoetal.,2020) 19.26 20.31 23.74 13.89\nCBDM(Qinetal.,2023) 21.26 21.68 26.07 15.15\nPlaces-LT\nOCLT(Zhangetal.,2024) 19.16 19.43 22.84 13.04\nCCUA(ours) 18.15 19.27 20.84 11.99\nResultsAcrossVariousCategoryIntervals Toseetheeffectivenessofourmethodontailclasses,\nwedividethePlaces-LTandTinyImageNet-LTdatasetsintothreesupercategories: â€˜Headâ€™,â€˜Bodyâ€™,\nand â€˜Tailâ€™, with classes sorted by the number of training images. For each dataset, the top 33%\nclasseswereallocatedtotheâ€˜Headâ€™category,thenext34%classestotheâ€˜Bodyâ€™category,andthe\nlast33%classestotheâ€˜Tailâ€™category. ThepercentageP oftrainingimagesbelongingto\ncategory\neachcategoryisshowninthefirstrowofTab.3. OurmethodachievesthebestFIDscoreacross\nallthreecategoriesonPlaces-LTandTinyImageNet-LTdatasets. Fortheâ€˜Tailâ€™classes,whichare\nonlyâˆ¼3%,ourmethodimprovestheirFIDfrom23.74to20.84onPlaces-LTand40.12to30.39on\nTinyImageNet-LT,respectively. Additionally,bothâ€˜Headâ€™andâ€˜Bodyâ€™classesshowimprovedFID\nscoreswithourmethod,asdemonstratedinTab.3.\n4.3 QUALITATIVERESULTSANDABLATIONSTUDY\nVisualization of Synthetic Images Fig. 4 shows synthetic images of SiT and with CCUA for\nImageNet-LTtailclassesâ€˜espressoâ€™andâ€˜windowshadeâ€™. ComparedtoSiT,CCUAachievesvisually\nhigher fidelity and diversity. More qualitative results on ImageNet-LT, TinyImageNet-LT, and\nCIFAR100-LTareshowninappendixA.3.1andA.3.3.\nAblationStudyofLossFunction Table4showsanablation Table4: LossFunctionAnalysis.\nstudyoftheproposedlossesincludingL andL ,whichis\nucl al\nconductedonTinyImageNet-LT.Forthelossfunctionablation\nLossFunction FIDâ†“\nstudy,wedisablebatchresamplestrategywhichisseparately\nL 18.67\nablated. OurmethodachieveslowerFIDwhenapplyingonly ddpm\nL w/L (Eq.1) 18.05\ntheL loss(Eq.1)oronlyL loss(Eq.6)comparedtothe ddpm ucl\nucl al L w/L (Eq.6) 17.84\nddpm al\nstandard L loss only. The best result is achieved with\nddpm L w/L (Eq.7) 17.16\nddpm ccua\nbothlosses,i.e.,L (Eq.7).\nccua\nAblationStudyofBatchResampleStrategy AsshowninTable5,wemeasuretheperformanceof\nSiTandCCUA,withorwithoutapplyingbatchresamplingstrategy. ComparedtoSiT,theproposed\n8"
      },
      {
        "page": 9,
        "content": "(a)SyntheticimagesfromSiT(Maetal.,2024).\n(b)SyntheticimagesfromCCUA(ours).\nFigure 4: Synthetic images of SiT and it with CCUA for ImageNet-LT tail classes â€˜espressoâ€™\nand â€˜window shadeâ€™. All methods start the denoising process from the same Gaussian noise at\ncorrespondinggridcells. CCUAshowsconsistentlyhigherdiversityandfidelitycomparedtoSiT.\nTable5: BatchResampleStrategyAnalysisonImageNet-LT.Allmodelsaretrainedfromscratchto\n40epochs. Blueâ€˜()â€™highlightstheimprovementofeachmethodcomparedtotheSiTbaseline,while\nRedâ€˜()â€™highlightsthedeclinecomparedtoSiTbaseline.\nMethod ISâ†‘ FIDâ†“ sFIDâ†“ Prec.(%)â†‘ Recall(%)â†‘\nSiT(baseline) 53.93 33.87 22.66 54.56 19.17\n+BatchResample 71.39(+17.46) 28.05(-5.82) 21.57(-1.09) 57.61(+3.05) 17.26(-1.91)\nL (Eq.7) 57.79(+3.86) 32.25(-1.62) 20.71(-1.95) 54.47(-0.09) 20.66(+1.49)\nccua\n+BatchResample 73.60(+19.67) 25.89(-7.98) 16.54(-6.12) 58.62(+4.06) 23.05(+3.88)\nCCUAlossachievesbetterIS,FID,sFIDandRecallwithonly0.09Precisionâ€™sdecline,illustratesour\nmethodonimprovingdiversitywithouttradingofffidelity. Withapplyingbatchresamplestrategy,\ntheCCUAfurtherimprovesallmetrics. NotesthattheCCUAincreasesRecallfrom20.66to23.05,\nwhileSiTsuffersadeclineinRecallfrom19.17to17.26whenapplyingbatchresampling. Aswe\ndiscussinSection.3.1,theL couldbenefitfrombatchresamplestrategyduetotheincreasing\nucl\ninstancesoftailclasses,leadingtomorediversedistributioninthelatentspace.\n5 CONCLUSION\nReal-worlddatafortrainingimagegenerationmodelsoftenexhibitlong-taileddistributions. Similar\nto class-imbalanced GANs (Khorram et al., 2024), class-imbalanced diffusion models generates\ninferior tail class images due to data scarcity. We propose a framework with two losses. Firstly,\nweemployanunsupervisedcontrastivelosswithnegativesamplesonlytocontrastthelatentsof\ndifferentsyntheticimagesateverydenoisingstep,promotingintra-classdiversityinparticularfor\ntailclasses. Oursecondlossalignsclass-conditionalgenerationwithunconditionalgenerationfor\nlargetimesteps. Thisencouragestheinitialdenoisingstepstobeclass-agnostic,therebyenrichingtail\nclassesthroughknowledgesharingfromheadclassesâ€“aprincipledemonstratedtoenhancelong-tailed\nGANperformance(Shahbazietal.,2022;Khorrametal.,2024),whichwesuccessfullyadaptto\ndiffusion models. With the two losses, our framework of contrastive conditional-unconditional\nalignment boosts the performane of DDPM (Ho et al., 2020) and SiT (Ma et al., 2024) for long-\ntailedimagegenerationandoutperformsalternativemethodsincludingCBDM(Qinetal.,2023),\nOCLT(Zhangetal.,2024), andDispersiveLoss(Wang&He,2025). Extensiveexperimentson\nmultipledatasetsinparticularImageNet-LTwith256x256resolutiondemonstratedtheeffectiveness\nofourmethodonbothUNet-basedarchitectureandDiffusionTransformer.\n9"
      },
      {
        "page": 10,
        "content": "REFERENCES\nQingzhongAi,PengyunWang,LirongHe,LiangjianWen,LujiaPan,andZenglinXu. Generative\noversampling for imbalanced data via majority-guided vae. In International Conference on\nArtificialIntelligenceandStatistics,pp.3315â€“3330.PMLR,2023.\nDmitryBaranchuk,IvanRubachev,AndreyVoynov,ValentinKhrulkov,andArtemBabenko. Label-\nefficientsemanticsegmentationwithdiffusionmodels. arXivpreprintarXiv:2112.03126,2021.\nMikoÅ‚ajBinÂ´kowski,DanicaJSutherland,MichaelArbel,andArthurGretton. Demystifyingmmd\ngans. arXivpreprintarXiv:1801.01401,2018.\nAndrewBrock,JeffDonahue,andKarenSimonyan. Largescalegantrainingforhighfidelitynatural\nimagesynthesis. InICLR,2019.\nKaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced\ndatasetswithlabel-distribution-awaremarginloss. Advancesinneuralinformationprocessing\nsystems,32,2019.\nTingChen,SimonKornblith,MohammadNorouzi,andGeoffreyHinton. Asimpleframeworkfor\ncontrastivelearningofvisualrepresentations. 2020.\nGuillaumeCouairon,JakobVerbeek,HolgerSchwenk,andMatthieuCord. Diffedit: Diffusion-based\nsemanticimageeditingwithmaskguidance. arXivpreprintarXiv:2210.11427,2022.\nYusuf Dalva and Pinar Yanardag. Noiseclr: A contrastive learning approach for unsupervised\ndiscovery of interpretable directions in diffusion models. In Proceedings of the IEEE/CVF\nConferenceonComputerVisionandPatternRecognition,pp.24209â€“24218,2024.\nPrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. Advances\ninneuralinformationprocessingsystems,34:8780â€“8794,2021.\nMichaelFuest,PingchuanMa,MingGui,JohannesSFischer,VincentTaoHu,andBjornOmmer.\nDiffusionmodelsandrepresentationlearning: Asurvey. arXivpreprintarXiv:2407.00783,2024.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervisedvisualrepresentationlearning. InCVPR,2020.\nJonathanHoandTimSalimans. Classifier-freediffusionguidance. arXivpreprintarXiv:2207.12598,\n2022.\nJonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin\nneuralinformationprocessingsystems,33:6840â€“6851,2020.\nJonathanHo,WilliamChan,ChitwanSaharia,JayWhang,RuiqiGao,AlexeyGritsenko,DiederikP\nKingma,BenPoole,MohammadNorouzi,DavidJFleet,etal. Imagenvideo: Highdefinition\nvideogenerationwithdiffusionmodels. arXivpreprintarXiv:2210.02303,2022a.\nJonathanHo,TimSalimans,AlexeyGritsenko,WilliamChan,MohammadNorouzi,andDavidJ\nFleet.Videodiffusionmodels.AdvancesinNeuralInformationProcessingSystems,35:8633â€“8646,\n2022b.\nZiyuJiang,TianlongChen,BobakJMortazavi,andZhangyangWang. Self-damagingcontrastive\nlearning. 2021.\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarialnetworks. InCVPR,2019.\nTeroKarras,MiikaAittala,JanneHellsten,SamuliLaine,JaakkoLehtinen,andTimoAila. Training\ngenerativeadversarialnetworkswithlimiteddata. Advancesinneuralinformationprocessing\nsystems,33:12104â€“12114,2020.\nBahjatKawar,ShiranZada,OranLang,OmerTov,HuiwenChang,TaliDekel,InbarMosseri,and\nMichalIrani. Imagic: Text-basedrealimageeditingwithdiffusionmodels. InProceedingsofthe\nIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.6007â€“6017,2023.\n10"
      },
      {
        "page": 11,
        "content": "SaeedKhorram,MingqiJiang,MohamadShahbazi,MohamadHDanesh,andLiFuxin. Tamingthe\ntailinclass-conditionalgans: Knowledgesharingviaunconditionaltrainingatlowerresolutions.\nInProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.\n7580â€“7590,2024.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron\nMaschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural\ninformationprocessingsystems,33:18661â€“18673,2020.\nDiederikPKingma,MaxWelling,etal. Anintroductiontovariationalautoencoders. Foundations\nandTrendsÂ®inMachineLearning,12(4):307â€“392,2019.\nTianhongLi,PengCao,YuanYuan,LijieFan,YuzheYang,RogerioSFeris,PiotrIndyk,andDina\nKatabi. Targetedsupervisedcontrastivelearningforlong-tailedrecognition. InProceedingsofthe\nIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.6918â€“6928,2022.\nZiwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and X Yu Stella. Open\nlong-tailedrecognitioninadynamicworld. IEEETransactionsonPatternAnalysisandMachine\nIntelligence,2022.\nNanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and\nSainingXie. Sit: Exploringflowanddiffusion-basedgenerativemodelswithscalableinterpolant\ntransformers. InEuropeanConferenceonComputerVision,pp.23â€“40.Springer,2024.\nYidong Ouyang, Liyan Xie, and Guang Cheng. Improving adversarial robustness through the\ncontrastive-guided diffusion process. In International Conference on Machine Learning, pp.\n26699â€“26723.PMLR,2023.\nBenPoole,AjayJain,JonathanT.Barron,andBenMildenhall. Dreamfusion: Text-to-3dusing2d\ndiffusion. arXiv,2022.\nYiming Qin, Huangjie Zheng, Jiangchao Yao, Mingyuan Zhou, and Ya Zhang. Class-balancing\ndiffusionmodels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern\nRecognition,pp.18434â€“18443,2023.\nHarshRangwani,NamanJaswani,TejanKarmali,VarunJampani,andRVenkateshBabu. Improving\ngans for long-tailed data through group spectral regularization. In European Conference on\nComputerVision,pp.426â€“442.Springer,2022.\nHarshRangwani,LavishBansal,KartikSharma,TejanKarmali,VarunJampani,andRVenkatesh\nBabu. Noisytwins: Class-consistentanddiverseimagegenerationthroughstylegans. InProceed-\ningsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.5987â€“5996,\n2023.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjoÂ¨rn Ommer. High-\nresolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-\nenceoncomputervisionandpatternrecognition,pp.10684â€“10695,2022.\nNatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfirAberman.\nDreambooth:Finetuningtext-to-imagediffusionmodelsforsubject-drivengeneration. InProceed-\ningsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pp.22500â€“22510,\n2023.\nMohamadShahbazi,MartinDanelljan,DandaPaniPaudel,andLucVanGool. Collapsebycondition-\ning: Trainingclass-conditionalGANswithlimiteddata. InInternationalConferenceonLearning\nRepresentations,2022. URLhttps://openreview.net/forum?id=7TZeCsNOUB_.\nChenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. Freeu: Free lunch in diffusion u-net.\nInProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.\n4733â€“4743,2024.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprintarXiv:2010.02502,2020.\n11"
      },
      {
        "page": 12,
        "content": "ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonShlens,andZbigniewWojna. Rethinking\ntheinceptionarchitectureforcomputervision. InProceedingsoftheIEEEconferenceoncomputer\nvisionandpatternrecognition,pp.2818â€“2826,2016.\nShuhanTan,YujunShen,andBoleiZhou. Improvingthefairnessofdeepgenerativemodelswithout\nretraining. arXivpreprintarXiv:2012.04842,2020.\nLaurensVanderMaatenandGeoffreyHinton. Visualizingdatausingt-sne. Journalofmachine\nlearningresearch,9(11),2008.\nRunqian Wang and Kaiming He. Diffuse and disperse: Image generation with representation\nregularization. arXivpreprintarXiv:2506.09027,June2025.\nXiWang,NicolasDufour,NefeliAndreou,Marie-PauleCani,VictoriaFernaÂ´ndezAbrevaya,David\nPicard, and Vicky Kalogeiton. Analysis of classifier-free guidance weight schedulers. arXiv\npreprintarXiv:2404.13040,2024.\nDivinYan,LuQi,VincentTaoHu,Ming-HsuanYang,andMengTang. Trainingclass-imbalanced\ndiffusionmodelviaoverlapoptimization. arXivpreprintarXiv:2402.10821,2024.\nSihyunYu,SangkyungKwak,HuiwonJang,JongheonJeong,JonathanHuang,JinwooShin,and\nSainingXie. Representationalignmentforgeneration: Trainingdiffusiontransformersiseasier\nthanyouthink. arXivpreprintarXiv:2410.06940,2024.\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image\ndiffusionmodels. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,\npp.3836â€“3847,2023.\nTianjiaoZhang,HuangjieZheng,JiangchaoYao,XiangfengWang,MingyuanZhou,YaZhang,and\nYanfengWang.Long-taileddiffusionmodelswithorientedcalibration.InTheTwelfthInternational\nConferenceonLearningRepresentations,2024.\n12"
      },
      {
        "page": 13,
        "content": "share weights\nkcolB\nseR\nnwoD elpmaS seR klB ...\nkcolB\nseR\nnwoD elpmaS seR klB ...\nseR klB pU\nelpmaS kcolB\nseR\n...\n(a)w/UNet-basedModel\nTiD/TiS kcolB TiD/TiS kcolB TiD/TiS kcolB ...\nTiD/TiS kcolB TiD/TiS kcolB TiD/TiS kcolB ...\nraeniL NLada\nshare weights\n(b)w/DiffusionTransformer\nFigure5: ModeldetailsofCCUAframeworkwithUNet-basedModelandDiffusionTransformer. As\nshowninFig.2,thenoiseestimationnetworkisdividedintotwoparts,alatentencodednetworke(âˆ—)\nandadecodednetworkd(âˆ—). (a)ForUNet-basedarchitecture,e(âˆ—)isdefinedastheUNetencoder,\nwhiled(âˆ—)isdefinedastheUNetdecoder. (b)ForDiffusionTransformer,e(âˆ—)isdefinedasallthe\nSiT/DiTblocks,whiled(âˆ—)isdefinedasthefinallinearandadaLNprojectionlayer.\nTable 6: Our method outperforms other baselines on all datasets. We also provide the results of\nDDPM trained on balanced datasets, which show the upper bound of performance. All models\naremeasuredwithDDIM(Songetal.,2020)100stepsforconditionalgenerationwithCFG.Blue\nâ€˜()â€™ shows improvement of our method over DDPM baseline (Ho et al., 2020). Green â€˜()â€™ shows\nimprovementofDDPMtrainedonbalancedversionovertrainedonlong-tailedversion.\nDataset Method FIDâ†“ FID â†“ KID â†“\ntail Ã—1k\nDDPMâˆ— (Hoetal.,2020) 4.90(-1.03) 6.27(-5.98) 1.32(-0.32)\nbal\nDDPM(Hoetal.,2020) 5.93 12.25 1.64\nCIFAR10-LT CBDM(Qinetal.,2023) 5.81 10.01 1.58\nOCLT(Zhangetal.,2024) 6.10 11.13 1.58\nCCUA(ours) 5.56(-0.37) 10.03(-2.22) 1.27(-0.37)\nDDPMâˆ— (Hoetal.,2020) 5.15(-1.80) 8.97(-8.48) 1.05(-0.66)\nbal\nDDPM(Hoetal.,2020) 6.95 17.45 1.71\nCIFAR100-LT CBDM(Qinetal.,2023) 6.50 17.36 1.41\nOCLT(Zhangetal.,2024) 6.45 17.22 1.42\nCCUA(ours) 6.24(-0.71) 16.35(-1.10) 1.36(-0.35)\nA TECHNICAL APPENDICES AND SUPPLEMENTARY MATERIAL\nA.1 IMPLEMENTATIONDETAILS\nDatasetDetails Wekeeptheoriginal32Ã—32resolutionforCIFAR10-LT/CIFAR100-LT,andresize\nimagesto64Ã—64forTinyImageNet-LTandPlaces-LT,while256Ã—256forImageNet-LT.Same\nas(Qinetal.,2023;Yanetal.,2024;Zhangetal.,2024),weadoptthesamemethodologypresented\nin(Caoetal.,2019)toconstructlong-tailedversiondatasetswithanimbalancefactorof0.01.\nModelArchitectureDetails AsdescribedinSec.3,theproposedCCUAframeworkcanbeapplied\nintoUNet-basedarchitectureandDiffusionTransformer. InFig.5,wedisplayourUNet-basedmodel\nandDiffusionTransformerindetails. AsshowninFig.2,thenoiseestimationnetworkisdividedinto\ntwoparts,alatentencodednetworke(âˆ—)andadecodednetworkd(âˆ—). Inoursetting,forUNet-based\nmodel,e(âˆ—)isdefinedastheUNetencoder,whiled(âˆ—)isdefinedastheUNetdecoder,asshownin\nFig.5(a). ForDiffusionTransformer,e(âˆ—)isdefinedasalltheSiT/DiTblocks,whiled(âˆ—)isdefined\nasthefinallinearandadaLNprojectionlayer,asshowninFig.5(b).\nTrainingDetails ForSiTpipeline,westrictlyfollowthesametraininghyper-parametersettings\nasDispersiveLoss(Wang&He,2025). Weuse4A6000GPUstotrainallSiTbasedmodelson\nImageNet-LTdatasetswithbatchsize48. Duringthetrainingprocess,allmethodsaretrainedfrom\nscratch. Wereport40epochs,80epochs,120epochs,and160epochsresultsinTable1. ForDDPM\npipeline,wefollowthesametrainingconfigurationsofthebaselinemodels(Hoetal.,2020;Qin\net al., 2023; Zhang et al., 2024). We use one RTX 4090 GPU to train each model on CIFAR10-\n13"
      },
      {
        "page": 14,
        "content": "Table7: WecompareourmethodwithotherbaselinesonCIFAR10-LT/CIFAR100-LTdatasets,with\nDDPM1000stepsforconditionalgenerationwithCFG.Wealsoreportthedataaugmentationmethod\nADA(Karrasetal.,2020)andÏ‰-scheduler(Wangetal.,2024),whichareorthogonaltoourmethod.\nCIFAR10-LT CIFAR100-LT\nMethod\nFIDâ†“ ISâ†‘ FIDâ†“ ISâ†‘\nDDPMâˆ— (Hoetal.,2020) 4.87 9.35 5.20 13.29\nbal\nDDPM(Hoetal.,2020) 5.81 9.36 7.09 12.64\n+ADA(Karrasetal.,2020) - - 6.69 12.87\n+Ï‰-Scheduler(Wangetal.,2024) 5.87 9.22 6.60 12.10\nCBDM(Qinetal.,2023) 5.92 9.38 6.52 12.79\nOCLT(Zhangetal.,2024) 5.69 9.42 6.23 13.18\nCCUA(ours) 5.57 9.42 5.99 13.01\nLT/CIFAR100-LTdatasetswithbatchsize64whileusing2A100GPUsforTinyImageNet-LTand\nPlaces-LT,withbatchsize128. Duringthetrainingprocess,allmethodsaretrainedfromscratchfor\n200kiterationsonCIFAR10-LT/CIFAR100-LTdatasets,100kiterationsonTinyImageNet-LTand\nPlaces-LTdatasets,whiletheyfollowtheclassifier-freeguidance(CFG)algorithm(Ho&Salimans,\n2022),whichrandomlydropslabelswithaprobabilityof10%. OnImageNet-LTandTinyImageNet-\nLTdatasets,weapplybatchresamplestrategywiththere-balancedfactor0.1,whilewedonâ€™tapply\nbatchresamplestrategyonPlaces-LTandCIFAR10-LT/CIFAR100-LTdatasets. ForTinyImageNet-\nLTandPlaces-LTdatasets,wealsoapplytimestepadaptiveweightt/T tounsupervisedcontrastive\nlosssincewefindtheunsupervisedcontrastivelosscouldalsobenefitfromsuchanadaptiveweight,\nlikealignmentloss.\nEvaluation Details In Table 2, FID denotes the FID score for synthetic images of the last\ntail\n30%classesineachdataset. Specifically,wecategorizetheâ€˜Tailâ€™classesasthelast66classesfor\nTinyImageLT(200classes),thelast121classesforPlaces-LT(365classes),respectively. InTable1,\ntheevaluationmetricsarebasedon50ksyntheticimagesgeneratedbyeachmethodwithaCFG\nstrength7.5. InTable2,theevaluationmetricsarebasedon10ksyntheticimagesgeneratedbyeach\nmethod. Duringinference,weperformagridsearchalgorithmforeachmethodtodeterminethe\noptimalguidancestrengthÏ‰ofCFG,ensuringeachmodelachievesitsbestperformance.\nA.2 MOREQUANTITATIVERESULTS\nClass-imbalancedGenerationonCIFAR10-LT/CIFAR100-LTdatasets Weconductmoreex-\nperimentsandanalysisonCIFAR10-LT/CIFAR100-LTdatasets,asshowninTable6andTable7.\nWe provide the metrics of the DDPM model trained on the balanced version, i.e., the original\nCIFAR10/CIFAR100 datasets, denoted by DDPMâˆ— , as the theoretical optimal reference. On\nbal\nTable8: WecompareourmethodwithotherbaselinesonCIFAR10-LT/CIFAR100-LTdatasetswith\nDDIM10samplingstepsforconditionalgenerationwithCFG.Blueâ€˜()â€™showsimprovementofour\nmethodoverDDPMbaseline(Hoetal.,2020). Greenâ€˜()â€™showsimprovementofDDPMtrainedon\nbalancedversionovertrainedonlong-tailedversion.\nDataset Method FIDâ†“ FID â†“ KID â†“\ntail Ã—1k\nDDPMâˆ— 13.28(-1.44) 13.26(-5.01) 6.06(-0.98)\nbal\nDDPM 14.72 18.27 7.04\nCIFAR10-LT CBDM 13.54 16.90 6.52\nOCLT 15.48 20.73 7.39\nCCUA(ours) 13.16(-1.56) 16.83(-1.44) 6.04(-1.00)\nDDPMâˆ— 13.34(-0.75) 18.27(-6.80) 5.56(-0.57)\nbal\nDDPM 14.09 25.07 6.13\nCIFAR100-LT CBDM 13.37 23.97 5.83\nOCLT 13.70 24.48 5.73\nCCUA(ours) 12.90(-1.19) 23.17(-1.90) 5.63(-0.50)\n14"
      },
      {
        "page": 15,
        "content": "CIFAR10-LT/CIFAR100-LT,ourmethodachievesthelowestFIDandKIDcomparedtobaseline\nmethods. NotethattheFIDgapbetweenDDPMandDDPMâˆ— is1.03onCIFAR10-LTand1.8on\nbal\nCIFAR100-LT,whileourmethodimprovesFID0.37over1.03onCIFAR10-LTand0.71over1.8on\nCIFAR100-LT,achieving>35%performanceimprovement. Toinvestigatetheconsistencyofsuch\nimprovements,wecompareourmethodandallbaselinemethodsonCIFAR10-LTandCIFAR100-LT\nwithDDPM1000steps. AsshowninTab.7,ourmethodachievesconsistentimprovementsofFIDfor\nfull1000samplingsteps. Wealsocomparetoawidelyuseddataaugmentationtechnique,Adaptive\nDiscriminatorAdaption(ADA)(Karrasetal.,2020)forgenerativemodelsonthefullDDPM1000\nsamplingsteps. Besides,weapplytheCFGguidancestrengthschedulerÏ‰-cos(Wangetal.,2024)on\nDDPM,whichgraduallyincreasestheguidancestrengthduringsamplingtimestepsdecreasingto\nforcethemodeltransferfromunconditionaltoconditionalgeneration.\nConsistentImprovementforFewerSamplingSteps Toinvestigatethemodelâ€™sperformanceon\nextremelyfewsamplingsteps,WeevaluateourmethodandallbaselinesforDDIM10stepswithCFG\nconditionalgenerationonCIFAR10-LT/CIFAR100-LT.AsshowninTab.8,ourmethodachievesthe\nbestFID,FID andKIDscores.Ourmethodachievesevenbetterresultsthanthetheoreticaloptimal\ntail\nmodelDDPMâˆ— undersuchanextremeexperimentalsetting. Suchanimprovementdemonstrates\nbal\ntheeffectivenessofourmethodfortraininglong-tailedimagegenerationdiffusionmodel.\nClass-imbalancedUnconditionalGeneration Wealso Table 9: Unconditional generation w/\nevaluate all the models for class-imbalanced uncondi- DDIM100steps.\ntional generation. As shown in Tab. 9, the proposed\nmethodreducesFIDfrom27.52to24.16forCIFAR10-\nCIFAR10-LT CIFAR100-LT\nLTandfrom18.53to15.97forCIFAR100-LT.Suchan Method\nFIDâ†“ ISâ†‘ FIDâ†“ ISâ†‘\nimprovementinunconditionalgenerationhighlightsthe\nDDPM 27.52 6.65 18.53 8.68\neffectiveness of the proposed contrastive learning loss,\nCBDM 25.60 6.70 17.06 9.00\nparticularlytheunsupervisedcontrastivelossL .\nucl OCLT 31.38 6.34 18.97 8.73\nOurs 24.16 6.80 15.97 9.23\nStatistical Significance Analysis\nToinspecttherobustnessofthepro- Table10: StatisticalAnalysisforrandomseedonCIFAR100-\nposed method, we conduct statisti- LT,samplingwithDDIM100steps.\ncalsignificanceanalysisontheddpm\nandourmethod,asshowninTab.10.\nWithapplyingdifferentrandomseed, Seed Method FIDâ†“ Method FIDâ†“\ntheFIDdeviationofDDPMisabout 0 DDPM 7.02 Ours 6.26\n0.04whileourmethodisabout0.01, 42 DDPM 6.95 Ours 6.24\n2025 DDPM 7.04 Ours 6.27\nillustrating the robustness of our\nmethodfordifferentrandomseeds.\nLimitationofourmethod OurMSElossinvolvesbothconditionalgenerationandunconditional\ngeneration,whichrequirestwopassesofthedenosingnetworkduringtrainingandincreasestraining\ntime. Inpractice,withapplyingourmethod,thetrainingtimeforoneepochis1.6xofthatofDDPM,\nand1.48xofthatofSiT.However,ourmethoddoesnâ€™tincreaseinferencelatency.\nA.3 MOREQUALITATIVERESULTS\nA.3.1 MOREVISUALIZATIONOFSYNTHETICIMAGESONIMAGENET-LTAND\nTINYIMAGENET-LT\nFig.6showssyntheticimagesofSiTandwiththeproposedCCUAforImageNet-LTtailclasses\nâ€˜bubbleâ€™,â€˜redwineâ€™,â€˜comicbookâ€™andâ€˜yawlâ€™. Fig.7showssyntheticimagesofDDPMandwiththe\nproposedCCUAforTinyImageNet-LTtailclassesâ€˜teapotâ€™,â€˜watertowerâ€™,â€˜pretzelâ€™,â€˜mushroomâ€™,\nâ€˜orangeâ€™,andâ€˜pizzaâ€™. ThesemethodsstartthedenoisingprocessfromthesameGaussiannoiseat\ncorrespondinggridcells. AsshowninFig.6andFig.7,syntheticimagesofCCUAshowconsistently\nhigherdiversityandfidelitycomparedtoSiT.\n15"
      },
      {
        "page": 16,
        "content": "(a)SyntheticimagesfromSiT(Maetal.,2024).\n(b)SyntheticimagesfromCCUA(ours).\nFigure6: SyntheticimagesofSiTandCCUAforImageNet-LTtailclasses(fromtop-lefttoright-\nbottom: â€˜bubbleâ€™,â€˜redwineâ€™,â€˜comicbookâ€™andâ€˜yawlâ€™). Allmethodsstartthedenoisingprocessfrom\nthesameGaussiannoiseatcorrespondinggridcells. CCUAshowsconsistentlyhigherdiversityand\nfidelitycomparedtoSiT.\n16"
      },
      {
        "page": 17,
        "content": "(a)SyntheticimagesfromDDPM(Hoetal.,2020).\n(b)SyntheticimagesfromCCUA(ours).\nFigure7: MoresyntheticresultsforTinyImageNet-LTtailclasses(fromtop-lefttoright-bottom:\nâ€˜teapotâ€™,â€˜watertowerâ€™,â€˜pretzelâ€™,â€˜mushroomâ€™,â€˜orangeâ€™,andâ€˜pizzaâ€™). Imagesincorrespondinggrid\ncellforDDPMandourmethodareinitializedfromthesameGaussiannoise. Ourmethodachieves\nmorediverseimageswithhigherfidelityfortailclasses. Notethatforâ€˜pretzelâ€™andâ€˜orangeâ€™classes,\nDDPMfailstosynthesizeimagescorrelatedtotheclasswhileCCUAsynthesizesdiverseimages\nwithhighquality.\n17"
      },
      {
        "page": 18,
        "content": "(a)Imagesinlow-dimensionalembeddings. (b)Densitydistributionofsyntheticimages.\nFigure8: (a)Visualizationoflow-dimensionalembeddingsoffivetrainingimagesxforatailclass\n(â€˜wormâ€™ in CIFAR100-LT) and synthetic images generated by DDPM (Ho et al., 2020) and our\nmethod. (b)Wealsoshowthedistributionsofrealimagesandsyntheticimagesgeneratedbyour\nmethodandtheoriginalDDPM.\nA.3.2 DISTRIBUTIONOFSYNTHETICIMAGESFORTAILCLASS\nWevisualizesyntheticimagesinthefeaturespaceandtheirdensityfunctionforourmethodand\ntheoriginalDDPM.Specifically,weusetheInception-V3(Szegedyetal.,2016)modeltoextract\n2048dimensionalfeaturesofeachimage,andthenapplyt-SNE(VanderMaaten&Hinton,2008)\ntoproject these featuresinto 2 dimensions. As shownin Fig.8, theoriginalDDPM overfitsand\ngenerateshighlysimilarimages,whilesyntheticimagesbasedonourmethodhavemorediversity.\nWevisualizetheimagedistributionmorespecificallybyusingkerneldensityestimationinthebottom.\nThegreyregionrepresentsthedistributionofrealâ€˜wormâ€™imagesfromtheclass-balancedCIFAR100\ndataset,whileredâ€˜xâ€™pointsdenoteall5â€˜wormâ€™imagesfromtheclass-imbalancedCIFAR100-LT\ndataset. Theblueregionrepresentsthedistributionofimagessynthesizedbyourmethod,whilethe\ngreen region is for the original DDPM. Areas with higher color saturation (dark, green, or blue)\nindicateregionsofhigherdensity,whichcorrespondtomodesofdistribution. Syntheticimagesfrom\nDDPMmostlyconcentratesaroundthetrainingimages,leadingtomodecollapse. Syntheticimages\nfromourmethodspansthespaceenclosedbyalltrainingimages,thedistributionofwhichisshown\ninbluein(b).\nA.3.3 MODECOLLAPSEISSUEONTAILCLASS\nFig.9showssyntheticimagesofbaselinemethodsandourmethodforCIFAR100-LTtailclassesâ€˜roseâ€™\nandâ€˜tableâ€™. AllmethodsstartthedenoisingprocessfromthesameGaussiannoiseatcorresponding\ngrid cells. Red dashed ellipses highlight the mode collapse issues observed in DDPM. With the\nsameinitialnoise,ourmethodgivessyntheticimageswithhigherdiversityandfidelitycomparedto\nDDPM.Forexample,DDPMalwaysgeneratesroseimagescontainingonlyonerose. Ourmethod\ncangenerateanimagecontainingtworoses(seethesixthcolumnofthelastrow). Tofurtherclarify,\nintheâ€˜tableâ€™class,DDPMrepeatedlyproducesnear-identicalimagesashighlightedinredellipses.\nIncontrast,ourmethodproducesamorevariedsetoftableimages. Thissuggeststhatourmethodcan\nbettercapturegreaterdiversityinimagegenerationwhencomparedtoDDPM(Hoetal.,2020). To\nfurtherillustratehowtheproposedmethodmitigatestheissueofoverfitting,wevisualizethetop-10\nnearestneighborsamong1000syntheticimagestoananchorimageinthetrainingsetfortheoriginal\nDDPMandourmethod. AsshowninFig.10,DDPMshowsthemodecollapsetothetrainingimage\nwhileourmethodâ€™sgeneratedimagesshowmuchbetterdiversity. Forexample,inthefirsttworows,\nDDPMgeneratesrepeatedverticalwormswhileourmethodgenerateswormswithdiversedirections.\nInthe9th-10throws,DDPMgeneratestableswithrepeatedmodeswhileourmethodgeneratestables\nwithdifferentstylesandcolors.\nA.3.4 MOREVISUALIZATIONOFREVERSEPROCESSOFDDPMFORDIFFERENTCLASSES\nToillustrateourobservationinSection3.2moreclearly,wedecomposex intoacombinationof\nt\nlow-frequencyimagesandhigh-frequencyimages,asshowninFig11. Thelow-frequencyimages\nwiththesameinitialnoiseareverysimilarfordifferentclassesfortheinitialsteps,whichisalso\nobservedinpriorwork(Sietal.,2024).\n18"
      },
      {
        "page": 19,
        "content": "(a)SyntheticimagesfromDDPM(Hoetal.,2020).Redellipsesshowsimilarimagesgenerated.\n(b)SyntheticimagesfromCCUA(ours)havemorediversityandarelessrepetitive.\nFigure 9: The synthesized results for CIFAR100-LT tail classes â€˜roseâ€™ and â€˜tableâ€™ are shown for\nourmethodandbaselinemethods. AllmethodsinitiatereverseprocessingfromthesameGaussian\nnoiseforimagesatcorrespondinggridcells. Reddashedellipseshighlightmodecollapseissues\nobserved in DDPM. Overall, our method demonstrates higher diversity and fidelity compared to\nDDPMbaseline.\nFigure10: Toseeoverfittingontailclasses,wefindTop-10nearestneighbors(3rdto12thcolumns\nsorted by distances) among 1000 synthetic images to an anchor image (Leftmost column) in the\ntrainingset. KNNisbasedonL distancesofInceptionV3embeddings. Foreachexample,thetop\n2\nrowistheresultsofDDPM,andthebottomrowshowsours. ThenearestneighborsfromDDPM\nshowthemodecollapsetothetrainingimage,whileourmethodgeneratedmorediverseimages.\n19"
      },
      {
        "page": 20,
        "content": "Figure11: Top: reverseprocessstartingfromthesameinitialGaussiannoisebutwithdifferentclass\nconditions(2nd-4throws)orwithoutcondition(1strow). Middle: low-frequencycomponentsof\neachnoisyimage. Bottom: high-frequencycomponentsofeachnoisyimage.\n20"
      },
      {
        "page": 21,
        "content": "B THE USE OF LARGE LANGUAGE MODELS (LLMS)\nWeusedalargelanguagemodeltorefineourwritingandcorrectgrammar,butnottogeneratethe\ninitialdraft. TheLLMservedonlyasasupportivetooltoimprovepresentation,notasasourceof\ncontent.\n21"
      }
    ]
  },
  "pdf_url": "/uploads/d8de67a90832b722167c092bc8e0c883.pdf"
}