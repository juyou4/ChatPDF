{
  "filename": "BoxPaste.pdf",
  "upload_time": "2025-12-02T09:12:32.447388",
  "data": {
    "total_pages": 15,
    "pages": [
      {
        "page": 1,
        "content": "remote   sensing                                                        \n                                                                                  \n                                                                                  \n                                                                                  \n     Article                                                                      \n     BoxPaste:  An   Effective Data  Augmentation     Method    for SAR           \n                                                                                  \n     Ship  Detection                                                              \n                                                                                  \n     ZhilingSuo,YongboZhao* ,ShengChen andYiliHu                                  \n                                                                                  \n                                                                                  \n                       NationalLaboratoryofRadarSignalProcessing,XidianUniversity,Xi’an710071,China\n                       * Correspondence:ybzhao@xidian.edu.cn                      \n                       Abstract:Dataaugmentationisacrucialtechniqueforconvolutionalneuralnetwork(CNN)-based\n                       object detection. Thus, this work proposes BoxPaste, a simple but powerful data augmentation\n                       methodappropriateforshipdetectioninSyntheticApertureRadar(SAR)imagery.BoxPastecrops\n                       shipobjectsfromoneSARimageusingboundingboxannotationsandpastesthemonanotherSAR\n                       imagetoartificiallyincreasetheobjectdensityineachtrainingimage.Furthermore,wedivedeep\n                       intothecharacteristicsoftheSARshipdetectiontaskanddrawaprinciplefordesigningaSARship\n                       detector—lightmodelsmayperformbetter.Ourproposeddataaugmentationmethodandmodified\n                       shipdetectorattaina95.5%AveragePrecision(AP)and96.6%recallontheSARShipDetection\n                       Dataset(SSDD),4.7%and5.5%higherthanthefullyconvolutionalone-stage(FCOS)objectdetection\n                       baselinemethod.Furthermore,wealsocombineourdataaugmentationschemewithtwocurrent\n                       detectors,RetinaNetandadaptivetrainingsampleselection(ATSS),tovalidateitseffectiveness.The\n                       experimentalresultsdemonstratethatournewlyproposedSAR-ATSSarchitectureachieves96.3%\n                       AP, employing ResNet-50 as the backbone. The experimental results show that the method can\n                       significantlyimprovedetectionperformance.                  \n                                                                                  \n                       Keywords:syntheticapertureradar;shipdetection;dataaugmentation;targetdetection\n                                                                                  \n                                                                                  \n     Citation:Suo,Z.;Zhao,Y.;Chen,S.;                                             \n     Hu,Y.BoxPaste:AnEffectiveData                                                \n                       1. Introduction                                            \n     AugmentationMethodforSARShip                                                 \n     Detection.RemoteSens.2022,14,5761. Monitoringandidentifyingmarineshipsisacrucialtaskguaranteeingnationalsecu-\n     https://doi.org/10.3390/rs14225761 rity. Specifically,itplaysavitalroleinmonitoringandmanagingfishingships,combating\n                       smuggling, and protecting marine resources [1]. Synthetic Aperture Radar (SAR) is an\n     AcademicEditor:GerardoDi                                                     \n                       appropriatesensorforshipdetection[2–4]becauseitcancreatehigh-resolutionimages\n     Martino                                                                      \n                       (Figure 1), regardless of the altitude and weather conditions, making ship detection a\n     Received:28September2022 computervisiontask.                                 \n     Accepted:9November2022                                                       \n     Published:15November2022                                                     \n     Publisher’sNote:MDPIstaysneutral                                             \n     withregardtojurisdictionalclaimsin                                           \n     publishedmapsandinstitutionalaffil-                                          \n     iations.                                                                     \n     Copyright: © 2022 by the authors.                                            \n     Licensee MDPI, Basel, Switzerland.                                           \n     This article is an open access article                                       \n     distributed under the terms and                                              \n     conditionsoftheCreativeCommons                                               \n     Attribution(CCBY)license(https:// Figure1.SyntheticApertureRadar(SAR)shipdetectionimagesfromtheSARShipDetectionDataset\n     creativecommons.org/licenses/by/                                             \n                       (SSDD)[5].Greenboxesareground-truthlabelsmanuallyannotated.\n     4.0/).                                                                       \n     RemoteSens.2022,14,5761.https://doi.org/10.3390/rs14225761 https://www.mdpi.com/journal/remotesensing",
        "char_count": 5880,
        "has_tables": false
      },
      {
        "page": 2,
        "content": "RemoteSens.2022,14,5761                                              2of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                          In the past decades, the literature has suggested several SAR ship detection meth-\n                       ods, mainly divided into two categories: traditional and convolutional neural network\n                       (CNN)-based methods. In the former category, the Constant False-Alarm Rate (CFAR)\n                       algorithmanditsvariantsaretheprimaryrepresentativetechniquesoftraditionalSAR\n                       shipdetection[6–10]. Technically,thisapproachestablishesathresholdtoidentifytargets\n                       statisticallyexceedingthebackgroundpixellevelwhileretainingalowfalsealarmrate.\n                       However,traditionalalgorithmsarenotrobusttolightandweatherconditionvariations.\n                       ThesecondcategoryinvolvesCNNs,whichhaverecentlypresentedgreatsuccessinobject\n                       detection [11–17]. Employing CNNs for SAR ship detection is also becoming a trend,\n                       with [18] designing a modified faster region-based CNN (R-CNN) scheme involving a\n                       denselyconnectednetworktosolvethescalevarianceissueinSARshipdetection. Further-\n                       more, [19]introducesaR-CNNtodetectshipswithinSARimagery. Theissueofsmallship\n                       detectionissolvedbyaggregatingcontextualfeaturesfromdifferentlayersandachieving\n                       improved performance. Commonly, the detection speed of ships within a SAR image\n                       isoftenneglectedandthus[20]suggestsalightweightnetworkwithfewerparameters\n                       bymainlyusingdepthwiseseparableCNN(DS-CNN)toachievehigh-speedSARship\n                       detection. Althoughthedetectionperformanceandspeedcontinuouslyimproved,their\n                       scopesarelimitedtomodifyingthenetworkstructure.            \n                          Opposingpreviousworks,thispapernoticesthatthestatisticalcharacteristicsofSAR\n                       shipdataaresignificantlydifferentfromthegeneralobjectdetectiondata. Considering\n                       theSARShipDetectionDataset(SSDD)[5]asanexample,wefirstcalculatethenumber\n                       of images with regard to the number of ships (Figure 2a). Figure 2a highlights that the\n                       dataset involves more than 700 images, each of which contains only one ground-truth\n                       target. The average number of ground-truth targets per image is 2.19, indicating that\n                       objects are sparsely distributed (see also Figure 2a), as most pixels in SAR imagery are\n                       background,revealingitsrelativelylowinformationdensity. Wealsocalculatethesizeof\n                       theshipsinpixels. ThecorrespondingresultsinFigure2bindicatethattheareasoccupied\n                       bymostshipsaresmallerthan2500px(around50×50),whichisabout0.95%ofthepicture,\n                       highlightingthatSARimageryobjectsareverysmallcomparedtoothergeneralobjects.\n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                 Number of Ships               Area of Ships      \n                                   (a)                           (b)              \n                       segami                                                     \n                       fo                                                         \n                       rebmuN                                                     \n                                                  segami                          \n                                                  fo                              \n                                                  rebmuN                          \n                       Figure2. StatisticsoftheSSDDdataset. (a)isthenumberofimageswithregardtothenumberof\n                       ground-truthships.(b)showsthenumberofshipswithregardtotheareaofground-truthships.\n                          Previous research [21] demonstrates that training an object detector is primarily a\n                       learningprocesstoidentifytheobjectsofinterest.Therefore,increasingtheobjectdensityin\n                       eachimageshouldbebeneficialfordetectionperformance.Knowingthesestatisticalcharac-\n                       teristicsinaSARshipdetectiontask,weproposeasimplebuteffectivedataaugmentation\n                       methodnamedBoxPastethatincreasestheobjectdensityineachSARimage. Concretely,\n                       during training, we crop objects from one image using bounding box annotations and\n                       pastethemintoanotherSARimage. Opposingcurrentdataaugmentationmethodssuchas\n                       randomflippingandcolorjittering,theproposedBoxPasteisspeciallydesignedtoincrease\n                       thenumberofground-truthshipsperimageandultimatelyenhancetrainingefficiency.\n                       OurexperimentsdemonstratethatBoxPastegreatlyimprovesthedetectionperformanceon",
        "char_count": 6288,
        "has_tables": false
      },
      {
        "page": 3,
        "content": "RemoteSens.2022,14,5761                                              3of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                       anchor-free(FullyConvolutionalOne-Stage(FCOS)[15])andanchor-based(RetinaNet[14])\n                       detectorsby4.7%and3.2%AP,respectively.                     \n                          Inaddition,SARshipdetectionisasingle-classdetectiontask. However,objectde-\n                       tectorssuchasRetinaNet,FCOS,andFasterR-CNN[11]aredesignedforgeneralobject\n                       detectiontasks,andthusdirectlyapplyingthosedetectorstoSARimageryleadstoover-\n                       fitting. Therefore, wealsointroduceaprinciplefordesigninganappropriateSARship\n                       detectorgovernedbytheconceptthatthelargermodelisnotalwaysthebetter. Following\n                       thisprinciple,wemodifythewell-knownanchor-freeobjectdetectorFCOSanddevelop\n                       itslightervariantentitledSAR-FCOS.ThelatteristwiceasfastasitsFCOSbaselineand\n                       achievesabetterdetectionperformance,demonstratingtheeffectivenessofourmodifica-\n                       tion.                                                      \n                          Insummary,ourcontributionsfromthisworkarethree-fold:    \n                       •  ProposingBoxPaste,aneasybutpowerfuldataaugmentationstrategyforSARship\n                          detection.                                              \n                       •  Developing a principle to designa SAR image detector and proposing a modified\n                          detectorSAR-FCOS.                                       \n                       •  Combiningthetwopreviouscontributionstoachieveagreatdetectionperformance\n                          ontheSSDDdatasetemployingResNet-50[22]asthebackbone.    \n                          Therestofthisarticleisorganizedasfollows.Thesecondsectionintroducestherelated\n                       workofSARshipdetection.Section3detailstheproposedmethods,includingSAR-FCOS\n                       andourproposedBoxPastedataenhancementstrategy.InSection4,theexperimentalresults\n                       andcorrespondinganalysisareprovided,andsomeconclusionsaremadeinSection5.\n                       2. RelatedWorks                                            \n                       2.1. TraditionalMethods                                    \n                          ThemostwidelyusedtraditionalshipdetectionalgorithmsaretheCFARalgorithm\n                       and its improved variants [6–10]. These methods set a threshold to detect statistically\n                       significant targets exceeding the background pixel while maintaining a constant false\n                       alarm rate. Concretely, [6] proposes a technique that computes the cross-correlation\n                       valuesbetweentwoimagesextractedbyslidingasmall-sizedwindowonthemulti-view\n                       SARintensity(oramplitude)imagery,producingacoherentimage. Furthermore,ref. [7]\n                       suggestsanewCFAR-basedshipdetectionalgorithmthatconsidersthenormaldistribution\n                       oftwo-dimensionaljointlogs,while[8]developsashipdetectionmethodbasedonfeature\n                       analysisforhigh-resolutionSARimages. Theauthorof[9]introducesabilateralCFAR\n                       algorithmforshipdetectioninSARimages,reducingtheinfluenceofSARambiguitiesand\n                       seaclutterbycombiningtheSARimages’intensityandspatialdistribution. Duetothe\n                       highsimilaritybetweentheharbor’sandship’sbodygrayandtexturefeatures,traditional\n                       methodscannoteffectivelydetectinshoreships. Thus,ref. [10]presentsanovelsaliency\n                       andcontextinformationapproachdealingwiththisissue. SinceCFARanditsimproved\n                       variantsseverelyrelyonthepresetdistributionormanuallydefinedcharacteristics,their\n                       adaptiveabilityisweak.                                     \n                                                                                  \n                       2.2. DeepLearning-BasedMethods                             \n                          WiththedevelopmentofdeeplearningtechnologyandtheestablishmentofaSARship\n                       database[5,23,24],manyshipdetectionalgorithmsbasedonconvolutionalneuralnetworks\n                       haveemerged. Forexample,ref. [25]introducesFasterR-CNNforshipdetectioninSAR\n                       imageryandsolvestheissueofsmallshipdetectionbyaggregatingcontextualfeatures\n                       fromdifferentlayers,achievingimprovedperformance. Theworkof[26]introducesanew\n                       networkarchitecture,namedYouOnlyLookOnceversion2(YOLOv2)-reduced,whichhas\n                       alowerdetectiontimethanYOLOv2[27]onanNVIDIATITANXGPU.Aimingatthe\n                       problemthatthedetectionspeedofSARshipsisoftenneglectedatpresent,abrand-new\n                       lightweightnetwork[20]isestablishedwithfewernetworkparametersbymainlyusingDS-\n                       CNNtoachievehigh-speedSARshipdetection,whichcanachievehigh-speedandaccurate\n                       shipdetectionsimultaneouslycomparedwithothermethods. In[28],theauthorsdevelopa",
        "char_count": 6049,
        "has_tables": false
      },
      {
        "page": 4,
        "content": "RemoteSens.2022,14,5761                                              4of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                       novelshipdetectionmethodbasedonahigh-resolutionshipdetectionnetwork(HR-SDNet)\n                       appropriate for high-resolution SAR images. This method is more accurate and robust\n                       for inshore and offshore ship detection of high-resolution SAR imagery. A two-staged\n                       detectornamedAttentionReceptivePyramidNetwork(ARPN)[29]issuggestedtoimprove\n                       detectingmulti-scaleshipsinSARimagesbyenhancingtherelationshipsamongnon-local\n                       featuresandrefininginformationatdifferentfeaturemaps. Thisstrategyiseffectivefor\n                       scenesofvarioussizesandcomplexbackgrounds. Toalleviatetheexcessivecomputational\n                       burdenandincreasedhyper-parametercardinalityproblems,ref.[30]suggestsanefficient\n                       and low-cost ship detection network for SAR imagery. This work utilizes an anchor-\n                       freeSARshipdetectionframeworkcomprisingaboundingboxregressionsub-netand\n                       a score map regression sub-net based on a simplified U-Net. This pipeline achieves a\n                       verycompetitivedetectionperformancewhilebeingextremelylightweight. Animproved\n                       algorithmbasedonCenterNet[31]hasalsobeenproposed[32]thatissignificantlybetter\n                       thanCenterNetforsmallshipdetectioninlow-resolutionSARimagery,addinglow-level\n                       featurerepresentationtothepyramidsforsmallobjectdetectionandoptimizingtheheadof\n                       detectortoeffectivelydistinguishforegroundfrombackground. Finally,ref.[33]introduces\n                       ananchorlessconvolutionnetworkaggregatinganintensiveattentionfunctionthatobtains\n                       higherprecisionandisfastertoexecutethanthemainstreamdetectionalgorithms.\n                       2.3. DataAugmentationforObjectDetectionandInstanceSegmentation\n                          Reference[34]leveragessegmentationannotationstoincreasethenumberofobject\n                       instancesby appropriately modelingthe visualcontextsurroundingobjects. The work\n                       of[35]automaticallyextractsobjectinstancemasksandrendersthemonrandomback-\n                       ground images. Mixup [36] randomly extracts two images from the training set and\n                       then performs a linear weighted summation of the pixel values of the extracted image\n                       data. Atthesametime,theOne-hotvectorlabelscorrespondingtothesamplesarealso\n                       weightedandsummed. Inthisway,anewimagewithafuzzyclassificationboundarycan\n                       beobtained,enhancingthegeneralizationabilityofthemodel. CutMix[37]replacesthe\n                       removedregionswithapatchfromanotherimageandchangesthegroundtruthlabels\n                       bythenumberofpixelsofthecombinedimages. Byrequiringthemodeltorecognizethe\n                       targetfromalocalperspective,thelocalizationabilitycanbeenhanced. CutMixisusually\n                       usedforclassificationtasksandisnotsuitablefordetectiontasksbecauseitusuallycrops\n                       imagepatchesrandomly,whichrequiresthattheimagedoesnotcontaintoomuchcontext.\n                       While[38]highlightsthatCopyPaste,i.e.,simplypastingobjectsrandomly,providessolid\n                       gains on the detectors’ performance. While being similar to BoxPaste in this work, we\n                       arguethatourworkisthefirsttomigratethekeyideologyofCopyPasteneglectingthe\n                       unnecessaryusageofinstancemaskannotationsintheSARshipdetectiontask.\n                       3. Methods                                                 \n                          ThissectionfirstrevisitsFCOS[15],awell-knownone-stagedanchor-freeobjectdetec-\n                       tor,whichwillbeusedasourbaselineinthispaper. ConsideringthatobjectsinaSARship\n                       detectionscenarioarestatisticallysmallandsparse,welightenthestructureofFCOSfrom\n                       theFeaturePyramidNetworks(FPN)[39]tothedetectionheadandsuggestSAR-FCOS.\n                       Finally, this section introduces our proposed BoxPaste, a powerful data augmentation\n                       strategyforSARshipdetection.                               \n                                                                                  \n                       3.1. RevisitingFCOS                                        \n                          Althoughanchor-basedobjectdetectorshaveachievedmassivesuccessonmanyobject\n                       detectiondatasets,theysufferfromtherequirementtodesignanchorboxes. Furthermore,\n                       theenormousamountofdetectionproposalsdramaticallyslowsdownthepost-processing\n                       method,prohibitingtheanchor-basedmechanismfromreal-timeapplications. Recently,\n                       anchor-freeobjectdetectorshaveshowngreatpotentialingeneralobjectdetection. They\n                       usuallyattainhigherperformancethantheircounterpartanchor-baseddetectorswhile",
        "char_count": 6170,
        "has_tables": false
      },
      {
        "page": 5,
        "content": "RemoteSens.2022,14,5761                                              5of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                       enjoying a more straightforward architectural design. Hence, this work considers the\n                       well-knownanchor-freeobjectdetectorFCOSasthebaselinemethod.\n                          Like other standard object detectors, FCOS comprises three parts: a backbone for\n                       featureextraction,FPNforfeatureintegration,andadetectionheadforprediction. Figure3\n                       illustratestheoverallFCOSstructure. Asthebackbonestructure,VisualGeometryGroup\n                       (VGG)[40],ResNet[22],Inception[41],oranyotherwell-knownarchitecturesdesignedfor\n                       classificationcanbeexploited.                              \n                          FPNcomprisesasequenceoftop-downlayersandseveralshortcutlayerstocombine\n                       theknowledgeencodedatdifferentlayers,whichisalsoabroadlyusedstructureinobject\n                       detection. ThepredictedresultsencodedbythedetectionheadofFCOSaredifferentfrom\n                       other anchor-based object detectors. Unlike RetinaNet, Single Shot MultiBox Detector\n                       (SSD)[13],YOLOv3[12],andFasterR-CNN,whichuseanchorboxes,FCOSdirectlyviews\n                       locationpointsastrainingsamplesandlearnstopredictthefouroffsetsfromeachlocation\n                       to the bounding boxes, i.e., left, top, right, and bottom (l∗, t∗, r∗, b∗). Concretely, the\n                       boundingboxregressiontargetsforlocation(x,y)isdefinedas:   \n                                           l∗ = x−x ,r∗ = x −x,                   \n                                                 0    1                           \n                                                                            (1)   \n                                           t∗ = y−y ,b∗ = y −y,                   \n                                                 0    1                           \n                       where(x ,y ),(x ,y )arethecoordinatesoftheleft-topandright-bottomcornersofthe\n                            0 0  1 1                                              \n                       ground-truthboundingbox. Inadditiontotheregressionprediction,onealsoneedsto\n                       knoweachlocation’scategory. Regardingclassification,ifalocationfallsintotheground-\n                       truthboundingbox,thatlocationisconsideredapositivesampleandisresponsiblefor\n                       predictingthatgroundtruth. Moreover,theFPNinthestandardFCOStypicallycontains\n                       fivelevels,i.e.,fromP3toP7,whiletoconstructvalidreceptivefieldscalesfortheneurons\n                       atdifferentFPNlevels,differentFPNlevelsareregressingdifferentobjectssizes. Giventhe\n                       regressiontargetsl∗,t∗,r∗,andb∗ foralocation,theobjectscaleperfeaturepyramidlevel\n                       followsthefollowingconstraint:                             \n                                          max(l∗ ,t∗ ,r∗ ,b∗) > m,                \n                                                        i                         \n                                                                            (2)   \n                                          max(l∗ ,t∗ ,r∗ ,b∗) < m i+1,            \n                       wherem,i ∈2,3,4,5,6,7intheoriginalFCOSaresetas0,64,128,256,512,and∞,respectively.\n                            i                                                     \n                          Tosuppressthelow-qualitypredictedboundingboxesgeneratedbythelocationsfar\n                       awayfromtheobjects’center,FCOSadoptsthecenternessbranch. Thecenternessscoreof\n                       eachlocationx,y andthecorrespondingboundingboxatfeatureleveliisdefinedas:\n                                i i                                               \n                                              (cid:115)                           \n                                               min(l∗,r∗) min(t∗,b∗)              \n                                     centerness∗ =    ×        .            (3)   \n                                               max(l∗,r∗) max(t∗,b∗)              \n                          Thecenternessscoreisthenmultipliedbytheclassificationscoretoprovidethefinal\n                       predictedconfidenceusedbytheNon-Maximum-Suppression(NMS).Itshouldbenoted\n                       that the pre-processing (e.g., normalizing images) and post-processing (e.g., decoding\n                       outputs,NMS)methodsofFCOSarethesameasinotherstandardobjectdetectors.\n                       3.2. SAR-FCOS                                              \n                          Unlikegeneralobjectdetectiontasks,objectsinSARshipdetectionscenariosusually\n                       have two features: the objects’ sizes are relatively small, and there is only one object\n                       category. Suchfeaturesrequiredesigninganewnetworkstructure,andthusthissection\n                       simplifiestheFCOSstructureandproposesSAR-FCOS.ComparedtotheoriginalFCOS,\n                       thecomplexityofSAR-FCOSisseverelyreduced,aimingatpreventingthenetworkfrom\n                       overfitting. Specifically,weonlymodifytheFPNanddetectionheadstructure,asthesetwo\n                       partsarethemaindifferencebetweendetectionandperformingothertaskssuchasimage",
        "char_count": 5989,
        "has_tables": false
      },
      {
        "page": 6,
        "content": "RemoteSens.2022,14,5761                                              6of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                       classificationandsegmentation. Moreover,wewishtoemphasizethatourmotivationis\n                       nottodesignalightweightdetectorforshipdetectioninSARimagesbuttohighlightthe\n                       criticalmeritofdesigninganappropriatedetectorfortheSARshipdetectiontask.\n                       3.2.1. LightFPN                                            \n                          In[39],theauthorsdevelopedtheFPNtohandlethelarge-scalevarianceingeneral\n                       objectdetectiontasks. ThecriticalinsightbehindFPNisthattheneurons’validreceptive\n                       fieldsatthedeeplayersaresignificant,andattheshallowlayers,thesearetypicallysmall.\n                       Henceclassifyingdifferentobjectsizesondifferentfeaturepyramidlevelscanbenefitfrom\n                       thescalealignmentbetweentheobjects’sizesandneurons’validreceptivefields.\n                                                                                  \n                                                                                  \n                                   P7      Head                                   \n                                   P6      Head                                   \n                          C5       P5      Head      C5       P5      Head        \n                                                                                  \n                          C4      P4       Head      C4      P4       Head        \n                                                                                  \n                         C3       P3                C3       P3                   \n                                           Head                       Head        \n                                                                                  \n                                                                                  \n                                                                                  \n                          Backbone Feature Pyramid DetectionHead Backbone Feature Pyramid DetectionHead\n                                 (a)FCOS                   (b)SAR-FCOS            \n                                                                                  \n                                                                                  \n                                        Classification            Classification  \n                                        H(cid:1)W(cid:1)C          H(cid:1)W(cid:1)C\n                                        Center-ness               Center-ness     \n                               (cid:1)4 H(cid:1)W(cid:1)1 (cid:1)2 H(cid:1)W(cid:1)1\n                           H(cid:1)W(cid:1)256 H(cid:1)W(cid:1)256 H(cid:1)W(cid:1)64 H(cid:1)W(cid:1)64\n                                        Regression                Regression      \n                                        H(cid:1)W(cid:1)4          H(cid:1)W(cid:1)4\n                               (cid:1)4                  (cid:1)2                 \n                           H(cid:1)W(cid:1)256 H(cid:1)W(cid:1)256 H(cid:1)W(cid:1)64 H(cid:1)W(cid:1)64\n                               (c)FCOSDetectionHead     (d)SAR-FCOSDetectionHead  \n                       Figure3.AdetailedillustrationofthestructuraldifferencebetweentheFullyConvolutionalOne-\n                       Stage(FCOS)andSAR-FCOS.                                    \n                          However,asstatedinSection1,theships’scalesinaSARdetectiontaskaresmall.\n                       Hence,byfollowingtheassigningruleofFCOS,onlyafewlargeobjectswillbeassignedto\n                       P6andP7,i.e.,thefeaturemapsP6andP7havelittlecontributiontotheaccuracyduring\n                       testingbutimposeahighcomputationalcost. Therefore,P6andP7areredundant,and\n                       weremoveboththeselayersandtheircorrespondingdetectionhead. Ourmodelinvolves\n                       fewer FPN layers, and, therefore, our light FPN executes faster, exploiting some extra\n                       modules to enhance the feature integration between the left layers. Knowing that the\n                       ground-truthobjectsaremainlyassignedtotheP3,P4,andP5layers,weusetheAdaptive\n                       SpatialFeatureFusion(ASFF)[42]tofusedifferentknowledgeencodedbetweendifferent\n                       FPNlayers.                                                 \n                       3.2.2. LightDetectionHead                                  \n                          Theoriginalstructureandhyper-parametersaredesignedfortheCommonObjects\n                       inContext(COCO)[43]benchmarkcontaining80categoriesofvariousobjects. However,\n                       thereisonlyoneShipcategoryintheSSDDdataset,makingthenumberofheadchannels\n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n\n[表格内容]\nBackbone | Feature Pyramid | DetectionHead\n\nBackbone | Feature Pyramid | DetectionHead",
        "char_count": 5851,
        "has_tables": true
      },
      {
        "page": 7,
        "content": "RemoteSens.2022,14,5761                                              7of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                       (CHead) and head blocks (NHead) redundant in the original network. Therefore, it is\n                       necessary to reduce the complexity of the detection head to prevent over-fitting. Our\n                       trialsinvolveextensiveexperimentswithdifferentheadparameters,andspecifically,we\n                       reducetheCHeadandNHeadfrom256/4to64/2,affordingabettertrade-offbetween\n                       latencyanddetectionperformance. Despiteourmodificationsbeingsimple,thesearenot\n                       trivial. In the experimental section, we demonstrate that the simplified detection head\n                       reducesthedetector’scomplexityandsurprisinglyimprovesthedetectionperformance,\n                       indicatingthesevereover-fittingoftheoriginalFCOSfortheSARshipdetectiontasks.\n                       Suchaphenomenonrevealstheimportanceofdesigningspecificmodelstructuresand\n                       hyper-parameters for a specific task. Our modified FCOS is named SAR-FCOS and is\n                       illustratedinFigure3.                                      \n                       3.3. BoxPaste                                              \n                       3.3.1. RevisitingCopyPaste                                 \n                          Dataaugmentationaimstoincreasethetrainingdataset’svariability,acriticalcompo-\n                       nentduringobjectdetectortraining,leadingtosignificantimprovementsinobjectdetection\n                       tasks. ThemostrecentandeffectiveaugmentationmethodisCopyPaste. Byrandomly\n                       croppingobjectsfromimageAutilizingground-truthmasksandpastingthemonimage\n                       B, CopyPaste creates more training samples and increases the number of ground-truth\n                       samplespermini-batch,whicharecrucialfortrainingobjectdetectors[21]. Asimpleillus-\n                       trationofCopyPasteispresentedinFigure4. CopyPasteaffordsaremarkableperformance\n                       gainoninstancesegmentationtasks,whileadditionally,italsosignificantlyimprovesthe\n                       performanceofobjectdetection. SpurredbytheadvantagesofCopyPaste,naturally,the\n                       followingquestionisraised: canwebringCopyPasteintotheSARshipdetectiontask?\n                                                                                  \n                       3.3.2. BoxPaste                                            \n                          A straightforward method is applying CopyPaste without instance segmentation\n                       masks and employing bounding boxes to crop objects. However, the objects’ scales in\n                       general object detection tasks vary greatly, and thus, the proposed method will lead to\n                       heavyocclusionbetweentheoriginalandpastedobjects.          \n                          However,therearethreepropertiesofSARshipdetectiontasks(consideringtheSSDD\n                       datasetasanexample). (1)Mostoftheshipsaresmall. (2)Thenumberoftargetshipsin\n                       eachimageisquitelimited. Therefore,mostpixelsinanimagearebackground,making\n                       each mini-batch less informative. (3) The diversity of backgrounds for SAR images is\n                       extremelylessthannaturalRGBimages; hence,SARimagescanbeeasilyconvertedto\n                       gray-scaleimages. Unlikegeneralobjectdetectiontasks,thesethreefeaturessuggestthat\n                       usingboundingboxestoperformCopyPastecancreatemorerealistictrainingsamples.\n                       WenamethismethodBoxPaste. Aclearillustrationofthedifferencebetweenapplying\n                       BoxPasteinSARshipdetectionandgeneralobjectdetectionisshowninFigure4.\n                          Inthefollowingexperiment,wedemonstratethatalthoughtheproposeddataaug-\n                       mentationschemeBoxPasteissimple,itsubstantiallyimprovestheSARshipdetection\n                       performance,revealingitsgreatvalue. Toprovideaclearimageofthetrainingsamples\n                       createdbyBoxPaste,wepresentmoreexamplesinFigure5.",
        "char_count": 5962,
        "has_tables": false
      },
      {
        "page": 8,
        "content": "RemoteSens.2022,14,5761                                              8of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                       (a) CopyPaste                              \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                       (b) BoxPaste                               \n                       Figure4. AnillustrationofCopyPaste[38]andourproposedBoxPaste. (a)isborrowedfromthe\n                       originalpaper.(b)showsacombinationoftwotrainingimagesintheSSDDdataset.Notethatfor\n                       CopyPaste,ground-truthinstancemasksarerequired.However,applyingBoxPasteintheSARship\n                       detectiontaskonlyrequirestheboundingboxannotations.        \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                       Figure5.ExamplesofthecreatedtrainingimagesbyBoxPaste.Bycopyingtheboundingboxesof\n                       themiddleimageandpastingthemintothetopimage,wegetthebottomimage.Onecanseethat\n                       BoxPastecaneffectivelyincreasethenumberofground-truthshipobjectsperimage.",
        "char_count": 5433,
        "has_tables": false
      },
      {
        "page": 9,
        "content": "RemoteSens.2022,14,5761                                              9of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                       4. Experiment                                              \n                          ThissectionevaluatestheeffectivenessofourproposedSAR-FCOSnetworkandthe\n                       BoxPastedataaugmentationmethodontheSSDDdataset.            \n                       4.1. DatasetandTrainingDetails                             \n                                                                                  \n                          SSDD[5]isapublicdatasetforSARshipdetection. Itcontains1160imagespresenting\n                       2540ships. Aswementionedatthebeginningofthispaper,theshipobjectsinthisdataset\n                       aresparselydistributed,whileaccordingtoourcalculation,morethan700imagescontain\n                       only one object, i.e., over 60% of all images. The average number of ships per image is\n                       2.19. WealsomeasuretheoverlapbetweenshipsusingtheIntersectionoverUnion(IoU)\n                       metric. TheSSDDhasanaverageIoUinvolvingoverlappingshipsof0.0048,illustrating\n                       thatthedataset’sobjectsaresparse. Wedividethetrainingandtestsetaccordingtotheway\n                       theauthoroftheSSDDdatasetdivided;thatis,thetrainingsetandthetestsetweresplit\n                       accordingtotheimages’name,i.e.,imageswiththeirnameendingwith1or9belongtothe\n                       testset,whiletheremainingimagesbelongtothetrainingset. Toevaluatethedetector’s\n                       performance,weadoptaverageprecision(AP)andrecall[44]. Precisionistheproportionof\n                       accuratelypredictedshipsinallforecasts,andrecallistheproportionofaccuratelypredicted\n                       shipsinallground-truthships,bothofwhicharethemostwidelyusedindicators. TheAP\n                       metricisusedtoevaluatethecomprehensiveperformanceofthedetector.Itcanbeobtained\n                       bysortingtheoutputresultsindescendingorderofthedetectionconfidence,drawingthe\n                       precision-recallcurve,andcalculatingtheareaofthecurve. Inthispaper,APiscalculated\n                       whentheIoUthresholdis0.5.                                  \n                          Becauseourworkfocusesonthenetworkdesigningstrategyanddataaugmentation\n                       method,whichisabletobeappliedtoeverydetectionalgorithm,wechooseFCOSasa\n                       simplebaseline. ThetrialscombinetheproposedFCOSwithResNet-50asourbaseline\n                       backbonenetwork,utilizingthepre-trainedonesfromImageNet[45]asinitialparameters.\n                       AllmodelsaretrainedononeNVIDIAGeForceGTX1080Tiinvolvingastochasticgradient\n                       descent (SGD) for 12 epochs. The initial learning rate is 0.01, which reduces by 10 at\n                       the8thand11thepochs. Theweightdecayfactorandmomentumaresetto0.0001and\n                       0.9, respectively, whiletheinputimagesareresizedto[512, 512]. InBoxPaste, boththe\n                       croppedimagepatchesfromtheoriginalimagesandthetargetimagesarerandomlyflipped\n                       horizontallyataratioof50%.                                 \n                       4.2. AblationStudy                                         \n                       4.2.1. ExperimentsonSAR-FCOS                               \n                          Initially,weinvestigatetheperformanceofSAR-FCOSfromtheperspectiveofevalu-\n                       atingthelightdetectionhead,lightFPN,andtheASFFfeatureintegrationmodule. Table1\n                       highlightsthattheFCOSbaselineachieves90.8%mAPat53.1framespersecond(FPS).\n                       Then,wereducethenumberoftheconvolutionlayersinthedetectionhead,i.e.,NHead,\n                       fromfourtotwo,andfindthatmAPreducesonlyby0.4%whileaffordinga13FPSim-\n                       provement,whichisanacceptabletrade-offbetweenperformanceandlatency. Afterthat,\n                       wereducethenumberofchannelsinboththedetectionheadandFPN,i.e.,CHead,from\n                       256 to 128. As expected, reducing the parameters alleviates over-fitting, increasing the\n                       detectionperformanceto91.1%mAP.                            \n                          FurtherreducingtheCHeadto64and32indicatesthatforCHead=64weobtain\n                       the best performance of 92.4% mAP at 101.2 FPS. These modifications highlight that\n                       alteringonlyafewhyper-parametersinthedetectionheadalmostdoublestheSARship\n                       detection speed while increasing performance by 1.6% mAP compared to the baseline\n                       network. Suchaphenomenonstronglyvalidatesourdesignintuitionforanappropriate\n                       SAR ship detector. Namely, the SAR ship detection dataset is easy to get over-fitted,\n                       therefore, it is better to use light models than heavy models. Note that we did not try\n                       differentbackbonesbecauseourgoalistoexpressthekeyideologyofdesigningappropriate\n                       detectorsforSARshipdetection,nottothoroughlyexploreeverycombinationofdifferent\n                       componentsindetectors.",
        "char_count": 6094,
        "has_tables": false
      },
      {
        "page": 10,
        "content": "RemoteSens.2022,14,5761                                             10of15   \n                                                                                  \n                                                                                  \n                       Table1.AblationstudyonthelightdetectionheadandthelightFeaturePyramidNetwork(FPN)in\n                       SAR-FCOS.                                                  \n                                                                                  \n                                 NHead  CHead    FPN    AP(%)  Recall(%) FPS      \n                         FCOS                                                     \n                                  4      256    P3–P7    90.8   91.1    53.1      \n                        (baseline)                                                \n                                  2      256    P3–P7    90.4   90.5    66.3      \n                                  2      128    P3–P7    91.1   91.5    96.3      \n                        LightHead                                                 \n                                  2       64    P3–P7    92.4   92.4    101.2     \n                                  2       32    P3–P7    89.2   89.4    102.8     \n                                  2       64    P3–P5    92.3   91.9    119.9     \n                        LightFPN                                                  \n                                  2       64    P3–P4    90.1   90.3    127.6     \n                        LightHead                                                 \n                                  2       64    P3–P5    93.0   94.4    110.1     \n                        +ASFF[42]                                                 \n                          AswestatedinSection3,shipobjectsintheSSDDdatasetarerelativelysmall,and\n                       mostofthemareassignedtoP3toP5levelsduringtheFCOSlabelassignment. Hence,\n                       P6andP7levelscontributelesstothefinalperformance. ThelastthreerowsinTable1\n                       presentourablationstudyinvolvingdifferentFPNlevels. Specifically,byremovingtheP6\n                       andP7levels,mAPdropsbyonly0.1%,butFPSincreasesbynearly20. However,further\n                       removingtheP5level,thedetectionperformancedropsby2.2%,indicatingthattheP5level\n                       isessential. Affordingadetectorthatistwiceasfastasthebaseline,wehavetheoption\n                       toaddextramodulessuchasASFForattentionmechanisms[46,47]. Thisworkenhances\n                       featureintegrationbyadoptingASFFbecausetheparameterreductionoccursintheFPN\n                       anddetectionhead. Indeed,thelastrowinTable1highlightsthatbyemployingASFF,our\n                       finalSAR-FCOSmodelattains93.0%mAPat110.1FPSontheSSDDdataset,whichisfaster\n                       andmorerobustthantheoriginalFCOSbaseline.                  \n                       4.2.2. AblationStudyonBoxPaste                             \n                          We also investigate the effect of BoxPaste from two aspects: scale jittering and total\n                       trainingepochs. Performingscalejitteringontwocombinedimagesisborrowedfromthe\n                       originalCopyPastepaper.Forexample,ifthescalejitteringrangeis[0.1,2],i.e.,thesampled\n                       image’ssizeis[int(512×0.1),int(512×2)],weperform,ifnecessary,paddingandcropping\n                       toaligntheimagesizeto512. Itiswellknownthatthegreaterthedataaugmentation,the\n                       longer the convergence time during training. Hence, to explore the upper bound of our\n                       BoxPasteonSAR-FCOS,weexploreavariousnumberoftrainingepochs. Table1shows\n                       thatourSAR-FCOSmodelachieves93.0%APand94.4%recall. However,whenapplying\n                       BoxPaste,theAPmetricimprovesby1.1%,reaching94.1%(Table2).Ifweapplyscalejittering\n                       from0.5to1.5,themAPfurtherimprovesto94.5%,andforscalejitteringwithintherange\n                       [0.1,2],ourmodelachieves94.6%APand95.5%recall. ItshouldbenotedthatBoxPasteis\n                       onlyusedduringtraining.Hence,thetestingFPSisnotaffected.   \n                          Thenweinvestigatetheeffectsofthetotaltrainingepochs.Theresultsdemonstratethat\n                       trainingthedetectorsfor36epochsyieldsthebestresults,whilewhenexceeding36epochs,\n                       themodeloverfits,reducingperformance.Figure6visualizesthedetectionresultstoprovide\n                       anintuitiveunderstandingoftheimprovedperformance,highlightingthattheSAR-FCOS\n                       modelcombinedwithBoxPasteyieldsfewermisseddetectionsandhigherrecall.",
        "char_count": 5810,
        "has_tables": false
      },
      {
        "page": 11,
        "content": "RemoteSens.2022,14,5761                                             11of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                               FCOS         SAR-FCOS w/ BoxPaste  GT              \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                       Figure6.VisualizationofthedetectionresultsofFCOS,SAR-FCOSwithBoxPaste,andground-truth.\n                       Thecyan,red,yellow,andgreenboxesindicatethetestresultsofFCOS,theshipobjectmissedthe\n                       testresultsofours,andthegroundtruth.                       \n                       Table2.AblationstudyonBoxPastewithregardtoscalejitteringandtrainingepochs.\n                                                                                  \n                                ScaleJit. Total                                   \n                                                BoxPaste AP(%) Recall(%) FPS      \n                                 Range  Epochs                                    \n                                  -       12      -      93.0   94.4              \n                                  -       12      (cid:88) 94.1 95.1              \n                                [0.5,1.5] 12      (cid:88) 94.5 95.2              \n                        SAR-FCOS [0.1,2]  12      (cid:88) 94.6 95.5    110.1     \n                                 [0.1,2]  24      (cid:88) 95.2 96.5              \n                                 [0.1,2]  36      (cid:88) 95.5 96.6              \n                                 [0.1,2]  48      (cid:88) 95.0 96.1              \n                       4.2.3. ComparingBoxPastetoCopyPaste                        \n                          SincetheSSDDdatasetprovidessegmentationannotation,wealsoperformCopyPaste\n                       dataaugmentationonSSDD.TheexperimentalresultsareshowninTable 3. Itshowsthat\n                       CopyPastemarginallyimprovesAPby0.3%,whichmaybeduetoCopyPaste’sprecise\n                       croppingoftheobject. Meanwhile,becausemostoftheSARshipdatabasebackgroundis\n                       simpleandclean,theperformanceofusingBoxPasteisveryclosetoCopyPaste. Sincethe\n                       segmentationmaskismoredifficulttoobtainthantheobjectbox,makingBoxPastemore\n                       feasibleinthereal-worldscenario.",
        "char_count": 5466,
        "has_tables": false
      },
      {
        "page": 12,
        "content": "RemoteSens.2022,14,5761                                             12of15   \n                                                                                  \n                                                                                  \n                       Table3.PerformancecomparisonbetweenBoxPaste+BBox-SSDDandCopyPaste+PSeg-SSDD.\n                                                                                  \n                                                AP(%)            Recall(%)        \n                          BoxPaste+BBox-SSDD     95.5              96.6           \n                          CopyPaste+PSeg-SSDD    95.8              97.0           \n                                                                                  \n                       4.3. WideApplicability                                     \n                          Thetrialspresentedintheprevioussub-sectionssolelyreliedonFCOS.Hence,weval-\n                       idateourmethods’broadapplicabilityinthissub-sectionbycombiningitwithRetinaNet\n                       andAdaptiveTrainingSampleSelection(ATSS)[48]. WhileFCOSisananchor-freeone-\n                       stagedetector,RetinaNetisanchor-based,andATSSadoptsanadvancedlabel-assigning\n                       strategy. BecauseRetinaNetandATSSleveragedifferentlabel-assigningstrategiesthatare\n                       probablymoresuitableinSARshipdetectiontasks,therefore,theirbaselineperformances\n                       arehigherthanFCOS.Onthesedetectors,weapplyboththelighthead/FPNandBoxPaste,\n                       whileinanycase,ResNet-50isthebackbonemodel. Thecorrespondingexperimentalre-\n                       sultsarepresentedinTable4,highlightingthatRetinaNetandATSSattain92.6%and94.4%\n                       AP,farbetterthanFCOS(90.8%AP).PossiblyduetoRetinaNetbeingananchor-based\n                       detector,whileATSSusesanadvancedlabel-assigningstrategy. Bothdetectorsincrease\n                       the number of positive training samples per mini-batch. Nevertheless, using the light\n                       head/FPNstillimprovestheirAPby0.7%and0.5%,respectively. AfteradoptingBoxPaste,\n                       thedetectionperformanceisfurtherimprovedto95.8%and96.3%,respectively,validating\n                       theapplicabilityofourproposedmethodsondifferentdetectors. Thenewdetectors,i.e.,\n                       SAR-RetinaNetandSAR-ATSS,arecomparedagainstotherstate-of-the-artSARshipde-\n                       tectorsonSSDD,utilizingResNet-50asthebackbonenetwork. Thecounterpartsinclude\n                       one-stagedetectors,suchasATSS,aswellaspowerfultwo-stagedetectors,suchasFaster\n                       R-CNNandCascadeR-CNN.TheresultsinFigure7showthatSAR-ATSSachieves96.3%\n                       AP,surpassingallotherpreviouswork.                         \n                                                                                  \n                       Table4.Theeffectsoflighthead/FPNandBoxPasteonRetinaNetandAdaptiveTrainingSample\n                       Selection(ATSS).                                           \n                                       RetinaNet                ATSS              \n                                                                                  \n                                Baseline +Light +BoxPaste Baseline +Light +BoxPaste\n                         AP(%)   92.6    93.3    95.8    94.4   94.9    96.3      \n                        Recall(%) 94.1   94.2    96.5    95.5   95.5    97.0      \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                          0.97                                        0.963       \n                                        0.959                    0.958            \n                          0.96               0.953                                \n                          0.95     0.944          0.947 0.946 0.949               \n                          0.94                                                    \n                          0.93 0.926                                              \n                          0.92                                                    \n                          0.91                                                    \n                           0.9                                                    \n                        PA                                                        \n                                                                                  \n                                                                                  \n                       Figure7.CombiningtheproposedmethodsonRetinaNetandATSSandcomparingthemagainst\n                       currentstate-of-the-artmethods.                            \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n\n[表格内容]\n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  | 0.953\n0.947 0.946 0.949 |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |",
        "char_count": 6176,
        "has_tables": true
      },
      {
        "page": 13,
        "content": "RemoteSens.2022,14,5761                                             13of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                       5. Conclusions                                             \n                          Thecharacteristicsofthedatasetareimportantforappropriatedetectionarchitecture\n                       designandtrainingrecipes. InSARshipdetection,objectsize,density,andbackground\n                       diversityareessentiallydifferentfromgeneralobjectdetection,suchasVOCandCOCO,\n                       motivatingustoexploredomain-specifictechniquesinit. Inthiswork,wefirstpresent\n                       BoxPaste,asimplebuteffectivedataaugmentationmethodforSARshipdetectionthat\n                       cropstheshipobjectsfromonetrainingimageusingboundingboxannotationsandpastes\n                       themonanotherimage. Despiteitssimplicity,BoxPastesignificantlyimprovesitsbaseline\n                       by 4.7% mAP. Given the SAR ship image dataset characteristics, we also introduce a\n                       principlefordesigningaSARshipdetector,i.e.,alargermodeldoesnotguaranteebetter\n                       performance. Withthisprinciple,weconductthoroughexperimentsonFCOSandpropose\n                       SAR-FCOS,whichrunstwiceasfastandachievesbetterdetectionperformance. Thorough\n                       experimentsareconductedthatvalidatetheeffectivenessofourproposedmethods.\n                       AuthorContributions: Conceptualization, Z.S.; methodology, Z.S.; software, Z.S., S.C.andY.H.;\n                       validation,Z.S.;formalanalysis,Z.S.;investigation,Z.S.,S.C.andY.H.;resources,Z.S.andY.Z.;data\n                       curation,Z.S.andY.Z.;writing—originaldraftpreparation,Z.S.;writing—reviewandediting,Y.Z.,\n                       S.C.andY.H.;visualization,Z.S.,S.C.andY.H.;supervision,Y.Z.;projectadministration,Y.Z.;funding\n                       acquisition,Y.Z.Allauthorshavereadandagreedtothepublishedversionofthemanuscript.\n                       Funding: ThisresearchwasfundedbytheFundforForeignScholarsinUniversityResearchand\n                       TeachingPrograms(the111Project),grantnumberB18039.         \n                       DataAvailabilityStatement:Notapplicable.                   \n                                                                                  \n                       ConflictsofInterest:Theauthorsdeclarenoconflictofinterest. \n     References                                                                   \n                                                                                  \n     1. Lee,H.J.;Huang,L.F.;Chen,Z. Multi-frameshipdetectionandtrackinginaninfraredimagesequence. PatternRecognit.1990,\n        23,785–798.[CrossRef]                                                     \n     2. Mingbo,Z.;Jianwu,Z.;Jianguo,H. ImagingsimulationofseasurfacewithfullpolarizationSAR. InProceedingsofthe2015IEEE\n        5thAsia-PacificConferenceonSyntheticApertureRadar(APSAR),Singapore,1–4September2015;pp.815–817.\n     3. Brusch,S.;Lehner,S.;Fritz,T.;Soccorsi,M.;Soloviev,A.;vanSchie,B. ShipSurveillancewithTerraSAR-X. IEEETrans.Geosci.\n        RemoteSens.2011,49,1092–1103.[CrossRef]                                   \n     4. Eldhuset,K.AnautomaticshipandshipwakedetectionsystemforspaceborneSARimagesincoastalregions. IEEETrans.Geosci.\n        RemoteSens. 1996,34,1010–1019.[CrossRef]                                  \n     5. Li,J.;Qu,C.;Shao,J. ShipdetectioninSARimagesbasedonanimprovedfasterR-CNN. InProceedingsofthe2017SARinBig\n        DataEra:Models,MethodsandApplications(BIGSARDATA),Beijing,China,13–14November2017;pp.1–6.\n     6. Ouchi, K.; Tamaki, S.; Yaguchi, H.; Iehara, M. Shipdetectionbasedoncoherenceimagesderivedfromcrosscorrelationof\n        multilookSARimages. IEEEGeosci.RemoteSens.Lett.2004,1,184–187.[CrossRef]  \n     7. Ai,J.;Qi,X.;Yu,W.;Deng,Y.;Liu,F.;Shi,L. AnewCFARshipdetectionalgorithmbasedon2-Djointlog-normaldistributionin\n        SARimages. IEEEGeosci.RemoteSens.Lett.2010,7,806–810.[CrossRef]           \n     8. Wang,C.;Jiang,S.;Zhang,H.;Wu,F.;Zhang,B. Shipdetectionforhigh-resolutionSARimagesbasedonfeatureanalysis. IEEE\n        Geosci.RemoteSens.Lett.2013,11,119–123.[CrossRef]                         \n     9. Leng,X.;Ji,K.;Yang,K.;Zou,H. AbilateralCFARalgorithmforshipdetectioninSARimages. IEEEGeosci.RemoteSens.Lett.\n        2015,12,1536–1540.[CrossRef]                                              \n     10. Zhai,L.;Li,Y.;Su,Y. Inshoreshipdetectionviasaliencyandcontextinformationinhigh-resolutionSARimages. IEEEGeosci.\n        RemoteSens.Lett.2016,13,1870–1874.[CrossRef]                              \n     11. Ren,S.;He,K.;Girshick,R.;Sun,J. Fasterr-cnn:Towardsreal-timeobjectdetectionwithregionproposalnetworks. Adv.Neural\n        Inf.Process.Syst. 2015,28,91–99.[CrossRef]                                \n     12. Redmon,J.;Farhadi,A. Yolov3:Anincrementalimprovement. arXiv2018,arXiv:1804.02767.\n     13. Liu,W.;Anguelov,D.;Erhan,D.;Szegedy,C.;Reed,S.;Fu,C.Y.;Berg,A.C. SSD:Singleshotmultiboxdetector. InProceedingsof\n        theEuropeanConferenceonComputerVision,Amsterdam,TheNetherlands,11–14October2016;pp.21–37.\n     14. Lin,T.Y.;Goyal,P.;Girshick,R.;He,K.;Dollár,P. Focallossfordenseobjectdetection. InProceedingsoftheIEEEInternational\n        ConferenceonComputerVision,Venice,Italy,22–29October2017;pp.2980–2988.    \n     15. Tian,Z.; Shen,C.; Chen,H.; He,T. Fcos: Fullyconvolutionalone-stageobjectdetection. InProceedingsoftheIEEE/CVF\n        InternationalConferenceonComputerVision,Seoul,Korea,27–28October2019;pp.9627–9636.\n     16. Ge,Z.;Liu,S.;Li,Z.;Yoshie,O.;Sun,J.OTA:OptimalTransportAssignmentforObjectDetection. InProceedingsoftheProceedings\n        oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,Nashville,TN,USA,20–25June2021;pp.303–312.",
        "char_count": 6351,
        "has_tables": false
      },
      {
        "page": 14,
        "content": "RemoteSens.2022,14,5761                                             14of15   \n                                                                                  \n                                                                                  \n                                                                                  \n     17. Ge,Z.;Liu,S.;Wang,F.;Li,Z.;Sun,J. Yolox:Exceedingyoloseriesin2021. arXiv2021,arXiv:2107.08430.\n     18. Jiao,J.;Zhang,Y.;Sun,H.;Yang,X.;Gao,X.;Hong,W.;Fu,K.;Sun,X. Adenselyconnectedend-to-endneuralnetworkfor\n        multiscaleandmultisceneSARshipdetection. IEEEAccess2018,6,20881–20892.[CrossRef]\n     19. Kang,M.;Leng,X.;Lin,Z.;Ji,K. AmodifiedfasterR-CNNbasedonCFARalgorithmforSARshipdetection. InProceedingsof\n        the2017InternationalWorkshoponRemoteSensingwithIntelligentProcessing(RSIP),Shanghai,China,18–21May2017;pp.1–4.\n     20. Zhang,T.;Zhang,X.;Shi,J.;Wei,S. Depthwiseseparableconvolutionneuralnetworkforhigh-speedSARshipdetection. Remote\n        Sens.2019,11,2483.[CrossRef]                                              \n     21. Ge,Z.;Jie,Z.;Huang,X.;Li,C.;Yoshie,O. Delvingdeepintotheimbalanceofpositiveproposalsintwo-stageobjectdetection.\n        Neurocomputing2021,425,107–116.[CrossRef]                                 \n     22. He,K.;Zhang,X.;Ren,S.;Sun,J. Deepresiduallearningforimagerecognition. InProceedingsoftheIEEEConferenceon\n        ComputerVisionandPatternRecognition,LasVegas,NV,USA,27–30June2016;pp.770–778.\n     23. Wang,Y.;Wang,C.;Zhang,H.;Dong,Y.;Wei,S. ASARdatasetofshipdetectionfordeeplearningundercomplexbackgrounds.\n        RemoteSens.2019,11,765.[CrossRef]                                         \n     24. Xian,S.;Zhirui,W.;Yuanrui,S.;Wenhui,D.;Yue,Z.;Kun,F. AIR-SARShip-1.0:High-resolutionSARshipdetectiondataset. J.\n        Radars2019,8,852–862.                                                     \n     25. Kang,M.;Ji,K.;Leng,X.;Lin,Z. Contextualregion-basedconvolutionalneuralnetworkwithmultilayerfusionforSARship\n        detection. RemoteSens.2017,9,860.[CrossRef]                               \n     26. Chang,Y.L.;Anagaw,A.;Chang,L.;Wang,Y.C.;Hsiao,C.Y.;Lee,W.H.ShipdetectionbasedonYOLOv2forSARimagery. Remote\n        Sens.2019,11,786.[CrossRef]                                               \n     27. Redmon,J.;Farhadi,A. YOLO9000: Better,faster,stronger. InProceedingsoftheIEEEConferenceonComputerVisionand\n        PatternRecognition,Honolulu,HI,USA,21–26July2017;pp.7263–7271.            \n     28. Wei,S.;Su,H.;Ming,J.;Wang,C.;Yan,M.;Kumar,D.;Shi,J.;Zhang,X. Preciseandrobustshipdetectionforhigh-resolutionSAR\n        imagerybasedonHR-SDNet. RemoteSens.2020,12,167.[CrossRef]                 \n     29. Zhao,Y.;Zhao,L.;Xiong,B.;Kuang,G. AttentionreceptivepyramidnetworkforshipdetectioninSARimages. IEEEJ.Sel.Top.\n        Appl.EarthObs.RemoteSens.2020,13,2738–2756.[CrossRef]                     \n     30. Mao,Y.;Yang,Y.;Ma,Z.;Li,M.;Su,H.;Zhang,J. Efficientlow-costshipdetectionforSARimagerybasedonsimplifiedU-net.\n        IEEEAccess2020,8,69742–69753.[CrossRef]                                   \n     31. Zhou,X.;Wang,D.;Krähenbühl,P. Objectsaspoints. arXiv2019,arXiv:1904.07850.\n     32. Guo,H.;Yang,X.;Wang,N.;Gao,X. ACenterNet++modelforshipdetectioninSARimages. PatternRecognit. 2021,112,107787.\n        [CrossRef]                                                                \n     33. Gao,F.;He,Y.;Wang,J.;Hussain,A.;Zhou,H. Anchor-freeconvolutionalnetworkwithdenseattentionfeatureaggregationfor\n        shipdetectioninSARimages. RemoteSens.2020,12,2619.[CrossRef]              \n     34. Dvornik,N.;Mairal,J.;Schmid,C. Modelingvisualcontextiskeytoaugmentingobjectdetectiondatasets. InProceedingsofthe\n        EuropeanConferenceonComputerVision(ECCV),Munich,Germany,8–14September2018;pp.364–380.\n     35. Dwibedi,D.;Misra,I.;Hebert,M. Cut,pasteandlearn:Surprisinglyeasysynthesisforinstancedetection. InProceedingsofthe\n        IEEEInternationalConferenceonComputerVision,Venice,Italy,22–29October2017;pp.1301–1310.\n     36. Zhang,H.;Cisse,M.;Dauphin,Y.N.;Lopez-Paz,D. mixup:Beyondempiricalriskminimization. arXiv2017,arXiv:1710.09412.\n     37. Yun,S.;Han,D.;Chun,S.;Oh,S.J.;Yoo,Y.;Choe,J. CutMix:RegularizationStrategytoTrainStrongClassifiersWithLocalizable\n        Features. InProceedingsofthe2019IEEE/CVFInternationalConferenceonComputerVision(ICCV),Seoul,Korea,27October–2\n        November2019;pp.6022–6031.                                                \n     38. Ghiasi,G.;Cui,Y.;Srinivas,A.;Qian,R.;Lin,T.Y.;Cubuk,E.D.;Le,Q.V.;Zoph,B.Simplecopy-pasteisastrongdataaugmentation\n        methodforinstancesegmentation. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,\n        Nashville,TN,USA,20–25June2021;pp.2918–2928.                              \n     39. Lin,T.Y.;Dollár,P.;Girshick,R.;He,K.;Hariharan,B.;Belongie,S. Featurepyramidnetworksforobjectdetection. InProceedings\n        oftheIEEEConferenceonComputerVisionandPatternRecognition,Honolulu,HI,USA,21–26July2017;pp.2117–2125.\n     40. Simonyan,K.;Zisserman,A. Verydeepconvolutionalnetworksforlarge-scaleimagerecognition. arXiv2014,arXiv:1409.1556.\n     41. Szegedy,C.; Ioffe,S.; Vanhoucke,V.; Alemi,A.A. Inception-v4,inception-resnetandtheimpactofresidualconnectionson\n        learning. InProceedingsoftheThirty-FrstAAAIConferenceonArtificialIntelligence,SanFrancisco,CA,USA,4–9February2017.\n     42. Liu,S.;Huang,D.;Wang,Y. Learningspatialfusionforsingle-shotobjectdetection. arXiv2019,arXiv:1911.09516.\n     43. Lin,T.Y.;Maire,M.;Belongie,S.;Hays,J.;Perona,P.;Ramanan,D.;Dollár,P.;Zitnick,C.L. Microsoftcoco:Commonobjectsin\n        context. InProceedingsoftheEuropeanConferenceonComputerVision,Zurich,Switzerland,6–12September2014;pp.740–755.\n     44. Everingham,M.;VanGool,L.;Williams,C.K.;Winn,J.;Zisserman,A. Thepascalvisualobjectclasses(voc)challenge. Int. J.\n        Comput.Vis. 2010,88,303–338.[CrossRef]                                    \n     45. Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.; Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.; etal.\n        Imagenetlargescalevisualrecognitionchallenge. Int.J.Comput.Vis. 2015,115,211–252.[CrossRef]\n     46. Hu,J.;Shen,L.;Sun,G. Squeeze-and-excitationnetworks. InProceedingsoftheIEEEConferenceonComputerVisionand\n        PatternRecognition,SaltLakeCity,UT,USA,18–23June2018;pp.7132–7141.",
        "char_count": 6698,
        "has_tables": false
      },
      {
        "page": 15,
        "content": "RemoteSens.2022,14,5761                                             15of15   \n                                                                                  \n                                                                                  \n                                                                                  \n     47. Woo,S.;Park,J.;Lee,J.Y.;Kweon,I.S. Cbam:Convolutionalblockattentionmodule. InProceedingsoftheEuropeanConference\n        onComputerVision(ECCV),Munich,Germany,8–14September2018;pp.3–19.          \n     48. Zhang,S.;Chi,C.;Yao,Y.;Lei,Z.;Li,S.Z. Bridgingthegapbetweenanchor-basedandanchor-freedetectionviaadaptivetraining\n        sampleselection. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,Seattle,WA,USA,\n        13–19June2020;pp.9759–9768.",
        "char_count": 5421,
        "has_tables": false
      }
    ],
    "full_text": "remote   sensing                                                        \n                                                                                  \n                                                                                  \n                                                                                  \n     Article                                                                      \n     BoxPaste:  An   Effective Data  Augmentation     Method    for SAR           \n                                                                                  \n     Ship  Detection                                                              \n                                                                                  \n     ZhilingSuo,YongboZhao* ,ShengChen andYiliHu                                  \n                                                                                  \n                                                                                  \n                       NationalLaboratoryofRadarSignalProcessing,XidianUniversity,Xi’an710071,China\n                       * Correspondence:ybzhao@xidian.edu.cn                      \n                       Abstract:Dataaugmentationisacrucialtechniqueforconvolutionalneuralnetwork(CNN)-based\n                       object detection. Thus, this work proposes BoxPaste, a simple but powerful data augmentation\n                       methodappropriateforshipdetectioninSyntheticApertureRadar(SAR)imagery.BoxPastecrops\n                       shipobjectsfromoneSARimageusingboundingboxannotationsandpastesthemonanotherSAR\n                       imagetoartificiallyincreasetheobjectdensityineachtrainingimage.Furthermore,wedivedeep\n                       intothecharacteristicsoftheSARshipdetectiontaskanddrawaprinciplefordesigningaSARship\n                       detector—lightmodelsmayperformbetter.Ourproposeddataaugmentationmethodandmodified\n                       shipdetectorattaina95.5%AveragePrecision(AP)and96.6%recallontheSARShipDetection\n                       Dataset(SSDD),4.7%and5.5%higherthanthefullyconvolutionalone-stage(FCOS)objectdetection\n                       baselinemethod.Furthermore,wealsocombineourdataaugmentationschemewithtwocurrent\n                       detectors,RetinaNetandadaptivetrainingsampleselection(ATSS),tovalidateitseffectiveness.The\n                       experimentalresultsdemonstratethatournewlyproposedSAR-ATSSarchitectureachieves96.3%\n                       AP, employing ResNet-50 as the backbone. The experimental results show that the method can\n                       significantlyimprovedetectionperformance.                  \n                                                                                  \n                       Keywords:syntheticapertureradar;shipdetection;dataaugmentation;targetdetection\n                                                                                  \n                                                                                  \n     Citation:Suo,Z.;Zhao,Y.;Chen,S.;                                             \n     Hu,Y.BoxPaste:AnEffectiveData                                                \n                       1. Introduction                                            \n     AugmentationMethodforSARShip                                                 \n     Detection.RemoteSens.2022,14,5761. Monitoringandidentifyingmarineshipsisacrucialtaskguaranteeingnationalsecu-\n     https://doi.org/10.3390/rs14225761 rity. Specifically,itplaysavitalroleinmonitoringandmanagingfishingships,combating\n                       smuggling, and protecting marine resources [1]. Synthetic Aperture Radar (SAR) is an\n     AcademicEditor:GerardoDi                                                     \n                       appropriatesensorforshipdetection[2–4]becauseitcancreatehigh-resolutionimages\n     Martino                                                                      \n                       (Figure 1), regardless of the altitude and weather conditions, making ship detection a\n     Received:28September2022 computervisiontask.                                 \n     Accepted:9November2022                                                       \n     Published:15November2022                                                     \n     Publisher’sNote:MDPIstaysneutral                                             \n     withregardtojurisdictionalclaimsin                                           \n     publishedmapsandinstitutionalaffil-                                          \n     iations.                                                                     \n     Copyright: © 2022 by the authors.                                            \n     Licensee MDPI, Basel, Switzerland.                                           \n     This article is an open access article                                       \n     distributed under the terms and                                              \n     conditionsoftheCreativeCommons                                               \n     Attribution(CCBY)license(https:// Figure1.SyntheticApertureRadar(SAR)shipdetectionimagesfromtheSARShipDetectionDataset\n     creativecommons.org/licenses/by/                                             \n                       (SSDD)[5].Greenboxesareground-truthlabelsmanuallyannotated.\n     4.0/).                                                                       \n     RemoteSens.2022,14,5761.https://doi.org/10.3390/rs14225761 https://www.mdpi.com/journal/remotesensing\n\nRemoteSens.2022,14,5761                                              2of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                          In the past decades, the literature has suggested several SAR ship detection meth-\n                       ods, mainly divided into two categories: traditional and convolutional neural network\n                       (CNN)-based methods. In the former category, the Constant False-Alarm Rate (CFAR)\n                       algorithmanditsvariantsaretheprimaryrepresentativetechniquesoftraditionalSAR\n                       shipdetection[6–10]. Technically,thisapproachestablishesathresholdtoidentifytargets\n                       statisticallyexceedingthebackgroundpixellevelwhileretainingalowfalsealarmrate.\n                       However,traditionalalgorithmsarenotrobusttolightandweatherconditionvariations.\n                       ThesecondcategoryinvolvesCNNs,whichhaverecentlypresentedgreatsuccessinobject\n                       detection [11–17]. Employing CNNs for SAR ship detection is also becoming a trend,\n                       with [18] designing a modified faster region-based CNN (R-CNN) scheme involving a\n                       denselyconnectednetworktosolvethescalevarianceissueinSARshipdetection. Further-\n                       more, [19]introducesaR-CNNtodetectshipswithinSARimagery. Theissueofsmallship\n                       detectionissolvedbyaggregatingcontextualfeaturesfromdifferentlayersandachieving\n                       improved performance. Commonly, the detection speed of ships within a SAR image\n                       isoftenneglectedandthus[20]suggestsalightweightnetworkwithfewerparameters\n                       bymainlyusingdepthwiseseparableCNN(DS-CNN)toachievehigh-speedSARship\n                       detection. Althoughthedetectionperformanceandspeedcontinuouslyimproved,their\n                       scopesarelimitedtomodifyingthenetworkstructure.            \n                          Opposingpreviousworks,thispapernoticesthatthestatisticalcharacteristicsofSAR\n                       shipdataaresignificantlydifferentfromthegeneralobjectdetectiondata. Considering\n                       theSARShipDetectionDataset(SSDD)[5]asanexample,wefirstcalculatethenumber\n                       of images with regard to the number of ships (Figure 2a). Figure 2a highlights that the\n                       dataset involves more than 700 images, each of which contains only one ground-truth\n                       target. The average number of ground-truth targets per image is 2.19, indicating that\n                       objects are sparsely distributed (see also Figure 2a), as most pixels in SAR imagery are\n                       background,revealingitsrelativelylowinformationdensity. Wealsocalculatethesizeof\n                       theshipsinpixels. ThecorrespondingresultsinFigure2bindicatethattheareasoccupied\n                       bymostshipsaresmallerthan2500px(around50×50),whichisabout0.95%ofthepicture,\n                       highlightingthatSARimageryobjectsareverysmallcomparedtoothergeneralobjects.\n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                 Number of Ships               Area of Ships      \n                                   (a)                           (b)              \n                       segami                                                     \n                       fo                                                         \n                       rebmuN                                                     \n                                                  segami                          \n                                                  fo                              \n                                                  rebmuN                          \n                       Figure2. StatisticsoftheSSDDdataset. (a)isthenumberofimageswithregardtothenumberof\n                       ground-truthships.(b)showsthenumberofshipswithregardtotheareaofground-truthships.\n                          Previous research [21] demonstrates that training an object detector is primarily a\n                       learningprocesstoidentifytheobjectsofinterest.Therefore,increasingtheobjectdensityin\n                       eachimageshouldbebeneficialfordetectionperformance.Knowingthesestatisticalcharac-\n                       teristicsinaSARshipdetectiontask,weproposeasimplebuteffectivedataaugmentation\n                       methodnamedBoxPastethatincreasestheobjectdensityineachSARimage. Concretely,\n                       during training, we crop objects from one image using bounding box annotations and\n                       pastethemintoanotherSARimage. Opposingcurrentdataaugmentationmethodssuchas\n                       randomflippingandcolorjittering,theproposedBoxPasteisspeciallydesignedtoincrease\n                       thenumberofground-truthshipsperimageandultimatelyenhancetrainingefficiency.\n                       OurexperimentsdemonstratethatBoxPastegreatlyimprovesthedetectionperformanceon\n\nRemoteSens.2022,14,5761                                              3of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                       anchor-free(FullyConvolutionalOne-Stage(FCOS)[15])andanchor-based(RetinaNet[14])\n                       detectorsby4.7%and3.2%AP,respectively.                     \n                          Inaddition,SARshipdetectionisasingle-classdetectiontask. However,objectde-\n                       tectorssuchasRetinaNet,FCOS,andFasterR-CNN[11]aredesignedforgeneralobject\n                       detectiontasks,andthusdirectlyapplyingthosedetectorstoSARimageryleadstoover-\n                       fitting. Therefore, wealsointroduceaprinciplefordesigninganappropriateSARship\n                       detectorgovernedbytheconceptthatthelargermodelisnotalwaysthebetter. Following\n                       thisprinciple,wemodifythewell-knownanchor-freeobjectdetectorFCOSanddevelop\n                       itslightervariantentitledSAR-FCOS.ThelatteristwiceasfastasitsFCOSbaselineand\n                       achievesabetterdetectionperformance,demonstratingtheeffectivenessofourmodifica-\n                       tion.                                                      \n                          Insummary,ourcontributionsfromthisworkarethree-fold:    \n                       •  ProposingBoxPaste,aneasybutpowerfuldataaugmentationstrategyforSARship\n                          detection.                                              \n                       •  Developing a principle to designa SAR image detector and proposing a modified\n                          detectorSAR-FCOS.                                       \n                       •  Combiningthetwopreviouscontributionstoachieveagreatdetectionperformance\n                          ontheSSDDdatasetemployingResNet-50[22]asthebackbone.    \n                          Therestofthisarticleisorganizedasfollows.Thesecondsectionintroducestherelated\n                       workofSARshipdetection.Section3detailstheproposedmethods,includingSAR-FCOS\n                       andourproposedBoxPastedataenhancementstrategy.InSection4,theexperimentalresults\n                       andcorrespondinganalysisareprovided,andsomeconclusionsaremadeinSection5.\n                       2. RelatedWorks                                            \n                       2.1. TraditionalMethods                                    \n                          ThemostwidelyusedtraditionalshipdetectionalgorithmsaretheCFARalgorithm\n                       and its improved variants [6–10]. These methods set a threshold to detect statistically\n                       significant targets exceeding the background pixel while maintaining a constant false\n                       alarm rate. Concretely, [6] proposes a technique that computes the cross-correlation\n                       valuesbetweentwoimagesextractedbyslidingasmall-sizedwindowonthemulti-view\n                       SARintensity(oramplitude)imagery,producingacoherentimage. Furthermore,ref. [7]\n                       suggestsanewCFAR-basedshipdetectionalgorithmthatconsidersthenormaldistribution\n                       oftwo-dimensionaljointlogs,while[8]developsashipdetectionmethodbasedonfeature\n                       analysisforhigh-resolutionSARimages. Theauthorof[9]introducesabilateralCFAR\n                       algorithmforshipdetectioninSARimages,reducingtheinfluenceofSARambiguitiesand\n                       seaclutterbycombiningtheSARimages’intensityandspatialdistribution. Duetothe\n                       highsimilaritybetweentheharbor’sandship’sbodygrayandtexturefeatures,traditional\n                       methodscannoteffectivelydetectinshoreships. Thus,ref. [10]presentsanovelsaliency\n                       andcontextinformationapproachdealingwiththisissue. SinceCFARanditsimproved\n                       variantsseverelyrelyonthepresetdistributionormanuallydefinedcharacteristics,their\n                       adaptiveabilityisweak.                                     \n                                                                                  \n                       2.2. DeepLearning-BasedMethods                             \n                          WiththedevelopmentofdeeplearningtechnologyandtheestablishmentofaSARship\n                       database[5,23,24],manyshipdetectionalgorithmsbasedonconvolutionalneuralnetworks\n                       haveemerged. Forexample,ref. [25]introducesFasterR-CNNforshipdetectioninSAR\n                       imageryandsolvestheissueofsmallshipdetectionbyaggregatingcontextualfeatures\n                       fromdifferentlayers,achievingimprovedperformance. Theworkof[26]introducesanew\n                       networkarchitecture,namedYouOnlyLookOnceversion2(YOLOv2)-reduced,whichhas\n                       alowerdetectiontimethanYOLOv2[27]onanNVIDIATITANXGPU.Aimingatthe\n                       problemthatthedetectionspeedofSARshipsisoftenneglectedatpresent,abrand-new\n                       lightweightnetwork[20]isestablishedwithfewernetworkparametersbymainlyusingDS-\n                       CNNtoachievehigh-speedSARshipdetection,whichcanachievehigh-speedandaccurate\n                       shipdetectionsimultaneouslycomparedwithothermethods. In[28],theauthorsdevelopa\n\nRemoteSens.2022,14,5761                                              4of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                       novelshipdetectionmethodbasedonahigh-resolutionshipdetectionnetwork(HR-SDNet)\n                       appropriate for high-resolution SAR images. This method is more accurate and robust\n                       for inshore and offshore ship detection of high-resolution SAR imagery. A two-staged\n                       detectornamedAttentionReceptivePyramidNetwork(ARPN)[29]issuggestedtoimprove\n                       detectingmulti-scaleshipsinSARimagesbyenhancingtherelationshipsamongnon-local\n                       featuresandrefininginformationatdifferentfeaturemaps. Thisstrategyiseffectivefor\n                       scenesofvarioussizesandcomplexbackgrounds. Toalleviatetheexcessivecomputational\n                       burdenandincreasedhyper-parametercardinalityproblems,ref.[30]suggestsanefficient\n                       and low-cost ship detection network for SAR imagery. This work utilizes an anchor-\n                       freeSARshipdetectionframeworkcomprisingaboundingboxregressionsub-netand\n                       a score map regression sub-net based on a simplified U-Net. This pipeline achieves a\n                       verycompetitivedetectionperformancewhilebeingextremelylightweight. Animproved\n                       algorithmbasedonCenterNet[31]hasalsobeenproposed[32]thatissignificantlybetter\n                       thanCenterNetforsmallshipdetectioninlow-resolutionSARimagery,addinglow-level\n                       featurerepresentationtothepyramidsforsmallobjectdetectionandoptimizingtheheadof\n                       detectortoeffectivelydistinguishforegroundfrombackground. Finally,ref.[33]introduces\n                       ananchorlessconvolutionnetworkaggregatinganintensiveattentionfunctionthatobtains\n                       higherprecisionandisfastertoexecutethanthemainstreamdetectionalgorithms.\n                       2.3. DataAugmentationforObjectDetectionandInstanceSegmentation\n                          Reference[34]leveragessegmentationannotationstoincreasethenumberofobject\n                       instancesby appropriately modelingthe visualcontextsurroundingobjects. The work\n                       of[35]automaticallyextractsobjectinstancemasksandrendersthemonrandomback-\n                       ground images. Mixup [36] randomly extracts two images from the training set and\n                       then performs a linear weighted summation of the pixel values of the extracted image\n                       data. Atthesametime,theOne-hotvectorlabelscorrespondingtothesamplesarealso\n                       weightedandsummed. Inthisway,anewimagewithafuzzyclassificationboundarycan\n                       beobtained,enhancingthegeneralizationabilityofthemodel. CutMix[37]replacesthe\n                       removedregionswithapatchfromanotherimageandchangesthegroundtruthlabels\n                       bythenumberofpixelsofthecombinedimages. Byrequiringthemodeltorecognizethe\n                       targetfromalocalperspective,thelocalizationabilitycanbeenhanced. CutMixisusually\n                       usedforclassificationtasksandisnotsuitablefordetectiontasksbecauseitusuallycrops\n                       imagepatchesrandomly,whichrequiresthattheimagedoesnotcontaintoomuchcontext.\n                       While[38]highlightsthatCopyPaste,i.e.,simplypastingobjectsrandomly,providessolid\n                       gains on the detectors’ performance. While being similar to BoxPaste in this work, we\n                       arguethatourworkisthefirsttomigratethekeyideologyofCopyPasteneglectingthe\n                       unnecessaryusageofinstancemaskannotationsintheSARshipdetectiontask.\n                       3. Methods                                                 \n                          ThissectionfirstrevisitsFCOS[15],awell-knownone-stagedanchor-freeobjectdetec-\n                       tor,whichwillbeusedasourbaselineinthispaper. ConsideringthatobjectsinaSARship\n                       detectionscenarioarestatisticallysmallandsparse,welightenthestructureofFCOSfrom\n                       theFeaturePyramidNetworks(FPN)[39]tothedetectionheadandsuggestSAR-FCOS.\n                       Finally, this section introduces our proposed BoxPaste, a powerful data augmentation\n                       strategyforSARshipdetection.                               \n                                                                                  \n                       3.1. RevisitingFCOS                                        \n                          Althoughanchor-basedobjectdetectorshaveachievedmassivesuccessonmanyobject\n                       detectiondatasets,theysufferfromtherequirementtodesignanchorboxes. Furthermore,\n                       theenormousamountofdetectionproposalsdramaticallyslowsdownthepost-processing\n                       method,prohibitingtheanchor-basedmechanismfromreal-timeapplications. Recently,\n                       anchor-freeobjectdetectorshaveshowngreatpotentialingeneralobjectdetection. They\n                       usuallyattainhigherperformancethantheircounterpartanchor-baseddetectorswhile\n\nRemoteSens.2022,14,5761                                              5of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                       enjoying a more straightforward architectural design. Hence, this work considers the\n                       well-knownanchor-freeobjectdetectorFCOSasthebaselinemethod.\n                          Like other standard object detectors, FCOS comprises three parts: a backbone for\n                       featureextraction,FPNforfeatureintegration,andadetectionheadforprediction. Figure3\n                       illustratestheoverallFCOSstructure. Asthebackbonestructure,VisualGeometryGroup\n                       (VGG)[40],ResNet[22],Inception[41],oranyotherwell-knownarchitecturesdesignedfor\n                       classificationcanbeexploited.                              \n                          FPNcomprisesasequenceoftop-downlayersandseveralshortcutlayerstocombine\n                       theknowledgeencodedatdifferentlayers,whichisalsoabroadlyusedstructureinobject\n                       detection. ThepredictedresultsencodedbythedetectionheadofFCOSaredifferentfrom\n                       other anchor-based object detectors. Unlike RetinaNet, Single Shot MultiBox Detector\n                       (SSD)[13],YOLOv3[12],andFasterR-CNN,whichuseanchorboxes,FCOSdirectlyviews\n                       locationpointsastrainingsamplesandlearnstopredictthefouroffsetsfromeachlocation\n                       to the bounding boxes, i.e., left, top, right, and bottom (l∗, t∗, r∗, b∗). Concretely, the\n                       boundingboxregressiontargetsforlocation(x,y)isdefinedas:   \n                                           l∗ = x−x ,r∗ = x −x,                   \n                                                 0    1                           \n                                                                            (1)   \n                                           t∗ = y−y ,b∗ = y −y,                   \n                                                 0    1                           \n                       where(x ,y ),(x ,y )arethecoordinatesoftheleft-topandright-bottomcornersofthe\n                            0 0  1 1                                              \n                       ground-truthboundingbox. Inadditiontotheregressionprediction,onealsoneedsto\n                       knoweachlocation’scategory. Regardingclassification,ifalocationfallsintotheground-\n                       truthboundingbox,thatlocationisconsideredapositivesampleandisresponsiblefor\n                       predictingthatgroundtruth. Moreover,theFPNinthestandardFCOStypicallycontains\n                       fivelevels,i.e.,fromP3toP7,whiletoconstructvalidreceptivefieldscalesfortheneurons\n                       atdifferentFPNlevels,differentFPNlevelsareregressingdifferentobjectssizes. Giventhe\n                       regressiontargetsl∗,t∗,r∗,andb∗ foralocation,theobjectscaleperfeaturepyramidlevel\n                       followsthefollowingconstraint:                             \n                                          max(l∗ ,t∗ ,r∗ ,b∗) > m,                \n                                                        i                         \n                                                                            (2)   \n                                          max(l∗ ,t∗ ,r∗ ,b∗) < m i+1,            \n                       wherem,i ∈2,3,4,5,6,7intheoriginalFCOSaresetas0,64,128,256,512,and∞,respectively.\n                            i                                                     \n                          Tosuppressthelow-qualitypredictedboundingboxesgeneratedbythelocationsfar\n                       awayfromtheobjects’center,FCOSadoptsthecenternessbranch. Thecenternessscoreof\n                       eachlocationx,y andthecorrespondingboundingboxatfeatureleveliisdefinedas:\n                                i i                                               \n                                              (cid:115)                           \n                                               min(l∗,r∗) min(t∗,b∗)              \n                                     centerness∗ =    ×        .            (3)   \n                                               max(l∗,r∗) max(t∗,b∗)              \n                          Thecenternessscoreisthenmultipliedbytheclassificationscoretoprovidethefinal\n                       predictedconfidenceusedbytheNon-Maximum-Suppression(NMS).Itshouldbenoted\n                       that the pre-processing (e.g., normalizing images) and post-processing (e.g., decoding\n                       outputs,NMS)methodsofFCOSarethesameasinotherstandardobjectdetectors.\n                       3.2. SAR-FCOS                                              \n                          Unlikegeneralobjectdetectiontasks,objectsinSARshipdetectionscenariosusually\n                       have two features: the objects’ sizes are relatively small, and there is only one object\n                       category. Suchfeaturesrequiredesigninganewnetworkstructure,andthusthissection\n                       simplifiestheFCOSstructureandproposesSAR-FCOS.ComparedtotheoriginalFCOS,\n                       thecomplexityofSAR-FCOSisseverelyreduced,aimingatpreventingthenetworkfrom\n                       overfitting. Specifically,weonlymodifytheFPNanddetectionheadstructure,asthesetwo\n                       partsarethemaindifferencebetweendetectionandperformingothertaskssuchasimage\n\nRemoteSens.2022,14,5761                                              6of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                       classificationandsegmentation. Moreover,wewishtoemphasizethatourmotivationis\n                       nottodesignalightweightdetectorforshipdetectioninSARimagesbuttohighlightthe\n                       criticalmeritofdesigninganappropriatedetectorfortheSARshipdetectiontask.\n                       3.2.1. LightFPN                                            \n                          In[39],theauthorsdevelopedtheFPNtohandlethelarge-scalevarianceingeneral\n                       objectdetectiontasks. ThecriticalinsightbehindFPNisthattheneurons’validreceptive\n                       fieldsatthedeeplayersaresignificant,andattheshallowlayers,thesearetypicallysmall.\n                       Henceclassifyingdifferentobjectsizesondifferentfeaturepyramidlevelscanbenefitfrom\n                       thescalealignmentbetweentheobjects’sizesandneurons’validreceptivefields.\n                                                                                  \n                                                                                  \n                                   P7      Head                                   \n                                   P6      Head                                   \n                          C5       P5      Head      C5       P5      Head        \n                                                                                  \n                          C4      P4       Head      C4      P4       Head        \n                                                                                  \n                         C3       P3                C3       P3                   \n                                           Head                       Head        \n                                                                                  \n                                                                                  \n                                                                                  \n                          Backbone Feature Pyramid DetectionHead Backbone Feature Pyramid DetectionHead\n                                 (a)FCOS                   (b)SAR-FCOS            \n                                                                                  \n                                                                                  \n                                        Classification            Classification  \n                                        H(cid:1)W(cid:1)C          H(cid:1)W(cid:1)C\n                                        Center-ness               Center-ness     \n                               (cid:1)4 H(cid:1)W(cid:1)1 (cid:1)2 H(cid:1)W(cid:1)1\n                           H(cid:1)W(cid:1)256 H(cid:1)W(cid:1)256 H(cid:1)W(cid:1)64 H(cid:1)W(cid:1)64\n                                        Regression                Regression      \n                                        H(cid:1)W(cid:1)4          H(cid:1)W(cid:1)4\n                               (cid:1)4                  (cid:1)2                 \n                           H(cid:1)W(cid:1)256 H(cid:1)W(cid:1)256 H(cid:1)W(cid:1)64 H(cid:1)W(cid:1)64\n                               (c)FCOSDetectionHead     (d)SAR-FCOSDetectionHead  \n                       Figure3.AdetailedillustrationofthestructuraldifferencebetweentheFullyConvolutionalOne-\n                       Stage(FCOS)andSAR-FCOS.                                    \n                          However,asstatedinSection1,theships’scalesinaSARdetectiontaskaresmall.\n                       Hence,byfollowingtheassigningruleofFCOS,onlyafewlargeobjectswillbeassignedto\n                       P6andP7,i.e.,thefeaturemapsP6andP7havelittlecontributiontotheaccuracyduring\n                       testingbutimposeahighcomputationalcost. Therefore,P6andP7areredundant,and\n                       weremoveboththeselayersandtheircorrespondingdetectionhead. Ourmodelinvolves\n                       fewer FPN layers, and, therefore, our light FPN executes faster, exploiting some extra\n                       modules to enhance the feature integration between the left layers. Knowing that the\n                       ground-truthobjectsaremainlyassignedtotheP3,P4,andP5layers,weusetheAdaptive\n                       SpatialFeatureFusion(ASFF)[42]tofusedifferentknowledgeencodedbetweendifferent\n                       FPNlayers.                                                 \n                       3.2.2. LightDetectionHead                                  \n                          Theoriginalstructureandhyper-parametersaredesignedfortheCommonObjects\n                       inContext(COCO)[43]benchmarkcontaining80categoriesofvariousobjects. However,\n                       thereisonlyoneShipcategoryintheSSDDdataset,makingthenumberofheadchannels\n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n\n[表格内容]\nBackbone | Feature Pyramid | DetectionHead\n\nBackbone | Feature Pyramid | DetectionHead\n\nRemoteSens.2022,14,5761                                              7of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                       (CHead) and head blocks (NHead) redundant in the original network. Therefore, it is\n                       necessary to reduce the complexity of the detection head to prevent over-fitting. Our\n                       trialsinvolveextensiveexperimentswithdifferentheadparameters,andspecifically,we\n                       reducetheCHeadandNHeadfrom256/4to64/2,affordingabettertrade-offbetween\n                       latencyanddetectionperformance. Despiteourmodificationsbeingsimple,thesearenot\n                       trivial. In the experimental section, we demonstrate that the simplified detection head\n                       reducesthedetector’scomplexityandsurprisinglyimprovesthedetectionperformance,\n                       indicatingthesevereover-fittingoftheoriginalFCOSfortheSARshipdetectiontasks.\n                       Suchaphenomenonrevealstheimportanceofdesigningspecificmodelstructuresand\n                       hyper-parameters for a specific task. Our modified FCOS is named SAR-FCOS and is\n                       illustratedinFigure3.                                      \n                       3.3. BoxPaste                                              \n                       3.3.1. RevisitingCopyPaste                                 \n                          Dataaugmentationaimstoincreasethetrainingdataset’svariability,acriticalcompo-\n                       nentduringobjectdetectortraining,leadingtosignificantimprovementsinobjectdetection\n                       tasks. ThemostrecentandeffectiveaugmentationmethodisCopyPaste. Byrandomly\n                       croppingobjectsfromimageAutilizingground-truthmasksandpastingthemonimage\n                       B, CopyPaste creates more training samples and increases the number of ground-truth\n                       samplespermini-batch,whicharecrucialfortrainingobjectdetectors[21]. Asimpleillus-\n                       trationofCopyPasteispresentedinFigure4. CopyPasteaffordsaremarkableperformance\n                       gainoninstancesegmentationtasks,whileadditionally,italsosignificantlyimprovesthe\n                       performanceofobjectdetection. SpurredbytheadvantagesofCopyPaste,naturally,the\n                       followingquestionisraised: canwebringCopyPasteintotheSARshipdetectiontask?\n                                                                                  \n                       3.3.2. BoxPaste                                            \n                          A straightforward method is applying CopyPaste without instance segmentation\n                       masks and employing bounding boxes to crop objects. However, the objects’ scales in\n                       general object detection tasks vary greatly, and thus, the proposed method will lead to\n                       heavyocclusionbetweentheoriginalandpastedobjects.          \n                          However,therearethreepropertiesofSARshipdetectiontasks(consideringtheSSDD\n                       datasetasanexample). (1)Mostoftheshipsaresmall. (2)Thenumberoftargetshipsin\n                       eachimageisquitelimited. Therefore,mostpixelsinanimagearebackground,making\n                       each mini-batch less informative. (3) The diversity of backgrounds for SAR images is\n                       extremelylessthannaturalRGBimages; hence,SARimagescanbeeasilyconvertedto\n                       gray-scaleimages. Unlikegeneralobjectdetectiontasks,thesethreefeaturessuggestthat\n                       usingboundingboxestoperformCopyPastecancreatemorerealistictrainingsamples.\n                       WenamethismethodBoxPaste. Aclearillustrationofthedifferencebetweenapplying\n                       BoxPasteinSARshipdetectionandgeneralobjectdetectionisshowninFigure4.\n                          Inthefollowingexperiment,wedemonstratethatalthoughtheproposeddataaug-\n                       mentationschemeBoxPasteissimple,itsubstantiallyimprovestheSARshipdetection\n                       performance,revealingitsgreatvalue. Toprovideaclearimageofthetrainingsamples\n                       createdbyBoxPaste,wepresentmoreexamplesinFigure5.\n\nRemoteSens.2022,14,5761                                              8of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                       (a) CopyPaste                              \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                       (b) BoxPaste                               \n                       Figure4. AnillustrationofCopyPaste[38]andourproposedBoxPaste. (a)isborrowedfromthe\n                       originalpaper.(b)showsacombinationoftwotrainingimagesintheSSDDdataset.Notethatfor\n                       CopyPaste,ground-truthinstancemasksarerequired.However,applyingBoxPasteintheSARship\n                       detectiontaskonlyrequirestheboundingboxannotations.        \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                       Figure5.ExamplesofthecreatedtrainingimagesbyBoxPaste.Bycopyingtheboundingboxesof\n                       themiddleimageandpastingthemintothetopimage,wegetthebottomimage.Onecanseethat\n                       BoxPastecaneffectivelyincreasethenumberofground-truthshipobjectsperimage.\n\nRemoteSens.2022,14,5761                                              9of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                       4. Experiment                                              \n                          ThissectionevaluatestheeffectivenessofourproposedSAR-FCOSnetworkandthe\n                       BoxPastedataaugmentationmethodontheSSDDdataset.            \n                       4.1. DatasetandTrainingDetails                             \n                                                                                  \n                          SSDD[5]isapublicdatasetforSARshipdetection. Itcontains1160imagespresenting\n                       2540ships. Aswementionedatthebeginningofthispaper,theshipobjectsinthisdataset\n                       aresparselydistributed,whileaccordingtoourcalculation,morethan700imagescontain\n                       only one object, i.e., over 60% of all images. The average number of ships per image is\n                       2.19. WealsomeasuretheoverlapbetweenshipsusingtheIntersectionoverUnion(IoU)\n                       metric. TheSSDDhasanaverageIoUinvolvingoverlappingshipsof0.0048,illustrating\n                       thatthedataset’sobjectsaresparse. Wedividethetrainingandtestsetaccordingtotheway\n                       theauthoroftheSSDDdatasetdivided;thatis,thetrainingsetandthetestsetweresplit\n                       accordingtotheimages’name,i.e.,imageswiththeirnameendingwith1or9belongtothe\n                       testset,whiletheremainingimagesbelongtothetrainingset. Toevaluatethedetector’s\n                       performance,weadoptaverageprecision(AP)andrecall[44]. Precisionistheproportionof\n                       accuratelypredictedshipsinallforecasts,andrecallistheproportionofaccuratelypredicted\n                       shipsinallground-truthships,bothofwhicharethemostwidelyusedindicators. TheAP\n                       metricisusedtoevaluatethecomprehensiveperformanceofthedetector.Itcanbeobtained\n                       bysortingtheoutputresultsindescendingorderofthedetectionconfidence,drawingthe\n                       precision-recallcurve,andcalculatingtheareaofthecurve. Inthispaper,APiscalculated\n                       whentheIoUthresholdis0.5.                                  \n                          Becauseourworkfocusesonthenetworkdesigningstrategyanddataaugmentation\n                       method,whichisabletobeappliedtoeverydetectionalgorithm,wechooseFCOSasa\n                       simplebaseline. ThetrialscombinetheproposedFCOSwithResNet-50asourbaseline\n                       backbonenetwork,utilizingthepre-trainedonesfromImageNet[45]asinitialparameters.\n                       AllmodelsaretrainedononeNVIDIAGeForceGTX1080Tiinvolvingastochasticgradient\n                       descent (SGD) for 12 epochs. The initial learning rate is 0.01, which reduces by 10 at\n                       the8thand11thepochs. Theweightdecayfactorandmomentumaresetto0.0001and\n                       0.9, respectively, whiletheinputimagesareresizedto[512, 512]. InBoxPaste, boththe\n                       croppedimagepatchesfromtheoriginalimagesandthetargetimagesarerandomlyflipped\n                       horizontallyataratioof50%.                                 \n                       4.2. AblationStudy                                         \n                       4.2.1. ExperimentsonSAR-FCOS                               \n                          Initially,weinvestigatetheperformanceofSAR-FCOSfromtheperspectiveofevalu-\n                       atingthelightdetectionhead,lightFPN,andtheASFFfeatureintegrationmodule. Table1\n                       highlightsthattheFCOSbaselineachieves90.8%mAPat53.1framespersecond(FPS).\n                       Then,wereducethenumberoftheconvolutionlayersinthedetectionhead,i.e.,NHead,\n                       fromfourtotwo,andfindthatmAPreducesonlyby0.4%whileaffordinga13FPSim-\n                       provement,whichisanacceptabletrade-offbetweenperformanceandlatency. Afterthat,\n                       wereducethenumberofchannelsinboththedetectionheadandFPN,i.e.,CHead,from\n                       256 to 128. As expected, reducing the parameters alleviates over-fitting, increasing the\n                       detectionperformanceto91.1%mAP.                            \n                          FurtherreducingtheCHeadto64and32indicatesthatforCHead=64weobtain\n                       the best performance of 92.4% mAP at 101.2 FPS. These modifications highlight that\n                       alteringonlyafewhyper-parametersinthedetectionheadalmostdoublestheSARship\n                       detection speed while increasing performance by 1.6% mAP compared to the baseline\n                       network. Suchaphenomenonstronglyvalidatesourdesignintuitionforanappropriate\n                       SAR ship detector. Namely, the SAR ship detection dataset is easy to get over-fitted,\n                       therefore, it is better to use light models than heavy models. Note that we did not try\n                       differentbackbonesbecauseourgoalistoexpressthekeyideologyofdesigningappropriate\n                       detectorsforSARshipdetection,nottothoroughlyexploreeverycombinationofdifferent\n                       componentsindetectors.\n\nRemoteSens.2022,14,5761                                             10of15   \n                                                                                  \n                                                                                  \n                       Table1.AblationstudyonthelightdetectionheadandthelightFeaturePyramidNetwork(FPN)in\n                       SAR-FCOS.                                                  \n                                                                                  \n                                 NHead  CHead    FPN    AP(%)  Recall(%) FPS      \n                         FCOS                                                     \n                                  4      256    P3–P7    90.8   91.1    53.1      \n                        (baseline)                                                \n                                  2      256    P3–P7    90.4   90.5    66.3      \n                                  2      128    P3–P7    91.1   91.5    96.3      \n                        LightHead                                                 \n                                  2       64    P3–P7    92.4   92.4    101.2     \n                                  2       32    P3–P7    89.2   89.4    102.8     \n                                  2       64    P3–P5    92.3   91.9    119.9     \n                        LightFPN                                                  \n                                  2       64    P3–P4    90.1   90.3    127.6     \n                        LightHead                                                 \n                                  2       64    P3–P5    93.0   94.4    110.1     \n                        +ASFF[42]                                                 \n                          AswestatedinSection3,shipobjectsintheSSDDdatasetarerelativelysmall,and\n                       mostofthemareassignedtoP3toP5levelsduringtheFCOSlabelassignment. Hence,\n                       P6andP7levelscontributelesstothefinalperformance. ThelastthreerowsinTable1\n                       presentourablationstudyinvolvingdifferentFPNlevels. Specifically,byremovingtheP6\n                       andP7levels,mAPdropsbyonly0.1%,butFPSincreasesbynearly20. However,further\n                       removingtheP5level,thedetectionperformancedropsby2.2%,indicatingthattheP5level\n                       isessential. Affordingadetectorthatistwiceasfastasthebaseline,wehavetheoption\n                       toaddextramodulessuchasASFForattentionmechanisms[46,47]. Thisworkenhances\n                       featureintegrationbyadoptingASFFbecausetheparameterreductionoccursintheFPN\n                       anddetectionhead. Indeed,thelastrowinTable1highlightsthatbyemployingASFF,our\n                       finalSAR-FCOSmodelattains93.0%mAPat110.1FPSontheSSDDdataset,whichisfaster\n                       andmorerobustthantheoriginalFCOSbaseline.                  \n                       4.2.2. AblationStudyonBoxPaste                             \n                          We also investigate the effect of BoxPaste from two aspects: scale jittering and total\n                       trainingepochs. Performingscalejitteringontwocombinedimagesisborrowedfromthe\n                       originalCopyPastepaper.Forexample,ifthescalejitteringrangeis[0.1,2],i.e.,thesampled\n                       image’ssizeis[int(512×0.1),int(512×2)],weperform,ifnecessary,paddingandcropping\n                       toaligntheimagesizeto512. Itiswellknownthatthegreaterthedataaugmentation,the\n                       longer the convergence time during training. Hence, to explore the upper bound of our\n                       BoxPasteonSAR-FCOS,weexploreavariousnumberoftrainingepochs. Table1shows\n                       thatourSAR-FCOSmodelachieves93.0%APand94.4%recall. However,whenapplying\n                       BoxPaste,theAPmetricimprovesby1.1%,reaching94.1%(Table2).Ifweapplyscalejittering\n                       from0.5to1.5,themAPfurtherimprovesto94.5%,andforscalejitteringwithintherange\n                       [0.1,2],ourmodelachieves94.6%APand95.5%recall. ItshouldbenotedthatBoxPasteis\n                       onlyusedduringtraining.Hence,thetestingFPSisnotaffected.   \n                          Thenweinvestigatetheeffectsofthetotaltrainingepochs.Theresultsdemonstratethat\n                       trainingthedetectorsfor36epochsyieldsthebestresults,whilewhenexceeding36epochs,\n                       themodeloverfits,reducingperformance.Figure6visualizesthedetectionresultstoprovide\n                       anintuitiveunderstandingoftheimprovedperformance,highlightingthattheSAR-FCOS\n                       modelcombinedwithBoxPasteyieldsfewermisseddetectionsandhigherrecall.\n\nRemoteSens.2022,14,5761                                             11of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                               FCOS         SAR-FCOS w/ BoxPaste  GT              \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                       Figure6.VisualizationofthedetectionresultsofFCOS,SAR-FCOSwithBoxPaste,andground-truth.\n                       Thecyan,red,yellow,andgreenboxesindicatethetestresultsofFCOS,theshipobjectmissedthe\n                       testresultsofours,andthegroundtruth.                       \n                       Table2.AblationstudyonBoxPastewithregardtoscalejitteringandtrainingepochs.\n                                                                                  \n                                ScaleJit. Total                                   \n                                                BoxPaste AP(%) Recall(%) FPS      \n                                 Range  Epochs                                    \n                                  -       12      -      93.0   94.4              \n                                  -       12      (cid:88) 94.1 95.1              \n                                [0.5,1.5] 12      (cid:88) 94.5 95.2              \n                        SAR-FCOS [0.1,2]  12      (cid:88) 94.6 95.5    110.1     \n                                 [0.1,2]  24      (cid:88) 95.2 96.5              \n                                 [0.1,2]  36      (cid:88) 95.5 96.6              \n                                 [0.1,2]  48      (cid:88) 95.0 96.1              \n                       4.2.3. ComparingBoxPastetoCopyPaste                        \n                          SincetheSSDDdatasetprovidessegmentationannotation,wealsoperformCopyPaste\n                       dataaugmentationonSSDD.TheexperimentalresultsareshowninTable 3. Itshowsthat\n                       CopyPastemarginallyimprovesAPby0.3%,whichmaybeduetoCopyPaste’sprecise\n                       croppingoftheobject. Meanwhile,becausemostoftheSARshipdatabasebackgroundis\n                       simpleandclean,theperformanceofusingBoxPasteisveryclosetoCopyPaste. Sincethe\n                       segmentationmaskismoredifficulttoobtainthantheobjectbox,makingBoxPastemore\n                       feasibleinthereal-worldscenario.\n\nRemoteSens.2022,14,5761                                             12of15   \n                                                                                  \n                                                                                  \n                       Table3.PerformancecomparisonbetweenBoxPaste+BBox-SSDDandCopyPaste+PSeg-SSDD.\n                                                                                  \n                                                AP(%)            Recall(%)        \n                          BoxPaste+BBox-SSDD     95.5              96.6           \n                          CopyPaste+PSeg-SSDD    95.8              97.0           \n                                                                                  \n                       4.3. WideApplicability                                     \n                          Thetrialspresentedintheprevioussub-sectionssolelyreliedonFCOS.Hence,weval-\n                       idateourmethods’broadapplicabilityinthissub-sectionbycombiningitwithRetinaNet\n                       andAdaptiveTrainingSampleSelection(ATSS)[48]. WhileFCOSisananchor-freeone-\n                       stagedetector,RetinaNetisanchor-based,andATSSadoptsanadvancedlabel-assigning\n                       strategy. BecauseRetinaNetandATSSleveragedifferentlabel-assigningstrategiesthatare\n                       probablymoresuitableinSARshipdetectiontasks,therefore,theirbaselineperformances\n                       arehigherthanFCOS.Onthesedetectors,weapplyboththelighthead/FPNandBoxPaste,\n                       whileinanycase,ResNet-50isthebackbonemodel. Thecorrespondingexperimentalre-\n                       sultsarepresentedinTable4,highlightingthatRetinaNetandATSSattain92.6%and94.4%\n                       AP,farbetterthanFCOS(90.8%AP).PossiblyduetoRetinaNetbeingananchor-based\n                       detector,whileATSSusesanadvancedlabel-assigningstrategy. Bothdetectorsincrease\n                       the number of positive training samples per mini-batch. Nevertheless, using the light\n                       head/FPNstillimprovestheirAPby0.7%and0.5%,respectively. AfteradoptingBoxPaste,\n                       thedetectionperformanceisfurtherimprovedto95.8%and96.3%,respectively,validating\n                       theapplicabilityofourproposedmethodsondifferentdetectors. Thenewdetectors,i.e.,\n                       SAR-RetinaNetandSAR-ATSS,arecomparedagainstotherstate-of-the-artSARshipde-\n                       tectorsonSSDD,utilizingResNet-50asthebackbonenetwork. Thecounterpartsinclude\n                       one-stagedetectors,suchasATSS,aswellaspowerfultwo-stagedetectors,suchasFaster\n                       R-CNNandCascadeR-CNN.TheresultsinFigure7showthatSAR-ATSSachieves96.3%\n                       AP,surpassingallotherpreviouswork.                         \n                                                                                  \n                       Table4.Theeffectsoflighthead/FPNandBoxPasteonRetinaNetandAdaptiveTrainingSample\n                       Selection(ATSS).                                           \n                                       RetinaNet                ATSS              \n                                                                                  \n                                Baseline +Light +BoxPaste Baseline +Light +BoxPaste\n                         AP(%)   92.6    93.3    95.8    94.4   94.9    96.3      \n                        Recall(%) 94.1   94.2    96.5    95.5   95.5    97.0      \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n                          0.97                                        0.963       \n                                        0.959                    0.958            \n                          0.96               0.953                                \n                          0.95     0.944          0.947 0.946 0.949               \n                          0.94                                                    \n                          0.93 0.926                                              \n                          0.92                                                    \n                          0.91                                                    \n                           0.9                                                    \n                        PA                                                        \n                                                                                  \n                                                                                  \n                       Figure7.CombiningtheproposedmethodsonRetinaNetandATSSandcomparingthemagainst\n                       currentstate-of-the-artmethods.                            \n                                                                                  \n                                                                                  \n                                                                                  \n                                                                                  \n\n[表格内容]\n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  | 0.953\n0.947 0.946 0.949 |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n\nRemoteSens.2022,14,5761                                             13of15   \n                                                                                  \n                                                                                  \n                                                                                  \n                       5. Conclusions                                             \n                          Thecharacteristicsofthedatasetareimportantforappropriatedetectionarchitecture\n                       designandtrainingrecipes. InSARshipdetection,objectsize,density,andbackground\n                       diversityareessentiallydifferentfromgeneralobjectdetection,suchasVOCandCOCO,\n                       motivatingustoexploredomain-specifictechniquesinit. Inthiswork,wefirstpresent\n                       BoxPaste,asimplebuteffectivedataaugmentationmethodforSARshipdetectionthat\n                       cropstheshipobjectsfromonetrainingimageusingboundingboxannotationsandpastes\n                       themonanotherimage. Despiteitssimplicity,BoxPastesignificantlyimprovesitsbaseline\n                       by 4.7% mAP. Given the SAR ship image dataset characteristics, we also introduce a\n                       principlefordesigningaSARshipdetector,i.e.,alargermodeldoesnotguaranteebetter\n                       performance. Withthisprinciple,weconductthoroughexperimentsonFCOSandpropose\n                       SAR-FCOS,whichrunstwiceasfastandachievesbetterdetectionperformance. Thorough\n                       experimentsareconductedthatvalidatetheeffectivenessofourproposedmethods.\n                       AuthorContributions: Conceptualization, Z.S.; methodology, Z.S.; software, Z.S., S.C.andY.H.;\n                       validation,Z.S.;formalanalysis,Z.S.;investigation,Z.S.,S.C.andY.H.;resources,Z.S.andY.Z.;data\n                       curation,Z.S.andY.Z.;writing—originaldraftpreparation,Z.S.;writing—reviewandediting,Y.Z.,\n                       S.C.andY.H.;visualization,Z.S.,S.C.andY.H.;supervision,Y.Z.;projectadministration,Y.Z.;funding\n                       acquisition,Y.Z.Allauthorshavereadandagreedtothepublishedversionofthemanuscript.\n                       Funding: ThisresearchwasfundedbytheFundforForeignScholarsinUniversityResearchand\n                       TeachingPrograms(the111Project),grantnumberB18039.         \n                       DataAvailabilityStatement:Notapplicable.                   \n                                                                                  \n                       ConflictsofInterest:Theauthorsdeclarenoconflictofinterest. \n     References                                                                   \n                                                                                  \n     1. Lee,H.J.;Huang,L.F.;Chen,Z. Multi-frameshipdetectionandtrackinginaninfraredimagesequence. PatternRecognit.1990,\n        23,785–798.[CrossRef]                                                     \n     2. Mingbo,Z.;Jianwu,Z.;Jianguo,H. ImagingsimulationofseasurfacewithfullpolarizationSAR. InProceedingsofthe2015IEEE\n        5thAsia-PacificConferenceonSyntheticApertureRadar(APSAR),Singapore,1–4September2015;pp.815–817.\n     3. Brusch,S.;Lehner,S.;Fritz,T.;Soccorsi,M.;Soloviev,A.;vanSchie,B. ShipSurveillancewithTerraSAR-X. IEEETrans.Geosci.\n        RemoteSens.2011,49,1092–1103.[CrossRef]                                   \n     4. Eldhuset,K.AnautomaticshipandshipwakedetectionsystemforspaceborneSARimagesincoastalregions. IEEETrans.Geosci.\n        RemoteSens. 1996,34,1010–1019.[CrossRef]                                  \n     5. Li,J.;Qu,C.;Shao,J. ShipdetectioninSARimagesbasedonanimprovedfasterR-CNN. InProceedingsofthe2017SARinBig\n        DataEra:Models,MethodsandApplications(BIGSARDATA),Beijing,China,13–14November2017;pp.1–6.\n     6. Ouchi, K.; Tamaki, S.; Yaguchi, H.; Iehara, M. Shipdetectionbasedoncoherenceimagesderivedfromcrosscorrelationof\n        multilookSARimages. IEEEGeosci.RemoteSens.Lett.2004,1,184–187.[CrossRef]  \n     7. Ai,J.;Qi,X.;Yu,W.;Deng,Y.;Liu,F.;Shi,L. AnewCFARshipdetectionalgorithmbasedon2-Djointlog-normaldistributionin\n        SARimages. IEEEGeosci.RemoteSens.Lett.2010,7,806–810.[CrossRef]           \n     8. Wang,C.;Jiang,S.;Zhang,H.;Wu,F.;Zhang,B. Shipdetectionforhigh-resolutionSARimagesbasedonfeatureanalysis. IEEE\n        Geosci.RemoteSens.Lett.2013,11,119–123.[CrossRef]                         \n     9. Leng,X.;Ji,K.;Yang,K.;Zou,H. AbilateralCFARalgorithmforshipdetectioninSARimages. IEEEGeosci.RemoteSens.Lett.\n        2015,12,1536–1540.[CrossRef]                                              \n     10. Zhai,L.;Li,Y.;Su,Y. Inshoreshipdetectionviasaliencyandcontextinformationinhigh-resolutionSARimages. IEEEGeosci.\n        RemoteSens.Lett.2016,13,1870–1874.[CrossRef]                              \n     11. Ren,S.;He,K.;Girshick,R.;Sun,J. Fasterr-cnn:Towardsreal-timeobjectdetectionwithregionproposalnetworks. Adv.Neural\n        Inf.Process.Syst. 2015,28,91–99.[CrossRef]                                \n     12. Redmon,J.;Farhadi,A. Yolov3:Anincrementalimprovement. arXiv2018,arXiv:1804.02767.\n     13. Liu,W.;Anguelov,D.;Erhan,D.;Szegedy,C.;Reed,S.;Fu,C.Y.;Berg,A.C. SSD:Singleshotmultiboxdetector. InProceedingsof\n        theEuropeanConferenceonComputerVision,Amsterdam,TheNetherlands,11–14October2016;pp.21–37.\n     14. Lin,T.Y.;Goyal,P.;Girshick,R.;He,K.;Dollár,P. Focallossfordenseobjectdetection. InProceedingsoftheIEEEInternational\n        ConferenceonComputerVision,Venice,Italy,22–29October2017;pp.2980–2988.    \n     15. Tian,Z.; Shen,C.; Chen,H.; He,T. Fcos: Fullyconvolutionalone-stageobjectdetection. InProceedingsoftheIEEE/CVF\n        InternationalConferenceonComputerVision,Seoul,Korea,27–28October2019;pp.9627–9636.\n     16. Ge,Z.;Liu,S.;Li,Z.;Yoshie,O.;Sun,J.OTA:OptimalTransportAssignmentforObjectDetection. InProceedingsoftheProceedings\n        oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,Nashville,TN,USA,20–25June2021;pp.303–312.\n\nRemoteSens.2022,14,5761                                             14of15   \n                                                                                  \n                                                                                  \n                                                                                  \n     17. Ge,Z.;Liu,S.;Wang,F.;Li,Z.;Sun,J. Yolox:Exceedingyoloseriesin2021. arXiv2021,arXiv:2107.08430.\n     18. Jiao,J.;Zhang,Y.;Sun,H.;Yang,X.;Gao,X.;Hong,W.;Fu,K.;Sun,X. Adenselyconnectedend-to-endneuralnetworkfor\n        multiscaleandmultisceneSARshipdetection. IEEEAccess2018,6,20881–20892.[CrossRef]\n     19. Kang,M.;Leng,X.;Lin,Z.;Ji,K. AmodifiedfasterR-CNNbasedonCFARalgorithmforSARshipdetection. InProceedingsof\n        the2017InternationalWorkshoponRemoteSensingwithIntelligentProcessing(RSIP),Shanghai,China,18–21May2017;pp.1–4.\n     20. Zhang,T.;Zhang,X.;Shi,J.;Wei,S. Depthwiseseparableconvolutionneuralnetworkforhigh-speedSARshipdetection. Remote\n        Sens.2019,11,2483.[CrossRef]                                              \n     21. Ge,Z.;Jie,Z.;Huang,X.;Li,C.;Yoshie,O. Delvingdeepintotheimbalanceofpositiveproposalsintwo-stageobjectdetection.\n        Neurocomputing2021,425,107–116.[CrossRef]                                 \n     22. He,K.;Zhang,X.;Ren,S.;Sun,J. Deepresiduallearningforimagerecognition. InProceedingsoftheIEEEConferenceon\n        ComputerVisionandPatternRecognition,LasVegas,NV,USA,27–30June2016;pp.770–778.\n     23. Wang,Y.;Wang,C.;Zhang,H.;Dong,Y.;Wei,S. ASARdatasetofshipdetectionfordeeplearningundercomplexbackgrounds.\n        RemoteSens.2019,11,765.[CrossRef]                                         \n     24. Xian,S.;Zhirui,W.;Yuanrui,S.;Wenhui,D.;Yue,Z.;Kun,F. AIR-SARShip-1.0:High-resolutionSARshipdetectiondataset. J.\n        Radars2019,8,852–862.                                                     \n     25. Kang,M.;Ji,K.;Leng,X.;Lin,Z. Contextualregion-basedconvolutionalneuralnetworkwithmultilayerfusionforSARship\n        detection. RemoteSens.2017,9,860.[CrossRef]                               \n     26. Chang,Y.L.;Anagaw,A.;Chang,L.;Wang,Y.C.;Hsiao,C.Y.;Lee,W.H.ShipdetectionbasedonYOLOv2forSARimagery. Remote\n        Sens.2019,11,786.[CrossRef]                                               \n     27. Redmon,J.;Farhadi,A. YOLO9000: Better,faster,stronger. InProceedingsoftheIEEEConferenceonComputerVisionand\n        PatternRecognition,Honolulu,HI,USA,21–26July2017;pp.7263–7271.            \n     28. Wei,S.;Su,H.;Ming,J.;Wang,C.;Yan,M.;Kumar,D.;Shi,J.;Zhang,X. Preciseandrobustshipdetectionforhigh-resolutionSAR\n        imagerybasedonHR-SDNet. RemoteSens.2020,12,167.[CrossRef]                 \n     29. Zhao,Y.;Zhao,L.;Xiong,B.;Kuang,G. AttentionreceptivepyramidnetworkforshipdetectioninSARimages. IEEEJ.Sel.Top.\n        Appl.EarthObs.RemoteSens.2020,13,2738–2756.[CrossRef]                     \n     30. Mao,Y.;Yang,Y.;Ma,Z.;Li,M.;Su,H.;Zhang,J. Efficientlow-costshipdetectionforSARimagerybasedonsimplifiedU-net.\n        IEEEAccess2020,8,69742–69753.[CrossRef]                                   \n     31. Zhou,X.;Wang,D.;Krähenbühl,P. Objectsaspoints. arXiv2019,arXiv:1904.07850.\n     32. Guo,H.;Yang,X.;Wang,N.;Gao,X. ACenterNet++modelforshipdetectioninSARimages. PatternRecognit. 2021,112,107787.\n        [CrossRef]                                                                \n     33. Gao,F.;He,Y.;Wang,J.;Hussain,A.;Zhou,H. Anchor-freeconvolutionalnetworkwithdenseattentionfeatureaggregationfor\n        shipdetectioninSARimages. RemoteSens.2020,12,2619.[CrossRef]              \n     34. Dvornik,N.;Mairal,J.;Schmid,C. Modelingvisualcontextiskeytoaugmentingobjectdetectiondatasets. InProceedingsofthe\n        EuropeanConferenceonComputerVision(ECCV),Munich,Germany,8–14September2018;pp.364–380.\n     35. Dwibedi,D.;Misra,I.;Hebert,M. Cut,pasteandlearn:Surprisinglyeasysynthesisforinstancedetection. InProceedingsofthe\n        IEEEInternationalConferenceonComputerVision,Venice,Italy,22–29October2017;pp.1301–1310.\n     36. Zhang,H.;Cisse,M.;Dauphin,Y.N.;Lopez-Paz,D. mixup:Beyondempiricalriskminimization. arXiv2017,arXiv:1710.09412.\n     37. Yun,S.;Han,D.;Chun,S.;Oh,S.J.;Yoo,Y.;Choe,J. CutMix:RegularizationStrategytoTrainStrongClassifiersWithLocalizable\n        Features. InProceedingsofthe2019IEEE/CVFInternationalConferenceonComputerVision(ICCV),Seoul,Korea,27October–2\n        November2019;pp.6022–6031.                                                \n     38. Ghiasi,G.;Cui,Y.;Srinivas,A.;Qian,R.;Lin,T.Y.;Cubuk,E.D.;Le,Q.V.;Zoph,B.Simplecopy-pasteisastrongdataaugmentation\n        methodforinstancesegmentation. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,\n        Nashville,TN,USA,20–25June2021;pp.2918–2928.                              \n     39. Lin,T.Y.;Dollár,P.;Girshick,R.;He,K.;Hariharan,B.;Belongie,S. Featurepyramidnetworksforobjectdetection. InProceedings\n        oftheIEEEConferenceonComputerVisionandPatternRecognition,Honolulu,HI,USA,21–26July2017;pp.2117–2125.\n     40. Simonyan,K.;Zisserman,A. Verydeepconvolutionalnetworksforlarge-scaleimagerecognition. arXiv2014,arXiv:1409.1556.\n     41. Szegedy,C.; Ioffe,S.; Vanhoucke,V.; Alemi,A.A. Inception-v4,inception-resnetandtheimpactofresidualconnectionson\n        learning. InProceedingsoftheThirty-FrstAAAIConferenceonArtificialIntelligence,SanFrancisco,CA,USA,4–9February2017.\n     42. Liu,S.;Huang,D.;Wang,Y. Learningspatialfusionforsingle-shotobjectdetection. arXiv2019,arXiv:1911.09516.\n     43. Lin,T.Y.;Maire,M.;Belongie,S.;Hays,J.;Perona,P.;Ramanan,D.;Dollár,P.;Zitnick,C.L. Microsoftcoco:Commonobjectsin\n        context. InProceedingsoftheEuropeanConferenceonComputerVision,Zurich,Switzerland,6–12September2014;pp.740–755.\n     44. Everingham,M.;VanGool,L.;Williams,C.K.;Winn,J.;Zisserman,A. Thepascalvisualobjectclasses(voc)challenge. Int. J.\n        Comput.Vis. 2010,88,303–338.[CrossRef]                                    \n     45. Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.; Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.; etal.\n        Imagenetlargescalevisualrecognitionchallenge. Int.J.Comput.Vis. 2015,115,211–252.[CrossRef]\n     46. Hu,J.;Shen,L.;Sun,G. Squeeze-and-excitationnetworks. InProceedingsoftheIEEEConferenceonComputerVisionand\n        PatternRecognition,SaltLakeCity,UT,USA,18–23June2018;pp.7132–7141.\n\nRemoteSens.2022,14,5761                                             15of15   \n                                                                                  \n                                                                                  \n                                                                                  \n     47. Woo,S.;Park,J.;Lee,J.Y.;Kweon,I.S. Cbam:Convolutionalblockattentionmodule. InProceedingsoftheEuropeanConference\n        onComputerVision(ECCV),Munich,Germany,8–14September2018;pp.3–19.          \n     48. Zhang,S.;Chi,C.;Yao,Y.;Lei,Z.;Li,S.Z. Bridgingthegapbetweenanchor-basedandanchor-freedetectionviaadaptivetraining\n        sampleselection. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,Seattle,WA,USA,\n        13–19June2020;pp.9759–9768."
  },
  "pdf_url": "/uploads/34ea07ad772980e13bf24cd1fdf282bf.pdf"
}