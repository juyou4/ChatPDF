{
  "filename": "Akyon 等 - 2022 - Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection.pdf",
  "upload_time": "2025-12-02T09:59:55.600105",
  "data": {
    "total_pages": 5,
    "pages": [
      {
        "page": 1,
        "content": "SLICINGAIDEDHYPERINFERENCEANDFINE-TUNING                        \n                             FORSMALLOBJECTDETECTION                                \n                                                                                    \n                                                                                    \n                    FatihCagatayAkyon1,2,SinanOnurAltinuc1,2,AlptekinTemizel2       \n                                                                                    \n                                 1OBSSAI,Ankara,Turkey                              \n               2GraduateSchoolofInformatics,MiddleEastTechnicalUniversity,Ankara,Turkey\n                                                                                    \n                                                                                    \n                     ABSTRACT                                                       \n        Detectionofsmallobjectsandobjectsfarawayinthesceneis                        \n        a major challenge in surveillance applications. Such objects                \n        are represented by small number of pixels in the image and                  \n        lack sufficient details, making them difficult to detect using              \n        conventional detectors. In this work, an open-source frame-                 \n        work called Slicing Aided Hyper Inference (SAHI) is pro-                    \n        posedthatprovidesagenericslicingaidedinferenceandfine-                      \n        tuningpipelineforsmallobjectdetection.Theproposedtech-                      \n                                           Fig. 1: Results for inference with TOOD detector (left),\n        nique is generic in the sense that it can be applied on top of              \n                                           Slicing-aided hyper inference (middle), Slicing-aided hyper\n        anyavailableobjectdetectorwithoutanyfine-tuning. Exper-                     \n                                           inferenceafterslicing-aidedfine-tuning(right).\n        imental evaluations, using object detection baselines on the                \n        VisdroneandxViewaerialobjectdetectiondatasetsshowthat The recent advances in drones, 4K cameras and deep\n        theproposedinferencemethodcanincreaseobjectdetection learning research have enabled long-range object detection\n        AP by 6.8%, 5.1% and 5.3% for FCOS, VFNet and TOOD that is met under Detection, Observation, Recognition and\n        detectors,respectively. Moreover,thedetectionaccuracycan Identification (DORI) criteria [8]. DORI criteria define the\n        be further increased with a slicing aided fine-tuning, result- minimumpixelheightoftheobjectsfordifferenttasks: 10%\n        inginacumulativeincreaseof12.7%,13.4%and14.5%AP oftheimageheightisrequiredtodetectand20%torecognize\n        in the same order. Proposed technique has been integrated the objects (108 pixels in full HD videos). Relatively small\n        with Detectron2, MMDetection and YOLOv5 models and it pixelcoveragepushesthelimitsofCNNbasedobjectdetec-\n        ispubliclyavailableathttps://github.com/obss/sahi.git tion methods, in addition, high-resolution images demands\n                                           greaterneedsintermsofmemoryrequirements. \n          Index Terms— small object detection, sliced inference,                    \n                                              Inthispaper,weproposeagenericsolutionbasedonslic-\n        windowedinference,visdrone,xview                                            \n                                           ingaidedinferenceandfine-tuningforsmallobjectdetection\n                  1. INTRODUCTION          onhigh-resolutionimageswhilemaintaininghighermemory\n                                           utilization. Fig. 1illustratestheimprovementofsmallobject\n        In recent years, object detection has been extensively stud-                \n                                           detectiononasampleimagefromVisdronetestset.\n        ied for different applications including face detection, video              \n        objectdetection,videosurveillance,self-drivingcars. Inthis                  \n                                                      2. RELATEDWORK                \n        field, adoption of deep learning architectures has resulted in              \n        highly accurate methods such as Faster R-CNN [1], Reti- Therecentlearning-basedobjectdetectiontechniquescanbe\n        naNet[2],thatarefurtherdevelopedasCascadeR-CNN[3], categorizedintotwomaintypes. Single-stagedetectors,such\n        VarifocalNet [4], and variants. All of these recent detectors as SSD [9], YOLO [10], RetinaNet [2], directly predict the\n        aretrainedandevaluatedonwell-knowndatasetssuchasIm- location of objects without an explicit proposal stage. Two-\n        ageNet[5],PascalVOC12[6],MSCOCO[7].Thesedatasets stage region proposal based methods, such as Fast R-CNN\n        mostlyinvolvelow-resolutionimages(640×480)including [11],FasterR-CNN[1],CascadeR-CNN[3],involveanini-\n        considerablylargeobjectswithlargepixelcoverage(covering tial region proposal stage. These proposals are then refined\n        60%oftheimageheightinaverage). Whilethetrainedmod- todefinethepositionandsizeoftheobject. Typically,single-\n        elshavesuccessfuldetectionperformancesforthosetypesof stageapproachesarefasterthantwo-stage,whilethelatterhas\n        input data, they yield significantly lower accuracy on small higheraccuracy.\n        objectdetectiontasksinhigh-resolutionimagesgeneratedby More recently, anchor-free detectors have started to at-\n        thehigh-enddroneandsurveillancecameras. tract attention. They eliminate the use of anchor boxes and\n  2202                                                                              \n  tcO                                                                               \n  42                                                                                \n  ]VC.sc[                                                                           \n  5v43960.2022:viXra",
        "char_count": 6788,
        "has_tables": false
      },
      {
        "page": 2,
        "content": "classifyeachpointonthefeaturepyramid[12]asforeground available objects detector can be boosted without any fine-\n        or background, and directly predict the distances from the tuning(byslicingaidedinference). Moreover,additionalper-\n        foregroundpointtothefoursidesoftheground-truthbound- formance boost can be gained by fine-tuning the pretrained\n        ing box, to produce the detection. FCOS [13] is the first models.           \n        object detector eliminating the need for predefined set of                  \n                                                    3. PROPOSEDAPPROACH             \n        anchor boxes and entailing computational need. Varifocal-                   \n        Net (VFNet) [4] learns to predict the IoU-aware classifica- Inordertohandlethesmallobjectdetectionproblem,wepro-\n        tion score which mixes the object presence confidence and poseagenericframeworkbasedonslicinginthefine-tuning\n        localization accuracy together as the detection score for a and inference stages. Dividing the input images into over-\n        bounding box. The learning is supervised by the proposed lappingpatchesresultsinrelativelargerpixelareasforsmall\n        Varifocal Loss (VFL), based on a new star-shaped bounding objectswithrespecttotheimagesfedintothenetwork.\n        box feature representation. TOOD [14] explicitly aligns the                 \n        twotasks(objectclassificationandlocalization)inalearning-                   \n        based manner utilizing novel task-aligned head which offers                 \n        a better balance between learning task-interactive and task-                \n        specific features and task alignment learning via a designed                \n        sampleassignmentschemeandatask-alignedloss.                                 \n          Thealgorithmsdesignedforgeneralobjectdetectionper-                        \n        formpoorlyonhighresolutionimagesthatcontainsmalland                         \n        dense objects, leading to specific approaches for small ob-                 \n        ject detection. In [15], a particle swarm optimization (PSO)                \n        and bacterial foraging optimization (BFO)-based learning                    \n        strategy (PBLS) is used to optimize the classifier and loss                 \n        function. However, these heavy modifications to the origi-                  \n        nal models prevent fine-tuning from pretrained weights and                  \n        require training from scratch. Moreover, due to unusual                     \n        optimization steps they are hard to adapt into a present de-                \n        tector. The method proposed in [16] oversamples images                      \n        with small objects and augments them by making several                      \n        copiesofsmallobjects. However,thisaugmentationrequires                      \n        segmentation annotations and, as such, it is not compatible                 \n        with the object detection datasets. The method in [17] can                  \n        learnricherfeaturesofsmallobjectsfromtheenlargedareas,                      \n        which are clipped from the raw image. The extra features                    \n        positively contribute to the detection performance but the Fig. 2: Slicing aided fine-tuning (top) and slicing aided hy-\n        selection of the areas to be enlarged brings a computational perinference(bottom)methods. Infinetuning, thedatasetis\n        burden. In[18],afullyconvolutionalnetworkisproposedfor augmented by extracting patches from the images and resiz-\n        small object detection that contains an early visual attention ingthemtoalargersize. Duringinference,imageisdivided\n        mechanism that is proposed to choose the most promising intosmallerpatchesandpredictionsaregeneratedfromlarger\n        regions with small objects and their context. In [19], a slic- resizedversionsofthesepatches. Thenthesepredictionsare\n        ing based technique is proposed but the implementation is converted back into original image coordinates after NMS.\n        not generic and only applicable to specific object detectors. Optionally,predictionsfromfullinferencecanalsobeadded.\n        In [20], a novel network (called JCS-Net) is proposed for SlicingAidedFine-tuning(SF):Widelyusedobjectde-\n        small-scale pedestrian detection, which integrates the classi- tection frameworks such as Detectron2 [22], MMDetection\n        ficationtaskandthesuper-resolutiontaskinaunifiedframe- [23] and YOLOv5 [24] provide pretrained weights on the\n        work. [21]proposedanalgorithmtodirectlygenerateaclear datasets such as ImageNet [5] and MS COCO [7]. This al-\n        high-resolution face from a blurry small one by adopting a lows us to fine-tune the model using smaller datasets and\n        generativeadversarialnetwork(GAN).However,sincethese overshortertrainingspansincontrasttotrainingfromscratch\n        techniques propose new detector architectures they require with large datasets. These common datasets mostly involve\n        pretrainingfromscratchwithlargedatasetswhichiscostly. low-resolutionimages(640×480)havingconsiderablylarge\n          In contrast to the mentioned techniques, we propose a objectswithlargepixelcoverage(covering60%oftheimage\n        generic slicing aided fine-tuning and inference pipeline that heightinaverage).Themodelspretrainedusingthesedatasets\n        can be utilized on top of any existing object detector. This provideverysuccessfuldetectionperformanceforsimilarin-\n        way, small object detection performance of any currently puts. Ontheotherhand, theyyieldsignificantlyloweraccu-",
        "char_count": 6208,
        "has_tables": false
      },
      {
        "page": 3,
        "content": "racyonsmallobjectdetectiontasksinhigh-resolutionimages collectedatdifferentlocationsbutinsimilarenvironments.\n        generatedbythehigh-enddroneandsurveillancecameras. xView[26]isoneofthelargestpubliclyavailabledatasets\n          In order to overcome this issue, we augment the dataset for object detection from satellite imagery. It contains im-\n        withbyextractingpatchesfromtheimagesfine-tuningdataset agesfromcomplexscenesaroundtheworld,annotatedusing\n        as seen in Fig. 2. Each image IF,IF,...,IF is sliced into bounding boxes. It contains over 1M object instances from\n                          1 2   j                                                   \n        overlappingpatches PF,PF,...PF withdimensions M and 60 different classes. During the experiments, randomly se-\n                    1 2   k                                                         \n        N are selected within predefined ranges [M ,M ] and lected75%and25%splitshavebeenusedasthetrainingand\n                                min max                                             \n        [N ,N  ] which are treated as hyper-parameters. Then validationsets,respectively.\n         min max                                                                    \n        during fine-tuning, patches are resized by preserving the as- Bothofthesedatasetscontainsmallobjects(objectwidth\n        pectratiosothatimagewidthisbetween800to1333pixels <1%ofimagewidth).         \n        toobtainaugmentationimagesI(cid:48),I(cid:48),...,I(cid:48),wherebytherel-  \n                         1 2   k                 Setup     AP  AP s AP m AP l       \n        ative object sizes are larger compared to the original image. 50 50 50 50   \n                                                FCOS+FI    25.8 14.2 39.6 45.1      \n        TheseimagesI(cid:48),I(cid:48),...,I(cid:48),togetherwiththeoriginalimages  \n                1 2  k                        FCOS+SAHI+PO 29.0 18.9 41.5 46.4      \n        IF,IF,...,IF (tofacilitatedetectionoflargeobjects),areuti-                  \n        1 2   j                              FCOS+SAHI+FI+PO 31.0 19.8 44.6 49.0    \n        lizedduringfine-tuning. Ithastobenotedthat, asthepatch                      \n                                             FCOS+SF+SAHI+PO 38.1 25.7 54.8 56.9    \n        sizesdecrease,largerobjectsmaynotfitwithinasliceandthe FCOS+SF+SAHI+FI+PO 38.5 25.9 55.4 59.8\n        intersectingareas,andthismayleadtopoordetectionperfor- VFNet+FI 28.8 16.8 44.0 47.5\n        manceforlargerobjects.                VFNet+SAHI+PO 32.0 21.4 45.8 45.5     \n          SlicingAidedHyperInference(SAHI):Slicingmethod VFNet+SAHI+FI+PO 33.9 22.4 49.1 49.4\n        isalsoutilizedduringtheinferencestepasdetailedinFig. 2. VFNet+SF+SAHI+PO 41.9 29.7 58.8 60.6\n        First, the original query image I is sliced into l number of VFNet+SF+SAHI+FI+PO 42.2 29.6 59.2 63.3\n        M×N overlappingpatchesPI,PI,...PI . Then,eachpatch TOOD+FI 29.4 18.1 44.1 50.0\n                        1 2   l                                                     \n                                               TOOD+SAHI   31.9 22.6 44.0 45.2      \n        is resized while preserving the aspect ratio. After that, ob-               \n                                              TOOD+SAHI+PO 32.5 22.8 45.2 43.6      \n        ject detection forward pass is applied independently to each                \n                                              TOOD+SAHI+FI 34.6 23.8 48.5 53.1      \n        overlapping patch. An optional full-inference (FI) using the                \n                                             TOOD+SAHI+FI+PO 34.7 23.8 48.9 50.3    \n        original image can be applied to detect larger objects. Fi-                 \n                                               TOOD+SF+FI  36.8 24.4 53.8 66.4      \n        nally, the overlapping prediction results and, if used, FI re-              \n                                              TOOD+SF+SAHI 42.5 31.6 58.0 61.1      \n        sults are merged back into to original size using NMS. Dur- TOOD+SF+SAHI+PO 43.1 31.7 59.0 60.2\n        ingNMS,boxeshavinghigherIntersectionoverUnion(IoU) TOOD+SF+SAHI+FI 43.4 31.7 59.6 65.6\n        ratios than a predefined matching threshold T are matched TOOD+SF+SAHI+FI+PO 43.5 31.7 59.8 65.4\n                                 m                                                  \n        and for each match, detections having detection probability                 \n                                           Table 1: Mean average precision values calculated on\n        thanlowerthanT areremoved.                                                  \n                 d                         Visdrone19-Detectiontest-devset.SF,SAHI,FI,andPOcor-\n                    4. RESULTS             respond to slicing aided fine-tuning, slicing aided inference,\n                                           fullimageinference,andoverlappingpatches,respectively.\n        The proposed method has been integrated into FCOS [13],                     \n        VarifocalNet [4] and TOOD [14] object detectors using                       \n                                                 Setup     AP  AP s AP m AP l       \n                                                             50  50   50   50       \n        MMDetection [23] framework for experimental evaluation.                     \n                                                FCOS+FI    2.20 0.10 1.80 7.30      \n        Related config files, conversion and evaluation scripts, eval- FCOS+SF+SAHI 15.8 11.9 18.4 11.0\n        uation result files have been publicly provided 1. All slicing FCOS+SF+SAHI+PO 17.1 12.2 20.2 12.8\n        related operations have also been made publicly available to FCOS+SF+SAHI+FI 15.7 11.9 18.4 14.3\n        enableintegrationintootherobjectdetectionframeworks2. FCOS+SF+SAHI+FI+PO 17.0 12.2 20.2 15.8\n          VisDrone2019-Detection [25] is an object detection VFNet+FI 2.10 0.50 1.80 6.80\n        dataset having 8599 images captured by drone platforms VFNet+SF+SAHI 16.0 11.9 17.6 13.1\n                                             VFNet+SF+SAHI+PO 17.7 13.7 19.7 15.4   \n        at different locations and at different heights. Most of the                \n                                             VFNet+SF+SAHI+FI 15.8 11.9 17.5 15.2   \n        objectsinthisdatasetaresmall, denselydistributedandpar-                     \n                                            VFNet+SF+SAHI+FI+PO 17.5 13.7 19.6 17.6 \n        tially occluded. There are also illumination and perspective                \n                                                TOOD+FI    2.10 0.10 2.00 5.20      \n        changes in different scenarios. More than 540k bounding                     \n                                              TOOD+SF+SAHI 19.4 14.6 22.5 14.2      \n        boxesoftargetsareannotatedwithtenpredefinedcategories:                      \n                                             TOOD+SF+SAHI+PO 20.6 14.9 23.6 17.0    \n        pedestrian,person,bicycle,car,van,truck,tricycle,awning-                    \n                                             TOOD+SF+SAHI+FI 19.2 14.6 22.3 14.7    \n        tricycle, bus, motor. Super categories are defined as pedes- TOOD+SF+SAHI+FI+PO 20.4 14.9 23.5 17.6\n        trian, motor, carandtruck. Thetrainingandvalidationsub-                     \n                                           Table2: MeanaverageprecisionvaluescalculatedonxView\n        setsconsistsof6471and548images,respectivelywhichare                         \n                                           validation split. SF, SAHI, FI, and PO correspond to slicing\n         1https://github.com/fcakyon/sahi-benchmark aided fine-tuning, slicing aided inference, full image infer-\n         2https://github.com/obss/sahi     ence,andoverlappingpatches,respectively.",
        "char_count": 8358,
        "has_tables": false
      },
      {
        "page": 4,
        "content": "theadditionalsmallobjecttruepositivespredictedfromslices\n                                           and decrease is caused by the false positives predicted from\n                                           slicesthatmatchinglargegroundtruthboxes. Bestsmallob-\n                                           ject detection AP is achieved by SF followed by SI, while\n                                           best large object detection AP is achieved by SF followed\n                                           by FI, confirming the contribution of FI for large object de-\n                                           tection. Results for xView dataset is presented in Table 2.\n                                           SincexViewtargetsareverysmall,regulartrainingwithorig-\n                                           inal images yields poor detection performance and SF im-\n        Fig. 3: Error analysis curve for TOOD object detector in                    \n                                           proves the results substantially. Integration of FI increases\n        SF+SAHIsettingcalculatedonVisdrone19-Dettest-devset.                        \n                                           largeobjectAPbyupto3.3%butresultsinslightlydecreased\n                                           small/medium object AP, which is expected as some of the\n                                           largerobjectsmaynotbedetectedfromsmallerslices. 25%\n                                           overlap between slices increase the detection AP by up to\n                                           1.7%. xViewcontainshighlyimbalanced60targetcategories\n                                           and despite being an older and, reportedly weaker detector,\n                                           FCOS yields a very close performance compared to VFNet\n                                           for this dataset. This observation confirms the effectiveness\n                                           of focal loss [2] in FCOS, which is designed to handle cate-\n                                           gory imbalance. TOOD also benefits from focal loss during\n                                           training and yields the best detection result among 3 detec-\n        Fig. 4: Error analysis bar plot for TOOD object detector in                 \n                                           tor. ErroranalysisresultsofTOODdetectoronVisdroneand\n        SF+SAHIsettingcalculatedonxViewvalidationsplit.                             \n                                           xView datasets are presented in Fig. 3 and 4, respectively.\n          During experiments, SGD optimizer with a learning rate                    \n                                           HereC75,C50,Loc,Sim,Oth,BG,FNcorrespondstoresults\n        of0.01,momentumof0.9,weightdecayof0.0001andlinear                           \n                                           atIoUthresholdof0.75and0.50,resultsafterignoringlocal-\n        warmup of 500 iteration is used. Learning rate scheduling                   \n                                           ization errors, supercategory false positives, category confu-\n        is done with exponential decay at 16th and 22nd epochs. For                 \n                                           sions,allfalsepositives,andallfalsenegatives,respectively.\n        theslicingaidedfine-tuning,patchesarecreatedbyslicingthe                    \n                                           As seen in Fig. 3, there is minor room for improving super\n        imagesandannotationsandthenVisdroneandxViewtraining                         \n                                           categoryfalsepositives,categoryconfusionsandlocalization\n        setsareaugmentedusingthesepatches. Sizeofeachpatchis                        \n                                           errorsandmajorroomforimprovingfalsepositivesandfalse\n        randomlyselectedtohaveawidthandheightintherangeof                           \n                                           negatives.Similarly,Fig.4showsthatthereismajorroomfor\n        480to640and300to500forVisdroneandxViewdatasets,                             \n                                           improvementafterfixingcategoryconfusionsandfalseposi-\n        respectively. Inputimagesareresizedtohaveawidthof800                        \n                                           tives.                                   \n        to 1333 (by preserving the aspect ratio). During inference,                 \n        NMSmatchingthresholdT issetas0.5.                                           \n                       m                                                            \n          TheMSCOCO[7]evaluationprotocolhasbeenadopted                              \n        for evaluation, including overall and size-wise AP scores. 5. CONCLUSION    \n                                    50                                              \n        Specifically, AP is computed at the single IoU threshold                    \n                 50                                                                 \n        0.5overallcategoriesandmaximumnumberofdetectionsis The proposed slicing aided hyper inference scheme can\n        set as 500. In Table 1 and 2 conventional inference on orig- directly be integrated into any object detection inference\n        inal image, FI (Full Inference), is taken as the baseline. SF pipeline and does not require pretraining. Experiments with\n        (Slicing Aided Fine-tuning) is the model fine-tuned on aug- FCOS,VFNet,andTOODdetectorsonVisdroneandxView\n        mented dataset with patch sizes in the range of 480 to 640 datasets show that it can result in up to 6.8% AP increase.\n        and 300 to 500 in Tables 1 and 2, respectively. SAHI (Slic- Moreover, applying slicing aided fine-tuning results in an\n        ing Aided Hyper Inference) refers to inference with patches additional14.5%APincreaseforsmallobjectsandapplying\n        of size 640×640 and 400×400 in Tables 1 and 2, respec- 25%overlapbetweenslicesresultsinafurther2.9%increase\n        tively. PO (Patch Overlap) means the there is 25% overlap in AP. Training a network with higher resolution images\n        between patches during sliced inference. As seen from Ta- throughlargerfeaturemapsresultinhighercomputationand\n        ble 1, SAHI increases object detection AP by 6.8%, 5.1% memoryrequirements. Theproposedapproachincreasesthe\n        and 5.3%. The detection accuracy can be further increased computational time linearly, while keeping memory require-\n        withaSF,resultinginacumulativeincreaseof12.7%,13.4% ments fixed. Computation and memory budgets can also be\n        and 14.5% AP for FCOS, VFNet and TOOD detectors, re- traded-offbyadjustingthepatchsizes,consideringthetarget\n        spectively. Applying 25% overlap between slices during in- platform. In the future, instance segmentation models will\n        ference, increases small/medium object AP and overall AP be benchmarked utilizing the proposed slicing approach and\n        but slightly decreases large object AP. Increase is caused by differentpost-processingtechniqueswillbeevaluated.",
        "char_count": 7733,
        "has_tables": false
      },
      {
        "page": 5,
        "content": "6. REFERENCES           [18] B.Bosquet,M.Mucientes,andV.M.Brea, “STDnet: Acon-\n                                              vnetforsmalltargetdetection.,” inBMVC,2018,p.253.\n        [1] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN:                  \n                                           [19] A. Van Etten, “Satellite imagery multiscale rapid detection\n          Towards real-time object detection with region proposal net-              \n                                              withwindowednetworks,” in2019IEEEWACV.IEEE,2019,\n          works,” Advances in neural information processing systems,                \n                                              pp.735–743.                           \n          vol.28,pp.91–99,2015.                                                     \n                                           [20] Y.Pang,J.Cao,J.Wang,andJ.Han, “JCS-Net:Jointclassifi-\n        [2] T.-Y.Lin,P.Goyal,R.Girshick,K.He,andP.Dolla´r, “Focal                   \n                                              cationandsuper-resolutionnetworkforsmall-scalepedestrian\n          loss for dense object detection,” in Proceedings of the IEEE              \n                                              detectioninsurveillanceimages,” IEEETrans.Inf.Forensics\n          ICCV,2017,pp.2980–2988.                                                   \n                                              Security,vol.14,no.12,pp.3322–3331,2019.\n        [3] Z. Cai and N. Vasconcelos, “Cascade R-CNN: Delving into                 \n                                           [21] Y.Bai,Y.Zhang,M.Ding,andB.Ghanem,“Findingtinyfaces\n          highqualityobjectdetection,”inProceedingsoftheIEEEcon-                    \n                                              inthewildwithgenerativeadversarialnetwork,” inProceed-\n          ferenceonCVPR,2018,pp.6154–6162.                                          \n                                              ingsoftheIEEECVPR,2018,pp.21–30.      \n        [4] H.Zhang,Y.Wang,F.Dayoub,andN.Sunderhauf, “Varifo-                       \n          calnet:Aniou-awaredenseobjectdetector,” inProceedingsof [22] Y. Wu, A. Kirillov, F. Massa, W.-Y. Lo, and R. Gir-\n          theIEEE/CVFConferenceonCVPR,2021,pp.8514–8523. shick, “Detectron2,” https://github.com/\n                                              facebookresearch/detectron2,2019.     \n        [5] J.Deng, W.Dong, R.Socher, L.-J.Li, K.Li, andL.Fei-Fei,                  \n          “Imagenet:Alarge-scalehierarchicalimagedatabase,”in2009 [23] K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, S.\n          IEEEconferenceonCVPR.Ieee,2009,pp.248–255. Sun, W. Feng, Z. Liu, J. Xu, et al., “MMDetection: Open\n                                              mmlab detection toolbox and benchmark,” arXiv preprint\n        [6] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and                \n                                              arXiv:1906.07155,2019.                \n          A.Zisserman, “Thepascalvisualobjectclasses(VOC)chal-                      \n          lenge,” ICCV,vol.88,no.2,pp.303–338,2010. [24] G.Jocher, A.Stoken, J.Borovec, A.Chaurasia, L.Changyu,\n                                              V.Laughing,A.Hogan,J.Hajek,L.Diaconu,Y.Kwon,etal.,\n        [7] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-            \n                                              “ultralytics/yolov5: v5.0-yolov5-p61280modelsawssuper-\n          manan,P.Dolla´r,andC.L.Zitnick, “MicrosoftCOCO:Com-                       \n                                              vise.lyandyoutubeintegrations,” Zenodo,vol.11,2021.\n          monobjectsincontext,” inECCV.Springer, 2014, pp.740–                      \n          755.                             [25] D. Du, P. Zhu, L. Wen, X. Bian, H. Lin, Q. Hu, T. Peng, J.\n        [8] E. C. for Electro-technical Standardization, “Alarm systems Zheng,X.Wang,Y.Zhang,etal., “Visdrone-det2019:Thevi-\n          - cctv surveillance systems for use in security applications,” sionmeetsdroneobjectdetectioninimagechallengeresults,”\n          August2012.                                                               \n                                              inProceedingsoftheIEEE/CVFICCVWorkshops,2019,pp.\n                                              0–0.                                  \n        [9] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.               \n          Fu,andA.C.Berg, “SSD:Singleshotmultiboxdetector,” in [26] D. Lam, R. Kuzma, K. McGee, S. Dooley, M. Laielli, M.\n          ECCV.Springer,2016,pp.21–37.        Klaric,Y.Bulatov,andB.McCord,“xView:Objectsincontext\n                                              inoverheadimagery,”arXivpreprintarXiv:1802.07856,2018.\n        [10] A.Bochkovskiy,C.-Y.Wang,andH.-Y.M.Liao,“Yolov4:Op-                     \n          timalspeedandaccuracyofobjectdetection,” arXivpreprint                    \n          arXiv:2004.10934,2020.                                                    \n        [11] R.Girshick, “Fastr-cnn,” inProceedingsoftheIEEEICCV,                   \n          2015,pp.1440–1448.                                                        \n        [12] T.-Y.Lin,P.Dolla´r,R.Girshick,K.He,B.Hariharan,andS.                   \n          Belongie, “Featurepyramidnetworksforobjectdetection,” in                  \n          ProceedingsoftheIEEECVPR,2017,pp.2117–2125.                               \n        [13] Z. Tian, C. Shen, H. Chen, and T. He, “FCOS: Fully con-                \n          volutionalone-stageobjectdetection,” inProceedingsofthe                   \n          IEEE/CVFICCV,2019,pp.9627–9636.                                           \n        [14] C. Feng, Y. Zhong, Y. Gao, M. R. Scott, and W. Huang,                  \n          “TOOD:Task-alignedone-stageobjectdetection,”inProceed-                    \n          ingsoftheIEEE/CVFICCV,2021,pp.3510–3519.                                  \n        [15] G. Wang, J. Guo, Y. Chen, Y. Li, and Q. Xu, “A PSO and                 \n          BFO-basedlearningstrategyappliedtofasterR-CNNforob-                       \n          jectdetectioninautonomousdriving,”IEEEAccess,vol.7,pp.                    \n          18840–18859,2019.                                                         \n        [16] M.Kisantal,Z.Wojna,J.Murawski,J.Naruniec,andK.Cho,                     \n          “Augmentation for small object detection,” arXiv preprint                 \n          arXiv:1902.07296,2019.                                                    \n        [17] Z.Chen,K.Wu,Y.Li,M.Wang,andW.Li, “SSD-MSN:An                           \n          improvedmulti-scaleobjectdetectionnetworkbasedonssd,”                     \n          IEEEAccess,vol.7,pp.80622–80632,2019.",
        "char_count": 7322,
        "has_tables": false
      }
    ],
    "full_text": "SLICINGAIDEDHYPERINFERENCEANDFINE-TUNING                        \n                             FORSMALLOBJECTDETECTION                                \n                                                                                    \n                                                                                    \n                    FatihCagatayAkyon1,2,SinanOnurAltinuc1,2,AlptekinTemizel2       \n                                                                                    \n                                 1OBSSAI,Ankara,Turkey                              \n               2GraduateSchoolofInformatics,MiddleEastTechnicalUniversity,Ankara,Turkey\n                                                                                    \n                                                                                    \n                     ABSTRACT                                                       \n        Detectionofsmallobjectsandobjectsfarawayinthesceneis                        \n        a major challenge in surveillance applications. Such objects                \n        are represented by small number of pixels in the image and                  \n        lack sufficient details, making them difficult to detect using              \n        conventional detectors. In this work, an open-source frame-                 \n        work called Slicing Aided Hyper Inference (SAHI) is pro-                    \n        posedthatprovidesagenericslicingaidedinferenceandfine-                      \n        tuningpipelineforsmallobjectdetection.Theproposedtech-                      \n                                           Fig. 1: Results for inference with TOOD detector (left),\n        nique is generic in the sense that it can be applied on top of              \n                                           Slicing-aided hyper inference (middle), Slicing-aided hyper\n        anyavailableobjectdetectorwithoutanyfine-tuning. Exper-                     \n                                           inferenceafterslicing-aidedfine-tuning(right).\n        imental evaluations, using object detection baselines on the                \n        VisdroneandxViewaerialobjectdetectiondatasetsshowthat The recent advances in drones, 4K cameras and deep\n        theproposedinferencemethodcanincreaseobjectdetection learning research have enabled long-range object detection\n        AP by 6.8%, 5.1% and 5.3% for FCOS, VFNet and TOOD that is met under Detection, Observation, Recognition and\n        detectors,respectively. Moreover,thedetectionaccuracycan Identification (DORI) criteria [8]. DORI criteria define the\n        be further increased with a slicing aided fine-tuning, result- minimumpixelheightoftheobjectsfordifferenttasks: 10%\n        inginacumulativeincreaseof12.7%,13.4%and14.5%AP oftheimageheightisrequiredtodetectand20%torecognize\n        in the same order. Proposed technique has been integrated the objects (108 pixels in full HD videos). Relatively small\n        with Detectron2, MMDetection and YOLOv5 models and it pixelcoveragepushesthelimitsofCNNbasedobjectdetec-\n        ispubliclyavailableathttps://github.com/obss/sahi.git tion methods, in addition, high-resolution images demands\n                                           greaterneedsintermsofmemoryrequirements. \n          Index Terms— small object detection, sliced inference,                    \n                                              Inthispaper,weproposeagenericsolutionbasedonslic-\n        windowedinference,visdrone,xview                                            \n                                           ingaidedinferenceandfine-tuningforsmallobjectdetection\n                  1. INTRODUCTION          onhigh-resolutionimageswhilemaintaininghighermemory\n                                           utilization. Fig. 1illustratestheimprovementofsmallobject\n        In recent years, object detection has been extensively stud-                \n                                           detectiononasampleimagefromVisdronetestset.\n        ied for different applications including face detection, video              \n        objectdetection,videosurveillance,self-drivingcars. Inthis                  \n                                                      2. RELATEDWORK                \n        field, adoption of deep learning architectures has resulted in              \n        highly accurate methods such as Faster R-CNN [1], Reti- Therecentlearning-basedobjectdetectiontechniquescanbe\n        naNet[2],thatarefurtherdevelopedasCascadeR-CNN[3], categorizedintotwomaintypes. Single-stagedetectors,such\n        VarifocalNet [4], and variants. All of these recent detectors as SSD [9], YOLO [10], RetinaNet [2], directly predict the\n        aretrainedandevaluatedonwell-knowndatasetssuchasIm- location of objects without an explicit proposal stage. Two-\n        ageNet[5],PascalVOC12[6],MSCOCO[7].Thesedatasets stage region proposal based methods, such as Fast R-CNN\n        mostlyinvolvelow-resolutionimages(640×480)including [11],FasterR-CNN[1],CascadeR-CNN[3],involveanini-\n        considerablylargeobjectswithlargepixelcoverage(covering tial region proposal stage. These proposals are then refined\n        60%oftheimageheightinaverage). Whilethetrainedmod- todefinethepositionandsizeoftheobject. Typically,single-\n        elshavesuccessfuldetectionperformancesforthosetypesof stageapproachesarefasterthantwo-stage,whilethelatterhas\n        input data, they yield significantly lower accuracy on small higheraccuracy.\n        objectdetectiontasksinhigh-resolutionimagesgeneratedby More recently, anchor-free detectors have started to at-\n        thehigh-enddroneandsurveillancecameras. tract attention. They eliminate the use of anchor boxes and\n  2202                                                                              \n  tcO                                                                               \n  42                                                                                \n  ]VC.sc[                                                                           \n  5v43960.2022:viXra\n\nclassifyeachpointonthefeaturepyramid[12]asforeground available objects detector can be boosted without any fine-\n        or background, and directly predict the distances from the tuning(byslicingaidedinference). Moreover,additionalper-\n        foregroundpointtothefoursidesoftheground-truthbound- formance boost can be gained by fine-tuning the pretrained\n        ing box, to produce the detection. FCOS [13] is the first models.           \n        object detector eliminating the need for predefined set of                  \n                                                    3. PROPOSEDAPPROACH             \n        anchor boxes and entailing computational need. Varifocal-                   \n        Net (VFNet) [4] learns to predict the IoU-aware classifica- Inordertohandlethesmallobjectdetectionproblem,wepro-\n        tion score which mixes the object presence confidence and poseagenericframeworkbasedonslicinginthefine-tuning\n        localization accuracy together as the detection score for a and inference stages. Dividing the input images into over-\n        bounding box. The learning is supervised by the proposed lappingpatchesresultsinrelativelargerpixelareasforsmall\n        Varifocal Loss (VFL), based on a new star-shaped bounding objectswithrespecttotheimagesfedintothenetwork.\n        box feature representation. TOOD [14] explicitly aligns the                 \n        twotasks(objectclassificationandlocalization)inalearning-                   \n        based manner utilizing novel task-aligned head which offers                 \n        a better balance between learning task-interactive and task-                \n        specific features and task alignment learning via a designed                \n        sampleassignmentschemeandatask-alignedloss.                                 \n          Thealgorithmsdesignedforgeneralobjectdetectionper-                        \n        formpoorlyonhighresolutionimagesthatcontainsmalland                         \n        dense objects, leading to specific approaches for small ob-                 \n        ject detection. In [15], a particle swarm optimization (PSO)                \n        and bacterial foraging optimization (BFO)-based learning                    \n        strategy (PBLS) is used to optimize the classifier and loss                 \n        function. However, these heavy modifications to the origi-                  \n        nal models prevent fine-tuning from pretrained weights and                  \n        require training from scratch. Moreover, due to unusual                     \n        optimization steps they are hard to adapt into a present de-                \n        tector. The method proposed in [16] oversamples images                      \n        with small objects and augments them by making several                      \n        copiesofsmallobjects. However,thisaugmentationrequires                      \n        segmentation annotations and, as such, it is not compatible                 \n        with the object detection datasets. The method in [17] can                  \n        learnricherfeaturesofsmallobjectsfromtheenlargedareas,                      \n        which are clipped from the raw image. The extra features                    \n        positively contribute to the detection performance but the Fig. 2: Slicing aided fine-tuning (top) and slicing aided hy-\n        selection of the areas to be enlarged brings a computational perinference(bottom)methods. Infinetuning, thedatasetis\n        burden. In[18],afullyconvolutionalnetworkisproposedfor augmented by extracting patches from the images and resiz-\n        small object detection that contains an early visual attention ingthemtoalargersize. Duringinference,imageisdivided\n        mechanism that is proposed to choose the most promising intosmallerpatchesandpredictionsaregeneratedfromlarger\n        regions with small objects and their context. In [19], a slic- resizedversionsofthesepatches. Thenthesepredictionsare\n        ing based technique is proposed but the implementation is converted back into original image coordinates after NMS.\n        not generic and only applicable to specific object detectors. Optionally,predictionsfromfullinferencecanalsobeadded.\n        In [20], a novel network (called JCS-Net) is proposed for SlicingAidedFine-tuning(SF):Widelyusedobjectde-\n        small-scale pedestrian detection, which integrates the classi- tection frameworks such as Detectron2 [22], MMDetection\n        ficationtaskandthesuper-resolutiontaskinaunifiedframe- [23] and YOLOv5 [24] provide pretrained weights on the\n        work. [21]proposedanalgorithmtodirectlygenerateaclear datasets such as ImageNet [5] and MS COCO [7]. This al-\n        high-resolution face from a blurry small one by adopting a lows us to fine-tune the model using smaller datasets and\n        generativeadversarialnetwork(GAN).However,sincethese overshortertrainingspansincontrasttotrainingfromscratch\n        techniques propose new detector architectures they require with large datasets. These common datasets mostly involve\n        pretrainingfromscratchwithlargedatasetswhichiscostly. low-resolutionimages(640×480)havingconsiderablylarge\n          In contrast to the mentioned techniques, we propose a objectswithlargepixelcoverage(covering60%oftheimage\n        generic slicing aided fine-tuning and inference pipeline that heightinaverage).Themodelspretrainedusingthesedatasets\n        can be utilized on top of any existing object detector. This provideverysuccessfuldetectionperformanceforsimilarin-\n        way, small object detection performance of any currently puts. Ontheotherhand, theyyieldsignificantlyloweraccu-\n\nracyonsmallobjectdetectiontasksinhigh-resolutionimages collectedatdifferentlocationsbutinsimilarenvironments.\n        generatedbythehigh-enddroneandsurveillancecameras. xView[26]isoneofthelargestpubliclyavailabledatasets\n          In order to overcome this issue, we augment the dataset for object detection from satellite imagery. It contains im-\n        withbyextractingpatchesfromtheimagesfine-tuningdataset agesfromcomplexscenesaroundtheworld,annotatedusing\n        as seen in Fig. 2. Each image IF,IF,...,IF is sliced into bounding boxes. It contains over 1M object instances from\n                          1 2   j                                                   \n        overlappingpatches PF,PF,...PF withdimensions M and 60 different classes. During the experiments, randomly se-\n                    1 2   k                                                         \n        N are selected within predefined ranges [M ,M ] and lected75%and25%splitshavebeenusedasthetrainingand\n                                min max                                             \n        [N ,N  ] which are treated as hyper-parameters. Then validationsets,respectively.\n         min max                                                                    \n        during fine-tuning, patches are resized by preserving the as- Bothofthesedatasetscontainsmallobjects(objectwidth\n        pectratiosothatimagewidthisbetween800to1333pixels <1%ofimagewidth).         \n        toobtainaugmentationimagesI(cid:48),I(cid:48),...,I(cid:48),wherebytherel-  \n                         1 2   k                 Setup     AP  AP s AP m AP l       \n        ative object sizes are larger compared to the original image. 50 50 50 50   \n                                                FCOS+FI    25.8 14.2 39.6 45.1      \n        TheseimagesI(cid:48),I(cid:48),...,I(cid:48),togetherwiththeoriginalimages  \n                1 2  k                        FCOS+SAHI+PO 29.0 18.9 41.5 46.4      \n        IF,IF,...,IF (tofacilitatedetectionoflargeobjects),areuti-                  \n        1 2   j                              FCOS+SAHI+FI+PO 31.0 19.8 44.6 49.0    \n        lizedduringfine-tuning. Ithastobenotedthat, asthepatch                      \n                                             FCOS+SF+SAHI+PO 38.1 25.7 54.8 56.9    \n        sizesdecrease,largerobjectsmaynotfitwithinasliceandthe FCOS+SF+SAHI+FI+PO 38.5 25.9 55.4 59.8\n        intersectingareas,andthismayleadtopoordetectionperfor- VFNet+FI 28.8 16.8 44.0 47.5\n        manceforlargerobjects.                VFNet+SAHI+PO 32.0 21.4 45.8 45.5     \n          SlicingAidedHyperInference(SAHI):Slicingmethod VFNet+SAHI+FI+PO 33.9 22.4 49.1 49.4\n        isalsoutilizedduringtheinferencestepasdetailedinFig. 2. VFNet+SF+SAHI+PO 41.9 29.7 58.8 60.6\n        First, the original query image I is sliced into l number of VFNet+SF+SAHI+FI+PO 42.2 29.6 59.2 63.3\n        M×N overlappingpatchesPI,PI,...PI . Then,eachpatch TOOD+FI 29.4 18.1 44.1 50.0\n                        1 2   l                                                     \n                                               TOOD+SAHI   31.9 22.6 44.0 45.2      \n        is resized while preserving the aspect ratio. After that, ob-               \n                                              TOOD+SAHI+PO 32.5 22.8 45.2 43.6      \n        ject detection forward pass is applied independently to each                \n                                              TOOD+SAHI+FI 34.6 23.8 48.5 53.1      \n        overlapping patch. An optional full-inference (FI) using the                \n                                             TOOD+SAHI+FI+PO 34.7 23.8 48.9 50.3    \n        original image can be applied to detect larger objects. Fi-                 \n                                               TOOD+SF+FI  36.8 24.4 53.8 66.4      \n        nally, the overlapping prediction results and, if used, FI re-              \n                                              TOOD+SF+SAHI 42.5 31.6 58.0 61.1      \n        sults are merged back into to original size using NMS. Dur- TOOD+SF+SAHI+PO 43.1 31.7 59.0 60.2\n        ingNMS,boxeshavinghigherIntersectionoverUnion(IoU) TOOD+SF+SAHI+FI 43.4 31.7 59.6 65.6\n        ratios than a predefined matching threshold T are matched TOOD+SF+SAHI+FI+PO 43.5 31.7 59.8 65.4\n                                 m                                                  \n        and for each match, detections having detection probability                 \n                                           Table 1: Mean average precision values calculated on\n        thanlowerthanT areremoved.                                                  \n                 d                         Visdrone19-Detectiontest-devset.SF,SAHI,FI,andPOcor-\n                    4. RESULTS             respond to slicing aided fine-tuning, slicing aided inference,\n                                           fullimageinference,andoverlappingpatches,respectively.\n        The proposed method has been integrated into FCOS [13],                     \n        VarifocalNet [4] and TOOD [14] object detectors using                       \n                                                 Setup     AP  AP s AP m AP l       \n                                                             50  50   50   50       \n        MMDetection [23] framework for experimental evaluation.                     \n                                                FCOS+FI    2.20 0.10 1.80 7.30      \n        Related config files, conversion and evaluation scripts, eval- FCOS+SF+SAHI 15.8 11.9 18.4 11.0\n        uation result files have been publicly provided 1. All slicing FCOS+SF+SAHI+PO 17.1 12.2 20.2 12.8\n        related operations have also been made publicly available to FCOS+SF+SAHI+FI 15.7 11.9 18.4 14.3\n        enableintegrationintootherobjectdetectionframeworks2. FCOS+SF+SAHI+FI+PO 17.0 12.2 20.2 15.8\n          VisDrone2019-Detection [25] is an object detection VFNet+FI 2.10 0.50 1.80 6.80\n        dataset having 8599 images captured by drone platforms VFNet+SF+SAHI 16.0 11.9 17.6 13.1\n                                             VFNet+SF+SAHI+PO 17.7 13.7 19.7 15.4   \n        at different locations and at different heights. Most of the                \n                                             VFNet+SF+SAHI+FI 15.8 11.9 17.5 15.2   \n        objectsinthisdatasetaresmall, denselydistributedandpar-                     \n                                            VFNet+SF+SAHI+FI+PO 17.5 13.7 19.6 17.6 \n        tially occluded. There are also illumination and perspective                \n                                                TOOD+FI    2.10 0.10 2.00 5.20      \n        changes in different scenarios. More than 540k bounding                     \n                                              TOOD+SF+SAHI 19.4 14.6 22.5 14.2      \n        boxesoftargetsareannotatedwithtenpredefinedcategories:                      \n                                             TOOD+SF+SAHI+PO 20.6 14.9 23.6 17.0    \n        pedestrian,person,bicycle,car,van,truck,tricycle,awning-                    \n                                             TOOD+SF+SAHI+FI 19.2 14.6 22.3 14.7    \n        tricycle, bus, motor. Super categories are defined as pedes- TOOD+SF+SAHI+FI+PO 20.4 14.9 23.5 17.6\n        trian, motor, carandtruck. Thetrainingandvalidationsub-                     \n                                           Table2: MeanaverageprecisionvaluescalculatedonxView\n        setsconsistsof6471and548images,respectivelywhichare                         \n                                           validation split. SF, SAHI, FI, and PO correspond to slicing\n         1https://github.com/fcakyon/sahi-benchmark aided fine-tuning, slicing aided inference, full image infer-\n         2https://github.com/obss/sahi     ence,andoverlappingpatches,respectively.\n\ntheadditionalsmallobjecttruepositivespredictedfromslices\n                                           and decrease is caused by the false positives predicted from\n                                           slicesthatmatchinglargegroundtruthboxes. Bestsmallob-\n                                           ject detection AP is achieved by SF followed by SI, while\n                                           best large object detection AP is achieved by SF followed\n                                           by FI, confirming the contribution of FI for large object de-\n                                           tection. Results for xView dataset is presented in Table 2.\n                                           SincexViewtargetsareverysmall,regulartrainingwithorig-\n                                           inal images yields poor detection performance and SF im-\n        Fig. 3: Error analysis curve for TOOD object detector in                    \n                                           proves the results substantially. Integration of FI increases\n        SF+SAHIsettingcalculatedonVisdrone19-Dettest-devset.                        \n                                           largeobjectAPbyupto3.3%butresultsinslightlydecreased\n                                           small/medium object AP, which is expected as some of the\n                                           largerobjectsmaynotbedetectedfromsmallerslices. 25%\n                                           overlap between slices increase the detection AP by up to\n                                           1.7%. xViewcontainshighlyimbalanced60targetcategories\n                                           and despite being an older and, reportedly weaker detector,\n                                           FCOS yields a very close performance compared to VFNet\n                                           for this dataset. This observation confirms the effectiveness\n                                           of focal loss [2] in FCOS, which is designed to handle cate-\n                                           gory imbalance. TOOD also benefits from focal loss during\n                                           training and yields the best detection result among 3 detec-\n        Fig. 4: Error analysis bar plot for TOOD object detector in                 \n                                           tor. ErroranalysisresultsofTOODdetectoronVisdroneand\n        SF+SAHIsettingcalculatedonxViewvalidationsplit.                             \n                                           xView datasets are presented in Fig. 3 and 4, respectively.\n          During experiments, SGD optimizer with a learning rate                    \n                                           HereC75,C50,Loc,Sim,Oth,BG,FNcorrespondstoresults\n        of0.01,momentumof0.9,weightdecayof0.0001andlinear                           \n                                           atIoUthresholdof0.75and0.50,resultsafterignoringlocal-\n        warmup of 500 iteration is used. Learning rate scheduling                   \n                                           ization errors, supercategory false positives, category confu-\n        is done with exponential decay at 16th and 22nd epochs. For                 \n                                           sions,allfalsepositives,andallfalsenegatives,respectively.\n        theslicingaidedfine-tuning,patchesarecreatedbyslicingthe                    \n                                           As seen in Fig. 3, there is minor room for improving super\n        imagesandannotationsandthenVisdroneandxViewtraining                         \n                                           categoryfalsepositives,categoryconfusionsandlocalization\n        setsareaugmentedusingthesepatches. Sizeofeachpatchis                        \n                                           errorsandmajorroomforimprovingfalsepositivesandfalse\n        randomlyselectedtohaveawidthandheightintherangeof                           \n                                           negatives.Similarly,Fig.4showsthatthereismajorroomfor\n        480to640and300to500forVisdroneandxViewdatasets,                             \n                                           improvementafterfixingcategoryconfusionsandfalseposi-\n        respectively. Inputimagesareresizedtohaveawidthof800                        \n                                           tives.                                   \n        to 1333 (by preserving the aspect ratio). During inference,                 \n        NMSmatchingthresholdT issetas0.5.                                           \n                       m                                                            \n          TheMSCOCO[7]evaluationprotocolhasbeenadopted                              \n        for evaluation, including overall and size-wise AP scores. 5. CONCLUSION    \n                                    50                                              \n        Specifically, AP is computed at the single IoU threshold                    \n                 50                                                                 \n        0.5overallcategoriesandmaximumnumberofdetectionsis The proposed slicing aided hyper inference scheme can\n        set as 500. In Table 1 and 2 conventional inference on orig- directly be integrated into any object detection inference\n        inal image, FI (Full Inference), is taken as the baseline. SF pipeline and does not require pretraining. Experiments with\n        (Slicing Aided Fine-tuning) is the model fine-tuned on aug- FCOS,VFNet,andTOODdetectorsonVisdroneandxView\n        mented dataset with patch sizes in the range of 480 to 640 datasets show that it can result in up to 6.8% AP increase.\n        and 300 to 500 in Tables 1 and 2, respectively. SAHI (Slic- Moreover, applying slicing aided fine-tuning results in an\n        ing Aided Hyper Inference) refers to inference with patches additional14.5%APincreaseforsmallobjectsandapplying\n        of size 640×640 and 400×400 in Tables 1 and 2, respec- 25%overlapbetweenslicesresultsinafurther2.9%increase\n        tively. PO (Patch Overlap) means the there is 25% overlap in AP. Training a network with higher resolution images\n        between patches during sliced inference. As seen from Ta- throughlargerfeaturemapsresultinhighercomputationand\n        ble 1, SAHI increases object detection AP by 6.8%, 5.1% memoryrequirements. Theproposedapproachincreasesthe\n        and 5.3%. The detection accuracy can be further increased computational time linearly, while keeping memory require-\n        withaSF,resultinginacumulativeincreaseof12.7%,13.4% ments fixed. Computation and memory budgets can also be\n        and 14.5% AP for FCOS, VFNet and TOOD detectors, re- traded-offbyadjustingthepatchsizes,consideringthetarget\n        spectively. Applying 25% overlap between slices during in- platform. In the future, instance segmentation models will\n        ference, increases small/medium object AP and overall AP be benchmarked utilizing the proposed slicing approach and\n        but slightly decreases large object AP. Increase is caused by differentpost-processingtechniqueswillbeevaluated.\n\n6. REFERENCES           [18] B.Bosquet,M.Mucientes,andV.M.Brea, “STDnet: Acon-\n                                              vnetforsmalltargetdetection.,” inBMVC,2018,p.253.\n        [1] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN:                  \n                                           [19] A. Van Etten, “Satellite imagery multiscale rapid detection\n          Towards real-time object detection with region proposal net-              \n                                              withwindowednetworks,” in2019IEEEWACV.IEEE,2019,\n          works,” Advances in neural information processing systems,                \n                                              pp.735–743.                           \n          vol.28,pp.91–99,2015.                                                     \n                                           [20] Y.Pang,J.Cao,J.Wang,andJ.Han, “JCS-Net:Jointclassifi-\n        [2] T.-Y.Lin,P.Goyal,R.Girshick,K.He,andP.Dolla´r, “Focal                   \n                                              cationandsuper-resolutionnetworkforsmall-scalepedestrian\n          loss for dense object detection,” in Proceedings of the IEEE              \n                                              detectioninsurveillanceimages,” IEEETrans.Inf.Forensics\n          ICCV,2017,pp.2980–2988.                                                   \n                                              Security,vol.14,no.12,pp.3322–3331,2019.\n        [3] Z. Cai and N. Vasconcelos, “Cascade R-CNN: Delving into                 \n                                           [21] Y.Bai,Y.Zhang,M.Ding,andB.Ghanem,“Findingtinyfaces\n          highqualityobjectdetection,”inProceedingsoftheIEEEcon-                    \n                                              inthewildwithgenerativeadversarialnetwork,” inProceed-\n          ferenceonCVPR,2018,pp.6154–6162.                                          \n                                              ingsoftheIEEECVPR,2018,pp.21–30.      \n        [4] H.Zhang,Y.Wang,F.Dayoub,andN.Sunderhauf, “Varifo-                       \n          calnet:Aniou-awaredenseobjectdetector,” inProceedingsof [22] Y. Wu, A. Kirillov, F. Massa, W.-Y. Lo, and R. Gir-\n          theIEEE/CVFConferenceonCVPR,2021,pp.8514–8523. shick, “Detectron2,” https://github.com/\n                                              facebookresearch/detectron2,2019.     \n        [5] J.Deng, W.Dong, R.Socher, L.-J.Li, K.Li, andL.Fei-Fei,                  \n          “Imagenet:Alarge-scalehierarchicalimagedatabase,”in2009 [23] K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, S.\n          IEEEconferenceonCVPR.Ieee,2009,pp.248–255. Sun, W. Feng, Z. Liu, J. Xu, et al., “MMDetection: Open\n                                              mmlab detection toolbox and benchmark,” arXiv preprint\n        [6] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and                \n                                              arXiv:1906.07155,2019.                \n          A.Zisserman, “Thepascalvisualobjectclasses(VOC)chal-                      \n          lenge,” ICCV,vol.88,no.2,pp.303–338,2010. [24] G.Jocher, A.Stoken, J.Borovec, A.Chaurasia, L.Changyu,\n                                              V.Laughing,A.Hogan,J.Hajek,L.Diaconu,Y.Kwon,etal.,\n        [7] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-            \n                                              “ultralytics/yolov5: v5.0-yolov5-p61280modelsawssuper-\n          manan,P.Dolla´r,andC.L.Zitnick, “MicrosoftCOCO:Com-                       \n                                              vise.lyandyoutubeintegrations,” Zenodo,vol.11,2021.\n          monobjectsincontext,” inECCV.Springer, 2014, pp.740–                      \n          755.                             [25] D. Du, P. Zhu, L. Wen, X. Bian, H. Lin, Q. Hu, T. Peng, J.\n        [8] E. C. for Electro-technical Standardization, “Alarm systems Zheng,X.Wang,Y.Zhang,etal., “Visdrone-det2019:Thevi-\n          - cctv surveillance systems for use in security applications,” sionmeetsdroneobjectdetectioninimagechallengeresults,”\n          August2012.                                                               \n                                              inProceedingsoftheIEEE/CVFICCVWorkshops,2019,pp.\n                                              0–0.                                  \n        [9] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.               \n          Fu,andA.C.Berg, “SSD:Singleshotmultiboxdetector,” in [26] D. Lam, R. Kuzma, K. McGee, S. Dooley, M. Laielli, M.\n          ECCV.Springer,2016,pp.21–37.        Klaric,Y.Bulatov,andB.McCord,“xView:Objectsincontext\n                                              inoverheadimagery,”arXivpreprintarXiv:1802.07856,2018.\n        [10] A.Bochkovskiy,C.-Y.Wang,andH.-Y.M.Liao,“Yolov4:Op-                     \n          timalspeedandaccuracyofobjectdetection,” arXivpreprint                    \n          arXiv:2004.10934,2020.                                                    \n        [11] R.Girshick, “Fastr-cnn,” inProceedingsoftheIEEEICCV,                   \n          2015,pp.1440–1448.                                                        \n        [12] T.-Y.Lin,P.Dolla´r,R.Girshick,K.He,B.Hariharan,andS.                   \n          Belongie, “Featurepyramidnetworksforobjectdetection,” in                  \n          ProceedingsoftheIEEECVPR,2017,pp.2117–2125.                               \n        [13] Z. Tian, C. Shen, H. Chen, and T. He, “FCOS: Fully con-                \n          volutionalone-stageobjectdetection,” inProceedingsofthe                   \n          IEEE/CVFICCV,2019,pp.9627–9636.                                           \n        [14] C. Feng, Y. Zhong, Y. Gao, M. R. Scott, and W. Huang,                  \n          “TOOD:Task-alignedone-stageobjectdetection,”inProceed-                    \n          ingsoftheIEEE/CVFICCV,2021,pp.3510–3519.                                  \n        [15] G. Wang, J. Guo, Y. Chen, Y. Li, and Q. Xu, “A PSO and                 \n          BFO-basedlearningstrategyappliedtofasterR-CNNforob-                       \n          jectdetectioninautonomousdriving,”IEEEAccess,vol.7,pp.                    \n          18840–18859,2019.                                                         \n        [16] M.Kisantal,Z.Wojna,J.Murawski,J.Naruniec,andK.Cho,                     \n          “Augmentation for small object detection,” arXiv preprint                 \n          arXiv:1902.07296,2019.                                                    \n        [17] Z.Chen,K.Wu,Y.Li,M.Wang,andW.Li, “SSD-MSN:An                           \n          improvedmulti-scaleobjectdetectionnetworkbasedonssd,”                     \n          IEEEAccess,vol.7,pp.80622–80632,2019."
  },
  "pdf_url": "/uploads/ec3b5668e762e620849c2a1e003de7b9.pdf"
}