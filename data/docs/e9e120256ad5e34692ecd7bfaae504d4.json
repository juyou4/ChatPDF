{
  "filename": "扩散模型长尾分布训练方法综述.pdf",
  "upload_time": "2025-12-12T15:37:18.189718",
  "data": {
    "full_text": "扩散模型长尾分布训练方法综述\n背景： 长尾分布的训练数据会导致扩散模型对头部类别生成质量较高，而尾部类别往往生成多样性和保真度不足\n1 2 。针对这一问题，近三年的研究提出了多种无需改动模型结构的训练策略，适用于Latent Diffusion\nModel (LDM) 这类扩散模型，并可结合目标检测器（如DINO）的固定架构使用。下面从数据层面、损失函数层面\n和推理调控三个方面总结相关方法，并比较各方法的核心思想、适配性和效果。\n数据层面的长尾均衡策略\n1. 类别均衡采样： 长尾数据下常用做法是调整各类别样本的采样频率，在训练批次中提升尾部类样本比例 3\n4 。例如ImageNet-LT等数据集，可采用过采样尾类或欠采样头类，使每个minibatch呈近似均匀的类分布 5\n4 。Chen等（2025）的长尾扩散模型训练中，就结合标准的重采样技巧来增加尾类样本的出现频率，显著提升了\n尾类生成的多样性 3 。需注意过度过采样可能导致尾类过拟合，实践中可在维持头类多样性的同时逐步增加尾类\n采样率。\n2. 数据增广与生成增强： 利用数据增广来扩充尾部类别的有效样本数也是即插即用的方法 6 。例如，可对尾类图\n像施加Differentiable Augmentation (DiffAug) 或 ADA 等技术（源自GAN训练）来生成带噪或变换的样本\n6 。Katsumata等（2024）提出标签增广（Label Augmentation）的方法：使用预训练分类器为图像预测软标\n签，将相似类的概率分布作为新的训练标签，从而在判别器（或扩散模型的条件）中融入跨类特征，实现高亲和且\n多样的增广 7 8 。这一思路在GAN中验证了对少样本类的增强效果 9 ；在扩散模型中，也有工作建议在潜变量\n空间进行类似的插值增广，以减少直接图像空间增广带来的分布偏移 10 。对于文本-图像扩散，可以通过丰富尾部\n概念的描述（如增加同义词、属性词）或对长尾概念组合进行额外增广来平衡训练语料。不过需要注意增广应保持\n语义正确性（高“亲和度”）同时引入足够变化（高“多样性”） 11 。总体而言，数据层面的方法对模型结构零改动，\n易于实现，常作为基础方案；但若尾类样本极少，仍需结合损失层面的特殊正则化来防止模型记忆少数样本。\n损失函数层面的长尾优化策略\n1. 类别损失重加权： 直接在扩散模型的噪声回归损失上给予尾部类别更高权重，是最简单的长尾补偿手段。可按每\n类频次的反比（如$\\alpha/(n_c+\\epsilon)$）为该类样本的损失乘以权重，或采用Effective Number公式计算平\n衡权重 12 。这一策略类似分类任务中的重加权损失，对扩散模型同样适用 13 。需要仔细调节权重强度，过高会削\n弱头类学习，通常与重采样结合使用以避免单纯重复少数样本造成的过拟合。\n2. Dispersive Loss分散损失： Wang和何（2025）提出了分散损失（Dispersive Loss），这是一种即插即用的正\n则项，旨在鼓励扩散模型内部表示在隐藏空间中更加分散 14 。其思想类似对比学习中的拉开分布，但不需要构造\n正样本对，从而不干扰扩散回归采样过程 14 。与之前需要外部表示对齐的方案（如REPA）不同，分散损失无需额\n外预训练或外部数据，仅通过在训练中正则化模型的中间特征，使模型摆脱对长尾类训练样本的记忆，提高对少样\n本模式的探索能力 14 。在ImageNet上，加入分散损失可对多种扩散模型架构取得一致性提升 15 。虽然作者主要\n验证了平衡数据集上的整体性能提升，但这种提升隐含地有利于尾类多样性，因为分散的内部表示能减少尾类模式\n塌陷（如训练集中某尾类样本被反复生成的问题）。该方法不依赖特定条件标签，对无条件和有条件扩散均适用；\n对于文本条件扩散，也能在不改变U-Net结构的情况下增强隐藏表示的覆盖范围，提升生成多样性。\n1\n\n3. 对比学习正则化： 将对比损失融入扩散模型，可有效缓解尾类模式坍塌 16 17 。Chen等（2025）提出对比学\n习扩散模型（CLDM）框架，包括两个正则项： 18 (a) 无监督InfoNCE损失（仅负样本）：不区分类别地将不同生\n成样本的内部嵌入拉远，特别是针对尾类样本，增大它们在表示空间的距离，从而鼓励同类生成的多样性 18 17 。\n这一损失相当于惩罚生成样本过于相似（尤其是尾类往往趋向于几个训练样本的模式 19 ），以提高尾类图像的多\n样性和覆盖率。 20 (b) 条件-无条件对齐损失：这是一个MSE损失，在扩散过程的大时间步对比有条件和无条件扩散\n分支预测的噪声，使二者在初始去噪阶段对齐 21 。其作用是使模型在早期不依赖过强的类条件信息，利用无条件\n扩散（学到整个数据分布）的知识来丰富尾类表示 3 21 。这一技巧与GAN领域的UTLO方法类似，将有条件和无\n条件生成共享低阶特征，从而头尾类知识共享 21 。CLDM方法无需改变U-Net结构，仅增加上述损失，易于在标准\nDDPM或扩散Transformer训练管线中实现 22 23 。实验证明，该对比正则框架在CIFAR-10/100-LT、ImageNet-\nLT等多数据集上显著提升了尾类图像的多样性，同时保持头类的保真度 24 。例如，与基础DDPM和其他SOTA方法\n（如CBDM、OCLT等）相比，CLDM在ImageNet-LT 256×256上的尾类FID有明显降低 25 。\n另一项工作是Yan等（2024）的Diffusion with Regularized Overlap (DiffROP) 26 。DiffROP关注不同类别生\n成分布的区分度，通过概率对比学习损失来最小化不同类别合成图像分布的重叠 26 27 。作者观察到长尾数据下扩\n散模型生成的罕见类图像常近似于频繁类 26 。为此，DiffROP引入针对类别条件分布的对比正则：对于任意两类\n$y_i, y_j$，取各自的样本在随机时刻$t$的Noisy表示$x_t^{(i)}, x_t^{(j)}$，惩罚模型预测从$x_t^{(i)}$反向去噪一\n步得到的分布与从$x_t^{(j)}$得到的分布之间的KL散度 26 27 。在高斯假设下，该KL正则可简化为模型对这两类\n样本预测的噪声$\\epsilon_\\theta$差异的L2损失，从而高效实现 28 29 （等价于拉开不同类别的预测噪声\n28 ）。DiffROP不需要特殊的采样策略即可融入训练，通过一个超参数$\\tau$控制对比损失权重，并随$t$增大而\n指数衰减，以反映后期噪声水平的趋同 30 30 。此外，作者引入hinge样式的变体损失，只在分布过度重叠时施加\n惩罚，以避免扰乱原有最优点 31 。DiffROP能有效提高尾类图像的保真度，因为不同类生成被拉开后，头类特征不\n至于“淹没”尾类，减少了尾类图像与训练集中头类过于相似的现象 26 27 。实验显示该方法在多个长尾数据集上提\n升了合成质量 32 。代码已开源提供实现细节 32 。需要注意的是，DiffROP只考虑跨类别分布的分离，未直接鼓励\n同类内部的多样性 33 ；因此，可与上述InfoNCE类正则组合，使模型既避免类别混淆，又增加尾类内部的多样覆\n盖。\n4. 分布校准与知识蒸馏： 一些方法通过隐含分布校准来提升尾类生成质量。Qin等（2023）的Class-Balancing\nDiffusion Models (CBDM)就是典型代表 34 。CBDM假设训练数据非均衡，提出在采样过程中调整条件转移概\n率，使每一步生成的中间分布都更加平衡 35 。这种调整对应于在训练目标中加入一个额外的MSE正则项，使扩散\n模型对不同类别的预测更加相似 36 。直观而言，该损失促进了模型对不同条件生成的隐藏表示相似，相当于将头\n类的共性特征传递给尾类，而不会损伤头类表达能力 37 。实现上，只需几行代码：例如，对每个训练样本除了使\n用其真实标签$y$计算噪声损失外，再随机指定一个伪标签$y'$（全局上服从均匀分布）计算另一份噪声预测，并最\n小化两次预测的差异 38 。这样模型被引导对任意标签条件都产出相似的初始预测，削弱了因类别频次差异导致的\n预测偏差 38 。CBDM本质是在不引入新参数的情况下实现了条件扩散的“均衡化”，可被看作对有条件扩散模型隐含\n先验分布的校准。实验表明，CBDM在CIFAR100-LT等数据集上能生成多样且质量更高的图像 39 ；尤其尾部类别的\n图像语义更清晰、多样性更好（FID显著降低，见论文附图） 1 2 。此外，平衡后的生成模型还提升了下游长尾\n分类任务的识别性能 40 。这说明通过这种隐式KL正则，扩散模型学到了更普适的表征，对尾类也有更强的生成能\n力。\n另一项定向校准策略是Zhang等（2024）的OCLT（Oriented Calibration for Long-Tail diffusion）方法 41 。\nOCLT利用贝叶斯观点直接实现头→尾知识转移，核心是在训练中将尾类样本的去噪目标部分替换为相似的头类样本\n42 43 。具体做法为：在每一训练步骤，采样一个均衡类别分布的minibatch，然后对于尾部类别的Noisy样本\n$x_t^{(T)}$，在batch内根据样本与$x_t^{(T)}$的相似度挑选一个来自头部类的干净样本$x_0^{(H)}$ 44 45 ，将\n$x_t^{(T)}$的真实去噪目标替换为$x_0^{(H)}$对应的噪声估计$\\hat{\\epsilon}_H$（而非其自身的$\\hat{\\epsilon}\n_T$） 46 47 。简言之，让模型用头类的“答案”指导尾类的去噪。这种转移通过一个门控机制严格控制：只有当候\n选头类的频次高于尾类时才执行替换，反之则保持原样（避免头类扰动尾类过度） 46 。通过这种定向的Score校准\n2\n\n（Tail-to-Head模式），模型在早期训练阶段就将头类多样的语义注入尾类生成过程，显著增加了尾类生成的多样性\n43 48 。理论分析表明，该知识转移的效果取决于标签分布和样本相似度，合理结合二者可以在再平衡类别比例时\n获得更优解 49 。实验中，OCLT在CIFAR-10/100-LT、Imagenet-LT等多项基准上性能优于已有方法 50 。它无需\n改变模型结构，仅通过定制的损失计算实现，对LDM同样适用；作者已开源代码便于集成 51 。值得注意的是，\nOCLT进一步强调了重采样与样本相似性结合的重要性：与纯粹增加尾类频率相比，选择具备相似特征的头类样本来\n指导尾类，可在保证生成图像质量的前提下提升多样性 49 。这种方法尤其适合类别标签明确的扩散模型；对于文\n本条件扩散，可探索将尾部概念对应的一组高频概念作为“头类”指导，实现类似的语义迁移（但要有可靠的相似度\n度量，如CLIP嵌入）。\n推理阶段的调整与其他即插即用技术\n1. Classifier-Free Guidance 调整： 对于已训练好的有条件扩散模型，推理时的指导系数(guidance scale)直接影\n响尾类图像的质量和多样性。长尾情况下，尾类条件下模型的预测往往不如头类可靠，若使用过强的Guidance，可\n能导致模型过度依赖有限的条件信号，反而生成模式单一的图像（甚至重复训练样本） 19 。因此，可对尾部类别\n采用较弱的指导力度，让模型保留更多无条件生成的多样性；而对头部类别则可使用正常或更高的guidance以确保\n细节和保真度。实践中可以为每个类别设定不同的guidance scale，或使用一种动态调节策略：例如在扩散采样初\n期使用低guidance（鼓励多样性），后期逐渐提高guidance细化图像。这种分阶段指导思想与前述训练对齐损失类\n似，都旨在先利用模型整体分布能力，再引入类别细节 21 。需要注意的是，不同类别的最优guidance可能需要通\n过在验证集上试验FID、多样性指标来确定。对于文本生成模型，亦可按罕见概念降低guidance，以避免模型忽视\n罕见词语——这类似于提升prompt中稀有词影响力的做法，如Attend-and-Excite方法通过优化使得稀有词获得更\n高注意力，从而更突出地出现在生成图像中 8 。总之，适度调整推理参数是一种不改模型而改善尾类生成的简单\n途径。\n2. 自蒸馏与生成校准: 自蒸馏是指利用模型本身的生成结果来改进模型。如Dhariwal等工作的启发，可以将扩散模\n型在高指导下生成的高质量样本，用于微调模型使其在较低指导甚至无指导情况下也能生成类似质量的图像。这种\n将模型推理行为反哺训练的思路，可视为对尾类的一种增强：先用模型在尾类上以宽松采样获得多样图像，再用这\n些合成图像联合原始数据继续训练，从而扩充尾类分布的支持。虽然截至目前没有直接以“自蒸馏”命名的长尾扩散\n论文，但这种思路与知识蒸馏和自训练在判别任务中的做法类似，值得在生成任务中尝试。另一个角度，Meng等\n的DINO探测器本身采用了无标签自蒸馏技术，可以在生成-检测闭环中启发新的训练策略：例如，用当前LDM生成\n图像，经DINO检测获得伪标签反馈，再用以训练扩散模型或调整其条件嵌入，从而校准模型对尾类的识别和生成能\n力。这属于更复杂的流水线，不在纯生成模型训练范畴内，在此提及以供拓展考虑。\n3. 分数校准方法： （注：此处“分数”指扩散模型中的score或判别导数） 除了训练时的KL正则化，推理时也可进行\n生成分布的校准。如果采用分类器引导的扩散（Classifier Guidance），可对分类器的输出进行后处理校准：例如\n减去各类别的对数频率（类似识别任务中的logit adjustment 52 ），使得指导偏向尾类。这等价于假设均匀的条件\n先验，从而鼓励生成尾部样本。对于无分类器的条件扩散，可借鉴“面向头尾的score重加权”思路：如在每次去噪\n时，检测当前样本属于尾类的可能性，适当增强模型预测中的头类成分，从概率流角度提高生成跳往头类模式的概\n率 53 。Zhang等（2024）的定向校准实际已在训练中实现了这一点：他们的发展公式将扩散得分视为可在尾→头\n和头→尾之间切换的混合，如果需要强化尾类多样性则执行Tail-to-Head迁移，若需要保护尾类特有细节也可以反向\n操作（论文中定义了H2T模式） 54 43 。虽然大多数方法将校准融合在训练，但一些后验调整也值得探索，例如针\n对生成结果的置信度校准：利用预训练模型评估生成图像的类别置信度，若发现尾类图像过于靠近头类特征空间，\n可reject并重新生成或调整噪声种子，直到达到预定的多样性阈值。此类方法类似于生成后的筛选，不改动扩散模型\n本身，可视为插件式提升尾类表现的方法。当然，这可能增加采样开销，但在需要高质量少样本合成时是可选的手\n段。\n3\n\n方法对比与适配总结\n下表汇总了以上方法的核心思想及适配性：\n无需\n适配LDM/文本\n方法 核心思想 改结 尾类提升 代码\n扩散\n构\n重采样(Over- 提高尾类采样频率，均 适用于任何条 尾类样本利用率 - （常规实\n是\nsampling) 衡每批次类分布 3 件形式 ↑，易过拟合 现）\n对尾类图像进行变换或\n数据增广 可用于图像或 尾类样本多样性 Softlabel-\n合成软标签，提高样本 是\n(DiffAug等) 潜空间增广 ↑，亲和度平衡 GAN开源 9\n多样性 7\n按类别频次反比赋予损\n条件/无条件均 尾类梯度↑，头 - （直接实\n损失重加权 失权重，平衡头尾损失 是\n可 类精度略降 现）\n贡献\n随机选嵌入对拉大隐藏 全局多样性↑\nDispersive 通用（不依赖 计划开源（论\n表示距离，防止模式塌 是 （隐含惠及尾\nLoss 标签） 文附）\n陷 14 类）\nInfoNCE负样本正则，\n对比损失 无监督通用， 尾类多样性↑ 附论文代码片\n拉远任意两生成样本嵌 是\n(CLDM-UCL) 条件均可 （模式去重） 段\n入 18\n需类别标签\n跨类分布对比 惩罚不同类噪声预测相 尾类FID↓，类\n是 （文本可选模 已开源 32\n(DiffROP) 近，拉开类间分布 26 混淆↓\n拟）\n对齐扩散初期的无条件/ 需有无条件分\n条件-无条件对 尾类表示更丰 - （几行代码\n有条件预测，知识共享 是 支（CFG训练\n齐 富，避免过拟合 实现）\n21 自带）\n加入正则使不同类预测 类别条件扩\n分布校准 尾类多样性↑， - （算法简洁\n更相似，平衡条件分布 是 散，文本需归\n(CBDM) 语义清晰度↑ 易实现）\n36 类\n扩散过程中以头类目标 类别条件，文 尾类FID大幅\n定向校准\n替换尾类目标，直接知 是 本可尝试相似 ↓，多样性↑ 已开源 51\n(OCLT)\n识迁移 46 概念 48 50\n降低尾类guidance防塌\n适用于CFG的 尾类样本覆盖 - （推理超\n指导尺度调整 陷，分阶段采样提升覆 是\n任何条件扩散 ↑，细节可控 参）\n盖率\n4\n\n无需\n适配LDM/文本\n方法 核心思想 改结 尾类提升 代码\n扩散\n构\n利用模型高质量输出反\n适用于尾类数 尾类质量↑（潜 - （需定制实\n生成自蒸馏 哺训练，提升低 是\n据极少场景 在），待验证 现）\nguidance下性能\n注：表中箭头↑↓表示相对基线的提升或下降趋势。\n结论与适配建议\n综上，近年来针对扩散模型长尾数据的训练提出了丰富的即插即用技巧。从数据重采样到损失正则，再到推理校\n准，这些方法均不更改模型架构，可以直接应用于固定结构的LDM+检测器管线中。在实际适配时，可考虑组合多\n种技术以取长补短。例如，在LDM训练中启用类别均衡采样+损失重加权打好基础，再加入对比正则或分布校准提\n高尾类多样性和质量；训练完毕后，在推理时针对尾类调低guidance或采用定向校准算法进一步提高生成覆盖面。\n对于涉及文本提示的扩散，可将上述基于类别的方法转换为基于概念频次或语义相似的方法：如识别出长尾概念集\n合，给予类似的重采样和正则策略。\n总体而言，不改结构的长尾训练法已证明能显著改善少样本类别的生成表现。例如，CLDM方法在ImageNet-LT上\n显著提高了尾类图像的多样性且不损伤头类质量 24 ；OCLT则在多个基准上取得最优的尾类FID和丰富的生成多样\n性 50 ；CBDM在保证图像质量的同时，让模型对尾部模式的掌握更加充分 1 40 。这些成果表明，通过训练策略\n的优化，扩散模型可以更好地“兼顾尾部”，生成既多样又逼真的长尾类别图像。对于固定结构的LDM+DINO系统，\n建议优先尝试上述插件式策略，并根据具体任务（纯生成或生成+检测）需求进行组合调优，从而在不改变模型架构\n的前提下最大化模型对长尾分布的适应性和生成性能。\n参考文献：\n1 2 ; 3 ; 4 ; 7 8 ; 10 ; 11 ; 6 ; 13 ; 14 ; 15 ; 18 17 ; 20 ; 21 ; ; 3 ; 24 ; 25 ; 26 27 ; 28\n29 ; 30 ; 31 ; 32 ; 33 ; 36 ; 38 ; 34 ; 41 ; 46 47 ; 49 ; 50 ; 43 ; 48 ; 54 ; ; 8 ; 50 ; 40 ; 24 ; 50\n1 2 6 13 34 35 36 37 39 40 52 Class-Balancing Diffusion Models\nhttps://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Class-Balancing_Diffusion_Models_CVPR_2023_paper.pdf\n3 [2507.09052] Contrastive Conditional-Unconditional Alignment for Long-tailed Diffusion Model\nhttps://arxiv.org/abs/2507.09052\n4 5 42 43 44 45 46 47 53 54 proceedings.iclr.cc\nhttps://proceedings.iclr.cc/paper_files/paper/2024/file/7dff60c3db3c0d2f3acf92f13b1b2472-Paper-Conference.pdf\n7 8 9 11 Label Augmentation As Inter-Class Data Augmentation for Conditional Image Synthesis With\nImbalanced Data\nhttps://openaccess.thecvf.com/content/WACV2024/papers/Katsumata_Label_Augmentation_As_Inter-\nClass_Data_Augmentation_for_Conditional_Image_Synthesis_WACV_2024_paper.pdf\n10 [PDF] arXiv:2404.04517v2 [cs.CV] 23 Apr 2024\nhttps://arxiv.org/pdf/2404.04517\n5\n\n12 Imbalance in Balance: Online Concept Balancing in Generation Models\nhttps://openaccess.thecvf.com/content/ICCV2025/papers/\nShi_Imbalance_in_Balance_Online_Concept_Balancing_in_Generation_Models_ICCV_2025_paper.pdf\n14 15 [2506.09027] Diffuse and Disperse: Image Generation with Representation Regularization\nhttps://arxiv.org/abs/2506.09027\n16 17 18 19 20 21 22 23 24 25 33 38 [2507.09052] Can Contrastive Learning Improve Class-\nImbalanced Diffusion Model?\nhttps://ar5iv.labs.arxiv.org/html/2507.09052v2\n26 27 28 29 30 31 [Literature Review] Training Class-Imbalanced Diffusion Model Via Overlap\nOptimization\nhttps://www.themoonlight.io/en/review/training-class-imbalanced-diffusion-model-via-overlap-optimization\n32 Training Class-Imbalanced Diffusion Model Via Overlap Optimization | OpenReview\nhttps://openreview.net/forum?\nid=3u9dvwPEeW&referrer=%5Bthe%20profile%20of%20Tao%20Hu%5D(%2Fprofile%3Fid%3D~Tao_Hu2)\n41 48 49 50 51 Long-tailed Diffusion Models with Oriented Calibration | OpenReview\nhttps://openreview.net/forum?id=NW2s5XXwXU\n6\n\n",
    "total_pages": 6,
    "pages": [
      {
        "page": 1,
        "content": "扩散模型长尾分布训练方法综述\n背景： 长尾分布的训练数据会导致扩散模型对头部类别生成质量较高，而尾部类别往往生成多样性和保真度不足\n1 2 。针对这一问题，近三年的研究提出了多种无需改动模型结构的训练策略，适用于Latent Diffusion\nModel (LDM) 这类扩散模型，并可结合目标检测器（如DINO）的固定架构使用。下面从数据层面、损失函数层面\n和推理调控三个方面总结相关方法，并比较各方法的核心思想、适配性和效果。\n数据层面的长尾均衡策略\n1. 类别均衡采样： 长尾数据下常用做法是调整各类别样本的采样频率，在训练批次中提升尾部类样本比例 3\n4 。例如ImageNet-LT等数据集，可采用过采样尾类或欠采样头类，使每个minibatch呈近似均匀的类分布 5\n4 。Chen等（2025）的长尾扩散模型训练中，就结合标准的重采样技巧来增加尾类样本的出现频率，显著提升了\n尾类生成的多样性 3 。需注意过度过采样可能导致尾类过拟合，实践中可在维持头类多样性的同时逐步增加尾类\n采样率。\n2. 数据增广与生成增强： 利用数据增广来扩充尾部类别的有效样本数也是即插即用的方法 6 。例如，可对尾类图\n像施加Differentiable Augmentation (DiffAug) 或 ADA 等技术（源自GAN训练）来生成带噪或变换的样本\n6 。Katsumata等（2024）提出标签增广（Label Augmentation）的方法：使用预训练分类器为图像预测软标\n签，将相似类的概率分布作为新的训练标签，从而在判别器（或扩散模型的条件）中融入跨类特征，实现高亲和且\n多样的增广 7 8 。这一思路在GAN中验证了对少样本类的增强效果 9 ；在扩散模型中，也有工作建议在潜变量\n空间进行类似的插值增广，以减少直接图像空间增广带来的分布偏移 10 。对于文本-图像扩散，可以通过丰富尾部\n概念的描述（如增加同义词、属性词）或对长尾概念组合进行额外增广来平衡训练语料。不过需要注意增广应保持\n语义正确性（高“亲和度”）同时引入足够变化（高“多样性”） 11 。总体而言，数据层面的方法对模型结构零改动，\n易于实现，常作为基础方案；但若尾类样本极少，仍需结合损失层面的特殊正则化来防止模型记忆少数样本。\n损失函数层面的长尾优化策略\n1. 类别损失重加权： 直接在扩散模型的噪声回归损失上给予尾部类别更高权重，是最简单的长尾补偿手段。可按每\n类频次的反比（如$\\alpha/(n_c+\\epsilon)$）为该类样本的损失乘以权重，或采用Effective Number公式计算平\n衡权重 12 。这一策略类似分类任务中的重加权损失，对扩散模型同样适用 13 。需要仔细调节权重强度，过高会削\n弱头类学习，通常与重采样结合使用以避免单纯重复少数样本造成的过拟合。\n2. Dispersive Loss分散损失： Wang和何（2025）提出了分散损失（Dispersive Loss），这是一种即插即用的正\n则项，旨在鼓励扩散模型内部表示在隐藏空间中更加分散 14 。其思想类似对比学习中的拉开分布，但不需要构造\n正样本对，从而不干扰扩散回归采样过程 14 。与之前需要外部表示对齐的方案（如REPA）不同，分散损失无需额\n外预训练或外部数据，仅通过在训练中正则化模型的中间特征，使模型摆脱对长尾类训练样本的记忆，提高对少样\n本模式的探索能力 14 。在ImageNet上，加入分散损失可对多种扩散模型架构取得一致性提升 15 。虽然作者主要\n验证了平衡数据集上的整体性能提升，但这种提升隐含地有利于尾类多样性，因为分散的内部表示能减少尾类模式\n塌陷（如训练集中某尾类样本被反复生成的问题）。该方法不依赖特定条件标签，对无条件和有条件扩散均适用；\n对于文本条件扩散，也能在不改变U-Net结构的情况下增强隐藏表示的覆盖范围，提升生成多样性。\n1"
      },
      {
        "page": 2,
        "content": "3. 对比学习正则化： 将对比损失融入扩散模型，可有效缓解尾类模式坍塌 16 17 。Chen等（2025）提出对比学\n习扩散模型（CLDM）框架，包括两个正则项： 18 (a) 无监督InfoNCE损失（仅负样本）：不区分类别地将不同生\n成样本的内部嵌入拉远，特别是针对尾类样本，增大它们在表示空间的距离，从而鼓励同类生成的多样性 18 17 。\n这一损失相当于惩罚生成样本过于相似（尤其是尾类往往趋向于几个训练样本的模式 19 ），以提高尾类图像的多\n样性和覆盖率。 20 (b) 条件-无条件对齐损失：这是一个MSE损失，在扩散过程的大时间步对比有条件和无条件扩散\n分支预测的噪声，使二者在初始去噪阶段对齐 21 。其作用是使模型在早期不依赖过强的类条件信息，利用无条件\n扩散（学到整个数据分布）的知识来丰富尾类表示 3 21 。这一技巧与GAN领域的UTLO方法类似，将有条件和无\n条件生成共享低阶特征，从而头尾类知识共享 21 。CLDM方法无需改变U-Net结构，仅增加上述损失，易于在标准\nDDPM或扩散Transformer训练管线中实现 22 23 。实验证明，该对比正则框架在CIFAR-10/100-LT、ImageNet-\nLT等多数据集上显著提升了尾类图像的多样性，同时保持头类的保真度 24 。例如，与基础DDPM和其他SOTA方法\n（如CBDM、OCLT等）相比，CLDM在ImageNet-LT 256×256上的尾类FID有明显降低 25 。\n另一项工作是Yan等（2024）的Diffusion with Regularized Overlap (DiffROP) 26 。DiffROP关注不同类别生\n成分布的区分度，通过概率对比学习损失来最小化不同类别合成图像分布的重叠 26 27 。作者观察到长尾数据下扩\n散模型生成的罕见类图像常近似于频繁类 26 。为此，DiffROP引入针对类别条件分布的对比正则：对于任意两类\n$y_i, y_j$，取各自的样本在随机时刻$t$的Noisy表示$x_t^{(i)}, x_t^{(j)}$，惩罚模型预测从$x_t^{(i)}$反向去噪一\n步得到的分布与从$x_t^{(j)}$得到的分布之间的KL散度 26 27 。在高斯假设下，该KL正则可简化为模型对这两类\n样本预测的噪声$\\epsilon_\\theta$差异的L2损失，从而高效实现 28 29 （等价于拉开不同类别的预测噪声\n28 ）。DiffROP不需要特殊的采样策略即可融入训练，通过一个超参数$\\tau$控制对比损失权重，并随$t$增大而\n指数衰减，以反映后期噪声水平的趋同 30 30 。此外，作者引入hinge样式的变体损失，只在分布过度重叠时施加\n惩罚，以避免扰乱原有最优点 31 。DiffROP能有效提高尾类图像的保真度，因为不同类生成被拉开后，头类特征不\n至于“淹没”尾类，减少了尾类图像与训练集中头类过于相似的现象 26 27 。实验显示该方法在多个长尾数据集上提\n升了合成质量 32 。代码已开源提供实现细节 32 。需要注意的是，DiffROP只考虑跨类别分布的分离，未直接鼓励\n同类内部的多样性 33 ；因此，可与上述InfoNCE类正则组合，使模型既避免类别混淆，又增加尾类内部的多样覆\n盖。\n4. 分布校准与知识蒸馏： 一些方法通过隐含分布校准来提升尾类生成质量。Qin等（2023）的Class-Balancing\nDiffusion Models (CBDM)就是典型代表 34 。CBDM假设训练数据非均衡，提出在采样过程中调整条件转移概\n率，使每一步生成的中间分布都更加平衡 35 。这种调整对应于在训练目标中加入一个额外的MSE正则项，使扩散\n模型对不同类别的预测更加相似 36 。直观而言，该损失促进了模型对不同条件生成的隐藏表示相似，相当于将头\n类的共性特征传递给尾类，而不会损伤头类表达能力 37 。实现上，只需几行代码：例如，对每个训练样本除了使\n用其真实标签$y$计算噪声损失外，再随机指定一个伪标签$y'$（全局上服从均匀分布）计算另一份噪声预测，并最\n小化两次预测的差异 38 。这样模型被引导对任意标签条件都产出相似的初始预测，削弱了因类别频次差异导致的\n预测偏差 38 。CBDM本质是在不引入新参数的情况下实现了条件扩散的“均衡化”，可被看作对有条件扩散模型隐含\n先验分布的校准。实验表明，CBDM在CIFAR100-LT等数据集上能生成多样且质量更高的图像 39 ；尤其尾部类别的\n图像语义更清晰、多样性更好（FID显著降低，见论文附图） 1 2 。此外，平衡后的生成模型还提升了下游长尾\n分类任务的识别性能 40 。这说明通过这种隐式KL正则，扩散模型学到了更普适的表征，对尾类也有更强的生成能\n力。\n另一项定向校准策略是Zhang等（2024）的OCLT（Oriented Calibration for Long-Tail diffusion）方法 41 。\nOCLT利用贝叶斯观点直接实现头→尾知识转移，核心是在训练中将尾类样本的去噪目标部分替换为相似的头类样本\n42 43 。具体做法为：在每一训练步骤，采样一个均衡类别分布的minibatch，然后对于尾部类别的Noisy样本\n$x_t^{(T)}$，在batch内根据样本与$x_t^{(T)}$的相似度挑选一个来自头部类的干净样本$x_0^{(H)}$ 44 45 ，将\n$x_t^{(T)}$的真实去噪目标替换为$x_0^{(H)}$对应的噪声估计$\\hat{\\epsilon}_H$（而非其自身的$\\hat{\\epsilon}\n_T$） 46 47 。简言之，让模型用头类的“答案”指导尾类的去噪。这种转移通过一个门控机制严格控制：只有当候\n选头类的频次高于尾类时才执行替换，反之则保持原样（避免头类扰动尾类过度） 46 。通过这种定向的Score校准\n2"
      },
      {
        "page": 3,
        "content": "（Tail-to-Head模式），模型在早期训练阶段就将头类多样的语义注入尾类生成过程，显著增加了尾类生成的多样性\n43 48 。理论分析表明，该知识转移的效果取决于标签分布和样本相似度，合理结合二者可以在再平衡类别比例时\n获得更优解 49 。实验中，OCLT在CIFAR-10/100-LT、Imagenet-LT等多项基准上性能优于已有方法 50 。它无需\n改变模型结构，仅通过定制的损失计算实现，对LDM同样适用；作者已开源代码便于集成 51 。值得注意的是，\nOCLT进一步强调了重采样与样本相似性结合的重要性：与纯粹增加尾类频率相比，选择具备相似特征的头类样本来\n指导尾类，可在保证生成图像质量的前提下提升多样性 49 。这种方法尤其适合类别标签明确的扩散模型；对于文\n本条件扩散，可探索将尾部概念对应的一组高频概念作为“头类”指导，实现类似的语义迁移（但要有可靠的相似度\n度量，如CLIP嵌入）。\n推理阶段的调整与其他即插即用技术\n1. Classifier-Free Guidance 调整： 对于已训练好的有条件扩散模型，推理时的指导系数(guidance scale)直接影\n响尾类图像的质量和多样性。长尾情况下，尾类条件下模型的预测往往不如头类可靠，若使用过强的Guidance，可\n能导致模型过度依赖有限的条件信号，反而生成模式单一的图像（甚至重复训练样本） 19 。因此，可对尾部类别\n采用较弱的指导力度，让模型保留更多无条件生成的多样性；而对头部类别则可使用正常或更高的guidance以确保\n细节和保真度。实践中可以为每个类别设定不同的guidance scale，或使用一种动态调节策略：例如在扩散采样初\n期使用低guidance（鼓励多样性），后期逐渐提高guidance细化图像。这种分阶段指导思想与前述训练对齐损失类\n似，都旨在先利用模型整体分布能力，再引入类别细节 21 。需要注意的是，不同类别的最优guidance可能需要通\n过在验证集上试验FID、多样性指标来确定。对于文本生成模型，亦可按罕见概念降低guidance，以避免模型忽视\n罕见词语——这类似于提升prompt中稀有词影响力的做法，如Attend-and-Excite方法通过优化使得稀有词获得更\n高注意力，从而更突出地出现在生成图像中 8 。总之，适度调整推理参数是一种不改模型而改善尾类生成的简单\n途径。\n2. 自蒸馏与生成校准: 自蒸馏是指利用模型本身的生成结果来改进模型。如Dhariwal等工作的启发，可以将扩散模\n型在高指导下生成的高质量样本，用于微调模型使其在较低指导甚至无指导情况下也能生成类似质量的图像。这种\n将模型推理行为反哺训练的思路，可视为对尾类的一种增强：先用模型在尾类上以宽松采样获得多样图像，再用这\n些合成图像联合原始数据继续训练，从而扩充尾类分布的支持。虽然截至目前没有直接以“自蒸馏”命名的长尾扩散\n论文，但这种思路与知识蒸馏和自训练在判别任务中的做法类似，值得在生成任务中尝试。另一个角度，Meng等\n的DINO探测器本身采用了无标签自蒸馏技术，可以在生成-检测闭环中启发新的训练策略：例如，用当前LDM生成\n图像，经DINO检测获得伪标签反馈，再用以训练扩散模型或调整其条件嵌入，从而校准模型对尾类的识别和生成能\n力。这属于更复杂的流水线，不在纯生成模型训练范畴内，在此提及以供拓展考虑。\n3. 分数校准方法： （注：此处“分数”指扩散模型中的score或判别导数） 除了训练时的KL正则化，推理时也可进行\n生成分布的校准。如果采用分类器引导的扩散（Classifier Guidance），可对分类器的输出进行后处理校准：例如\n减去各类别的对数频率（类似识别任务中的logit adjustment 52 ），使得指导偏向尾类。这等价于假设均匀的条件\n先验，从而鼓励生成尾部样本。对于无分类器的条件扩散，可借鉴“面向头尾的score重加权”思路：如在每次去噪\n时，检测当前样本属于尾类的可能性，适当增强模型预测中的头类成分，从概率流角度提高生成跳往头类模式的概\n率 53 。Zhang等（2024）的定向校准实际已在训练中实现了这一点：他们的发展公式将扩散得分视为可在尾→头\n和头→尾之间切换的混合，如果需要强化尾类多样性则执行Tail-to-Head迁移，若需要保护尾类特有细节也可以反向\n操作（论文中定义了H2T模式） 54 43 。虽然大多数方法将校准融合在训练，但一些后验调整也值得探索，例如针\n对生成结果的置信度校准：利用预训练模型评估生成图像的类别置信度，若发现尾类图像过于靠近头类特征空间，\n可reject并重新生成或调整噪声种子，直到达到预定的多样性阈值。此类方法类似于生成后的筛选，不改动扩散模型\n本身，可视为插件式提升尾类表现的方法。当然，这可能增加采样开销，但在需要高质量少样本合成时是可选的手\n段。\n3"
      },
      {
        "page": 4,
        "content": "方法对比与适配总结\n下表汇总了以上方法的核心思想及适配性：\n无需\n适配LDM/文本\n方法 核心思想 改结 尾类提升 代码\n扩散\n构\n重采样(Over- 提高尾类采样频率，均 适用于任何条 尾类样本利用率 - （常规实\n是\nsampling) 衡每批次类分布 3 件形式 ↑，易过拟合 现）\n对尾类图像进行变换或\n数据增广 可用于图像或 尾类样本多样性 Softlabel-\n合成软标签，提高样本 是\n(DiffAug等) 潜空间增广 ↑，亲和度平衡 GAN开源 9\n多样性 7\n按类别频次反比赋予损\n条件/无条件均 尾类梯度↑，头 - （直接实\n损失重加权 失权重，平衡头尾损失 是\n可 类精度略降 现）\n贡献\n随机选嵌入对拉大隐藏 全局多样性↑\nDispersive 通用（不依赖 计划开源（论\n表示距离，防止模式塌 是 （隐含惠及尾\nLoss 标签） 文附）\n陷 14 类）\nInfoNCE负样本正则，\n对比损失 无监督通用， 尾类多样性↑ 附论文代码片\n拉远任意两生成样本嵌 是\n(CLDM-UCL) 条件均可 （模式去重） 段\n入 18\n需类别标签\n跨类分布对比 惩罚不同类噪声预测相 尾类FID↓，类\n是 （文本可选模 已开源 32\n(DiffROP) 近，拉开类间分布 26 混淆↓\n拟）\n对齐扩散初期的无条件/ 需有无条件分\n条件-无条件对 尾类表示更丰 - （几行代码\n有条件预测，知识共享 是 支（CFG训练\n齐 富，避免过拟合 实现）\n21 自带）\n加入正则使不同类预测 类别条件扩\n分布校准 尾类多样性↑， - （算法简洁\n更相似，平衡条件分布 是 散，文本需归\n(CBDM) 语义清晰度↑ 易实现）\n36 类\n扩散过程中以头类目标 类别条件，文 尾类FID大幅\n定向校准\n替换尾类目标，直接知 是 本可尝试相似 ↓，多样性↑ 已开源 51\n(OCLT)\n识迁移 46 概念 48 50\n降低尾类guidance防塌\n适用于CFG的 尾类样本覆盖 - （推理超\n指导尺度调整 陷，分阶段采样提升覆 是\n任何条件扩散 ↑，细节可控 参）\n盖率\n4"
      },
      {
        "page": 5,
        "content": "无需\n适配LDM/文本\n方法 核心思想 改结 尾类提升 代码\n扩散\n构\n利用模型高质量输出反\n适用于尾类数 尾类质量↑（潜 - （需定制实\n生成自蒸馏 哺训练，提升低 是\n据极少场景 在），待验证 现）\nguidance下性能\n注：表中箭头↑↓表示相对基线的提升或下降趋势。\n结论与适配建议\n综上，近年来针对扩散模型长尾数据的训练提出了丰富的即插即用技巧。从数据重采样到损失正则，再到推理校\n准，这些方法均不更改模型架构，可以直接应用于固定结构的LDM+检测器管线中。在实际适配时，可考虑组合多\n种技术以取长补短。例如，在LDM训练中启用类别均衡采样+损失重加权打好基础，再加入对比正则或分布校准提\n高尾类多样性和质量；训练完毕后，在推理时针对尾类调低guidance或采用定向校准算法进一步提高生成覆盖面。\n对于涉及文本提示的扩散，可将上述基于类别的方法转换为基于概念频次或语义相似的方法：如识别出长尾概念集\n合，给予类似的重采样和正则策略。\n总体而言，不改结构的长尾训练法已证明能显著改善少样本类别的生成表现。例如，CLDM方法在ImageNet-LT上\n显著提高了尾类图像的多样性且不损伤头类质量 24 ；OCLT则在多个基准上取得最优的尾类FID和丰富的生成多样\n性 50 ；CBDM在保证图像质量的同时，让模型对尾部模式的掌握更加充分 1 40 。这些成果表明，通过训练策略\n的优化，扩散模型可以更好地“兼顾尾部”，生成既多样又逼真的长尾类别图像。对于固定结构的LDM+DINO系统，\n建议优先尝试上述插件式策略，并根据具体任务（纯生成或生成+检测）需求进行组合调优，从而在不改变模型架构\n的前提下最大化模型对长尾分布的适应性和生成性能。\n参考文献：\n1 2 ; 3 ; 4 ; 7 8 ; 10 ; 11 ; 6 ; 13 ; 14 ; 15 ; 18 17 ; 20 ; 21 ; ; 3 ; 24 ; 25 ; 26 27 ; 28\n29 ; 30 ; 31 ; 32 ; 33 ; 36 ; 38 ; 34 ; 41 ; 46 47 ; 49 ; 50 ; 43 ; 48 ; 54 ; ; 8 ; 50 ; 40 ; 24 ; 50\n1 2 6 13 34 35 36 37 39 40 52 Class-Balancing Diffusion Models\nhttps://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Class-Balancing_Diffusion_Models_CVPR_2023_paper.pdf\n3 [2507.09052] Contrastive Conditional-Unconditional Alignment for Long-tailed Diffusion Model\nhttps://arxiv.org/abs/2507.09052\n4 5 42 43 44 45 46 47 53 54 proceedings.iclr.cc\nhttps://proceedings.iclr.cc/paper_files/paper/2024/file/7dff60c3db3c0d2f3acf92f13b1b2472-Paper-Conference.pdf\n7 8 9 11 Label Augmentation As Inter-Class Data Augmentation for Conditional Image Synthesis With\nImbalanced Data\nhttps://openaccess.thecvf.com/content/WACV2024/papers/Katsumata_Label_Augmentation_As_Inter-\nClass_Data_Augmentation_for_Conditional_Image_Synthesis_WACV_2024_paper.pdf\n10 [PDF] arXiv:2404.04517v2 [cs.CV] 23 Apr 2024\nhttps://arxiv.org/pdf/2404.04517\n5"
      },
      {
        "page": 6,
        "content": "12 Imbalance in Balance: Online Concept Balancing in Generation Models\nhttps://openaccess.thecvf.com/content/ICCV2025/papers/\nShi_Imbalance_in_Balance_Online_Concept_Balancing_in_Generation_Models_ICCV_2025_paper.pdf\n14 15 [2506.09027] Diffuse and Disperse: Image Generation with Representation Regularization\nhttps://arxiv.org/abs/2506.09027\n16 17 18 19 20 21 22 23 24 25 33 38 [2507.09052] Can Contrastive Learning Improve Class-\nImbalanced Diffusion Model?\nhttps://ar5iv.labs.arxiv.org/html/2507.09052v2\n26 27 28 29 30 31 [Literature Review] Training Class-Imbalanced Diffusion Model Via Overlap\nOptimization\nhttps://www.themoonlight.io/en/review/training-class-imbalanced-diffusion-model-via-overlap-optimization\n32 Training Class-Imbalanced Diffusion Model Via Overlap Optimization | OpenReview\nhttps://openreview.net/forum?\nid=3u9dvwPEeW&referrer=%5Bthe%20profile%20of%20Tao%20Hu%5D(%2Fprofile%3Fid%3D~Tao_Hu2)\n41 48 49 50 51 Long-tailed Diffusion Models with Oriented Calibration | OpenReview\nhttps://openreview.net/forum?id=NW2s5XXwXU\n6"
      }
    ]
  },
  "pdf_url": "/uploads/e9e120256ad5e34692ecd7bfaae504d4.pdf"
}