{
  "filename": "BEV.pdf",
  "upload_time": "2025-12-13T09:48:37.584054",
  "data": {
    "full_text": "538 IEEETRANSACTIONSONIMAGEPROCESSING,VOL.34,2025\nPhysically Realizable Adversarial Creating Attack\nAgainst Vision-Based BEV Space\n3D Object Detection\nJian Wang , Fan Li , Senior Member, IEEE, Song Lv, Lijun He , Member, IEEE,\nand Chao Shen , Senior Member, IEEE\nAbstract—Vision-based 3D object detection, a cost-effective I. INTRODUCTION\nalternativetoLiDAR-basedsolutions,playsacrucialroleinmod-\nern autonomous driving systems. Meanwhile, deep models have VISION-BASED 3D object detection [1], [25], [26] has\nbeen proven susceptible to adversarial examples, and attacking\nwidespread applications in fields such as autonomous\ndetection models can lead to serious driving consequences. Most\ndrivingandrobotics,whichutilizesmultiplecamerascovering\nprevious adversarial attacks targeted 2D detectors by placing\nthe patch in a specific region within the object’s bounding a 360◦ field of view to perceive and locate the foreground\nbox in the image, allowing it to evade detection. However, objects in the environment and benefits from low deployment\nattacking 3D detector is more difficult because the adversary costs. Recently, 3D perception tasks [2], [3], [4] conducted\nmay be observed from different viewpoints and distances, and\nin Bird’s Eye View (BEV) space [5], [6], [7] have attracted\nthere is a lack of effective methods to differentiably render the\ntremendous attention, thanks to the holistic representation,\n3D space poster onto the image. In this paper, we propose\na novel attack setting where a carefully crafted adversarial rich semantic, precise localization, and naturally support most\nposter (looks like meaningless graffiti) is learned and pasted downstream tasks. The vision-based BEV detectors typically\non the road surface, inducing the vision-based 3D detectors to transformtheimagefeaturestotheunifiedBEVspacethrough\nperceive a non-existent object. We show that even a single 2D\ndepth estimation or transformers and perform the detection\nposter is sufficient to deceive the 3D detector with the desired\nprocess upon these BEV feature maps. Despite the great suc-\nattack effect, and the poster is universal, which is effective\nacrossvariousscenes,viewpoints,anddistances.Togeneratethe cess that has been made, DNN-based models are found to be\nposter, an image-3D applying algorithm is devised to establish vulnerable to the carefully crafted adversarial examples [10],\nthe pixel-wise mapping relationship between the image area [11], [12], which can manipulate the model to produce any\nand the 3D space poster so that the poster can be optimized\ndesired outputs [8], [9]. For the 3D detection task, creating\nthrough standard backpropagation. Moreover, a ground-truth\nfake targets and hiding real objects correspond to two types\nmaskedoptimizationstrategyispresentedtoeffectivelylearnthe\nposter without interference from scene objects. Extensive results oferrorsthatthedetectormayproduce,namelyfalsepositives\nincludingreal-worldexperimentsvalidatetheeffectivenessofour (FP) and false negatives (FN). FN attacks can cause the\nadversarial attack. The transferability and defense strategy are perception system to fail to perceive real objects, such as a\nalso investigated to comprehensively understand the proposed\ncar driving on the lane, leading the autonomous vehicle to\nattack.\ncontinuemovingandultimatelyresultinginacollision.Incon-\nIndexTerms—Physicaladversarialattack,visual3Ddetection,\ntrast, FP predictions can mistakenly identify non-existent\nuniversal patch, adversarial robustness.\nobjects,triggeringunnecessaryorinappropriateactions—such\nas sudden braking or evasive maneuvers—that may endanger\nReceived10June2024;revised3December2024and27December2024;\naccepted 29 December 2024. Date of publication 10 January 2025; date of the passengers’ safety. The underlying vulnerabilities, in con-\ncurrent version 17 January 2025. This work was supported in part by the junctionwiththesafetyandreliabilityconcerns,motivateusto\nNationalScienceandTechnologyMajorProjectunderGrant2022ZD0115803,\ninvestigatetherealisticadversarialattacksonwidelydeployed\nin part by the Natural Science Basic Research Plan in Shaanxi Province\nof China under Grant 2023-JC-JQ-51, in part by the Fundamental Research 3D detection models in autonomous driving systems.\nFundsfortheCentralUniversitiesunderGrantxzy022023053,andinpartby Most adversarial attacks on 2D perception tasks can be\ntheKeyResearchandDevelopmentProgramofShaanxiProvinceunderGrant\ncategorized into adversarial perturbations [12], [13], [14] and\n2024GH-ZDXM-41.Theassociateeditorcoordinatingthereviewofthisarti-\ncleandapprovingitforpublicationwasProf.AykutErdem.(Corresponding adversarial patches [15], [16], [17]. The former slightly alters\nauthor:FanLi.) each pixel value of the image to induce mispredictions of\nJian Wang, Fan Li, Song Lv, and Lijun He are with Shaanxi Key\nthe model, while the latter modifies the image content within\nLaboratory of Deep Space Exploration Intelligent Information Technology,\nSchool of Information and Communications Engineering, Xi’an Jiaotong a restricted region. Although patch attacks may be more\nUniversity, Xi’an 710049, China (e-mail: wj851329121@stu.xjtu.edu.cn; perceptible to the human eyes, they are feasible to implement\nlifan@mail.xjtu.edu.cn;net666@stu.xjtu.edu.cn;lijunhe@mail.xjtu.edu.cn).\nin the physical world compared to the perturbation. Physical\nChaoSheniswiththeSchoolofCyberScienceandEngineering,MOEKey\nLaboratory for Intelligent Networks and Network Security, Xi’an Jiaotong patch attacks first require training adversarial patches in\nUniversity,Xi’an710049,China(e-mail:chaoshen@mail.xjtu.edu.cn). the digital domain before printing them, which necessitates\nThis article has supplementary downloadable material available at\neffectively and realistically rendering the patch onto images\nhttps://doi.org/10.1109/TIP.2025.3526056,providedbytheauthors.\nDigitalObjectIdentifier10.1109/TIP.2025.3526056 in the digital space. However, vanilla 2D patch rendering\n1941-0042©2025IEEE.Allrightsreserved,includingrightsfortextanddatamining,andtrainingofartificialintelligence\nandsimilartechnologies.Personaluseispermitted,butrepublication/redistributionrequiresIEEEpermission.\nSeehttps://www.ieee.org/publications/rights/index.htmlformoreinformation.\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply.\n\nWANGetal.:PHYSICALLYREALIZABLEADVERSARIALCREATINGATTACKAGAINSTBEVSPACE3DOBJECTDETECTION 539\napplied to 3D detection scenarios faces the following issues,\nthereby limiting their physical feasibility. Firstly, physical\nperspective transformation—image data are captured from\na perspective view, and the image region covered by a\nreal patch in 3D space can appear at various distances,\nangles, and viewpoints, where the patch region cannot be\nsimulated simply through a linear transformation and pasted\non images. Secondly, lacking the depth information in 3D\nspace—attackers cannot determine the spoofing location of\nthe patch in 3D space given only the image inputs, making\nit difficult to reproduce in the physical world.\nTo perform the physically realizable adversarial attack\nagainst visual 3D detectors, Zhu et al. [18] propose a 3D\nconsistent patch by pasting it to the side of a target vehicle,\nallowing it to evade the detection process from 3D detectors.\nHowever, their attack only considers learning an adversarial\npatch for a few continuous frames or overlaps with multiple\ncameras. So, the learned patch is not universal and hard to\ngeneralizetootherscenarios.Tomitigatethisproblem,Lietal.\n[19] propose Adv3D upon NeRF techniques to learn an Fig.1. Ourproposedadversarialposterattack.Bypastingthelearnedposter\nadversarialvehiclewithcamouflagetexture,sothatthecrafted ontheroadsurface,thedetectorswillperceivea‘ghost’objectattheposter\nlocation.\nvehicle together with surrounding targets can be hidden.\nDifferent from hiding an existing target, this paper investi-\nwith other foreground objects and is easily overlooked\ngates how to create a ‘ghost’ object at the desired location in\nby the driver. This further increases the safety risks of\nthe3Dscenethroughthepatchmanner.Thecraftedadversary\nautonomous driving.\nmust be universal so that it can generalize to the unseen\n2) We propose I3DA and GTMO techniques to build\nscenesandkeepattackefficacyacrossawiderangeofviewing\nthe attack pipeline and optimize the spoofing poster.\nanglesanddistances.Toachievethisgoal,weproposeanovel\nSpecifically, I3DA serves as a differentiable rendering\nattack setting dubbed 3D adversarial poster attack to induce\nalgorithm to establish the pixel-wise mapping relation-\nprediction errors of the 3D detectors. Specifically, we learn\nship between the image patch area and 3D space poster,\na carefully crafted adversarial patch with predefined physical\nwhich is realized by solving the spatial position con-\nsizeandresolution,andpasteittotheroadsurface,causingthe\nstraints. I3DA enables the gradients to propagate from\n3D detector to perceive a non-existent object. Here, we refer\nthe image to the 3D space poster and is network- and\nto this attack as the 3D poster attack (3D-PA) because it is\nparameter-free.TheGTMOisdevisedtoeliminateinter-\nplaced in 3D space and then captured by the cameras. Such\nference from scene objects on the adversarial objective,\nan attack can lead to severe driving consequences such as\nallowing the poster to learn more deceptive content.\nemergency brakes that may injure passengers. Learning such\n3) We report a thorough evaluation, including attacks on a\na poster mainly faces the following two challenges:\ntotalofsixvictimmodelscomprisingthreetypicalvisual\n1) How to render the 3D space poster onto the image and\nBEV 3D detectors, transferability, and defense strat-\nbackpropagate gradients?\negy, to comprehensively understand the attack effect.\n2) How to design effective optimization objectives to\nSpecifically, real-world experiments with printed poster\nensure the poster learns useful content in complex\nachieve an average attack success rate of more than\nautonomous driving scenarios?\n90% within 12m. The learned posters have cross-model\nIn this paper, we address the above issues by proposing transferability and can be effectively defended through\nan image-3D applying algorithm (I3DA) and a ground-truth adversarial augmentation.\nmaskedoptimizationstrategy(GTMO).Extensiveexperiments Theremainderofthispaperisorganizedasfollows.Wefirst\nincluding digital and physical-world attacks demonstrate the recall some related works including adversarial attacks on 2D\neffectiveness of 3D-PA. We detail the contributions of this images and 3D detectors in Section II. Then we elaborate the\nwork as follows: attack pipeline of 3D-PA in Section III. The attack results\n1) We introduce 3D-PA, the first instance-level creating in digital space and the physical world are presented in\nattack for visual 3D detectors, to spoof fake objects in Section IV and V respectively, followed by the defense strat-\nautonomous driving scenarios with physically printable egy in Section VI. Finally, Section VII concludes the paper.\nposters.Thelearnedadversaryisuniversal,whichmeans\nit can be applied to various road scenes and keep effi-\nII. RELATEDWORK\ncacy under viewpoint changes. Attackers can precisely\nA. Vision-Based BEV Space 3D Object Detection\ncontrolthe positionof thespoofed objectby pastingthe\nposter in the corresponding region. Since the poster is The image inputs are typically captured from a perspec-\ninherently 2D and lacks thickness, it does not interact tive view, and the key to vision-based 3D detection lies in\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply.\n\n540 IEEETRANSACTIONSONIMAGEPROCESSING,VOL.34,2025\nhow to use image features to construct BEV features in 3D object on top of a vehicle so that the entire host vehicle can\nspace. According to the view transformation process, current be hidden. The following works demonstrate the feasibility\nadvanced works can be divided into depth-based methods ofattackingtheLiDAR-camerafusion-baseddetectionsystem\nand network-based methods. Depth-based methods explicitly using a 3D-printed object [41] or simply attacking the camera\nestimate a depth distribution and a context feature for each modality [42]. The works above cause FN predictions from\nimage feature pixel and then lift 2D features to the 3D space the detector so that the real objects can be hidden. Another\naccording to the camera’s intrinsic and extrinsic parameters. set of works achieves the goal of creating fake objects [43]\nFinally, the BEV feature for each position is aggregated from and hiding real ones [44] by strategically injecting fake laser\nthose lifted features falling into the same BEV grid. This points into the target LiDAR sensor [45]. Although these\nparadigmwasfirstproposedbyLSS[20],andBEVDet[21]is attackshaveshownpromisingresults,theyallrequirecomplex\nthe first to apply it to the 3D detection task. BEVDet4D [22] implementation conditions such as 3D printing of adversarial\nand BEVDepth [23] further boost this pipeline by introducing objects or sophisticated electro-optical emission and control\ntemporal cues and explicit depth supervision. The network- modules. In this work, we propose a more accessible and\nbasedmethodsutilizeatop-downstrategybyfirstconstructing feasible attack method against vision-based 3D detectors by\nthe predefined BEV queries and then searching each BEV simply printing and pasting the learned poster on the road\ncontext from the multi-view image features by transformer surface thus leading to significant detection errors.\narchitectures [57], [58]. Among them, Tesla first leverages\ncross-attention between BEV queries and image features\nIII. METHOD\nto perform view transformation. BEVFormer [24] further\naggregates the history BEV information and uses deformable Inthissection,weintroducetheattackpipelineforlearning\nattention to reduce the computational budgets. theadversarialposter,whichcanbepastedontheroadsurface\nand induces 3D detectors to perceive a non-existent object\nB. Physical Adversarial Attack on 2D Image such as a car. Moreover, the learned poster content, appearing\nas seemingly meaningless graffiti to the human eyes, can\nThe adversarial attacks on 2D images have been well\neffectively mislead the deep models. Generally, we argue the\nstudied and more and more works are devoted to designing\ncraftedpostertobe(1)universal—theposteriseffectiveacross\nphysical space attacks. A popular topic is to use adversarial\ndifferent road scenes, which is scene-agnostic; (2) robust to\npatches to mislead a well-trained person detector to make\nviewing conditions—the poster can withstand observations\nfalse negative prediction results, which can be realized by\nfrom different views and distances; (3) transferable—the\nprinted cardboard [27], [28], cloak [30], [31], and portable\nlearnedposterexhibitsacertaindegreeoftransferabilityacross\nmonitor[29].Toinducemodelfailureintrafficroadscenarios,\ndifferent models and parameter initialization.\nmany efforts have been made to mislead road sign classifiers\nIn the training phase, we learn the poster on the training\nby physical perturbations [32], adversarial natural styles [33],\nscenes following Expectation of Transformation (EOT) [51],\nor simply use a laser beam [35]. Sato et al. [34] propose\n[52], where we iteratively render the poster to each frame and\ndirty road patches to effectively attack the automated lane\nupdatethecontent.Therefore,themethodsectionisorganized\ncenteringsystems.Aseriesofworks[46],[47],[48],[49],[50]\nto answer: where to place the poster? how to differentiably\nare proposed to learn adversarial camouflage applying on 3D\nrender the poster onto the image? and how to optimize it\nobject surfaces so that the target object can evade detection.\neffectively?\nWhile this approach demonstrates the feasibility of physical\nimplementation, spray-painting large areas on vehicles for\ncamouflage is not easy. Therefore, most of the works are A. Overall\nlimited to rendering camouflaged vehicles in synthetic scenes\nThe attack starts from a predefined poster with fixed phys-\nfor experimentation, and they only target 2D detectors.\nical size h × w meters, digital resolution H × W pixels,\np p\nand expected object size O ×O ×O meters outputted by\nx y z\nC. Adversarial Attack on 3D Object Detection\nthe3Ddetector.Formally,thepostercanberepresentedbythe\nAttacking 3D object detection models is more challenging learnableparameters P ∈[0,255]Hp×Wp×3 andeachpixelhas\nas the targets appear in 3D space and exhibit more poses and a real physical size h m× w m. The poster is pasted on the\nHp Wp\nviewing angles, thus requiring the adversarial patterns to be road surface in 3D space thus requiring a rendering function\nrobust enough to cope with the various observations. Previous to project the poster to the 2D image plane. Then, multi-view\nworksproposetoattackvision3Ddetectorsbysimplyextend- images with adversarial patterns are fed into the model for\ning 2D image-based perturbations [36] and patches [37], [38], inference, and the content of the poster is optimized under\nwhich is difficult to implement in the physical world. Apart the supervision of adversarial loss. To enable the gradient\nfromattackingvision3Ddetectors,Caoetal.[39]proposean to propagate from the corresponding image region to the\nadversarial object that can evade the detection process from a 3D space poster, the entire process, especially the applying\nLiDAR 3D detector by optimizing a group of learnable object function, must be differentiable. The overall training pipeline\nvertices. Tu et al. [40] further places the learned adversarial is shown in Fig. 2.\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply.\n\nWANGetal.:PHYSICALLYREALIZABLEADVERSARIALCREATINGATTACKAGAINSTBEVSPACE3DOBJECTDETECTION 541\nFig.2. Theattackpipelineofouradversarialposter.Wepredefinearandomlyinitializedposterwithafixedphysicalsizeanddigitalresolution.Theposter\nisplacedontheroadsurfacewithoutoverlappingwithsceneobjects.Thenweprojectthepostercornerstotheimageplaneandfindthecorrespondingimage\nposterarea.Byestablishingthecorrespondencebetweeneachpixelintheimageposterareawiththe3Dspaceposter,wecanapplythepostertotheimage\nwhilebackpropagatingthegradients.\nB. Sampling the Poster Position in 3D Space\nWeplacetheposterinthe3Dspacesatisfying(1)theposter\nmust be placed on the road surface to ensure reproducibility\nin the physical scenario. (2) the expected bounding box of the\nposter does not overlap with existing 3D objects in the scene.\nFormally, let x ∈ RN×3×H×W denote a frame of image\ninputswithcorrespondinggroundtruthboundingboxes B ∈\ngt\nRm×9, where N is the number of cameras, m is the number\nof objects and each of which includes center location, object\nsize, yaw angle, and velocity. We adopt the following designs\nto ensure the placement of the poster on the road. First,\nwe only place the poster within the view of the front and\nbackcameras,asthesetwocamerasaremostlyfacingtheroad. Fig.3. Illustrationofthespoofingregion.Weonlyplacetheposterwithina\nSecond,fortheposterpositioninthe X−Y plane,welimitthe fixedanglerangetowardsthecenterdirectionofthefrontandbackcameras.\nposter within a fixed angle range towards the target camera’s\ncenter direction, which is illustrated in Fig. 3. Specifically,\nwe randomly select spoof distance d ∈ (d ,d ) and 3D LiDAR coordinates:\nmin max\nposter’s location angle θ ∈ (θ − (cid:49)θ,θ + (cid:49)θ), where θ w h\nis the camera’s center directionc (θ\nc\n= π 2c and −π\n2\nfor fronc t p 1init =[ 2, 2,0]T,\nand back cameras). Then, the poster center in X−Y plane is w h\np =[ ,− ,0]T\n(d·cosθ,d·sinθ).Third,fortheheightoftheposter,weselect 2init\n2 2\nthe bottom height of the nearest ground truth box in X −Y w h\np =[− ,− ,0]T\nplane as the poster height z p. We further filter out the case 3init 2 2\nwhen z is higher than a height threshold z to prevent the w h\np thr p =[− , ,0]T (2)\nposter from ‘flying’ in the air. The poster’s yaw angle θ y is 4init 2 2\nrandomly selected from (−π,π) and the expected velocity of  cosθ −sinθ 0\ny y\nthe spoofed object is set to 0. Finally, we get the expected p\ni\n= sinθ\ny\ncosθ\ny\n0× p\niinit\n+[dcosθ,dsinθ,z p]T\nobject box B : 0 0 1\nspoof\nB =[c ,c ,c ,length,width,height,yaw,v ,v ] (3)\nspoof x y z x y\nO\n=[dcosθ,dsinθ,z + z,O ,O ,O ,θ ,0,0] (1)\np 2 x y z y C. Image-3D Applying Algorithm\nTo avoid overlap with existing scene objects, we repeat the The image-3D applying algorithm serves as a differentiable\nabove process until the maximum Intersection over Union renderingfunctiontorenderthe3Dspaceposterontheimage,\n(IOU) between B and B is 0. which consists of two steps: First, find the poster area on\nspoof gt\nAfter getting the poster position in 3D space with 2D images; Second, for each pixel in the image poster area,\nthe expected bounding box B , we can calculate reversely find its position on the 3D space poster and assign\nspoof\nthe four poster corners {p ,p ,p ,p |p ∈ R3} in the pixel value. We now detail this process.\n1 2 3 4 i\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply.\n\n542 IEEETRANSACTIONSONIMAGEPROCESSING,VOL.34,2025\nGiven the camera intrinsic and extrinsic parameters, we get Algorithm1PseudocodeDiagramofTrainingtheAdversarial\nthe projection matrix M ∈ R4×4 and then project the 3D Poster\nL2C\ncorner p =(x ,y ,z ) to the image points p˜ ∈R2:\ni i i i i\n[u,v,z,1]T = M ×[x ,y ,z ,1]T\nL2C i i i\np˜ =[u/z,v/z] (4)\ni\nAccording to the Eq. 4, we get four poster corners\n{p˜ ,p˜ ,p˜ ,p˜ } in image plane. The image pixels falling into\n1 2 3 4\nthe quadrangle region defined by {p˜ ,p˜ ,p˜ ,p˜ } constitute\n1 2 3 4\nthe image poster area.\nFor each pixel in the image poster area, we need to find its\nposition in the 3D space for (1) applying the poster onto the\nimage according to the pixel-wise mapping relationship, and\n(2) propagating the gradients from the image to the poster\nto update its content. Specifically, for an image pixel p˜ =\ni\n(u ,v ) inside the poster area, there is a ray passing through\ni i\nthe camera center, and every point on this ray is mapped to\np˜ due to the lack of depth information. However, we have\ni\nthe prior that the transformed 3D point lies on the poster,\nwhich means its Z−coordinate values are equal to the poster\nheight. With this information, we can calculate the accurate\n3D position of each pixel in the image poster area. Formally,\nlet p =(x ,y ,z ) be the transformed 3D point and c be the\ni i i i i\ndepth for the image pixel p˜ satisfying:\ni\nM −1×[u c ,v c ,c ,1]T =[x ,y ,z ,1]T\nL2C i i i i i i i i\nz =z (5)\ni p\nBy jointly solving the Eq. 5, we can obtain the coordinates of\np and the pixel color for p˜ can be interpolated across the\ni i\nneighbor values around p on the poster. Notably, the entire\ni\nprocess of applying the poster to the image is differentiable,\nallowingustooptimizethecontentoftheposterthroughstan-\ndard backpropagation and the detailed optimization strategy\nwill be discussed in the next section.\nBased on that, we propose a ground-truth masked opti-\nD. Effective Optimization for the 3D Space Poster\nmization strategy to directly optimize the poster to avoid\nGenerally, the optimization goal consists of the adversarial interferencefromexistingsceneobjects.Specifically,wemask\npart and total variation of the poster. The adversarial loss is outtheimageareasofallsceneobjectsaccordingtotheanno-\nused to mislead the model to output corresponding detection tations B , ensuring that the training images only contain the\ngt\nresults at the poster location, while the total variation loss spoofedobjectsintroducedbyourposter.Thustheadversarial\nmakes sure that the generated poster has a smooth color loss becomes:\ntransition and prevents noisy content.\n1) Ground-TruthMaskedOptimization: Formally,let F(cid:50) be L adv = J(F(cid:50)(A(M(x,B gt),P)),B spoof) (6)\nthe3Dobjectdetectorparameterizedby(cid:50);x isthemulti-view\nwhere M is the mask operation.\nimage inputs with corresponding ground truth boxes B for\ngt 2) TotalVariationLossofthePoster: Tomakethegenerated\neachframe; J(F(cid:50)(x),B gt)betheoriginaldetectionlossfunc-\nposter appear more natural, we utilize the total variation loss\ntion; A(x,P) is the applying function that applies the poster following [53] to keep it smoother and consistent:\nP to the image x. An intuitive way to build the adversarial\nloss is to add the spoofed bounding box B spoof to the ground L tv\n=Xq\n(q i,j −q i+1,j)2+(q i,j −q i,j+1)2 (7)\ntruths and then minimize the J(F(cid:50)(A(x,P)),B\ngt\n∪ B spoof).\ni,j\nHowever, the ground truth objects in the image will affect\nthe poster learning in such an optimization objective. First, where q i,j is a color value in poster P. Finally, the overall\nloss function is:\nthe overall adversarial loss includes contributions from real\ntargets, resulting in the spoofed part indirectly acting on the L = L adv +λL tv (8)\nposter. Second, the optimization objective introduces certain\nbiases because the current form still requires improving the To ensure the poster is robust to viewing changes, we ran-\ndetection accuracy of existing targets. domly sample the poster position with different yaw angles\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply.\n\nWANGetal.:PHYSICALLYREALIZABLEADVERSARIALCREATINGATTACKAGAINSTBEVSPACE3DOBJECTDETECTION 543\nwithin the spoofing region as illustrated in Fig. 3 during TABLEI\ntraining. Moreover, we apply random brightness and contrast DETECTIONPERFORMANCEOFALLVICTIM\ntransformations and random noise on the poster to further\nDETECTORSONTHENUSCENESVALSET\nimprove the robustness. The final poster is generated through\ntheexpectationovertransformationandscenedistribution.The\nentire learning process is summarized in Algorithm 1.\nIV. DIGITALATTACKEXPERIMENT\nA. Experimental Setup\n1) Dataset: We conduct the digital attack experiments\non the nuScenes dataset [59], which contains 28130 and During training, we use Adam with an initial learning rate\n6019 keyframes for training and validation. Each frame con- of 0.01 to optimize the poster for 8 epochs and decay the\ntains image data from six cameras providing a 360◦ FOV learning rate by half after each epoch. The weight factor λ is\naround the self-vehicle with corresponding 3D object annota- set to 10.\ntions. During training the poster, we apply it to the front and\nbackcamerasofeachframeinthetrainsettoiterativelyupdate\nB. Effectiveness of the Learned Poster\nthe poster content supervised by Eq. 8. During the evaluation,\nwe choose a random subset of 1000 frames in the validation To the best of our knowledge, we are the first to perform\nset as benign scenes to be attacked. the universal instance-level creating attack against visual 3D\n2) Evaluation Metrics: The attack success rate (ASR) is detectors, where we can spoof the fake object at the desired\nutilized to evaluate the creating attack, which measures the position with an adversarial poster. To verify the effectiveness\npercentage of successfully detected poster objects out of the of our methods and exclude the influence of ground-truth\ntotal spoofing objects. We consider a spoofed poster suc- objects on the attack results, we further conduct the following\ncessfully detected if the output IoU with B is greater control experiments for comparison: (1) Benign, the original\nspoof\nthan {0.1,0.3,0.5,0.7}. Moreover, the BEV center distance scene without attacks, however, we fix the random seed and\nunder thresholds {0.5m,1.0m,1.5m,2.0m} is also adopted to recordthepositionsofspoofedobjectsinthefollowingexper-\ndetermine whether a spoofing is successful. The IoU metric iments so that we can calculate the number of original scene\ncomprehensively considers the position, size, and orientation objectsmiscalculatedasspoofedobjects.(2)Rand,arandomly\nof the predicted object, and a larger IoU threshold results in initialized poster. (3) Real, taking a real vehicle image as the\na stricter evaluation of the attack. The center distance metric poster. (4) Adv. Poster, the adversarial poster learned from\nonly considers the Euclidean distance between the prediction the corresponding detector by our methods. The Rand, Real,\nand the poster, with a smaller threshold indicating a stricter and Adv. Poster share the same physical size and resolution.\nevaluation. In fact, for our creating attacks, it is not crucial We showcase these four situations in Fig. 5.\nwhether the fake bounding box is completely aligned with We calculate the ASR for a total of 2000 spoofed objects\nthe poster. As long as the detector can be induced to make in 1000 scenes and the results under different evaluation\nFP predictions near the poster, it can pose a security threat. metrics are summarized in Table II. Firstly, from Benign\nTherefore, we prefer using the center distance metric. The experimentsweobservethatthepredictionsforexistingscene\nconfidence score for all detectors is set as 0.1. objects hardly overlap with the spoofed locations because we\n3) Implementation Details: The BEVFormer [24], deliberately avoided such situations when selecting the poster\nBEVDet [21], and BEVDet4D [22] are selected as positions. Specifically, less than 1% detected objects have an\nvictim 3D detectors, each equipped with ResNet50 [61] IoU greater than 0.1 with B , and only about 2% have a\nspoof\nand SwinTransformer-Tiny [62] as backbone networks, center distance less than 2 meters from the spoofed posters\nrespectively. We train all models following their official for all models. Secondly, the attack impact of a randomly\nsettings and the performance on nuScenes validation set is initialized poster is negligible, however, a real vehicle poster\ngiven in Table I. We set the spoofing category of the poster to can to some extent cause the detectors to make false positive\nthe most widely used Vehicle in this paper. The poster starts predictions. For example, the real vehicle poster can achieve\nfrom a randomly initialized patch with fixed physical size the highest 18.2%, 29.4%, and 39.3% ASRs (IoU= 0.1) for\n2m × 3m and digital resolution 400 × 600 pixels, resulting BEVFormer-SwinT, BEVDet-Res50, and BEVDet4D-Res50\nin each poster pixel having a real size of 0.5cm × 0.5cm. respectively, which indicates that existing vision-based 3D\nWe set the expected object size predicted by the detectors as object detectors may still make incorrect predictions when\n4m ×1.8m ×1.6m, which is the common size for a typical faced with a realistic object pattern. Thirdly, our 3D adver-\nvehicle. To ensure that the poster yields sufficient area on sarial posters consistently achieved a high ASR across all\nthe image to interfere with the detector, we place the poster models. Specifically, 73.9%, 85.2%, 79.0%, 67.6%, 80.5%,\nlocated 6-12 meters from the self-vehicle and the angle range and 81.3% ASRs when IoU = 0.1, and 66.5%, 76.2%,\nlimitation (cid:49)θ = 15◦. Attacking the detector’s predictions 74.3%, 65.2%, 75.3%, and 76.9% ASRs when CD = 2.0m\nwithin this region is more likely to induce serious driving for BEVFormer-Res50, -SwinT, BEVDet-Res50, -SwinT and\nconsequences, such as emergency braking. The poster height BEVDet4D-Res50, -SwinT respectively. Such results demon-\nthreshold z is empirically set to −1m. strate the effectiveness of our proposed adversarial attacks\nthr\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply.\n\n544 IEEETRANSACTIONSONIMAGEPROCESSING,VOL.34,2025\nTABLEII\nATTACKSUCCESSRATE(%)FORVISION-BASED3DDETECTORSWITHDIFFERENTBACKBONES\nTABLEIII\nTRANSFERABILITYOFTHEPOSTERSACROSSDIFFERENTMODELS.THEATTACKSUCCESSRATES(%)UNDERIoU =0.1AREPRESENTED\nFig.4. Visualizationresultsofour3Dposterattackindigitalspace.The3Ddetectorperceivesa‘ghost’objectintheposterlocation.\nand reveal the adversarial vulnerabilities of existing visual its precise position in 3D space and some other fine-grained\n3D detectors. However, the adversarial poster is essentially characteristics, such as yaw angle. So, under stricter detection\n2D without thickness and lacks corresponding 3D geometric metrics like IoU = 0.7 and CD = 0.5m, ASR will decrease\ninformation, making it difficult for the detector to estimate significantly. Moreover, compared to the real vehicle pictures,\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply.\n\nWANGetal.:PHYSICALLYREALIZABLEADVERSARIALCREATINGATTACKAGAINSTBEVSPACE3DOBJECTDETECTION 545\nTABLEIV\nTHEATTACKSUCCESSRATES(%)WHENATTACKINGKITTIDATASET\nin each frame is much smaller than the ground truth objects,\nthe adversarial part we aim for may be overwhelmed by the\nirrelevant foreground loss, leading to indirect and inefficient\nFig.5. Weshowfourtypesofattackedscenesonasingleframe.Theattack\noptimization of the poster. By eliminating the side effects of\npatternsandcorrespondingspoofingboundingboxesarepresented.\nsceneobjectsontheoptimizationobjective,theGTMOfurther\nboosts the ASRs by up to 20%. We also visualize some of\nthelearnedposterhasmoreabstractcontent,resemblingcasual the learned posters w/ and w/o masking GT as shown in\ndoodles(visualizationresultsareshowninFig.4),soitishard Fig. 6(a), (j), (c), and (k). It is interesting to find that the\nfor the human eyes to recognize it as a foreground object. posters generated by the masking strategy exhibit more vivid\nand discriminative patterns.\n2) Physical Size: We fix the pixel density (0.5cm×0.5cm)\nC. Transferability\nand train the poster with different physical sizes on BEVDet.\nWe investigate whether the learned posters transfer to TheresultsareshowninFig.7(c).Weobservethatthephysical\ndifferent detectors, backbone networks, and parameter initial- size of the poster is the primary factor influencing the ASR.\nization. We further train three posters learned on the retrained Alargerposterexhibitsmorepixelareaintheimageandthus\nBEVFormer, BEVDet, and BEVDet4D with different random has stronger attack capabilities. However, larger posters are\nseeds, which we denote as BF-Res50-R, BD-Res50-R, and more noticeable to humans and may be challenging to print,\n4D-Res50-Rrespectively.Thetransferresultsonatotalofnine so, attackers need to make a trade-off between the physical\nmodelsarepresentedinTableIIIandallpostersarevisualized size and attack capability.\ninFig.6.Theposterislearnedfromthecorrespondingsource 3) Digital Resolution: We further fix the physical size as\nmodel and the attacks are performed on the target model. 2m×3m andlearntheposterwithdifferentdigitalresolutions.\n“BF”, “BD”, and “4D” denote the BEVFormer, BEVDet, and As shown in Fig. 7(d), the ASR is less sensitive to the pixel\nBEVDet4D respectively. density, and even a poster with only 100×150 pixels exhibits\nFirstly, we observe that all posters can effectively deceive nearlythesameadversarialperformancewithlargerresolution\nthe target detectors even attacking models that they have ones. However, when the poster resolution is set too high,\nnot been trained on, demonstrating a certain transferability. the ASR drops significantly. We interpret this as insufficient\nThis enables attackers to perform practical black-box attacks training on high-resolution posters within the same training\nwithout access to the target model information. Secondly, epoch.\nwe empirically find that the posters learned from BEVFormer\nexhibit stronger transferability. We speculate that this is\nE. Discussion\nbecause BEVFormer constructs BEV features using a query-\n1) Attacking KITTI Dataset: The KITTI dataset [60] only\nbased approach, where each BEV query focuses more on the\nannotates 3D objects in front of the self-vehicle, specifically\noverallsemanticsoftheimageposterregionduringthequery-\nthosecapturedbythefront-viewcamera.WetraintheBEVDet\ning process. As a result, the learned poster exhibits smoother\nonthetrainsplit(3712samples)andfurtherlearntheadversar-\ndetailsandmorecompactoverallsemanticinformationforthe\nialposter.Theattackresultsonvalidationsplit(3769samples)\ndetector.\nare given in Table IV. Our attack algorithm can successfully\nextend to the KITTI dataset, achieving 64.1% ASR under\nD. Ablation Study CD = 2.0m, which demonstrates the generalizability of our\n1) Ground-Truth Masked Optimization Strategy: We study I3DAandGTMOalgorithmsacrossdifferentdatasets(scenes).\ntheeffectivenessoftheproposedGTMOandresultsareshown The visualization results are also provided in Fig. 8.\nin Fig. 7(a) and (b). It can be seen that masking GT during 2) AttackingLiDAR-CameraFusionModel: Weinvestigate\ntraininghelpsthelearnedpostermoredeceptivetothedetector theattackeffectontheLiDAR-camerafusion-based3Ddetec-\nbothforBEVDetandBEVFormer.Withoutthemaskingstrat- tionandmakeacasestudyonBEVFusion[56].Followingthe\negy, minimizing the J(F(cid:50)(A(x,P)),B\ngt\n∪ B spoof) consists GTMOstrategy,wemasktheforegroundobjectregionsinthe\nof two parts: First, updating the poster to reduce the loss image along with the corresponding point cloud within the\nvalue on ground truth objects, which need to improve the 3D bounding boxes, and then train the adversarial posters on\ndetection quality of existing scene objects. Second, updating BEVFusion. The results are shown in Table V.\nthepostertocausethedetectortoidentifythespoofedobjects First, attacking only the camera modality does not\nintroducedbyourposters.Sincethenumberofspoofedobjects effectively spoof fake objects for the fusion model. Given\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply.\n\n546 IEEETRANSACTIONSONIMAGEPROCESSING,VOL.34,2025\nFig.6. Visualizationofthelearnedposters.\nFig.7. Ablationexperimentsforoptimizationstrategy,physicalsize,anddigitalresolution.\nTABLEV\nTHE ATTACK SUCCESS RATES (%) WHEN ATTACKING DIFFERENT\nMODALITIESONBEVFUSION\nperformance. Previous works [43], [44], [45] have demon-\nstrated the feasibility of injecting laser points into LiDAR in\nthephysicalworld.Withaplatformthatincludesphotodiodes,\nlaser diodes, and control circuits, it is possible to inject up\nto thousands of laser points into the target scene. In our\nexperiments, when we randomly inject a small number of\nFig.8. VisualizationofcreatingattacksonKITTIdataset.\nlaser points (no more than 20, which is a relatively loose\nbudget)nearthecenteroftheadversarialposter,thedeception\nperformance can be significantly improved compared to the\nthe fact that the LiDAR modality contributes more to the single-modality attack. Even though the positions of these\nfusion model for the 3D object detection task, we find that laser points are not optimized, they can effectively guide the\nmanipulatingonlytheimagedatadoesnotresultinaneffective fusionmodel’sattentiontotheposterregion,leadingtoafalse\nattack (ASR less than 5%). Since our poster is essentially a positive prediction.\n2D plane and placed on the road surface, it does not affect 3) Attack Range: When training the posters, we only place\nthe point cloud data. Second, injecting a small number them within the front and back camera regions to ensure\nof laser points at the poster location, along with our that they are placed on the road with the highest probability,\nadversarial poster, can significantly enhance the attack whileplacingthemintheremainingcamerasmayoverlapwith\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply.\n\nWANGetal.:PHYSICALLYREALIZABLEADVERSARIALCREATINGATTACKAGAINSTBEVSPACE3DOBJECTDETECTION 547\nTABLEVI\nPHYSICALATTACKSUNDERDIFFERENTDISTANCESOFTHEPOSTER\nB. Physical Attack Result\nWe place the poster in the following two typical scenarios:\n1) Parking spaces on the side of the road, and 2) Directly\nin front of the ego-vehicle. The attack results are shown in\nFig. 10. Compared to the digital space attack, the physical\nFig.9. Attacksattheoverlappingregionbetweenmultiplecameras.\nattackmayfacemorecomplexfactors,suchaslightingcondi-\ntions,sensornoise,printingcolordifferences,posterwrinkles,\nthe non-road background. During testing, however, the attack and so on. However, our poster can effectively induce the\npositions are not restricted to these regions, for example, the detector to output false predictions in the physical world.\nattack can be carried out in other cameras or the overlapping Such attacks could potentially cause autonomous vehicles to\nregion between multiple cameras. Some visualization results brake suddenly, posing a safety risk to passengers. Moreover,\nare shown in Fig. 9. When rendering, we consider the poster we evaluate the poster attack under 1) indoor scenes, and\nis captured by a specific camera when four projected corner 2) large area distortion. Fig. 12(a) shows that the poster can\npoints have positive depth values, and at least one projected still keep attack efficacy in indoor environments, where there\ncorner falls into the image area. Therefore, when the poster is are significant background differences compared to outdoor\ncaptured by multiple cameras, we can still render it onto each scenes. This also reflects the insensitivity of the learned\nimageandperformeffectiveattacks.Infact,whentheposteris poster to the attack scenarios and demonstrates its universal\nfullycapturedbyacameraandweobtainfourcornerpositions characteristic. In the real world, the poster may experience\nin the image, the subsequent applying process is equivalent to varying degrees of occlusion or distortion. We simulate large\nusing a perspective transformation to render the poster onto area distortion in the physical world and demonstrate the\nthe image. robustness of our poster in Fig. 12(b). The generality and\nrobustness are achieved by optimizing the poster across the\nentire dataset and sampling positions (expectation over scene\nV. PHYSICALWORLDATTACK\ndistribution and transformation).\nWe introduce the real-world attack experiments in this\nsection. Unlike physical attacks against 2D object detectors, C. Attack Distance\nthemodelstrainedoncommondatasetscanbedirectlyusedin\nWe place the poster at different distances from the ego-\ncustomscenes,e.g.apersondetector,sothe2Dpatchattackis\nvehicle. At each distance, the yaw angle of the poster ranges\neasy to verify in real-world scenarios. Visual 3D detectors are continuouslyfrom−45◦ to45◦.TableVIprovidesquantitative\ntypicallysceneandcamera-dependent.Forexample,thedepth\nresults of the physical attacks. It can be seen that when the\nestimation process is camera intrinsic and extrinsic relative.\nspoofdistanceisbetween5to12meters,theASRapproaches\nIf inferring the custom data with a 3D detector pre-trained\n100%, demonstrating the significant threat of our adversarial\non the common dataset, the detector will produce completely\nattack in the real world. We attribute the differences in ASR\nwrong detection results. Therefore, we construct a simple\nbetween the digital and physical domains to the possibility of\nLiDAR-cameraacquisitionsystemtocollectandannotatedata\nunreasonable sampling positions for the poster in the digital\nin our scenes, train the corresponding visual 3D detector, and\nsetting.Althoughwedeliberatelyavoidcollisionsbetweenthe\nuse it for subsequent physical attacks.\nposterandforegroundobjectsin3Dspace,thepostermaystill\noverlap with other objects in the 2D image and may appear\nA. Implementation Details in non-road locations. Because the poster is essentially a 2D\nplane, when the distance is too far, the number of pixels in\nWe build a simple LiDAR-camera acquisition system as\nthe poster area on the image will decrease rapidly, greatly\nshowninFig.11.TheLiDARsensorisonlyusedforannotat-\nreducing its deceptive ability. Moreover, when the distance\ning the 3D objects. We collect paired data from 60 scenes\nis too close, the camera may capture an incomplete poster,\nwith a total of more than 6000 frames and annotate them\nleading to missing key information and thus resulting in a\nevery 1s. Due to the sparsity of the 16-line LiDAR, we only\nfailure attack.\nannotate vehicle class within 30m in front of the ego-vehicle\n(100◦ FOV). We finetune the BEVDet inherited weight from\nD. Theoretical Analysis\nthe nuScenes dataset for 20 epochs with the input resolution\n448×960 pixels. Finally, we train a 2m×4m poster on the In general, our adversarial posters can provide the essential\ncustom detector and physically print it. visualfeaturesrequiredbythedetector,makingitrecognizable\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply.\n\n548 IEEETRANSACTIONSONIMAGEPROCESSING,VOL.34,2025\nFig.10. Posterattacksinthephysical-world.\nTABLEVII\nATTACKSUCCESSRATES(%)BEFOREANDAFTERAPPLYINGADVERSARIALAUGMENTATIONASDEFENSE\nand detectable by the model. These features are usually local model making an incorrect prediction. Take a classification\nandlow-levelsemantic,thusdifficulttorecognizebyhumans. model as an example: attackers can optimize a patch such\nHowever,thereasonwhythesevisualcontentscanbeadopted that any image attacked by it can be misclassified into a\nby detectors is that deep models have not yet learned general, fixed incorrect category [15]. Such a patch must contain\nhigh-level visual semantics from the natural data. the critical class features [54], [55], enabling it to dominate\nConsider constructing a universal patch P that is effective the model’s focus regardless of the original input. Interest-\nacross different images: ingly, these patches often consist of abstract, unrecognizable\nargminE x∼DJ(F(cid:50)(A(x,P)),y adv) (9) patterns to humans, which reveals that the model has not\nP learnedthegeneralvisualrepresentationfromthenaturaldata,\nDuring the optimization, each step updates the patch along instead, its decisions can be manipulated by these adversarial\nthe gradient direction which maximizes the probability of the patterns.\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply.\n\nWANGetal.:PHYSICALLYREALIZABLEADVERSARIALCREATINGATTACKAGAINSTBEVSPACE3DOBJECTDETECTION 549\nTABLEVIII\nATTACKSUCCESSRATES(%)ONTHEDEFENDEDMODEL\nIn conclusion, 1) Why does the creating attack work?\nThe poster can provide the necessary object features from\nthe perspective view; 2) Why are the posters difficult for\nhumans to recognize? Deep models have not yet learned\nhigh-level visual semantics, making these local and low-level\nvisual contents sufficient for the model; 3) Why does it still\nwork when the poster is deformed or even incomplete?\nThe object information contained in the poster is potentially\nredundant.\nVI. DEFENSE\nFig.11. Thebuilt3Ddataacquisitionsystem.\nWe investigate the defense method by finetuning the detec-\ntor with the adversarial poster for another 2 epochs (with\nCBGS [63]). From Table VII we observe that introduc-\ning adversarial augmentation can significantly defend against\nposterattacks,notonlytheseenposterduringtrainingbutalso\nthe unseen posters learned from other detectors. Specifically,\nthe ASRs of the models after adversarial augmentation are\nconsistently reduced to about 1% at IOU 0.1, which is even\nmuch lower than the attack effect of using real vehicle photos\nasposters.Thisgreatlyimprovestheadversarialrobustnessof\nthe model. We analyze the defense capability of the unseen\nposterscanbeattributedtothestrongtransferabilityofposters\nbetween different models. From Fig. 6(a)-(i), we can see that\nthere are many similar features among different posters, such\nas numerous bright spots under the overall black background.\nMoreover,were-trainanadversarialposteronthedefended\nFig. 12. Attack results in indoor scene and poster distortion. We crumple\nthepostertotestagainstphysicaldeformationsthatarenoteasilysimulated. model (after adversarial augmentation) and present the results\nin Table VIII. The poster learned from the defended model\ncan still spoof the fake object, but the ASR does drop.\nWe show a re-trained poster in Fig. 6(l), it exhibits sharper\ncolor changes locally and shows significant differences in\noverall style compared to the previous posters. This indicates\nthatthepostercanstilllearnusefuldeceptivecontenttargeting\nweaknesses in the detection model. We consider that future\nFig.13. Visualizationoftheposteratdifferentdistances. works could apply the bi-level loop of adversarial training to\nfurther enhance the model’s adversarial robustness.\nBack to our creating attack, the attack effectiveness of the\noptimized content even surpasses using real vehicle images as\nVII. CONCLUSION\nposters. This shows that after the loss of the 3D geometric In this paper, we investigate the vulnerability of current\nstructure, the object information inside the vehicle image will vision-based 3D object detectors and propose a universal\nbe greatly compressed, and the learned poster can provide and physically realizable adversarial poster attack capable of\nthe most sufficient class features under the perspective view. generating fake objects. Attackers can physically print the\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply.\n\n550 IEEETRANSACTIONSONIMAGEPROCESSING,VOL.34,2025\nposter and place it on the road surface to ‘create’ a non- [18] Z. Zhu et al., “Understanding the robustness of 3D object detection\nexistent object at the desired location. The poster is generated with bird’s-eye-view representations in autonomous driving,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2023,\nbytheproposedimage-3DapplyingalgorithmandGT-masked\npp.21600–21610.\noptimization strategy. Extensive experiments demonstrate the [19] L. Li, Q. Lian, and Y.-C. Chen, “Adv3D: Generating 3D adversarial\neffectivenessof3D-PAandshowthatitcansuccessfullymake examplesfor3DobjectdetectionindrivingscenarioswithNeRF,”2023,\narXiv:2309.01351.\nreal-world threats in the physical attack setting. We further\n[20] J.PhilionandS.Fidler,“Lift,splat,shoot:Encodingimagesfromarbi-\nshow that introducing adversaries for training is an effective\ntrarycamerarigsbyimplicitlyunprojectingto3D,”inProc.Eur.Conf.\ndefense method and can improve the adversarial robustness Comput.Vis.,Cham,Switzerland.Springer,Aug.2020,pp.194–210.\nof the model. Our future works will focus on: 1) improving [21] J. Huang, G. Huang, Z. Zhu, Y. Ye, and D. Du, “BEVDet: High-\nperformancemulti-camera3Dobjectdetectioninbird-eye-view,”2021,\nthe stealthiness and naturalness of adversarial posters, mak-\narXiv:2112.11790.\ning them harder to detect by humans; (2) further extending [22] J.HuangandG.Huang,“BEVDet4D:Exploittemporalcuesinmulti-\nadversarialposterstothehidingattack(FN),achievingamore camera3Dobjectdetection,”2022,arXiv:2203.17054.\ncomplete attack pipeline. We hope this work could promote [23] Y.Lietal.,“BEVDepth:Acquisitionofreliabledepthformulti-view3D\nobjectdetection,”inProc.AAAIConf.Artif.Intell.,Jun.2023,vol.37,\nrobust 3D perception and provide valuable insights for safety-\nno.2,pp.1477–1485.\ncritical applications like self-driving. [24] Z. Li et al., “Bevformer: Learning bird’s-eye-view representation from\nmulti-cameraimagesviaspatiotemporaltransformers,”inProc.17thEur.\nREFERENCES\nConf.Comput.Vis.(ECCV),2022,pp.1–18.\n[25] W. Bao, B. Xu, and Z. Chen, “MonoFENet: Monocular 3D object\n[1] J.Mao,S.Shi,X.Wang,andH.Li,“3Dobjectdetectionforautonomous detection with feature enhancement networks,” IEEE Trans. Image\ndriving:Acomprehensivesurvey,”Int.J.Comput.Vis.,vol.131,no.8, Process.,vol.29,pp.2753–2765,2020.\npp.1909–1963,Aug.2023. [26] C.Huang,T.He,H.Ren,W.Wang,B.Lin,andD.Cai,“OBMO:One\n[2] Y. Hu et al., “Planning-oriented autonomous driving,” in Proc. boundingboxmultipleobjectsformonocular3Dobjectdetection,”IEEE\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2023, Trans.ImageProcess.,vol.32,pp.6570–6581,2023.\npp.17853–17862. [27] S.Thys,W.V.Ranst,andT.Goedemé,“Foolingautomatedsurveillance\n[3] Q.Song,Q.Hu,C.Zhang,Y.Chen,andR.Huang,“Divideandconquer: cameras: Adversarial patches to attack person detection,” in Proc.\nImproving multi-camera 3D perception with 2D semantic-depth priors IEEE/CVFConf.Comput.Vis.PatternRecognit.Workshops(CVPRW),\nand input-dependent queries,” IEEE Trans. Image Process., vol. 33, Jun.2019,pp.49–55.\npp.897–909,2024. [28] T.Kim,H.J.Lee,andY.M.Ro,“Map:Multispectraladversarialpatch\n[4] L. Peng, Z. Chen, Z. Fu, P. Liang, and E. Cheng, “BEVSegFormer: to attack person detection,” in Proc. IEEE Int. Conf. Acoust., Speech\nBird’s eye view semantic segmentation from arbitrary camera rigs,” in SignalProcess.(ICASSP),May2022,pp.4853–4857.\nProc. IEEE/CVF Winter Conf. Appl. Comput. Vis. (WACV), Jan. 2023,\n[29] Y.Wangetal.,“Towardsaphysical-worldadversarialpatchforblinding\npp.5935–5943.\nobjectdetectionmodels,”Inf.Sci.,vol.556,pp.459–471,May2021.\n[5] H. Li et al., “Delving into the devils of bird’s-eye-view perception: A\n[30] Z.Wu,S.-N.Lim,L.S.Davis,andT.Goldstein,“Makinganinvisibility\nreview,evaluationandrecipe,”IEEETrans.PatternAnal.Mach.Intell.,\ncloak:Realworldadversarialattacksonobjectdetectors,”inProc.Eur.\nvol.46,no.4,pp.2151–2170,Apr.2024.\nConf.Comput.Vis.,Cham,Switzerland.Springer,2020,pp.1–17.\n[6] Y. Ma et al., “Vision-centric BEV perception: A survey,” 2022,\n[31] X. Wei, Y. Huang, Y. Sun, and J. Yu, “Unified adversarial patch\narXiv:2208.02797.\nfor visible-infrared cross-modal attacks in the physical world,” IEEE\n[7] J. Wang, F. Li, Y. An, X. Zhang, and H. Sun, “Toward robust\nTrans. Pattern Anal. Mach. Intell., vol. 46, no. 4, pp.2348–2363,\nLiDAR-camera fusion in BEV space via mutual deformable attention\nApr.2024.\nand temporal aggregation,” IEEE Trans. Circuits Syst. Video Technol.,\n[32] K.Eykholtetal.,“Robustphysical-worldattacksondeeplearningvisual\nvol.34,no.7,pp.5753–5764,Jul.2024.\nclassification,”inProc.IEEE/CVFConf.Comput.Vis.PatternRecognit.,\n[8] J. Zheng, C. Lin, J. Sun, Z. Zhao, Q. Li, and C. Shen, “Physical 3D\nJun.2018,pp.1625–1634.\nadversarial attacks against monocular depth estimation in autonomous\ndriving,”2024,arXiv:2403.17301. [33] R.Duan,X.Ma,Y.Wang,J.Bailey,A.K.Qin,andY.Yang,“Adver-\nsarial camouflage: Hiding physical-world attacks with natural styles,”\n[9] C. Ma, N. Wang, Q. A. Chen, and C. Shen, “Slowtrack: Increasing\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2020,\nthe latency of camera-based perception in autonomous driving using\npp.1000–1008.\nadversarialexamples,”inProc.AAAIConf.Artif.Intell.,2024,vol.38,\nno.5,pp.4062–4070. [34] T.Sato,J.Shen,N.Wang,Y.Jia,X.Lin,andQ.A.Chen,“Dirtyroadcan\n[10] I.J.Goodfellow,J.Shlens,andC.Szegedy,“Explainingandharnessing attack:Securityofdeeplearningbasedautomatedlanecenteringunder\nadversarialexamples,”2014,arXiv:1412.6572. physical-world attack,” in Proc. 30th USENIX Secur. Symp. (USENIX\nSecur.),2021,pp.3309–3326.\n[11] X. Wei, Y. Guo, J. Yu, and B. Zhang, “Simultaneously optimizing\nperturbations and positions for black-box adversarial patch attacks,” [35] R.Duanetal.,“Adversariallaserbeam:Effectivephysical-worldattack\nIEEETrans.PatternAnal.Mach.Intell.,vol.45,no.7,pp.9041–9054, toDNNsinablink,”inProc.IEEEConf.Comput.Vis.PatternRecognit.,\nJul.2023. Jun.2021,pp.16062–16071.\n[12] S.-M.Moosavi-Dezfooli,A.Fawzi,O.Fawzi,andP.Frossard,“Univer- [36] W.Park,N.Liu,Q.A.Chen,andZ.M.Mao,“Sensoradversarialtraits:\nsaladversarialperturbations,”inProc.IEEEConf.Comput.Vis.Pattern Analyzingrobustnessof3Dobjectdetectionsensorfusionmodels,”in\nRecognit.(CVPR),Jul.2017,pp.1765–1773. Proc.IEEEInt.Conf.ImageProcess.(ICIP),Sep.2021,pp.484–488.\n[13] Z. Wei et al., “Towards transferable adversarial attacks on image [37] J.Zhang,Y.Lou,J.Wang,K.Wu,K.Lu,andX.Jia,“Evaluatingadver-\nand video transformers,” IEEE Trans. Image Process., vol. 32, sarial attacks on driving safety in vision-based autonomous vehicles,”\npp.6346–6358,2023. IEEEInternetThingsJ.,vol.9,no.5,pp.3443–3456,Mar.2021.\n[14] D.Wang,W.Yao,T.Jiang,andX.Chen,“Improvingtransferabilityof [38] S. Xie, Z. Li, Z. Wang, and C. Xie, “On the adversarial robustness of\nuniversaladversarialperturbationwithfeaturedisruption,”IEEETrans. camera-based3Dobjectdetection,”2023,arXiv:2301.10766.\nImageProcess.,vol.33,pp.722–737,2024. [39] Y. Cao et al., “Adversarial objects against LiDAR-based autonomous\n[15] T.B.Brown,D.Mané,A.Roy,M.Abadi,andJ.Gilmer,“Adversarial drivingsystems,”2019,arXiv:1907.05418.\npatch,”2017,arXiv:1712.09665. [40] J.Tuetal.,“PhysicallyrealizableadversarialexamplesforLiDARobject\n[16] J.Wang,A.Liu,X.Bai,andX.Liu,“Universaladversarialpatchattack detection,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\nfor automatic checkout using perceptual and attentional bias,” IEEE (CVPR),Jun.2020,pp.13716–13725.\nTrans.ImageProcess.,vol.31,pp.598–611,2022. [41] Y.Caoetal.,“InvisibleforbothcameraandLiDAR:Securityofmulti-\n[17] Y. Yu, H. J. Lee, H. Lee, and Y. M. Ro, “Defending person detection sensor fusion based perception in autonomous driving under physical-\nagainst adversarial patch attack by using universal defensive frame,” world attacks,” in Proc. IEEE Symp. Secur. Privacy (SP), May 2021,\nIEEETrans.ImageProcess.,vol.31,pp.6976–6990,2022. pp.176–194.\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply.\n\nWANGetal.:PHYSICALLYREALIZABLEADVERSARIALCREATINGATTACKAGAINSTBEVSPACE3DOBJECTDETECTION 551\n[42] Z.Chengetal.,“Fusionisnotenough:Singlemodalattacksonfusion JianWangreceivedtheB.S.degreeininformation\nmodelsfor3Dobjectdetection,”2023,arXiv:2304.14614. engineering from Xi’an Jiaotong University, Xi’an,\n[43] J.Wang,F.Li,X.Zhang,andH.Sun,“Adversarialobstaclegeneration China, in 2020, where he is currently pursuing the\nagainst LiDAR-based 3D object detection,” IEEE Trans. Multimedia, Ph.D. degree with the School of Information and\nvol.26,pp.2686–2699,2024. CommunicationsEngineering.Hisresearchinterests\n[44] Z. Jin, X. Ji, Y. Cheng, B. Yang, C. Yan, and W. Xu, “PLA- includeadversarialattack,3Dperception,andtrust-\nLiDAR:PhysicallaserattacksagainstLiDAR-based3Dobjectdetection worthyAI.\nin autonomous vehicle,” in Proc. IEEE Symp. Secur. Privacy (SP),\nMay2023,pp.1822–1839.\n[45] Y.Caoetal.,“AdversarialsensorattackonLiDAR-basedperceptionin\nautonomous driving,” in Proc. ACM SIGSAC Conf. Comput. Commun.\nSecur.,Nov.2019,pp.2267–2281.\n[46] N. Suryanto et al., “DTA: Physical camouflage attacks using differen-\ntiabletransformationnetwork,”inProc.IEEE/CVFConf.Comput.Vis.\nPatternRecognit.(CVPR),Jun.2022,pp.15305–15314. Fan Li (Senior Member, IEEE) received the B.S.\n[47] N.Suryantoetal.,“ACTIVE:Towardshighlytransferable3Dphysical and Ph.D. degrees from the School of Infor-\ncamouflageforuniversalandrobustvehicleevasion,”inProc.IEEE/CVF mation and Communications Engineering, Xi’an\nInt.Conf.Comput.Vis.(ICCV),Oct.2023,pp.4305–4314. JiaotongUniversity,Xi’an,China,in2003and2010,\n[48] D.Wangetal.,“FCA:Learninga3Dfull-coveragevehiclecamouflage respectively. From 2017 to 2018, he was a Visit-\nfor multi-view physical adversarial attack,” in Proc. AAAI Conf. Artif. ing Scholar with the Department of Electrical and\nIntell.,vol.36,2022,pp.2414–2422. Computer Engineering, University of California at\nSan Diego, San Diego. He is currently a Professor\n[49] J. Wang, A. Liu, Z. Yin, S. Liu, S. Tang, and X. Liu, “Dual attention\nwiththeSchoolofInformationandCommunications\nsuppressionattack:Generateadversarialcamouflageinphysicalworld,”\nEngineering,Xi’anJiaotongUniversity.Hisresearch\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR),\ninterestsincludemultimediasignalprocessing.\nJun.2021,pp.8565–8574.\n[50] Y. Zhang, H. Foroosh, P. David, and B. Gong, “CAMOU: Learn-\ning physical vehicle camouflages to adversarially attack detectors\nin the wild,” in Proc. Int. Conf. Learn. Represent., Sep. 2018,\npp.1–20.\n[51] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok, “Synthesizing Song Lv received the B.S. degree in information\nrobust adversarial examples,” in Proc. Int. Conf. Mach. Learn., 2018, engineering from Xi’an Jiaotong University, Xi’an,\npp.284–293. China, in 2022, where he is currently pursuing the\nM.S. degree with the School of Information and\n[52] M. Lee and Z. Kolter, “On physical adversarial patches for object\nCommunications Engineering. His research inter-\ndetection,”2019,arXiv:1906.11897.\nests include 3D point cloud processing and object\n[53] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, “Accessorize\ndetection.\nto a crime: Real and stealthy attacks on state-of-the-art face recogni-\ntion,” in Proc. ACM SIGSAC Conf. Comput. Commun. Secur., 2016,\npp.1528–1540.\n[54] A.Ilyas,S.Santurkar,D.Tsipras,L.Engstrom,B.Tran,andA.Ma¸dry,\n“Adversarial examples are not bugs, they are features,” in Proc. Adv.\nNeuralInf.Process.Syst.,2019,pp.1–12.\n[55] S. Kumano, H. Kera, and T. Yamasaki, “Theoretical understanding of\nlearningfromadversarialperturbations,”2024,arXiv:2402.10470.\nLijun He (Member, IEEE) received the B.S. and\n[56] T.Liangetal.,“BEVFusion:AsimpleandrobustLiDAR-camerafusion\nPh.D. degrees from the School of Information and\nframework,”inProc.Adv.NeuralInf.Process.Syst.,vol.35,Dec.2022,\nCommunications Engineering, Xi’an Jiaotong Uni-\npp.10421–10434.\nversity,Xi’an,China,in2008and2016,respectively.\n[57] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nSheiscurrentlyaProfessorwiththeSchoolofInfor-\nS.Zagoruyko,“End-to-endobjectdetectionwithtransformers,”inProc.\nmation and Communications Engineering, Xi’an\nEur.Conf.Comput.Vis.,2020,pp.213–229.\nJiaotong University. Her research interests include\n[58] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable videocommunicationandtransmission,videoanal-\nDETR:Deformabletransformersforend-to-endobjectdetection,”2020, ysis,processing,andcompressiontechniques.\narXiv:2010.04159.\n[59] H. Caesar et al., “nuScenes: A multimodal dataset for autonomous\ndriving,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n(CVPR),Jun.2020,pp.11621–11631.\n[60] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous\ndriving? The KITTI vision benchmark suite,” in Proc. IEEE Conf.\nComput.Vis.PatternRecognit.,Jun.2012,pp.3354–3361. Chao Shen (Senior Member, IEEE) received the\n[61] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for B.S. degree in automation and the Ph.D. degree\nimagerecognition,”inProc.IEEEConf.Comput.Vis.PatternRecognit. in control theory and control engineering from\n(CVPR),Jun.2016,pp.770–778. Xi’anJiaotongUniversity,China,in2007and2014,\nrespectively. He is currently a Professor with the\n[62] Z.Liuetal.,“Swintransformer:Hierarchicalvisiontransformerusing\nFaculty of Electronic and Information Engineering,\nshifted windows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),\nXi’anJiaotongUniversity.Hiscurrentresearchinter-\nOct.2021,pp.10012–10022.\nestsincludeAIsecurity,insider/intrusiondetection,\n[63] B. Zhu, Z. Jiang, X. Zhou, Z. Li, and G. Yu, “Class-balanced\nbehavioralbiometrics,andmeasurementandexper-\ngrouping and sampling for point cloud 3D object detection,” 2019,\nimentalmethodology.\narXiv:1908.09492.\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply.\n\n",
    "total_pages": 14,
    "pages": [
      {
        "page": 1,
        "content": "538 IEEETRANSACTIONSONIMAGEPROCESSING,VOL.34,2025\nPhysically Realizable Adversarial Creating Attack\nAgainst Vision-Based BEV Space\n3D Object Detection\nJian Wang , Fan Li , Senior Member, IEEE, Song Lv, Lijun He , Member, IEEE,\nand Chao Shen , Senior Member, IEEE\nAbstract—Vision-based 3D object detection, a cost-effective I. INTRODUCTION\nalternativetoLiDAR-basedsolutions,playsacrucialroleinmod-\nern autonomous driving systems. Meanwhile, deep models have VISION-BASED 3D object detection [1], [25], [26] has\nbeen proven susceptible to adversarial examples, and attacking\nwidespread applications in fields such as autonomous\ndetection models can lead to serious driving consequences. Most\ndrivingandrobotics,whichutilizesmultiplecamerascovering\nprevious adversarial attacks targeted 2D detectors by placing\nthe patch in a specific region within the object’s bounding a 360◦ field of view to perceive and locate the foreground\nbox in the image, allowing it to evade detection. However, objects in the environment and benefits from low deployment\nattacking 3D detector is more difficult because the adversary costs. Recently, 3D perception tasks [2], [3], [4] conducted\nmay be observed from different viewpoints and distances, and\nin Bird’s Eye View (BEV) space [5], [6], [7] have attracted\nthere is a lack of effective methods to differentiably render the\ntremendous attention, thanks to the holistic representation,\n3D space poster onto the image. In this paper, we propose\na novel attack setting where a carefully crafted adversarial rich semantic, precise localization, and naturally support most\nposter (looks like meaningless graffiti) is learned and pasted downstream tasks. The vision-based BEV detectors typically\non the road surface, inducing the vision-based 3D detectors to transformtheimagefeaturestotheunifiedBEVspacethrough\nperceive a non-existent object. We show that even a single 2D\ndepth estimation or transformers and perform the detection\nposter is sufficient to deceive the 3D detector with the desired\nprocess upon these BEV feature maps. Despite the great suc-\nattack effect, and the poster is universal, which is effective\nacrossvariousscenes,viewpoints,anddistances.Togeneratethe cess that has been made, DNN-based models are found to be\nposter, an image-3D applying algorithm is devised to establish vulnerable to the carefully crafted adversarial examples [10],\nthe pixel-wise mapping relationship between the image area [11], [12], which can manipulate the model to produce any\nand the 3D space poster so that the poster can be optimized\ndesired outputs [8], [9]. For the 3D detection task, creating\nthrough standard backpropagation. Moreover, a ground-truth\nfake targets and hiding real objects correspond to two types\nmaskedoptimizationstrategyispresentedtoeffectivelylearnthe\nposter without interference from scene objects. Extensive results oferrorsthatthedetectormayproduce,namelyfalsepositives\nincludingreal-worldexperimentsvalidatetheeffectivenessofour (FP) and false negatives (FN). FN attacks can cause the\nadversarial attack. The transferability and defense strategy are perception system to fail to perceive real objects, such as a\nalso investigated to comprehensively understand the proposed\ncar driving on the lane, leading the autonomous vehicle to\nattack.\ncontinuemovingandultimatelyresultinginacollision.Incon-\nIndexTerms—Physicaladversarialattack,visual3Ddetection,\ntrast, FP predictions can mistakenly identify non-existent\nuniversal patch, adversarial robustness.\nobjects,triggeringunnecessaryorinappropriateactions—such\nas sudden braking or evasive maneuvers—that may endanger\nReceived10June2024;revised3December2024and27December2024;\naccepted 29 December 2024. Date of publication 10 January 2025; date of the passengers’ safety. The underlying vulnerabilities, in con-\ncurrent version 17 January 2025. This work was supported in part by the junctionwiththesafetyandreliabilityconcerns,motivateusto\nNationalScienceandTechnologyMajorProjectunderGrant2022ZD0115803,\ninvestigatetherealisticadversarialattacksonwidelydeployed\nin part by the Natural Science Basic Research Plan in Shaanxi Province\nof China under Grant 2023-JC-JQ-51, in part by the Fundamental Research 3D detection models in autonomous driving systems.\nFundsfortheCentralUniversitiesunderGrantxzy022023053,andinpartby Most adversarial attacks on 2D perception tasks can be\ntheKeyResearchandDevelopmentProgramofShaanxiProvinceunderGrant\ncategorized into adversarial perturbations [12], [13], [14] and\n2024GH-ZDXM-41.Theassociateeditorcoordinatingthereviewofthisarti-\ncleandapprovingitforpublicationwasProf.AykutErdem.(Corresponding adversarial patches [15], [16], [17]. The former slightly alters\nauthor:FanLi.) each pixel value of the image to induce mispredictions of\nJian Wang, Fan Li, Song Lv, and Lijun He are with Shaanxi Key\nthe model, while the latter modifies the image content within\nLaboratory of Deep Space Exploration Intelligent Information Technology,\nSchool of Information and Communications Engineering, Xi’an Jiaotong a restricted region. Although patch attacks may be more\nUniversity, Xi’an 710049, China (e-mail: wj851329121@stu.xjtu.edu.cn; perceptible to the human eyes, they are feasible to implement\nlifan@mail.xjtu.edu.cn;net666@stu.xjtu.edu.cn;lijunhe@mail.xjtu.edu.cn).\nin the physical world compared to the perturbation. Physical\nChaoSheniswiththeSchoolofCyberScienceandEngineering,MOEKey\nLaboratory for Intelligent Networks and Network Security, Xi’an Jiaotong patch attacks first require training adversarial patches in\nUniversity,Xi’an710049,China(e-mail:chaoshen@mail.xjtu.edu.cn). the digital domain before printing them, which necessitates\nThis article has supplementary downloadable material available at\neffectively and realistically rendering the patch onto images\nhttps://doi.org/10.1109/TIP.2025.3526056,providedbytheauthors.\nDigitalObjectIdentifier10.1109/TIP.2025.3526056 in the digital space. However, vanilla 2D patch rendering\n1941-0042©2025IEEE.Allrightsreserved,includingrightsfortextanddatamining,andtrainingofartificialintelligence\nandsimilartechnologies.Personaluseispermitted,butrepublication/redistributionrequiresIEEEpermission.\nSeehttps://www.ieee.org/publications/rights/index.htmlformoreinformation.\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply."
      },
      {
        "page": 2,
        "content": "WANGetal.:PHYSICALLYREALIZABLEADVERSARIALCREATINGATTACKAGAINSTBEVSPACE3DOBJECTDETECTION 539\napplied to 3D detection scenarios faces the following issues,\nthereby limiting their physical feasibility. Firstly, physical\nperspective transformation—image data are captured from\na perspective view, and the image region covered by a\nreal patch in 3D space can appear at various distances,\nangles, and viewpoints, where the patch region cannot be\nsimulated simply through a linear transformation and pasted\non images. Secondly, lacking the depth information in 3D\nspace—attackers cannot determine the spoofing location of\nthe patch in 3D space given only the image inputs, making\nit difficult to reproduce in the physical world.\nTo perform the physically realizable adversarial attack\nagainst visual 3D detectors, Zhu et al. [18] propose a 3D\nconsistent patch by pasting it to the side of a target vehicle,\nallowing it to evade the detection process from 3D detectors.\nHowever, their attack only considers learning an adversarial\npatch for a few continuous frames or overlaps with multiple\ncameras. So, the learned patch is not universal and hard to\ngeneralizetootherscenarios.Tomitigatethisproblem,Lietal.\n[19] propose Adv3D upon NeRF techniques to learn an Fig.1. Ourproposedadversarialposterattack.Bypastingthelearnedposter\nadversarialvehiclewithcamouflagetexture,sothatthecrafted ontheroadsurface,thedetectorswillperceivea‘ghost’objectattheposter\nlocation.\nvehicle together with surrounding targets can be hidden.\nDifferent from hiding an existing target, this paper investi-\nwith other foreground objects and is easily overlooked\ngates how to create a ‘ghost’ object at the desired location in\nby the driver. This further increases the safety risks of\nthe3Dscenethroughthepatchmanner.Thecraftedadversary\nautonomous driving.\nmust be universal so that it can generalize to the unseen\n2) We propose I3DA and GTMO techniques to build\nscenesandkeepattackefficacyacrossawiderangeofviewing\nthe attack pipeline and optimize the spoofing poster.\nanglesanddistances.Toachievethisgoal,weproposeanovel\nSpecifically, I3DA serves as a differentiable rendering\nattack setting dubbed 3D adversarial poster attack to induce\nalgorithm to establish the pixel-wise mapping relation-\nprediction errors of the 3D detectors. Specifically, we learn\nship between the image patch area and 3D space poster,\na carefully crafted adversarial patch with predefined physical\nwhich is realized by solving the spatial position con-\nsizeandresolution,andpasteittotheroadsurface,causingthe\nstraints. I3DA enables the gradients to propagate from\n3D detector to perceive a non-existent object. Here, we refer\nthe image to the 3D space poster and is network- and\nto this attack as the 3D poster attack (3D-PA) because it is\nparameter-free.TheGTMOisdevisedtoeliminateinter-\nplaced in 3D space and then captured by the cameras. Such\nference from scene objects on the adversarial objective,\nan attack can lead to severe driving consequences such as\nallowing the poster to learn more deceptive content.\nemergency brakes that may injure passengers. Learning such\n3) We report a thorough evaluation, including attacks on a\na poster mainly faces the following two challenges:\ntotalofsixvictimmodelscomprisingthreetypicalvisual\n1) How to render the 3D space poster onto the image and\nBEV 3D detectors, transferability, and defense strat-\nbackpropagate gradients?\negy, to comprehensively understand the attack effect.\n2) How to design effective optimization objectives to\nSpecifically, real-world experiments with printed poster\nensure the poster learns useful content in complex\nachieve an average attack success rate of more than\nautonomous driving scenarios?\n90% within 12m. The learned posters have cross-model\nIn this paper, we address the above issues by proposing transferability and can be effectively defended through\nan image-3D applying algorithm (I3DA) and a ground-truth adversarial augmentation.\nmaskedoptimizationstrategy(GTMO).Extensiveexperiments Theremainderofthispaperisorganizedasfollows.Wefirst\nincluding digital and physical-world attacks demonstrate the recall some related works including adversarial attacks on 2D\neffectiveness of 3D-PA. We detail the contributions of this images and 3D detectors in Section II. Then we elaborate the\nwork as follows: attack pipeline of 3D-PA in Section III. The attack results\n1) We introduce 3D-PA, the first instance-level creating in digital space and the physical world are presented in\nattack for visual 3D detectors, to spoof fake objects in Section IV and V respectively, followed by the defense strat-\nautonomous driving scenarios with physically printable egy in Section VI. Finally, Section VII concludes the paper.\nposters.Thelearnedadversaryisuniversal,whichmeans\nit can be applied to various road scenes and keep effi-\nII. RELATEDWORK\ncacy under viewpoint changes. Attackers can precisely\nA. Vision-Based BEV Space 3D Object Detection\ncontrolthe positionof thespoofed objectby pastingthe\nposter in the corresponding region. Since the poster is The image inputs are typically captured from a perspec-\ninherently 2D and lacks thickness, it does not interact tive view, and the key to vision-based 3D detection lies in\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply."
      },
      {
        "page": 3,
        "content": "540 IEEETRANSACTIONSONIMAGEPROCESSING,VOL.34,2025\nhow to use image features to construct BEV features in 3D object on top of a vehicle so that the entire host vehicle can\nspace. According to the view transformation process, current be hidden. The following works demonstrate the feasibility\nadvanced works can be divided into depth-based methods ofattackingtheLiDAR-camerafusion-baseddetectionsystem\nand network-based methods. Depth-based methods explicitly using a 3D-printed object [41] or simply attacking the camera\nestimate a depth distribution and a context feature for each modality [42]. The works above cause FN predictions from\nimage feature pixel and then lift 2D features to the 3D space the detector so that the real objects can be hidden. Another\naccording to the camera’s intrinsic and extrinsic parameters. set of works achieves the goal of creating fake objects [43]\nFinally, the BEV feature for each position is aggregated from and hiding real ones [44] by strategically injecting fake laser\nthose lifted features falling into the same BEV grid. This points into the target LiDAR sensor [45]. Although these\nparadigmwasfirstproposedbyLSS[20],andBEVDet[21]is attackshaveshownpromisingresults,theyallrequirecomplex\nthe first to apply it to the 3D detection task. BEVDet4D [22] implementation conditions such as 3D printing of adversarial\nand BEVDepth [23] further boost this pipeline by introducing objects or sophisticated electro-optical emission and control\ntemporal cues and explicit depth supervision. The network- modules. In this work, we propose a more accessible and\nbasedmethodsutilizeatop-downstrategybyfirstconstructing feasible attack method against vision-based 3D detectors by\nthe predefined BEV queries and then searching each BEV simply printing and pasting the learned poster on the road\ncontext from the multi-view image features by transformer surface thus leading to significant detection errors.\narchitectures [57], [58]. Among them, Tesla first leverages\ncross-attention between BEV queries and image features\nIII. METHOD\nto perform view transformation. BEVFormer [24] further\naggregates the history BEV information and uses deformable Inthissection,weintroducetheattackpipelineforlearning\nattention to reduce the computational budgets. theadversarialposter,whichcanbepastedontheroadsurface\nand induces 3D detectors to perceive a non-existent object\nB. Physical Adversarial Attack on 2D Image such as a car. Moreover, the learned poster content, appearing\nas seemingly meaningless graffiti to the human eyes, can\nThe adversarial attacks on 2D images have been well\neffectively mislead the deep models. Generally, we argue the\nstudied and more and more works are devoted to designing\ncraftedpostertobe(1)universal—theposteriseffectiveacross\nphysical space attacks. A popular topic is to use adversarial\ndifferent road scenes, which is scene-agnostic; (2) robust to\npatches to mislead a well-trained person detector to make\nviewing conditions—the poster can withstand observations\nfalse negative prediction results, which can be realized by\nfrom different views and distances; (3) transferable—the\nprinted cardboard [27], [28], cloak [30], [31], and portable\nlearnedposterexhibitsacertaindegreeoftransferabilityacross\nmonitor[29].Toinducemodelfailureintrafficroadscenarios,\ndifferent models and parameter initialization.\nmany efforts have been made to mislead road sign classifiers\nIn the training phase, we learn the poster on the training\nby physical perturbations [32], adversarial natural styles [33],\nscenes following Expectation of Transformation (EOT) [51],\nor simply use a laser beam [35]. Sato et al. [34] propose\n[52], where we iteratively render the poster to each frame and\ndirty road patches to effectively attack the automated lane\nupdatethecontent.Therefore,themethodsectionisorganized\ncenteringsystems.Aseriesofworks[46],[47],[48],[49],[50]\nto answer: where to place the poster? how to differentiably\nare proposed to learn adversarial camouflage applying on 3D\nrender the poster onto the image? and how to optimize it\nobject surfaces so that the target object can evade detection.\neffectively?\nWhile this approach demonstrates the feasibility of physical\nimplementation, spray-painting large areas on vehicles for\ncamouflage is not easy. Therefore, most of the works are A. Overall\nlimited to rendering camouflaged vehicles in synthetic scenes\nThe attack starts from a predefined poster with fixed phys-\nfor experimentation, and they only target 2D detectors.\nical size h × w meters, digital resolution H × W pixels,\np p\nand expected object size O ×O ×O meters outputted by\nx y z\nC. Adversarial Attack on 3D Object Detection\nthe3Ddetector.Formally,thepostercanberepresentedbythe\nAttacking 3D object detection models is more challenging learnableparameters P ∈[0,255]Hp×Wp×3 andeachpixelhas\nas the targets appear in 3D space and exhibit more poses and a real physical size h m× w m. The poster is pasted on the\nHp Wp\nviewing angles, thus requiring the adversarial patterns to be road surface in 3D space thus requiring a rendering function\nrobust enough to cope with the various observations. Previous to project the poster to the 2D image plane. Then, multi-view\nworksproposetoattackvision3Ddetectorsbysimplyextend- images with adversarial patterns are fed into the model for\ning 2D image-based perturbations [36] and patches [37], [38], inference, and the content of the poster is optimized under\nwhich is difficult to implement in the physical world. Apart the supervision of adversarial loss. To enable the gradient\nfromattackingvision3Ddetectors,Caoetal.[39]proposean to propagate from the corresponding image region to the\nadversarial object that can evade the detection process from a 3D space poster, the entire process, especially the applying\nLiDAR 3D detector by optimizing a group of learnable object function, must be differentiable. The overall training pipeline\nvertices. Tu et al. [40] further places the learned adversarial is shown in Fig. 2.\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply."
      },
      {
        "page": 4,
        "content": "WANGetal.:PHYSICALLYREALIZABLEADVERSARIALCREATINGATTACKAGAINSTBEVSPACE3DOBJECTDETECTION 541\nFig.2. Theattackpipelineofouradversarialposter.Wepredefinearandomlyinitializedposterwithafixedphysicalsizeanddigitalresolution.Theposter\nisplacedontheroadsurfacewithoutoverlappingwithsceneobjects.Thenweprojectthepostercornerstotheimageplaneandfindthecorrespondingimage\nposterarea.Byestablishingthecorrespondencebetweeneachpixelintheimageposterareawiththe3Dspaceposter,wecanapplythepostertotheimage\nwhilebackpropagatingthegradients.\nB. Sampling the Poster Position in 3D Space\nWeplacetheposterinthe3Dspacesatisfying(1)theposter\nmust be placed on the road surface to ensure reproducibility\nin the physical scenario. (2) the expected bounding box of the\nposter does not overlap with existing 3D objects in the scene.\nFormally, let x ∈ RN×3×H×W denote a frame of image\ninputswithcorrespondinggroundtruthboundingboxes B ∈\ngt\nRm×9, where N is the number of cameras, m is the number\nof objects and each of which includes center location, object\nsize, yaw angle, and velocity. We adopt the following designs\nto ensure the placement of the poster on the road. First,\nwe only place the poster within the view of the front and\nbackcameras,asthesetwocamerasaremostlyfacingtheroad. Fig.3. Illustrationofthespoofingregion.Weonlyplacetheposterwithina\nSecond,fortheposterpositioninthe X−Y plane,welimitthe fixedanglerangetowardsthecenterdirectionofthefrontandbackcameras.\nposter within a fixed angle range towards the target camera’s\ncenter direction, which is illustrated in Fig. 3. Specifically,\nwe randomly select spoof distance d ∈ (d ,d ) and 3D LiDAR coordinates:\nmin max\nposter’s location angle θ ∈ (θ − (cid:49)θ,θ + (cid:49)θ), where θ w h\nis the camera’s center directionc (θ\nc\n= π 2c and −π\n2\nfor fronc t p 1init =[ 2, 2,0]T,\nand back cameras). Then, the poster center in X−Y plane is w h\np =[ ,− ,0]T\n(d·cosθ,d·sinθ).Third,fortheheightoftheposter,weselect 2init\n2 2\nthe bottom height of the nearest ground truth box in X −Y w h\np =[− ,− ,0]T\nplane as the poster height z p. We further filter out the case 3init 2 2\nwhen z is higher than a height threshold z to prevent the w h\np thr p =[− , ,0]T (2)\nposter from ‘flying’ in the air. The poster’s yaw angle θ y is 4init 2 2\nrandomly selected from (−π,π) and the expected velocity of  cosθ −sinθ 0\ny y\nthe spoofed object is set to 0. Finally, we get the expected p\ni\n= sinθ\ny\ncosθ\ny\n0× p\niinit\n+[dcosθ,dsinθ,z p]T\nobject box B : 0 0 1\nspoof\nB =[c ,c ,c ,length,width,height,yaw,v ,v ] (3)\nspoof x y z x y\nO\n=[dcosθ,dsinθ,z + z,O ,O ,O ,θ ,0,0] (1)\np 2 x y z y C. Image-3D Applying Algorithm\nTo avoid overlap with existing scene objects, we repeat the The image-3D applying algorithm serves as a differentiable\nabove process until the maximum Intersection over Union renderingfunctiontorenderthe3Dspaceposterontheimage,\n(IOU) between B and B is 0. which consists of two steps: First, find the poster area on\nspoof gt\nAfter getting the poster position in 3D space with 2D images; Second, for each pixel in the image poster area,\nthe expected bounding box B , we can calculate reversely find its position on the 3D space poster and assign\nspoof\nthe four poster corners {p ,p ,p ,p |p ∈ R3} in the pixel value. We now detail this process.\n1 2 3 4 i\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply."
      },
      {
        "page": 5,
        "content": "542 IEEETRANSACTIONSONIMAGEPROCESSING,VOL.34,2025\nGiven the camera intrinsic and extrinsic parameters, we get Algorithm1PseudocodeDiagramofTrainingtheAdversarial\nthe projection matrix M ∈ R4×4 and then project the 3D Poster\nL2C\ncorner p =(x ,y ,z ) to the image points p˜ ∈R2:\ni i i i i\n[u,v,z,1]T = M ×[x ,y ,z ,1]T\nL2C i i i\np˜ =[u/z,v/z] (4)\ni\nAccording to the Eq. 4, we get four poster corners\n{p˜ ,p˜ ,p˜ ,p˜ } in image plane. The image pixels falling into\n1 2 3 4\nthe quadrangle region defined by {p˜ ,p˜ ,p˜ ,p˜ } constitute\n1 2 3 4\nthe image poster area.\nFor each pixel in the image poster area, we need to find its\nposition in the 3D space for (1) applying the poster onto the\nimage according to the pixel-wise mapping relationship, and\n(2) propagating the gradients from the image to the poster\nto update its content. Specifically, for an image pixel p˜ =\ni\n(u ,v ) inside the poster area, there is a ray passing through\ni i\nthe camera center, and every point on this ray is mapped to\np˜ due to the lack of depth information. However, we have\ni\nthe prior that the transformed 3D point lies on the poster,\nwhich means its Z−coordinate values are equal to the poster\nheight. With this information, we can calculate the accurate\n3D position of each pixel in the image poster area. Formally,\nlet p =(x ,y ,z ) be the transformed 3D point and c be the\ni i i i i\ndepth for the image pixel p˜ satisfying:\ni\nM −1×[u c ,v c ,c ,1]T =[x ,y ,z ,1]T\nL2C i i i i i i i i\nz =z (5)\ni p\nBy jointly solving the Eq. 5, we can obtain the coordinates of\np and the pixel color for p˜ can be interpolated across the\ni i\nneighbor values around p on the poster. Notably, the entire\ni\nprocess of applying the poster to the image is differentiable,\nallowingustooptimizethecontentoftheposterthroughstan-\ndard backpropagation and the detailed optimization strategy\nwill be discussed in the next section.\nBased on that, we propose a ground-truth masked opti-\nD. Effective Optimization for the 3D Space Poster\nmization strategy to directly optimize the poster to avoid\nGenerally, the optimization goal consists of the adversarial interferencefromexistingsceneobjects.Specifically,wemask\npart and total variation of the poster. The adversarial loss is outtheimageareasofallsceneobjectsaccordingtotheanno-\nused to mislead the model to output corresponding detection tations B , ensuring that the training images only contain the\ngt\nresults at the poster location, while the total variation loss spoofedobjectsintroducedbyourposter.Thustheadversarial\nmakes sure that the generated poster has a smooth color loss becomes:\ntransition and prevents noisy content.\n1) Ground-TruthMaskedOptimization: Formally,let F(cid:50) be L adv = J(F(cid:50)(A(M(x,B gt),P)),B spoof) (6)\nthe3Dobjectdetectorparameterizedby(cid:50);x isthemulti-view\nwhere M is the mask operation.\nimage inputs with corresponding ground truth boxes B for\ngt 2) TotalVariationLossofthePoster: Tomakethegenerated\neachframe; J(F(cid:50)(x),B gt)betheoriginaldetectionlossfunc-\nposter appear more natural, we utilize the total variation loss\ntion; A(x,P) is the applying function that applies the poster following [53] to keep it smoother and consistent:\nP to the image x. An intuitive way to build the adversarial\nloss is to add the spoofed bounding box B spoof to the ground L tv\n=Xq\n(q i,j −q i+1,j)2+(q i,j −q i,j+1)2 (7)\ntruths and then minimize the J(F(cid:50)(A(x,P)),B\ngt\n∪ B spoof).\ni,j\nHowever, the ground truth objects in the image will affect\nthe poster learning in such an optimization objective. First, where q i,j is a color value in poster P. Finally, the overall\nloss function is:\nthe overall adversarial loss includes contributions from real\ntargets, resulting in the spoofed part indirectly acting on the L = L adv +λL tv (8)\nposter. Second, the optimization objective introduces certain\nbiases because the current form still requires improving the To ensure the poster is robust to viewing changes, we ran-\ndetection accuracy of existing targets. domly sample the poster position with different yaw angles\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply."
      },
      {
        "page": 6,
        "content": "WANGetal.:PHYSICALLYREALIZABLEADVERSARIALCREATINGATTACKAGAINSTBEVSPACE3DOBJECTDETECTION 543\nwithin the spoofing region as illustrated in Fig. 3 during TABLEI\ntraining. Moreover, we apply random brightness and contrast DETECTIONPERFORMANCEOFALLVICTIM\ntransformations and random noise on the poster to further\nDETECTORSONTHENUSCENESVALSET\nimprove the robustness. The final poster is generated through\ntheexpectationovertransformationandscenedistribution.The\nentire learning process is summarized in Algorithm 1.\nIV. DIGITALATTACKEXPERIMENT\nA. Experimental Setup\n1) Dataset: We conduct the digital attack experiments\non the nuScenes dataset [59], which contains 28130 and During training, we use Adam with an initial learning rate\n6019 keyframes for training and validation. Each frame con- of 0.01 to optimize the poster for 8 epochs and decay the\ntains image data from six cameras providing a 360◦ FOV learning rate by half after each epoch. The weight factor λ is\naround the self-vehicle with corresponding 3D object annota- set to 10.\ntions. During training the poster, we apply it to the front and\nbackcamerasofeachframeinthetrainsettoiterativelyupdate\nB. Effectiveness of the Learned Poster\nthe poster content supervised by Eq. 8. During the evaluation,\nwe choose a random subset of 1000 frames in the validation To the best of our knowledge, we are the first to perform\nset as benign scenes to be attacked. the universal instance-level creating attack against visual 3D\n2) Evaluation Metrics: The attack success rate (ASR) is detectors, where we can spoof the fake object at the desired\nutilized to evaluate the creating attack, which measures the position with an adversarial poster. To verify the effectiveness\npercentage of successfully detected poster objects out of the of our methods and exclude the influence of ground-truth\ntotal spoofing objects. We consider a spoofed poster suc- objects on the attack results, we further conduct the following\ncessfully detected if the output IoU with B is greater control experiments for comparison: (1) Benign, the original\nspoof\nthan {0.1,0.3,0.5,0.7}. Moreover, the BEV center distance scene without attacks, however, we fix the random seed and\nunder thresholds {0.5m,1.0m,1.5m,2.0m} is also adopted to recordthepositionsofspoofedobjectsinthefollowingexper-\ndetermine whether a spoofing is successful. The IoU metric iments so that we can calculate the number of original scene\ncomprehensively considers the position, size, and orientation objectsmiscalculatedasspoofedobjects.(2)Rand,arandomly\nof the predicted object, and a larger IoU threshold results in initialized poster. (3) Real, taking a real vehicle image as the\na stricter evaluation of the attack. The center distance metric poster. (4) Adv. Poster, the adversarial poster learned from\nonly considers the Euclidean distance between the prediction the corresponding detector by our methods. The Rand, Real,\nand the poster, with a smaller threshold indicating a stricter and Adv. Poster share the same physical size and resolution.\nevaluation. In fact, for our creating attacks, it is not crucial We showcase these four situations in Fig. 5.\nwhether the fake bounding box is completely aligned with We calculate the ASR for a total of 2000 spoofed objects\nthe poster. As long as the detector can be induced to make in 1000 scenes and the results under different evaluation\nFP predictions near the poster, it can pose a security threat. metrics are summarized in Table II. Firstly, from Benign\nTherefore, we prefer using the center distance metric. The experimentsweobservethatthepredictionsforexistingscene\nconfidence score for all detectors is set as 0.1. objects hardly overlap with the spoofed locations because we\n3) Implementation Details: The BEVFormer [24], deliberately avoided such situations when selecting the poster\nBEVDet [21], and BEVDet4D [22] are selected as positions. Specifically, less than 1% detected objects have an\nvictim 3D detectors, each equipped with ResNet50 [61] IoU greater than 0.1 with B , and only about 2% have a\nspoof\nand SwinTransformer-Tiny [62] as backbone networks, center distance less than 2 meters from the spoofed posters\nrespectively. We train all models following their official for all models. Secondly, the attack impact of a randomly\nsettings and the performance on nuScenes validation set is initialized poster is negligible, however, a real vehicle poster\ngiven in Table I. We set the spoofing category of the poster to can to some extent cause the detectors to make false positive\nthe most widely used Vehicle in this paper. The poster starts predictions. For example, the real vehicle poster can achieve\nfrom a randomly initialized patch with fixed physical size the highest 18.2%, 29.4%, and 39.3% ASRs (IoU= 0.1) for\n2m × 3m and digital resolution 400 × 600 pixels, resulting BEVFormer-SwinT, BEVDet-Res50, and BEVDet4D-Res50\nin each poster pixel having a real size of 0.5cm × 0.5cm. respectively, which indicates that existing vision-based 3D\nWe set the expected object size predicted by the detectors as object detectors may still make incorrect predictions when\n4m ×1.8m ×1.6m, which is the common size for a typical faced with a realistic object pattern. Thirdly, our 3D adver-\nvehicle. To ensure that the poster yields sufficient area on sarial posters consistently achieved a high ASR across all\nthe image to interfere with the detector, we place the poster models. Specifically, 73.9%, 85.2%, 79.0%, 67.6%, 80.5%,\nlocated 6-12 meters from the self-vehicle and the angle range and 81.3% ASRs when IoU = 0.1, and 66.5%, 76.2%,\nlimitation (cid:49)θ = 15◦. Attacking the detector’s predictions 74.3%, 65.2%, 75.3%, and 76.9% ASRs when CD = 2.0m\nwithin this region is more likely to induce serious driving for BEVFormer-Res50, -SwinT, BEVDet-Res50, -SwinT and\nconsequences, such as emergency braking. The poster height BEVDet4D-Res50, -SwinT respectively. Such results demon-\nthreshold z is empirically set to −1m. strate the effectiveness of our proposed adversarial attacks\nthr\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply."
      },
      {
        "page": 7,
        "content": "544 IEEETRANSACTIONSONIMAGEPROCESSING,VOL.34,2025\nTABLEII\nATTACKSUCCESSRATE(%)FORVISION-BASED3DDETECTORSWITHDIFFERENTBACKBONES\nTABLEIII\nTRANSFERABILITYOFTHEPOSTERSACROSSDIFFERENTMODELS.THEATTACKSUCCESSRATES(%)UNDERIoU =0.1AREPRESENTED\nFig.4. Visualizationresultsofour3Dposterattackindigitalspace.The3Ddetectorperceivesa‘ghost’objectintheposterlocation.\nand reveal the adversarial vulnerabilities of existing visual its precise position in 3D space and some other fine-grained\n3D detectors. However, the adversarial poster is essentially characteristics, such as yaw angle. So, under stricter detection\n2D without thickness and lacks corresponding 3D geometric metrics like IoU = 0.7 and CD = 0.5m, ASR will decrease\ninformation, making it difficult for the detector to estimate significantly. Moreover, compared to the real vehicle pictures,\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply."
      },
      {
        "page": 8,
        "content": "WANGetal.:PHYSICALLYREALIZABLEADVERSARIALCREATINGATTACKAGAINSTBEVSPACE3DOBJECTDETECTION 545\nTABLEIV\nTHEATTACKSUCCESSRATES(%)WHENATTACKINGKITTIDATASET\nin each frame is much smaller than the ground truth objects,\nthe adversarial part we aim for may be overwhelmed by the\nirrelevant foreground loss, leading to indirect and inefficient\nFig.5. Weshowfourtypesofattackedscenesonasingleframe.Theattack\noptimization of the poster. By eliminating the side effects of\npatternsandcorrespondingspoofingboundingboxesarepresented.\nsceneobjectsontheoptimizationobjective,theGTMOfurther\nboosts the ASRs by up to 20%. We also visualize some of\nthelearnedposterhasmoreabstractcontent,resemblingcasual the learned posters w/ and w/o masking GT as shown in\ndoodles(visualizationresultsareshowninFig.4),soitishard Fig. 6(a), (j), (c), and (k). It is interesting to find that the\nfor the human eyes to recognize it as a foreground object. posters generated by the masking strategy exhibit more vivid\nand discriminative patterns.\n2) Physical Size: We fix the pixel density (0.5cm×0.5cm)\nC. Transferability\nand train the poster with different physical sizes on BEVDet.\nWe investigate whether the learned posters transfer to TheresultsareshowninFig.7(c).Weobservethatthephysical\ndifferent detectors, backbone networks, and parameter initial- size of the poster is the primary factor influencing the ASR.\nization. We further train three posters learned on the retrained Alargerposterexhibitsmorepixelareaintheimageandthus\nBEVFormer, BEVDet, and BEVDet4D with different random has stronger attack capabilities. However, larger posters are\nseeds, which we denote as BF-Res50-R, BD-Res50-R, and more noticeable to humans and may be challenging to print,\n4D-Res50-Rrespectively.Thetransferresultsonatotalofnine so, attackers need to make a trade-off between the physical\nmodelsarepresentedinTableIIIandallpostersarevisualized size and attack capability.\ninFig.6.Theposterislearnedfromthecorrespondingsource 3) Digital Resolution: We further fix the physical size as\nmodel and the attacks are performed on the target model. 2m×3m andlearntheposterwithdifferentdigitalresolutions.\n“BF”, “BD”, and “4D” denote the BEVFormer, BEVDet, and As shown in Fig. 7(d), the ASR is less sensitive to the pixel\nBEVDet4D respectively. density, and even a poster with only 100×150 pixels exhibits\nFirstly, we observe that all posters can effectively deceive nearlythesameadversarialperformancewithlargerresolution\nthe target detectors even attacking models that they have ones. However, when the poster resolution is set too high,\nnot been trained on, demonstrating a certain transferability. the ASR drops significantly. We interpret this as insufficient\nThis enables attackers to perform practical black-box attacks training on high-resolution posters within the same training\nwithout access to the target model information. Secondly, epoch.\nwe empirically find that the posters learned from BEVFormer\nexhibit stronger transferability. We speculate that this is\nE. Discussion\nbecause BEVFormer constructs BEV features using a query-\n1) Attacking KITTI Dataset: The KITTI dataset [60] only\nbased approach, where each BEV query focuses more on the\nannotates 3D objects in front of the self-vehicle, specifically\noverallsemanticsoftheimageposterregionduringthequery-\nthosecapturedbythefront-viewcamera.WetraintheBEVDet\ning process. As a result, the learned poster exhibits smoother\nonthetrainsplit(3712samples)andfurtherlearntheadversar-\ndetailsandmorecompactoverallsemanticinformationforthe\nialposter.Theattackresultsonvalidationsplit(3769samples)\ndetector.\nare given in Table IV. Our attack algorithm can successfully\nextend to the KITTI dataset, achieving 64.1% ASR under\nD. Ablation Study CD = 2.0m, which demonstrates the generalizability of our\n1) Ground-Truth Masked Optimization Strategy: We study I3DAandGTMOalgorithmsacrossdifferentdatasets(scenes).\ntheeffectivenessoftheproposedGTMOandresultsareshown The visualization results are also provided in Fig. 8.\nin Fig. 7(a) and (b). It can be seen that masking GT during 2) AttackingLiDAR-CameraFusionModel: Weinvestigate\ntraininghelpsthelearnedpostermoredeceptivetothedetector theattackeffectontheLiDAR-camerafusion-based3Ddetec-\nbothforBEVDetandBEVFormer.Withoutthemaskingstrat- tionandmakeacasestudyonBEVFusion[56].Followingthe\negy, minimizing the J(F(cid:50)(A(x,P)),B\ngt\n∪ B spoof) consists GTMOstrategy,wemasktheforegroundobjectregionsinthe\nof two parts: First, updating the poster to reduce the loss image along with the corresponding point cloud within the\nvalue on ground truth objects, which need to improve the 3D bounding boxes, and then train the adversarial posters on\ndetection quality of existing scene objects. Second, updating BEVFusion. The results are shown in Table V.\nthepostertocausethedetectortoidentifythespoofedobjects First, attacking only the camera modality does not\nintroducedbyourposters.Sincethenumberofspoofedobjects effectively spoof fake objects for the fusion model. Given\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply."
      },
      {
        "page": 9,
        "content": "546 IEEETRANSACTIONSONIMAGEPROCESSING,VOL.34,2025\nFig.6. Visualizationofthelearnedposters.\nFig.7. Ablationexperimentsforoptimizationstrategy,physicalsize,anddigitalresolution.\nTABLEV\nTHE ATTACK SUCCESS RATES (%) WHEN ATTACKING DIFFERENT\nMODALITIESONBEVFUSION\nperformance. Previous works [43], [44], [45] have demon-\nstrated the feasibility of injecting laser points into LiDAR in\nthephysicalworld.Withaplatformthatincludesphotodiodes,\nlaser diodes, and control circuits, it is possible to inject up\nto thousands of laser points into the target scene. In our\nexperiments, when we randomly inject a small number of\nFig.8. VisualizationofcreatingattacksonKITTIdataset.\nlaser points (no more than 20, which is a relatively loose\nbudget)nearthecenteroftheadversarialposter,thedeception\nperformance can be significantly improved compared to the\nthe fact that the LiDAR modality contributes more to the single-modality attack. Even though the positions of these\nfusion model for the 3D object detection task, we find that laser points are not optimized, they can effectively guide the\nmanipulatingonlytheimagedatadoesnotresultinaneffective fusionmodel’sattentiontotheposterregion,leadingtoafalse\nattack (ASR less than 5%). Since our poster is essentially a positive prediction.\n2D plane and placed on the road surface, it does not affect 3) Attack Range: When training the posters, we only place\nthe point cloud data. Second, injecting a small number them within the front and back camera regions to ensure\nof laser points at the poster location, along with our that they are placed on the road with the highest probability,\nadversarial poster, can significantly enhance the attack whileplacingthemintheremainingcamerasmayoverlapwith\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply."
      },
      {
        "page": 10,
        "content": "WANGetal.:PHYSICALLYREALIZABLEADVERSARIALCREATINGATTACKAGAINSTBEVSPACE3DOBJECTDETECTION 547\nTABLEVI\nPHYSICALATTACKSUNDERDIFFERENTDISTANCESOFTHEPOSTER\nB. Physical Attack Result\nWe place the poster in the following two typical scenarios:\n1) Parking spaces on the side of the road, and 2) Directly\nin front of the ego-vehicle. The attack results are shown in\nFig. 10. Compared to the digital space attack, the physical\nFig.9. Attacksattheoverlappingregionbetweenmultiplecameras.\nattackmayfacemorecomplexfactors,suchaslightingcondi-\ntions,sensornoise,printingcolordifferences,posterwrinkles,\nthe non-road background. During testing, however, the attack and so on. However, our poster can effectively induce the\npositions are not restricted to these regions, for example, the detector to output false predictions in the physical world.\nattack can be carried out in other cameras or the overlapping Such attacks could potentially cause autonomous vehicles to\nregion between multiple cameras. Some visualization results brake suddenly, posing a safety risk to passengers. Moreover,\nare shown in Fig. 9. When rendering, we consider the poster we evaluate the poster attack under 1) indoor scenes, and\nis captured by a specific camera when four projected corner 2) large area distortion. Fig. 12(a) shows that the poster can\npoints have positive depth values, and at least one projected still keep attack efficacy in indoor environments, where there\ncorner falls into the image area. Therefore, when the poster is are significant background differences compared to outdoor\ncaptured by multiple cameras, we can still render it onto each scenes. This also reflects the insensitivity of the learned\nimageandperformeffectiveattacks.Infact,whentheposteris poster to the attack scenarios and demonstrates its universal\nfullycapturedbyacameraandweobtainfourcornerpositions characteristic. In the real world, the poster may experience\nin the image, the subsequent applying process is equivalent to varying degrees of occlusion or distortion. We simulate large\nusing a perspective transformation to render the poster onto area distortion in the physical world and demonstrate the\nthe image. robustness of our poster in Fig. 12(b). The generality and\nrobustness are achieved by optimizing the poster across the\nentire dataset and sampling positions (expectation over scene\nV. PHYSICALWORLDATTACK\ndistribution and transformation).\nWe introduce the real-world attack experiments in this\nsection. Unlike physical attacks against 2D object detectors, C. Attack Distance\nthemodelstrainedoncommondatasetscanbedirectlyusedin\nWe place the poster at different distances from the ego-\ncustomscenes,e.g.apersondetector,sothe2Dpatchattackis\nvehicle. At each distance, the yaw angle of the poster ranges\neasy to verify in real-world scenarios. Visual 3D detectors are continuouslyfrom−45◦ to45◦.TableVIprovidesquantitative\ntypicallysceneandcamera-dependent.Forexample,thedepth\nresults of the physical attacks. It can be seen that when the\nestimation process is camera intrinsic and extrinsic relative.\nspoofdistanceisbetween5to12meters,theASRapproaches\nIf inferring the custom data with a 3D detector pre-trained\n100%, demonstrating the significant threat of our adversarial\non the common dataset, the detector will produce completely\nattack in the real world. We attribute the differences in ASR\nwrong detection results. Therefore, we construct a simple\nbetween the digital and physical domains to the possibility of\nLiDAR-cameraacquisitionsystemtocollectandannotatedata\nunreasonable sampling positions for the poster in the digital\nin our scenes, train the corresponding visual 3D detector, and\nsetting.Althoughwedeliberatelyavoidcollisionsbetweenthe\nuse it for subsequent physical attacks.\nposterandforegroundobjectsin3Dspace,thepostermaystill\noverlap with other objects in the 2D image and may appear\nA. Implementation Details in non-road locations. Because the poster is essentially a 2D\nplane, when the distance is too far, the number of pixels in\nWe build a simple LiDAR-camera acquisition system as\nthe poster area on the image will decrease rapidly, greatly\nshowninFig.11.TheLiDARsensorisonlyusedforannotat-\nreducing its deceptive ability. Moreover, when the distance\ning the 3D objects. We collect paired data from 60 scenes\nis too close, the camera may capture an incomplete poster,\nwith a total of more than 6000 frames and annotate them\nleading to missing key information and thus resulting in a\nevery 1s. Due to the sparsity of the 16-line LiDAR, we only\nfailure attack.\nannotate vehicle class within 30m in front of the ego-vehicle\n(100◦ FOV). We finetune the BEVDet inherited weight from\nD. Theoretical Analysis\nthe nuScenes dataset for 20 epochs with the input resolution\n448×960 pixels. Finally, we train a 2m×4m poster on the In general, our adversarial posters can provide the essential\ncustom detector and physically print it. visualfeaturesrequiredbythedetector,makingitrecognizable\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply."
      },
      {
        "page": 11,
        "content": "548 IEEETRANSACTIONSONIMAGEPROCESSING,VOL.34,2025\nFig.10. Posterattacksinthephysical-world.\nTABLEVII\nATTACKSUCCESSRATES(%)BEFOREANDAFTERAPPLYINGADVERSARIALAUGMENTATIONASDEFENSE\nand detectable by the model. These features are usually local model making an incorrect prediction. Take a classification\nandlow-levelsemantic,thusdifficulttorecognizebyhumans. model as an example: attackers can optimize a patch such\nHowever,thereasonwhythesevisualcontentscanbeadopted that any image attacked by it can be misclassified into a\nby detectors is that deep models have not yet learned general, fixed incorrect category [15]. Such a patch must contain\nhigh-level visual semantics from the natural data. the critical class features [54], [55], enabling it to dominate\nConsider constructing a universal patch P that is effective the model’s focus regardless of the original input. Interest-\nacross different images: ingly, these patches often consist of abstract, unrecognizable\nargminE x∼DJ(F(cid:50)(A(x,P)),y adv) (9) patterns to humans, which reveals that the model has not\nP learnedthegeneralvisualrepresentationfromthenaturaldata,\nDuring the optimization, each step updates the patch along instead, its decisions can be manipulated by these adversarial\nthe gradient direction which maximizes the probability of the patterns.\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply."
      },
      {
        "page": 12,
        "content": "WANGetal.:PHYSICALLYREALIZABLEADVERSARIALCREATINGATTACKAGAINSTBEVSPACE3DOBJECTDETECTION 549\nTABLEVIII\nATTACKSUCCESSRATES(%)ONTHEDEFENDEDMODEL\nIn conclusion, 1) Why does the creating attack work?\nThe poster can provide the necessary object features from\nthe perspective view; 2) Why are the posters difficult for\nhumans to recognize? Deep models have not yet learned\nhigh-level visual semantics, making these local and low-level\nvisual contents sufficient for the model; 3) Why does it still\nwork when the poster is deformed or even incomplete?\nThe object information contained in the poster is potentially\nredundant.\nVI. DEFENSE\nFig.11. Thebuilt3Ddataacquisitionsystem.\nWe investigate the defense method by finetuning the detec-\ntor with the adversarial poster for another 2 epochs (with\nCBGS [63]). From Table VII we observe that introduc-\ning adversarial augmentation can significantly defend against\nposterattacks,notonlytheseenposterduringtrainingbutalso\nthe unseen posters learned from other detectors. Specifically,\nthe ASRs of the models after adversarial augmentation are\nconsistently reduced to about 1% at IOU 0.1, which is even\nmuch lower than the attack effect of using real vehicle photos\nasposters.Thisgreatlyimprovestheadversarialrobustnessof\nthe model. We analyze the defense capability of the unseen\nposterscanbeattributedtothestrongtransferabilityofposters\nbetween different models. From Fig. 6(a)-(i), we can see that\nthere are many similar features among different posters, such\nas numerous bright spots under the overall black background.\nMoreover,were-trainanadversarialposteronthedefended\nFig. 12. Attack results in indoor scene and poster distortion. We crumple\nthepostertotestagainstphysicaldeformationsthatarenoteasilysimulated. model (after adversarial augmentation) and present the results\nin Table VIII. The poster learned from the defended model\ncan still spoof the fake object, but the ASR does drop.\nWe show a re-trained poster in Fig. 6(l), it exhibits sharper\ncolor changes locally and shows significant differences in\noverall style compared to the previous posters. This indicates\nthatthepostercanstilllearnusefuldeceptivecontenttargeting\nweaknesses in the detection model. We consider that future\nFig.13. Visualizationoftheposteratdifferentdistances. works could apply the bi-level loop of adversarial training to\nfurther enhance the model’s adversarial robustness.\nBack to our creating attack, the attack effectiveness of the\noptimized content even surpasses using real vehicle images as\nVII. CONCLUSION\nposters. This shows that after the loss of the 3D geometric In this paper, we investigate the vulnerability of current\nstructure, the object information inside the vehicle image will vision-based 3D object detectors and propose a universal\nbe greatly compressed, and the learned poster can provide and physically realizable adversarial poster attack capable of\nthe most sufficient class features under the perspective view. generating fake objects. Attackers can physically print the\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply."
      },
      {
        "page": 13,
        "content": "550 IEEETRANSACTIONSONIMAGEPROCESSING,VOL.34,2025\nposter and place it on the road surface to ‘create’ a non- [18] Z. Zhu et al., “Understanding the robustness of 3D object detection\nexistent object at the desired location. The poster is generated with bird’s-eye-view representations in autonomous driving,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2023,\nbytheproposedimage-3DapplyingalgorithmandGT-masked\npp.21600–21610.\noptimization strategy. Extensive experiments demonstrate the [19] L. Li, Q. Lian, and Y.-C. Chen, “Adv3D: Generating 3D adversarial\neffectivenessof3D-PAandshowthatitcansuccessfullymake examplesfor3DobjectdetectionindrivingscenarioswithNeRF,”2023,\narXiv:2309.01351.\nreal-world threats in the physical attack setting. We further\n[20] J.PhilionandS.Fidler,“Lift,splat,shoot:Encodingimagesfromarbi-\nshow that introducing adversaries for training is an effective\ntrarycamerarigsbyimplicitlyunprojectingto3D,”inProc.Eur.Conf.\ndefense method and can improve the adversarial robustness Comput.Vis.,Cham,Switzerland.Springer,Aug.2020,pp.194–210.\nof the model. Our future works will focus on: 1) improving [21] J. Huang, G. Huang, Z. Zhu, Y. Ye, and D. Du, “BEVDet: High-\nperformancemulti-camera3Dobjectdetectioninbird-eye-view,”2021,\nthe stealthiness and naturalness of adversarial posters, mak-\narXiv:2112.11790.\ning them harder to detect by humans; (2) further extending [22] J.HuangandG.Huang,“BEVDet4D:Exploittemporalcuesinmulti-\nadversarialposterstothehidingattack(FN),achievingamore camera3Dobjectdetection,”2022,arXiv:2203.17054.\ncomplete attack pipeline. We hope this work could promote [23] Y.Lietal.,“BEVDepth:Acquisitionofreliabledepthformulti-view3D\nobjectdetection,”inProc.AAAIConf.Artif.Intell.,Jun.2023,vol.37,\nrobust 3D perception and provide valuable insights for safety-\nno.2,pp.1477–1485.\ncritical applications like self-driving. [24] Z. Li et al., “Bevformer: Learning bird’s-eye-view representation from\nmulti-cameraimagesviaspatiotemporaltransformers,”inProc.17thEur.\nREFERENCES\nConf.Comput.Vis.(ECCV),2022,pp.1–18.\n[25] W. Bao, B. Xu, and Z. Chen, “MonoFENet: Monocular 3D object\n[1] J.Mao,S.Shi,X.Wang,andH.Li,“3Dobjectdetectionforautonomous detection with feature enhancement networks,” IEEE Trans. Image\ndriving:Acomprehensivesurvey,”Int.J.Comput.Vis.,vol.131,no.8, Process.,vol.29,pp.2753–2765,2020.\npp.1909–1963,Aug.2023. [26] C.Huang,T.He,H.Ren,W.Wang,B.Lin,andD.Cai,“OBMO:One\n[2] Y. Hu et al., “Planning-oriented autonomous driving,” in Proc. boundingboxmultipleobjectsformonocular3Dobjectdetection,”IEEE\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2023, Trans.ImageProcess.,vol.32,pp.6570–6581,2023.\npp.17853–17862. [27] S.Thys,W.V.Ranst,andT.Goedemé,“Foolingautomatedsurveillance\n[3] Q.Song,Q.Hu,C.Zhang,Y.Chen,andR.Huang,“Divideandconquer: cameras: Adversarial patches to attack person detection,” in Proc.\nImproving multi-camera 3D perception with 2D semantic-depth priors IEEE/CVFConf.Comput.Vis.PatternRecognit.Workshops(CVPRW),\nand input-dependent queries,” IEEE Trans. Image Process., vol. 33, Jun.2019,pp.49–55.\npp.897–909,2024. [28] T.Kim,H.J.Lee,andY.M.Ro,“Map:Multispectraladversarialpatch\n[4] L. Peng, Z. Chen, Z. Fu, P. Liang, and E. Cheng, “BEVSegFormer: to attack person detection,” in Proc. IEEE Int. Conf. Acoust., Speech\nBird’s eye view semantic segmentation from arbitrary camera rigs,” in SignalProcess.(ICASSP),May2022,pp.4853–4857.\nProc. IEEE/CVF Winter Conf. Appl. Comput. Vis. (WACV), Jan. 2023,\n[29] Y.Wangetal.,“Towardsaphysical-worldadversarialpatchforblinding\npp.5935–5943.\nobjectdetectionmodels,”Inf.Sci.,vol.556,pp.459–471,May2021.\n[5] H. Li et al., “Delving into the devils of bird’s-eye-view perception: A\n[30] Z.Wu,S.-N.Lim,L.S.Davis,andT.Goldstein,“Makinganinvisibility\nreview,evaluationandrecipe,”IEEETrans.PatternAnal.Mach.Intell.,\ncloak:Realworldadversarialattacksonobjectdetectors,”inProc.Eur.\nvol.46,no.4,pp.2151–2170,Apr.2024.\nConf.Comput.Vis.,Cham,Switzerland.Springer,2020,pp.1–17.\n[6] Y. Ma et al., “Vision-centric BEV perception: A survey,” 2022,\n[31] X. Wei, Y. Huang, Y. Sun, and J. Yu, “Unified adversarial patch\narXiv:2208.02797.\nfor visible-infrared cross-modal attacks in the physical world,” IEEE\n[7] J. Wang, F. Li, Y. An, X. Zhang, and H. Sun, “Toward robust\nTrans. Pattern Anal. Mach. Intell., vol. 46, no. 4, pp.2348–2363,\nLiDAR-camera fusion in BEV space via mutual deformable attention\nApr.2024.\nand temporal aggregation,” IEEE Trans. Circuits Syst. Video Technol.,\n[32] K.Eykholtetal.,“Robustphysical-worldattacksondeeplearningvisual\nvol.34,no.7,pp.5753–5764,Jul.2024.\nclassification,”inProc.IEEE/CVFConf.Comput.Vis.PatternRecognit.,\n[8] J. Zheng, C. Lin, J. Sun, Z. Zhao, Q. Li, and C. Shen, “Physical 3D\nJun.2018,pp.1625–1634.\nadversarial attacks against monocular depth estimation in autonomous\ndriving,”2024,arXiv:2403.17301. [33] R.Duan,X.Ma,Y.Wang,J.Bailey,A.K.Qin,andY.Yang,“Adver-\nsarial camouflage: Hiding physical-world attacks with natural styles,”\n[9] C. Ma, N. Wang, Q. A. Chen, and C. Shen, “Slowtrack: Increasing\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2020,\nthe latency of camera-based perception in autonomous driving using\npp.1000–1008.\nadversarialexamples,”inProc.AAAIConf.Artif.Intell.,2024,vol.38,\nno.5,pp.4062–4070. [34] T.Sato,J.Shen,N.Wang,Y.Jia,X.Lin,andQ.A.Chen,“Dirtyroadcan\n[10] I.J.Goodfellow,J.Shlens,andC.Szegedy,“Explainingandharnessing attack:Securityofdeeplearningbasedautomatedlanecenteringunder\nadversarialexamples,”2014,arXiv:1412.6572. physical-world attack,” in Proc. 30th USENIX Secur. Symp. (USENIX\nSecur.),2021,pp.3309–3326.\n[11] X. Wei, Y. Guo, J. Yu, and B. Zhang, “Simultaneously optimizing\nperturbations and positions for black-box adversarial patch attacks,” [35] R.Duanetal.,“Adversariallaserbeam:Effectivephysical-worldattack\nIEEETrans.PatternAnal.Mach.Intell.,vol.45,no.7,pp.9041–9054, toDNNsinablink,”inProc.IEEEConf.Comput.Vis.PatternRecognit.,\nJul.2023. Jun.2021,pp.16062–16071.\n[12] S.-M.Moosavi-Dezfooli,A.Fawzi,O.Fawzi,andP.Frossard,“Univer- [36] W.Park,N.Liu,Q.A.Chen,andZ.M.Mao,“Sensoradversarialtraits:\nsaladversarialperturbations,”inProc.IEEEConf.Comput.Vis.Pattern Analyzingrobustnessof3Dobjectdetectionsensorfusionmodels,”in\nRecognit.(CVPR),Jul.2017,pp.1765–1773. Proc.IEEEInt.Conf.ImageProcess.(ICIP),Sep.2021,pp.484–488.\n[13] Z. Wei et al., “Towards transferable adversarial attacks on image [37] J.Zhang,Y.Lou,J.Wang,K.Wu,K.Lu,andX.Jia,“Evaluatingadver-\nand video transformers,” IEEE Trans. Image Process., vol. 32, sarial attacks on driving safety in vision-based autonomous vehicles,”\npp.6346–6358,2023. IEEEInternetThingsJ.,vol.9,no.5,pp.3443–3456,Mar.2021.\n[14] D.Wang,W.Yao,T.Jiang,andX.Chen,“Improvingtransferabilityof [38] S. Xie, Z. Li, Z. Wang, and C. Xie, “On the adversarial robustness of\nuniversaladversarialperturbationwithfeaturedisruption,”IEEETrans. camera-based3Dobjectdetection,”2023,arXiv:2301.10766.\nImageProcess.,vol.33,pp.722–737,2024. [39] Y. Cao et al., “Adversarial objects against LiDAR-based autonomous\n[15] T.B.Brown,D.Mané,A.Roy,M.Abadi,andJ.Gilmer,“Adversarial drivingsystems,”2019,arXiv:1907.05418.\npatch,”2017,arXiv:1712.09665. [40] J.Tuetal.,“PhysicallyrealizableadversarialexamplesforLiDARobject\n[16] J.Wang,A.Liu,X.Bai,andX.Liu,“Universaladversarialpatchattack detection,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\nfor automatic checkout using perceptual and attentional bias,” IEEE (CVPR),Jun.2020,pp.13716–13725.\nTrans.ImageProcess.,vol.31,pp.598–611,2022. [41] Y.Caoetal.,“InvisibleforbothcameraandLiDAR:Securityofmulti-\n[17] Y. Yu, H. J. Lee, H. Lee, and Y. M. Ro, “Defending person detection sensor fusion based perception in autonomous driving under physical-\nagainst adversarial patch attack by using universal defensive frame,” world attacks,” in Proc. IEEE Symp. Secur. Privacy (SP), May 2021,\nIEEETrans.ImageProcess.,vol.31,pp.6976–6990,2022. pp.176–194.\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply."
      },
      {
        "page": 14,
        "content": "WANGetal.:PHYSICALLYREALIZABLEADVERSARIALCREATINGATTACKAGAINSTBEVSPACE3DOBJECTDETECTION 551\n[42] Z.Chengetal.,“Fusionisnotenough:Singlemodalattacksonfusion JianWangreceivedtheB.S.degreeininformation\nmodelsfor3Dobjectdetection,”2023,arXiv:2304.14614. engineering from Xi’an Jiaotong University, Xi’an,\n[43] J.Wang,F.Li,X.Zhang,andH.Sun,“Adversarialobstaclegeneration China, in 2020, where he is currently pursuing the\nagainst LiDAR-based 3D object detection,” IEEE Trans. Multimedia, Ph.D. degree with the School of Information and\nvol.26,pp.2686–2699,2024. CommunicationsEngineering.Hisresearchinterests\n[44] Z. Jin, X. Ji, Y. Cheng, B. Yang, C. Yan, and W. Xu, “PLA- includeadversarialattack,3Dperception,andtrust-\nLiDAR:PhysicallaserattacksagainstLiDAR-based3Dobjectdetection worthyAI.\nin autonomous vehicle,” in Proc. IEEE Symp. Secur. Privacy (SP),\nMay2023,pp.1822–1839.\n[45] Y.Caoetal.,“AdversarialsensorattackonLiDAR-basedperceptionin\nautonomous driving,” in Proc. ACM SIGSAC Conf. Comput. Commun.\nSecur.,Nov.2019,pp.2267–2281.\n[46] N. Suryanto et al., “DTA: Physical camouflage attacks using differen-\ntiabletransformationnetwork,”inProc.IEEE/CVFConf.Comput.Vis.\nPatternRecognit.(CVPR),Jun.2022,pp.15305–15314. Fan Li (Senior Member, IEEE) received the B.S.\n[47] N.Suryantoetal.,“ACTIVE:Towardshighlytransferable3Dphysical and Ph.D. degrees from the School of Infor-\ncamouflageforuniversalandrobustvehicleevasion,”inProc.IEEE/CVF mation and Communications Engineering, Xi’an\nInt.Conf.Comput.Vis.(ICCV),Oct.2023,pp.4305–4314. JiaotongUniversity,Xi’an,China,in2003and2010,\n[48] D.Wangetal.,“FCA:Learninga3Dfull-coveragevehiclecamouflage respectively. From 2017 to 2018, he was a Visit-\nfor multi-view physical adversarial attack,” in Proc. AAAI Conf. Artif. ing Scholar with the Department of Electrical and\nIntell.,vol.36,2022,pp.2414–2422. Computer Engineering, University of California at\nSan Diego, San Diego. He is currently a Professor\n[49] J. Wang, A. Liu, Z. Yin, S. Liu, S. Tang, and X. Liu, “Dual attention\nwiththeSchoolofInformationandCommunications\nsuppressionattack:Generateadversarialcamouflageinphysicalworld,”\nEngineering,Xi’anJiaotongUniversity.Hisresearch\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR),\ninterestsincludemultimediasignalprocessing.\nJun.2021,pp.8565–8574.\n[50] Y. Zhang, H. Foroosh, P. David, and B. Gong, “CAMOU: Learn-\ning physical vehicle camouflages to adversarially attack detectors\nin the wild,” in Proc. Int. Conf. Learn. Represent., Sep. 2018,\npp.1–20.\n[51] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok, “Synthesizing Song Lv received the B.S. degree in information\nrobust adversarial examples,” in Proc. Int. Conf. Mach. Learn., 2018, engineering from Xi’an Jiaotong University, Xi’an,\npp.284–293. China, in 2022, where he is currently pursuing the\nM.S. degree with the School of Information and\n[52] M. Lee and Z. Kolter, “On physical adversarial patches for object\nCommunications Engineering. His research inter-\ndetection,”2019,arXiv:1906.11897.\nests include 3D point cloud processing and object\n[53] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, “Accessorize\ndetection.\nto a crime: Real and stealthy attacks on state-of-the-art face recogni-\ntion,” in Proc. ACM SIGSAC Conf. Comput. Commun. Secur., 2016,\npp.1528–1540.\n[54] A.Ilyas,S.Santurkar,D.Tsipras,L.Engstrom,B.Tran,andA.Ma¸dry,\n“Adversarial examples are not bugs, they are features,” in Proc. Adv.\nNeuralInf.Process.Syst.,2019,pp.1–12.\n[55] S. Kumano, H. Kera, and T. Yamasaki, “Theoretical understanding of\nlearningfromadversarialperturbations,”2024,arXiv:2402.10470.\nLijun He (Member, IEEE) received the B.S. and\n[56] T.Liangetal.,“BEVFusion:AsimpleandrobustLiDAR-camerafusion\nPh.D. degrees from the School of Information and\nframework,”inProc.Adv.NeuralInf.Process.Syst.,vol.35,Dec.2022,\nCommunications Engineering, Xi’an Jiaotong Uni-\npp.10421–10434.\nversity,Xi’an,China,in2008and2016,respectively.\n[57] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nSheiscurrentlyaProfessorwiththeSchoolofInfor-\nS.Zagoruyko,“End-to-endobjectdetectionwithtransformers,”inProc.\nmation and Communications Engineering, Xi’an\nEur.Conf.Comput.Vis.,2020,pp.213–229.\nJiaotong University. Her research interests include\n[58] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable videocommunicationandtransmission,videoanal-\nDETR:Deformabletransformersforend-to-endobjectdetection,”2020, ysis,processing,andcompressiontechniques.\narXiv:2010.04159.\n[59] H. Caesar et al., “nuScenes: A multimodal dataset for autonomous\ndriving,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n(CVPR),Jun.2020,pp.11621–11631.\n[60] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous\ndriving? The KITTI vision benchmark suite,” in Proc. IEEE Conf.\nComput.Vis.PatternRecognit.,Jun.2012,pp.3354–3361. Chao Shen (Senior Member, IEEE) received the\n[61] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for B.S. degree in automation and the Ph.D. degree\nimagerecognition,”inProc.IEEEConf.Comput.Vis.PatternRecognit. in control theory and control engineering from\n(CVPR),Jun.2016,pp.770–778. Xi’anJiaotongUniversity,China,in2007and2014,\nrespectively. He is currently a Professor with the\n[62] Z.Liuetal.,“Swintransformer:Hierarchicalvisiontransformerusing\nFaculty of Electronic and Information Engineering,\nshifted windows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),\nXi’anJiaotongUniversity.Hiscurrentresearchinter-\nOct.2021,pp.10012–10022.\nestsincludeAIsecurity,insider/intrusiondetection,\n[63] B. Zhu, Z. Jiang, X. Zhou, Z. Li, and G. Yu, “Class-balanced\nbehavioralbiometrics,andmeasurementandexper-\ngrouping and sampling for point cloud 3D object detection,” 2019,\nimentalmethodology.\narXiv:1908.09492.\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on October 29,2025 at 04:05:13 UTC from IEEE Xplore. Restrictions apply."
      }
    ]
  },
  "pdf_url": "/uploads/b945a0ceb179533ad9b7822098821cbd.pdf"
}