{
  "filename": "Protecting_Minorities_in_.pdf",
  "upload_time": "2025-12-11T14:00:11.425996",
  "data": {
    "full_text": "UnderreviewasaconferencepaperatICLR2025\n000 PROTECTING MINORITIES IN DIFFUSION MODELS\n001\n002 VIA CAPACITY ALLOCATION\n003\n004\nAnonymousauthors\n005\nPaperunderdouble-blindreview\n006\n007\n008\n009 ABSTRACT\n010\n011 Diffusionmodelshaveadvancedquicklyinimagegeneration. However,theirper-\n012 formancedeclinessignificantlyontheimbalanceddatacommonlyencounteredin\n013 real-world scenarios. Current research on imbalanced diffusion models focuses\n014 onimprovingtheobjectivefunctiontofacilitateknowledgetransferbetweenma-\n015 joritiesandminorities,therebyenhancingthegenerationofminoritysamples. In\nthispaper,wemakethefirstattempttoaddresstheimbalanceddatachallengesin\n016\ndiffusionmodelsfromtheperspectiveofmodelcapacity. Specifically,majorities\n017\noccupy most of the model capacity because of their larger representation, con-\n018\nsequently restricting the capacity available for minority classes. To tackle this\n019\nchallenge, we propose Protecting Minorities via Capacity ALLocation (CALL).\n020\nWe reserve capacity for minority expertise by low-rank decomposing the model\n021 parameters and allocate the corresponding knowledge to the reserved model ca-\n022 pacitythroughacapacityallocationlossfunction. Extensiveexperimentsdemon-\n023 stratethatourmethod,whichisorthogonaltoexistingmethods,consistentlyand\n024 significantlyimprovestherobustnessofdiffusionmodelsonimbalanceddata.\n025\n026\n027\n1 INTRODUCTION\n028\n029 Inrecentyears, diffusionmodelshavedemonstratedexceptionalpotentialandeffectivenessinim-\n030 age generation, leading to increasing adoption by both industry and individuals (Ho et al., 2020;\nSong et al., 2021b; Dhariwal & Nichol, 2021). Diffusion model-based products such as DALL-E\n031\n2 (Ramesh et al., 2022) and the open-source Stable Diffusion (SD) (Rombach et al., 2022) have\n032\ndrawn millions of users, with numbers continuing to rise. However, recent studies reveal that dif-\n033\nfusion models suffer from significant performance degradation when trained on class-imbalanced\n034\ndatasets (Qinetal.,2023;Zhangetal.,2024),whichisparticularlyconcerninggiventheprevalence\n035\noftheimbalancenatureinthereal-worldscenarios(Reed,2001;Zhangetal.,2023).\n036\nCurrentresearchonimbalancedlearningprimarilyfocusesonimprovingtherobustnessofdiscrim-\n037\ninative models (Buda et al., 2018; He & Garcia, 2009; Wang et al., 2021a; Menon et al., 2021;\n038\nCuietal.,2021)orgenerativeadversarialnetworks(GANs)(Rangwanietal.,2021;2022)toclass\n039\nimbalance. However, most of them cannot be directly applied to diffusion models due to the sig-\n040\nnificantlydifferentmodelstructuresandtrainingandinferenceprocesses. Forimbalanceddiffusion\n041\nmodels,existingeffortsattempttoenhancetherobustnesstoimbalanceddistributionsbyimproving\n042 the objective function. Class Balancing Diffusion Models (CBDM) (Qin et al., 2023) introduced\n043 a loss function regularizer that implicitly encourages generated images to follow a balanced prior\n044 distributionateachsamplingstep. Yanetal.(2024)designedacontrastivelearningregularization\n045 to enhance inter-class separability. Oriented Calibration (OC) (Zhang et al., 2024) enhanced the\n046 generationqualityofminoritiesthroughknowledgetransferbetweenmajoritiesandminorities.\n047\nInthispaper, whileexistingeffortshaveprimarilyfocusedontheobjectivefunction, weapproach\n048 the challenges of class-imbalanced diffusion models from a new perspective: model capacity. In\n049 scenarios with significant class imbalance, majority classes dominate most of the model capacity\n050 duetotheirlargerrepresentation,squeezingthecapacityavailableforminorityclasses.Asshownin\n051 Figure1(a),minorityclassesexperienceamorepronouncedchangeinlossbeforeandafterpruning\n052 thetrainedmodel. Thisbehaviorindicatesthatminorityclassesutilizelessofthemodelâ€™scapacity,\n053 making them more vulnerable to pruning. We aim to enhance the robustness of diffusion models\nagainstimbalanceddatabysafeguardingthemodelcapacityforminorities.\n1\n\nUnderreviewasaconferencepaperatICLR2025\n054\nToaddressthechallengeofmodelcapacityencroachment,weproposeanewmethodforimbalanced\n055 diffusionmodels: ProtectingMinoritiesviaCapacityALLocation(CALL).Ourcoreconceptisto\n056 allocatededicatedmodelcapacityforminorityexpertise,reservedinadvancetopreventencroach-\n057 mentbymajorities,therebysafeguardingthetrainingprocessofminoritysamples. Specifically,we\n058 firstdecomposethemodelparametersintotwopartsusinglow-ranktechniques:oneformajorityand\n059 generalknowledge, andtheotherreservedforminorityexpertise(Eq.(3)). Byintroducingtheca-\n060 pacityallocationloss(Eq.(4)),weeffectivelyallocatethecorrespondingknowledgetothereserved\nmodelcapacityduringtraining. Duetothenatureoflow-rankparameterdecompositionandaggre-\n061\ngation, thecapacityallocationdoesnotintroduceadditionalinferencelatency, whichiscrucialfor\n062\nreal-worlddeploymentofdiffusionmodels. Additionally,CALLisorthogonaltoexistingmethods\n063\nandcanbecombinedtoachievefurtherimprovements. Thecontributionsaresummarizedas:\n064\n065 â€¢ Weexplorethechallengeofimbalanceddiffusionmodelsfromanewperspective:modelcapacity.\n066 Wehighlightthatthekeyliesinprotectingthemodelcapacityallocatedtominorities, settingit\n067 apartfromexistingeffortsfocusingonimprovingtheobjectivefunctiontoenhanceminorities.\n068 â€¢ Totackletheissueofmajoritiesencroachingonthemodelcapacityrequiredforminorities,pro-\n069 poseanovelmethod,CALL,whichprotectsminoritiesbyreservingmodelcapacityforminority\n070 expertise and effectively allocating the corresponding knowledge during training. CALL is or-\n071 thogonaltoexistingmethods,allowingforcomplementaryintegration.\n072 â€¢ Weconductextensiveexperimentstoshowcasethesuperiorityofourmethod,CALL,inenhanc-\n073 ingtherobustnessofdiffusionmodelsagainstimbalanceddataacrossvarioussettings,including\n074 trainingdiffusionmodelsfromscratchandfine-tuningpre-trainedStableDiffusion.\n075\n076 2 RELATED WORK\n077\n078 Diffusion Models. Diffusion models, a powerful class of generative models, are originally in-\n079 spiredbynon-equilibriumthermodynamics(Sohl-Dicksteinetal.,2015)andarenowsuccessfully\n080 appliedtoimagegeneration(Hoetal.,2020),showingremarkablyeffectiveperformance(Dhariwal\n081 &Nichol,2021;Rombachetal.,2022). Hoetal.(2020)conductthetrainingofdiffusionmodels\n082 using a weighted variational bound. (Song et al., 2021b) propose an alternative method for con-\n083 structing diffusion models by using a stochastic differential equation (SDE). Karras et al. (2022)\n084 introduce a design space that clearly outlines the key design choices in previous works. Denois-\ningdiffusionimplicitmodels(DDIMs)(Songetal.,2021a)employsanalternativenon-Markovian\n085\ngenerationprocess,enablingfastersamplingfordiffusionmodels.\n086\n087 Imbalanced Generation. Several works have investigated imbalanced generation based on gen-\n088 erativeadversarialnetworks(GANs)(Goodfellowetal.,2014). CB-GAN(Rangwanietal.,2021)\n089 mitigates class imbalance during training by using a pre-trained classifier. Rangwani et al. (2022)\n090 notethatperformancedeclineinlong-tailedgenerationmainlyresultsfromclass-specificmodecol-\nlapse in minority classes, which is linked to the spectral explosion of the conditioning parameter\n091\nmatrix. To address this, they propose a corresponding group spectral regularizer. With diffusion\n092\nmodelsdemonstratingexceptionalgenerativecapabilities,recentworkhasbeguntoexploretraining\n093\na robust diffusion model from imbalanced data. CBDM (Qin et al., 2023) employs a distribution\n094\nadjustmentregularizerduringtrainingtoaugmenttheminorities. Yanetal.(2024)introduceacon-\n095\ntrastive learning regularization loss to strengthen the minorities. OC (Zhang et al., 2024) utilizes\n096 transferlearningbetweenmajoritiesandminoritiestoenhancethequalityofminoritygeneration.\n097\n098\n3 PRELIMINARIES\n099\n100\n3.1 PROBLEMFORMULATION\n101\n102 Let and = 1,2,...,C betheimagespaceandtheclassspace,whereC representstheclass\n103 numX ber. AY n imb{ alanced trai} ning set can be denoted as = (xn,yn) N ( , )N, where\nD { }n=1 âˆˆ X Y\n104 N is the size of the training set. The sample number N of each class c in the descending\nc\nâˆˆ Y\n105 orderexhibitsalong-taileddistribution. Thegoalistolearnagenerativediffusionmodelp Î¸(xy),\n|\n106 parameterizedbyÎ¸ fromtheimbalancedtrainingset ,capableofgeneratingrealisticanddiverse\nD\n107 samplesacrossallclasses. Forunconditionalgenerationusingp Î¸(xy), theclassconditioncanbe\n|\nsettoNull,resultinginp (x)=p (xNull).\nÎ¸ Î¸\n|\n2\n\nUnderreviewasaconferencepaperatICLR2025\n108\n3.2 DIFFUSIONMODELS\n109\n110 Webrieflyreviewdiscrete-timediffusionmodels,specificallydenoisingdiffusionprobabilisticmod-\n111 els(DDPMs)(Hoetal.,2020). Givenarandomvariablex andaforwarddiffusionprocesson\n112 xdefinedasx := x ,...,x withT\nN+,theMarkovâˆˆ tX\nransitionprobabilityfromx tox\n1:T 1 T tâˆ’1 t\n113 is q(x t |x tâˆ’1) = N(x t;âˆš1 âˆ’Î² tx tâˆ’1,Î² tIâˆˆ ), where x 0 := x âˆ¼ q(x 0), and {Î² t }T t=1 is the variance\n114 schedule. Theforwardprocessallowsustosamplex t atanarbitrarytimesteptdirectlyfromx 0 in\n115 aclosedformq(x t |x 0)= N(x t;âˆšÎ±Â¯ tx 0,(1 âˆ’Î±Â¯ t)I),whereÎ± t :=1 âˆ’Î² t andÎ±Â¯ t\n:=(cid:81)t\ni=1Î± i. The\n116 variancescheduleisprescribedsuchthatx T isnearlyanisotropicGaussiandistribution.\n117 Training objective. The reverse process for DDPMs is defined as a Markov chain that aims\n118 to approximate q(x ) by gradually denoising from the standard Gaussian distribution p(x ) =\n0 T\n119 N(x T;0,I): p Î¸(x\ntâˆ’1\n|x t) = N(p Î¸(x tâˆ’1;Âµ Î¸(x t,t),Ïƒ t2I), where Âµ Î¸(x t,t) = âˆš1 Î±t(x\nt\nâˆ’\n1 12 20\n1\nâˆš 1Î² âˆ’t Î±Â¯tÏµ Î¸(x t,t)) is parameterized by a time-conditioned noise prediction network Ïµ Î¸(x t,t) and\nÏƒ ,...,Ïƒ are time dependent constants that can be predefined or analytically computed (Bao\n1 T\n122\net al., 2022). The reverse process can be learned by optimizing the variational lower bound on\n123 log-likelihoodas\n124\n125 logp (x)â‰¥E [âˆ’D (q(x |x )âˆ¥p(x ))+logp (x |x )âˆ’(cid:88) D (q(x |x ,x )âˆ¥p (x |x ))]\nÎ¸ q KL T 0 T Î¸ 0 1 KL tâˆ’1 t 0 Î¸ tâˆ’1 t\n126\nt>1\n127 =âˆ’E Ïµ,t[w tâˆ¥Ïµ Î¸(x t,t)âˆ’Ïµâˆ¥2 2]+C 1, (1)\n128\n129 where Ïµ (Ïµ;0,1), x = âˆšÎ±Â¯ x + âˆš1 Î±Â¯ Ïµ according to the forward process, w =\nt t 0 t t\n130 Î²2 âˆ¼ N âˆ’\n131 2Ïƒ t2Î±t(1t âˆ’Î±Â¯t)), and C 1 is typically small and can be dropped (Ho et al., 2020; Song et al., 2021b).\n132 Theterm LDiff(x,Î¸) = E Ïµ,t[w t âˆ¥Ïµ Î¸(x t,t) âˆ’Ïµ âˆ¥2 2]iscalledthediffusionloss(Kingmaetal.,2021).\nTobenefitsamplequality,Hoetal.(2020)applyasimplifiedtrainingobjectivebysettingw =1.\n133 t\n134 Class-conditional diffusion models. When the class labels of the training set are available, the\n135 class-conditional diffusion model p (xy) can be parameterized by Ïµ(x ,t,y). And the uncondi-\nÎ¸ t\n|\n136 tionaldiffusionmodelp Î¸(x)canbeviewedasaspecialcasewithanullconditionÏµ(x t,t,Null). A\n137 similarlowerboundontheclass-conditionallog-likelihoodtoEq.(1)is\n138\n139 logp Î¸(x |y) â‰¥âˆ’E Ïµ,t[w t âˆ¥Ïµ Î¸(x t,t,y) âˆ’Ïµ âˆ¥2 2]+C 2, (2)\n140\n141 whereC 2 isanothersmallconstantandcanbedropped(Hoetal.,2020;Songetal.,2021b). The\nclass-conditionaldiffusionlosscanbewrittenas (x,y,Î¸)=E [w Ïµ (x ,t,y) Ïµ 2].\n142 LDiff Ïµ,t t âˆ¥ Î¸ t âˆ’ âˆ¥2\n143\n144\n4 METHOD\n145\n146\n147 4.1 MOTIVATION\n148\n149 Although diffusion models have demonstrated significant advantages in terms of fidelity and di-\n150 versity in generation, most existing diffusion models implicitly assume that the training data is\napproximately uniformly distributed across classes. When training data exhibits real-world class\n151\nimbalance, diffusion models struggle to generate high-quality and diverse samples for the minori-\n152\nties (Qin et al., 2023; Yan et al., 2024; Zhang et al., 2024). Current efforts focus on adjusting the\n153\nobjectivetogivemoreattentiontominorityclasses,improvingtherobustnessofdiffusionmodelsto\n154\nimbalanceddistributions. Wetackletherobustnesschallengeofimbalanceddistributionsfromthe\n155 newperspectiveofmodelcapacity. Majoritiestakeupmostofthemodelcapacityduetoquantity\n156 dominance,leavingminoritieswithlimitedcapacityandpoorperformance. InFigure1(a),weshow\n157 thesamplesizeforeachclassandthelosschangeafterthemodelpruning(Hanetal.,2015;Lietal.,\n158 2017)operation. Itisclearthatpruninghasagreaterimpactontheoutputofminorities,indicating\n159 thatminorityclassesoccupylessmodelcapacityandarethereforelessrobusttopruning.Jiangetal.\n160 (2021)alsodiscussasimilarphenomenoninimbalanceddiscriminativemodels. Ifwecanreserve\n161 andallocateaportionofthemodelcapacityspecificallyforminorities,wecanpreventtheadverse\neffectsofcapacitydominationandimprovetherobustnesstoimbalanceddistributions.\n3\n\nUnderreviewasaconferencepaperatICLR2025\n162\n(a)Motivation (b)CapacityReservation (c)CapacityAllocation\n163\n164\nğœ½ğ’ˆ\n165\n166\n167 â„’â€™()) C Don ivs eis rt se en ft of ror mm ina oj ro ir tii eti ses (â„’(â„’ $%! &\" )#)\n168\n169\nğœ½= ğœ½ğ’ˆ â¨ ğœ½ğ’†\n170\n171 â„’#$%&\n172\n173 Figure 1: (a) The class distribution of training data in Imb. CIFAR-100 with imbalance ratio of\n174 IR = 100,alongwiththeaveragelossvaluechangesperclassbeforeandafterpruningtheDDPM\n175 model trained on it. The x-axis shows classes arranged in descending order of sample size. The\n176 pruning rate is set to 0.1. The images are for illustration purposes only. (b) An illustration of\n177 thecapacityreservationpartofourmethod, CALL.(c)AnillustrationofhowCALLallocatesthe\ncorrespondingknowledgetothereservedmodelcapacityduringtraining.\n178\n179\n180\n4.2 PROTECTINGMINORITIESVIACAPACITYALLOCATION\n181\n182 4.2.1 CAPACITYRESERVATION\n183\nToallocatesufficientmodelcapacityforminorities,wefirstneedtoexplicitlypartitionthemodelca-\n184\npacity. HereweachievethispurposebyatechniquesimilartoLow-RankAdaptations(LoRAs)(Hu\n185\netal.,2022), whichhasdemonstratedexcellentperformanceandversatilityinthefieldofefficient\n186\nfine-tuning.Whileourtaskandgoaldiffer,weapplyitslow-rankdecompositionconcepttopartition\n187\nthemodelcapacity. ForadiffusionmodelparameterizedbyÎ¸ = W ,W ,... ,whereeachW Î¸\n1 2\n188 representsaparametermatrixinthenetwork,wedecomposeany{ W RdÃ—k a} s âˆˆ\n189 âˆˆ\n190 W =Wg+BA=Wg+We, W Î¸, (3)\nâˆ€ âˆˆ\n191 whereWg RdÃ—krepresentsthepartofW toberetainedformajoritiesandgeneralizedknowledge,\n192 We = BAâˆˆ RdÃ—k represents the part to be allocated to the expertise of minorities, B RdÃ—r,\n193 A RrÃ—k,aâˆˆ ndtherankr < min(d,k). FromEq.(3),wedecomposeÎ¸ intoÎ¸g = Wg,Wâˆˆg,...\n194 andâˆˆ Î¸e = We,We,... andmergethembyÎ¸ =Î¸g Î¸e,where meanstheelement{ -wi1 sead2 dition} .\n{ 1 2 } âŠ• âŠ•\n195 AnillustrationofCapacityReservationisshowninFigure1(b).\n196\n197 4.2.2 CAPACITYALLOCATION\n198\nWiththemodelparametersdecomposedasÎ¸ =Î¸g Î¸e,ourgoalduringtrainingistostoreminority\n199 expertise in Î¸e and general knowledge in Î¸g, ensuâŠ• ring protection for minorities through capacity\n200\nallocation. Toachievethis,thediffusionmodelp Î¸(xy) = p Î¸gâŠ•Î¸e(xy)shouldperformwellonall\n201 samples, both majorities and minorities. Meanwhile| , p Î¸g(xy) shou| ld perform well on majorities\n202 butpoorlyonminorities,asÎ¸g isnotintendedtolearnthemi| norityexpertise.\n203\nCapacity allocation loss. For Î¸ = Î¸g Î¸e, we use a loss function ( ,Î¸) that balances\n204 âŠ• Lbase D\nperformance across majorities and minorities. This is not our primary focus, so we directly adopt\n205\nthe loss functions from existing imbalanced diffusion models, e.g., Zhang et al. (2024); Qin et al.\n206 (2023),as . Forimbalanceddata,weproposeacapacityallocationloss,whichencouragesÎ¸e\nbase\n207 tolearnmiL norityexpertiseandÎ¸g tocapturegeneralknowledge:\n208\n209 Capacityallocationloss: CALL(x,y,Î¸g,Î¸e)= Con(x,y,Î¸g,Î¸e)+ Div(x,y,Î¸g,Î¸e),\nL L L\n210 Consistencyloss: LCon(x,y,Î¸g,Î¸e)=Ï‰ Cy onE\nt\nâˆ¥Ïµ Î¸gâŠ•Î¸e(x t,t,y) âˆ’Ïµ Î¸g(x t,t,y) âˆ¥2 2, (4)\n2 21 11\n2\nDiversityloss: LDiv(x,y,Î¸g,Î¸e)= âˆ’Ï‰ Dy ivE\nt\nâˆ¥Ïµ Î¸gâŠ•Î¸e(x t,t,y) âˆ’Ïµ Î¸g(x t,t,y) âˆ¥2 2.\n213 WevarytheconsistencyclassweightÏ‰ Con andthediversityclassweightÏ‰ Div appliedtodifferent\n214 classes. Forclassc withN c instances,alargerN c (majorities)resultsinahigherconsistency\n215 class weight Ï‰ Cc on, âˆˆ leaY ding to more consistent outputs between Ïµ Î¸gâŠ•Î¸e(x t,t,y) and Ïµ Î¸g(x t,t,y).\nConversely,forthediversityclassweight,asmallerN (minorities)resultsinahigherÏ‰c ,leading\nc Div\n4\n\nUnderreviewasaconferencepaperatICLR2025\n216 tomorediverseoutputsbetweenÏµ Î¸gâŠ•Î¸e(x t,t,y)andÏµ Î¸g(x t,t,y).Thus,p Î¸g(xy)excelsonmajori-\n217 ties,asitsoutputalignswithÎ¸,butunderperformsonminoritiesduetothediv| ergencebetweenthe\n218 outputsofÎ¸g andÎ¸. Specifically,\n219\n220 Ï‰y = CN y , Ï‰y = C . (5)\n221 Con (cid:80)C N Div N (cid:80)C 1\n222\nc=1 c y c=1 Nc\n223 HereÏ‰ scaleslinearlywithclasssamplesize,whileÏ‰ isinverselyproportionaltoclasssample\nCon Div\n224 size,ensuringÏ‰ =Ï‰ =1, (x,y,Î¸g,Î¸e)=0forabalancedtrainingset.\nCon Div CALL\n225 L\nJointoptimization. ForÎ¸ = Î¸g Î¸e,weoptimizethebaseloss andthecapacityallocation\n226 âŠ• Lbase\nloss ,weightedbyhyperparameterÎ»:\n227 LCALL\n228\n(cid:88) 1\n229 min Total( ,Î¸)= base( ,Î¸)+Î» CALL(x,y,Î¸g,Î¸e), (6)\nÎ¸ L D L D NL\n230 (x,y)âˆˆD\n231\n232 where the base loss base optimizes Î¸ for both majorities and minorities, while the capacity allo-\nL\ncation loss acts as a regularizer to allocate capacity and protect minorities. This guides Î¸\n233 CALL\nL\ntoward more balanced and effective model weights. An illustration of the training process of our\n234\nCALLispresentedinFigure1(c).\n235\n236 Inference. For inference, we can explicitly compute and store Î¸ = Î¸g Î¸e, and sample images\nâŠ•\n237 fromp Î¸(xy). Thus,ourmethoddoesnotincreasemodelcapacity,ensuringnoadditionalinference\n|\n238\nlatencycomparedtoastandarddiffusionmodel,whichiscrucialasinferencespeedisakeybottle-\nneckinreal-worlddeployment(Songetal.,2021a). ThisadvantagecomesfromusingaLoRA-like\n239\nparameterdecompositioninEq.(3)andexplicitlyaggregatingtheparametersduringinference.\n240\n241\n242 4.3 DISSCUSSION\n243\n244 Comparisonwithexistingimbalanceddiffusionmodels. UnlikecurrentmethodssuchasCBDM\n245 andOC,whichprioritizedesigningmoresuitableobjectivefunctionsforimbalanceddata,ourCALL\n246 improves the robustness of diffusion models to imbalanced distributions from a new perspective:\n247 allocatingmodelcapacitytoprotectminorities. CALLisorthogonalandcanbenefitfromimproved\n248 objectivefunctionstoachievefurtherenhancements(asshownempiricallyinTable5).\n249 Comparison with LoRA. While the capacity reservation mechanism in CALL shares a similar\n250 structurewithLoRA,ourgoalistodecomposeandallocatemodelcapacitypriortotraining,whereas\n251 LoRA is aimed at efficiently fine-tuning pre-trained models. Additionally, our method involves a\n252 jointtrainingstrategy,whereasLoRAfocusessolelyonoptimizingthelow-rankcomponents.\n253\nComparisonwithensemble-basedimbalancedclassificationmethods. Severalensemble-based\n254\nmethods (Cui et al., 2023; Wang et al., 2021b; Zhang et al., 2022) leverage multiple experts to\n255 capturediverseknowledge,achievingstrongperformanceinclassificationtasksthroughprediction\n256 ensemble. However, most of these methods are tailored for classification networks in terms of ar-\n257 chitecture, training paradigm, and loss functions, making them unsuitable for direct application in\n258 diffusionmodels. Whiletheyalsoinvolveknowledgeallocation,theirgainmainlycomesfromin-\n259 creasedcapacityandensemblepredictions. Additionally,theyoftenrequirestructuralmodifications\n260 to the network and incur higher inference latency, further limiting applicability. In contrast, our\nmethodintroducesnochangestonetworkstructure, doesnotincreasemodelcapacityorinference\n261\nlatency,andenhancesimbalanceddiffusionmodelspurelythroughcapacityallocation.\n262\n263 ExtensiontoLoRA-finetuning. OurmethodcanbeseamlesslyextendedtoLoRA-finetuningsce-\n264 nariosbymodifyingEq.(3)totheform: W = Wf +BgAg +BeAe. Here,Î¸f = Wf,Wf,...\n265 representsthefrozenpre-trainedmodelparameters,Î¸g = BgAg,BgAg,... denot{ est1 hetra2 inabl} e\n2 26 66\n7\nrp ea sr pa om ne dt ser ts oa tl hl eoc tra ate ind af bo lerm paa rj ao mri eti te es rsa rn ed seg re vn ee dra foli rze md ink on ro iw tyl{ e ed xg p1 e e, rt1 a isn ed .2 Î¸ Fe o2 r= W{B} 1e RA de 1 Ã—,B k,2e BA ge 2,.. R. } dÃ—co rgr-\n,\n268 Ag RrgÃ—k, Be RdÃ—re , Ae RreÃ—k, we have re < rg < min(d,k).âˆˆ During inferâˆˆ ence, the\n269 modâˆˆ elparametersaâˆˆ remergedbyÎ¸âˆˆ =Î¸f Î¸g Î¸e. ThisextensionpreservesthestructureofLoRA\nâŠ• âŠ•\nwhileenhancingthefine-tuningprocessbycapacityallocationforimbalanceddata.\n5\n\nUnderreviewasaconferencepaperatICLR2025\n270\nTable1:FIDs( ),KIDs( ),Recalls( ),andISs( )ofCALLandvariousbaselinemethodsonImb.\n271 â†“ â†“ â†‘ â†‘\nCIFAR-10 and Imb. CIFAR-100 with different imbalance ratios IR = 100,50 . All results are\n272 reportedasMean Std. Bestandsecond-bestresultsarehighlighted. { }\n273 Â±\n274 Imb.CIFAR-10,IR=100\n275 Method FID KID Recall IS\nâ†“ â†“ â†‘ â†‘\n276 DDPM(Hoetal.,2020) 10.697 0.079 0.0035 0.0008 0.47 0.01 9.39 0.12\nÂ± Â± Â± Â±\n277 +ADA(Karrasetal.,2020) 9.266 0.133 0.0029 0.0003 0.49 0.02 9.26 0.14\nÂ± Â± Â± Â±\n278 +RS(Mahajanetal.,2018) 12.332 0.064 0.0037 0.0003 0.45 0.02 9.25 0.08\nÂ± Â± Â± Â±\n+Focal(Linetal.,2017) 10.842 0.134 0.0034 0.0001 0.46 0.03 9.42 0.18\n279\nCBDM(Qinetal.,2023) 8.233\nÂ±\n0.152\n0.0026Â±\n0.0001\n0.53Â±0.02 9.23Â±\n0.11\n280 OC(Zhangetal.,2024) 8.390Â± 0.063 0.0027Â± 0.0002 0.52Â± 0.03 9.53Â±0.12\n281 CALL\n7.727Â±0.124 0.0023Â±0.0001 0.53Â±0.01 9.52Â±\n0.10\nÂ± Â± Â± Â±\n282 Imb.CIFAR-10,IR=50\n283\nMethod FID KID Recall IS\n284 â†“ â†“ â†‘ â†‘\nDDPM(Hoetal.,2020) 10.216 0.138 0.0035 0.0002 0.47 0.03 9.37 0.13\n285 Â± Â± Â± Â±\n+ADA(Karrasetal.,2020) 9.132 0.215 0.0030 0.0002 0.51 0.04 9.28 0.21\n286 +RS(Mahajanetal.,2018) 11.231Â± 0.177 0.0038Â± 0.0002 0.47Â± 0.02 9.31Â± 0.14\nÂ± Â± Â± Â±\n287 +Focal(Linetal.,2017) 10.315 0.263 0.0034 0.0003 0.48 0.01 9.38 0.23\nCBDM(Qinetal.,2023) 7.933\nÂ±\n0.082\n0.0026Â±\n0.0002\n0.54Â±0.02 9.42Â±\n0.14\n288 Â± Â± Â± Â±\nOC(Zhangetal.,2024) 8.034 0.225 0.0027 0.0001 0.53 0.01 9.65 0.09\n289 CALL 7.372Â±0.125 0.0024Â±0.0002 0.54Â±0.01 9.69Â±0.09\nÂ± Â± Â± Â±\n290\nImb.CIFAR-100,IR=100\n291\nMethod FID KID Recall IS\n292 â†“ â†“ â†‘ â†‘\n293 DDPM(Hoetal.,2020) 10.163 0.077 0.0029 0.0005 0.46 0.01 13.45 0.15\nÂ± Â± Â± Â±\n+ADA(Karrasetal.,2020) 9.482 0.125 0.0032 0.0002 0.51 0.01 12.44 0.16\n294 Â± Â± Â± Â±\n+RS(Mahajanetal.,2018) 11.432 0.287 0.0038 0.0007 0.44 0.03 12.12 0.18\n295 +Focal(Linetal.,2017) 10.212Â± 0.110 0.0032Â± 0.0004 0.47Â± 0.02 13.07Â± 0.26\nÂ± Â± Â± Â±\n296 CBDM(Qinetal.,2023) 10.051 0.391 0.0036 0.0003 0.51 0.01 12.35 0.12\n297 CO AC L(Z Lhangetal.,2024) 8 7. .3 50 19 9Â±Â± 0 0. .2 13 33\n2\n0 0. .0 00 02 16 7Â± Â±0 0. .0 00 00 02\n3\n0 0. .5 52 2Â± Â±0 0. .0 02\n2\n1 13 3. .4 44 5Â±Â± 0 0. .2 20\n3\n298 Â± Â± Â± Â±\n299 Imb.CIFAR-100,IR=50\n300 Method FID KID Recall IS\nâ†“ â†“ â†‘ â†‘\n301 DDPM(Hoetal.,2020) 9.363 0.069 0.0032 0.0002 0.47 0.02 14.27 0.22\nÂ± Â± Â± Â±\n302 +ADA(Karrasetal.,2020) 8.927 0.138 0.0033 0.0001 0.51 0.02 12.89 0.17\nÂ± Â± Â± Â±\n+RS(Mahajanetal.,2018) 10.259 0.217 0.0037 0.0003 0.47 0.03 12.38 0.23\n303 Â± Â± Â± Â±\n+Focal(Linetal.,2017) 9.477 0.114 0.0034 0.0002 0.49 0.03 13.31 0.15\n304\nCBDM(Qinetal.,2023)\n8.946Â±\n0.178\n0.0036Â±\n0.0003\n0.55Â±0.02 12.59Â±\n0.19\n305 OC(Zhangetal.,2024) 7.188Â± 0.274 0.0024Â± 0.0002 0.54Â± 0.01 13.99Â± 0.22\n306 CALL\n6.732Â±0.052 0.0021Â±0.0001 0.55Â±0.03 14.12Â±\n0.15\nÂ± Â± Â± Â±\n307\n308\n5 EXPERIMENTS\n309\n310\n311\n5.1 EXPERIMENTALSETUP\n312\nDatasets. We conduct experiments on the imbalanced versions of commonly used datasets in the\n313\nfield of image synthesis, including CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky\n314 etal.,2009),CelebA-HQ(Karrasetal.,2018),andArtBench-10(Liaoetal.,2022). CIFAR-10and\n315 CIFAR-100havearesolutionof32 32,whileforCelebA-HQ,weusethe64 64version,andfor\nÃ— Ã—\n316 ArtBench-10,weusetheoriginalresolutionof256 256. WefollowCaoetal.(2019)toconstruct\nÃ—\n317 imbalancedversionsofthesedatasetsbydownsampling,resultinginanexponentialdecreaseinthe\n318 samplesizeofeachclasswithitsindex.WerefertotheseimbalanceddatasetsasImb.dataset,e.g.,,\n319 Imb.CIFAR-10.Wecontrolthelevelofimbalanceinthedatasetbysettingdifferentimbalanceratios\nIR 50,100 ,whereIRistheratioofthenumberofsamplesinthemostpopulousclasstothat\n320\nâˆˆ { }\n321 intheleastpopulousclass,definedasIR= m ma inx cc âˆˆâˆˆ YY NN cc. ForImb. CIFAR-10andImb. ArtBench-10,\n322 wedividethedatasetintothreesplits: Many(classes0-2),Medium(classes3-5),andFew(classes\n323 6-9)basedonclasssizesindescendingorder. Similarly,forImb. CIFAR-100,thesplitsareMany\n(classes0-32),Medium(classes33-65),andFew(classes66-99).\n6\n\nUnderreviewasaconferencepaperatICLR2025\n324\nTable 2: FIDs ( ), KIDs ( ), and per-class FIDs ( ) of CALL and baselines on Imb. CelebA-HQ\n325 â†“ â†“ â†“\nwithdifferentimbalanceratiosIR= 100,50 .\n326 { }\nImb.CelebA-HQ,IR=100\n327\n328 Method FemaleFID MaleFID OverallFID KID\nâ†“ â†“ â†“ â†“\n329 DDPM(Hoetal.,2020) 7.143 0.147 16.425 0.032 8.727 0.126 0.0037 0.0001\nÂ± Â± Â± Â±\n330 CBDM(Qinetal.,2023) 7.043 0.079 14.273 0.183 7.823 0.115 0.0043 0.0002\nÂ± Â± Â± Â±\n331 OC(Zhangetal.,2024) 7.092 0.323 13.962 0.221 7.871 0.237 0.0034 0.0002\nÂ± Â± Â± Â±\nCALL 6.815 0.241 12.788 0.316 7.538 0.201 0.0033 0.0002\n332 Â± Â± Â± Â±\n333 Imb.CelebA-HQ,IR=50\n334 Method FemaleFID MaleFID OverallFID KID\nâ†“ â†“ â†“ â†“\n335\nDDPM(Hoetal.,2020) 7.348 0.219 14.808 0.152 8.007 0.265 0.0034 0.0002\n336 CBDM(Qinetal.,2023) 7.317Â± 0.273 12.592Â± 0.181 7.423Â± 0.139 0.0042Â± 0.0001\nÂ± Â± Â± Â±\n337 OC(Zhangetal.,2024) 7.283 0.226 12.938 0.277 7.438 0.247 0.0034 0.0003\nÂ± Â± Â± Â±\n338 CALL 7.147 0.182 11.273 0.146 7.193 0.282 0.0033 0.0002\nÂ± Â± Â± Â±\n339\n340 Baselines. We consider baselines including: (1) the base denoising diffusion probabilistic model\n341 (DDPM);(2)methodsspecificallytargetingimbalanceddiffusionmodels: theclass-balancingdif-\n342 fusionmodel(CBDM)(Qinetal.,2023)andOrientedCalibration(OC)(Zhangetal.,2024);(3)ap-\n343 plyingimbalancelearningmethodsfromdiscriminativemodelsorgenerativeadversarialnetworks\n344 (GANs)todiffusionmodels: re-sampling(RS)(Mahajanetal.,2018),adaptivediscriminatoraug-\n345 mentation(ADA)(Karrasetal.,2020),andfocalloss(Linetal.,2017). Notethatmanyimbalanced\nlearningmethodsfordiscriminativemodelsandGANsheavilyrelyonspecificmodelarchitectures\n346\nortrainingparadigms,e.g.,Menonetal.(2021);Zhouetal.(2023);Rangwanietal.(2022),making\n347\nthemincompatiblewithimbalanceddiffusionmodels.\n348\n349 Implementationdetails. FollowingHoetal.(2020),weutilizeaU-Net(Ronnebergeretal.,2015)\n350 basedonaWideResNet(Zagoruyko&Komodakis,2016)asthenoisepredictionnetwork. Weset\n351 thehyperparametersforDDPMasÎ² 1 =10âˆ’4 andÎ² T =0.02,withmaximumtimestepT =1000.\nThe Adam optimizer (Kingma & Ba, 2015) is used with betas = (0.9,0.999) and a learning rate\n352\nof 2 10âˆ’4. The dropout rate is set to 0.1. We use a batch size of 64 and train the model for\n353 Ã—\n300,000steps,includingawarm-upperiodof5,000steps. FortherankofBAinEq.(3),wefixitat\n354\n1 min(d,k). Weonlyapplythelow-rankdecompositiontotheupsamplingpartoftheU-Net,i.e.,\n355 10\nthelatterhalfofthemodel,astheshallowlayerstendtocapturemoregeneralknowledge(Alzubaidi\n356\netal.,2021). ForthehyperparameterÎ»inEq.(6),wefixitasÎ»=1. ForthebaselossinEq.(6),we\n357 adopttheobjectivefunctionfromZhangetal.(2024),unlessotherwisespecified. Duringinference,\n358 newimagesaregeneratedutilizingthe50-stepDDIMsolver(Songetal.,2021a).\n359\nMetrics. The performance of our method and all baselines is evaluated using the metrics Frechet\n360\nInceptionDistance(FID)(Heuseletal.,2017),KernelInceptionDistance(KID)(Binkowskietal.,\n361\n2018), Recall (KynkaÂ¨aÂ¨nniemi et al., 2019), and Inception Score (IS) (Salimans et al., 2016). All\n362\nmetricsarecalculatedbasedonfeaturesextractedfromapre-trainedInception-V3(Szegedyetal.,\n363 2016)model1. Duringevaluation,themetricsarecalculatedusingabalancedsetofrealimagesand\n364 50,000generatedimages. Themetricsforeach many,medium,few splitarecomputedusingthe\n{ }\n365 correspondingsplitâ€™srealimagesand20,000generatedimages.\n366\n367 5.2 MAINRESULTS\n368\n369 Performance on Imb. CIFAR-10 and Imb. CIFAR-100. In Table 1, we summarize the FIDs,\n370 KIDs, Recalls, ISs of our CALL and all baseline methods on Imb. CIFAR-10 and Imb. CIFAR-\n100 with different imbalance ratios IR = 50,100 . Our CALL achieves the best results on 16\n371\n{ }\nmetricsacrossallfoursettings,exceptfortwoslightlylowerISs. NotethatIScannotdetectmode\n372\ncollapse (Barratt & Sharma, 2018), e.g., if the generated minority samples are overwhelmed by\n373\nmajority characteristics, such low-quality images would not lead to a drop in IS, which explains\n374\nwhy vanilla DDPM still performs well on some ISs. Additionally, IS lacks a reference to real\n375\nimages, makingitgenerallyconsideredalessreliablemetric(Borji,2019;Nunnetal.,2021). On\n376\n377 1https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/\nweights-inception-2015-12-05-6726825d.pth\n7\n\nUnderreviewasaconferencepaperatICLR2025\n378\nTable3:FIDs( ),KIDs( ),Recalls( ),andISs( )ofCALLandvariousbaselinemethodsonImb.\n379 â†“ â†“ â†‘ â†‘\nArtBench-10(imbalanceratiosIR= 100,50 )usingLoRAtofine-tuneStableDiffusion.\n380 { }\n381\nImb.ArtBench-10,IR=100\n382 Method FID KID Recall IS\nâ†“ â†“ â†‘ â†‘\n383 DDPM(Hoetal.,2020) 27.083 0.438 0.0142 0.0003 0.39 0.01 8.47 0.19\nÂ± Â± Â± Â±\n384 CBDM(Qinetal.,2023) 25.723 0.263 0.0122 0.0002 0.43 0.01 7.97 0.22\nÂ± Â± Â± Â±\n385 OC(Zhangetal.,2024) 24.315 0.162 0.0106 0.0005 0.42 0.01 8.71 0.20\nÂ± Â± Â± Â±\nCALL 22.776 0.078 0.0087 0.0002 00.44 0.02 8.71 0.18\n386 Â± Â± Â± Â±\n387 Imb.ArtBench-10,IR=50\n388 Method FID KID Recall IS\nâ†“ â†“ â†‘ â†‘\n389\nDDPM(Hoetal.,2020) 25.557 0.082 0.0134 0.0004 0.39 0.02 8.41 0.15\n390 CBDM(Qinetal.,2023) 24.487Â± 0.153 0.0114Â± 0.0002 0.43Â± 0.02 8.03Â± 0.23\n391 OC(Zhangetal.,2024) 23.287Â± 0.232 0.0097Â± 0.0003 0.43Â± 0.02 8.48Â± 0.17\nÂ± Â± Â± Â±\n392 CALL 21.733 0.153 0.0080 0.0002 0.44 0.01 8.51 0.23\nÂ± Â± Â± Â±\n393\n394\nthemostwidelyusedmetricFID,CALLachievesignificantimprovementsoverDDPMwithgains\n395\nof2.725,2.844,2.644,and2.571,andconsistentimprovementsoverthebestbaselineineachsetting\n396\nby0.506,0.561,0.790,and0.456,respectively.Forbaselinemethods,CBDMperformswellonImb.\n397\nCIFAR-10, while OC shows better results on Imb. CIFAR-100. DDPM + RS generally performs\n398 worse than DDPM. DDPM + ADA, although still weaker than specialized methods like CBDM\n399 andOC,demonstratesstableimprovementsoverDDPM,suggestingthepotentialofexploringdata\n400 augmentationtoaddresschallengesofimbalanceddataindiffusionmodels.DDPM+Focalachieves\n401 comparableresultstoDDPM,likelybecausethelossdifferencesbetweenclassesindiffusionmodels\n402 arelessdistinct,makingFocallosslesseffectiveforloss-basedhardexamplemining.\n403\n404\n20\n405\n18 406\n407 16\n408 14\n409 12\n410 10\nMany Medium Few\n411 Split\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n) (DIF â†“\nImb.CIFAR10,IR=100\n22.5\nDDPM\nCBDM 20.0\nOC\nCALL 17.5\n15.0\n12.5\n10.0\nMany Medium Few\nSplit\n) (DIF â†“\nMany/Medium/Few analysis. In\nFigure 2, we show the fine-grained Imb.CIFAR100,IR=100\nmany, medium, few per-split DDPM\n{ } CBDM FIDs of different methods on Imb.\nOC\nCIFAR-10 and Imb. CIFAR-100 CALL\nwith imbalance ratio IR = 100.\nOur method achieves the best re-\nsultsacrossallthreesplits, withthe\nprimary improvements observed in\nthe Medium and Few classes. It is\nFigure 2: Per-split FIDs of CALL and baselines on Imb.\nnoteworthythatonImb. CIFAR-10,\nCIFAR-10(IR=100)andImb. CIFAR-100(IR=100).\nthe generation quality for Medium\nclasses is worse than for Few classes. Similar observations have been made on imbalanced con-\ntrastive learning (Zhou et al., 2023). This could be attributed to the inherent difficulty differences\nbetween classes, suggesting a promising direction of addressing imbalanced diffusion models by\ncombininginherentdifficultyimbalancewithquantityimbalance.\nPerformance on Imb. CelebA-HQ. In Table 2, we report the FIDs, KIDs, and per-class FIDs of\nCALLandbaselinemethodsonImb. CelebA-HQwithdifferentimbalanceratiosIR = 100,50 .\n{ }\nImb. CelebA-HQ contains two classes: Female and Male, with Female being the majority class.\nOur CALL achieves the best performance across all eight metrics in both settings. Specifically, it\nimprovestheOverallFIDby1.189and0.814comparedtoDDPMandby0.285and0.230compared\nto the best baselines in each setting. For the minority class (Male), our method enhances FID\nby 3.637 and 3.535 over DDPM and by 1.174 and 1.319 over the best baselines. In Figure 6 in\nAppendix,weshowcasethegeneratedresultsfortheâ€œMaleâ€classwithimbalanceratioIR = 100.\nItisevidentthatourmethodgeneratesmorerealisticanddiversefaces.\nPerformanceofFine-tuningStableDiffusiononImb. ArtBench-10. OnImb. ArtBench-10,we\nfine-tunetheStableDiffusionmodel2(Rombachetal.,2022)byLoRA(Huetal.,2022)witharank\nof128. AndforÎ¸e,therankissetto8. Wetrainthemodelinaclass-conditionalmannerwherethe\ntextualpromptissimplysetasâ€œa class paintingâ€suchasâ€œarenaissancepaintingâ€. Thedropout\n{ }\n2https://huggingface.co/lambdalabs/miniSD-diffusers\n8\n\nUnderreviewasaconferencepaperatICLR2025\n432\n433\nDDPM\n434\n435\n436\nCALL\n437\n438\n439 Real\nImages\n440\n441\n442\nFigure 3: The visualization of generated images on Imb. ArtBench-10 with imbalance ratio\n443\nIR = 100. The figure showcases the generated outputs for the class â€œRealismâ€, which is one of\n444\nthefewclasses,frombothDDPMandCALL.Thelastrowdisplaysrealimagesfromthedatasetfor\n445 reference. ItisevidentthatCALLgeneratesresultsthataresignificantlymorediverseandstylisti-\n446 callyclosertotherealimagescomparedtoDDPM.Theimagesshownarerandomlyselected.\n447\n448\n449 8.5\n450 8.0\n451\n452 7.5\n453 0 0.5 0.81.01.2 1.5\nÎ»\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n)(DIF â†“\nIR=100\n8.00\n7.75\n7.50 CALL\nOC 7.25\n7.00\n0 0.5 0.81.01.2 1.5\nÎ»\n)(DIF â†“\nIR=50\n8.5\n8.0\nCALL\nOC 7.5\n0 0.5 0.81.01.2 1.5\nÎ»\n(a) Imb.CIFAR-10,IR={100,50}\n)(DIF â†“\nIR=100\n7.2\n7.0\nCALL 6.8\nOC 6.6\n0 0.5 0.81.01.2 1.5\nÎ»\n)(DIF â†“\nIR=50\nCALL\nOC\n(b) Imb.CIFAR-100,IR={100,50}\nFigure 4: Ablation study on the hyperparameter Î» in Eq. (6). We use OC as a reference because\nitshowsthebestoverallperformanceamongthebaselinesandservesasourbaseloss. Figures(a)\nand(b)showresultsonImb. CIFAR-10andImb. CIFAR-100, respectively, withimbalanceratios\nofIR=100andIR=50fromlefttoright. WereportFIDsforÎ»= 0.0,0.5,0.8,1.0,1.2,1.5 .\n{ }\nrateissetto0.1,andthemodelistrainedfor100epochswithabatchsizeof64,usingtheAdamW\noptimizer(Loshchilov&Hutter,2019)withaweightdecayof10âˆ’6 andaninitiallearningrateof\n3 10âˆ’4. During inference, we generate new images using a 50-step DDIM solver (Song et al.,\nÃ—\n2021a). InTable3,wecompareourCALLagainstDDPMandthetwostrongestbaselines,CBDM\nandOC,onImb. ArtBench-10withimbalanceratiosIR= 100,50 . OurCALLachievesthebest\n{ }\nresults across all eight metrics. Specifically, it outperforms DDPM in terms of FID by 4.307 and\n3.824,andthebestbaselineineachsettingby1.539and1.554,respectively. NotethatISshowsa\ndecreasingtrendastheimbalanceratiodecreasesfrom100to50,indicatingitsunreliabilityonImb.\nArtBench. ThisisbecausetheoutputsoftheImageNet-pretrainedInception-V3arelessreliablefor\nartwork images, and IS does not use real images as a reference. The generated images for one of\nthefewclassesâ€œRealismâ€onImb. ArtBench-10withIR=100areshowninFigure3. Ourmethod\ngeneratesmorediverseimages,andthegeneratedstylesareclosertotherealimages.\n5.3 FURTHERANALYSIS\nCALLasauniversalframework. Table5summarizestheperformanceofourCALLwheninte-\ngrated with DDPM, CBDM, and OC (i.e., using the corresponding objective function for in\nbase\nL\nEq.(6))onImb. CIFAR-100withIR = 100. Itcanbeobservedthatourmethodconsistentlyim-\nprovestheperformanceofimbalancedgenerationwhencombinedwithdifferentbaselines. Dueto\ntheorthogonalityofCALLtoexistingmethods,itcanconsistentlybenefitfromimprovedobjective\nfunctions,includingpotentialfutureadvancements.\nEffectofknowledgeallocationbetweenÎ¸g andÎ¸e. ToinvestigatetheeffectofCALLonknowl-\nedgeallocationbetweenÎ¸gandÎ¸e,wepresenttheresultsofgeneratingimagesusingonlyÎ¸g(CALL\n(Î¸g))andusingÎ¸ =Î¸g Î¸e(CALL)onImb. CIFAR-100withimbalanceratioIR=100inTable4.\nCALL(Î¸g)performswâŠ•\nellontheManyandMediumclassesbutstruggleswiththefewclasses. In\ncontrast,CALLshowsstrongperformanceacrossallsplits. ThisindicatesthatCALLsuccessfully\nallocatesminorityexpertisetoÎ¸e,whilereservingmajorityandgeneralknowledgeforÎ¸g.\n9\n\nUnderreviewasaconferencepaperatICLR2025\n486\nTable4:Per-splitFIDsandoverallFIDs( ,Mean Std)ofDDPM,CALL(Î¸g),andCALLonImb.\n487 â†“ Â±\nCIFAR-100withimbalanceratioIR = 100. Many,Medium,andFewarethethreesplitsbasedon\n488 thetrainingimbalance. Bestresultsarehighlighted.\n489\nMethod ManyFID Med.FID FewFID OverallFID\n490 â†“ â†“ â†“ â†“\n491 DDPM(Hoetal.,2020) 14.068 0.193 15.660 0.047 22.188 0.241 10.163 0.077\nCALL(Î¸g) 11.923Â± 0.139 14.872Â± 0.157 29.357Â± 0.318 13.732Â± 0.240\n492 CALL(Î¸=Î¸g Î¸e) 11.732Â± 0.247 13.043Â± 0.138 18.729Â± 0.141 7.519 Â± 0.132\n493 âŠ• Â± Â± Â± Â±\n494\n495\nTable5: FIDs( ),KIDs( ),Recalls( ),andISs( )ofdifferentbaselinesonImb. CIFAR-100with\n496 imbalanceratioâ†“ IR=100â†“ andtheirreâ†‘ sultswhencâ†‘\nombinedwithCALL.Thelasttworowsshowthe\n497 resultsofCALLafterremoving and ,respectively.\nCon Div\nL L\n498\nMethod FID KID Recall IS\n499 â†“ â†“ â†‘ â†‘\n500 DDPM(Hoetal.,2020) 10.163 0.077 0.0029 0.0005 0.46 0.01 13.45 0.15\nÂ± Â± Â± Â±\n+CALL 9.281 0.251 0.0027 0.0002 0.49 0.01 13.37 0.19\n501 Â± Â± Â± Â±\nCBDM(Qinetal.,2023) 10.051 0.391 0.0036 0.0003 0.51 0.01 12.35 0.12\n502 +CALL 8.837 Â± 0.245 0.0029Â± 0.0001 0.51Â± 0.02 13.07Â± 0.16\nÂ± Â± Â± Â±\n503 OC(Zhangetal.,2024) 8.309 0.233 0.0026 0.0002 0.52 0.02 13.44 0.20\nÂ± Â± Â± Â±\n504 +CALL 7.519 0.132 0.0017 0.0003 0.52 0.02 13.45 0.23\nÂ± Â± Â± Â±\n505 +CALLw/o LCon 8.412 Â±0.227 0.0029 Â±0.0002 0.50 Â±0.01 13.23 Â±0.22\n+CALLw/o Div 8.073 0.174 0.0025 0.0001 0.51 0.01 13.42 0.16\n506 L Â± Â± Â± Â±\n507\n508\nAblationonthehyperparameterÎ»inEq.(6). ToinvestigatetheimpactofthehyperparameterÎ»,\n509\ntheweightoftheCALLlossinEq.(6),ontheperformanceofourmethod,weconductablationex-\n510\nperimentsonImb. CIFAR-10andImb. CIFAR-100withdifferentimbalanceratiosIR= 100,50 .\n511 Figure 4 illustrates how the FID of CALL changes with varying Î» values under differen{ t setting} s.\n512 WeobservethatCALLmaintainsaconsistentadvantageoverOCacrossawiderangeofÎ»values,\n513 withitsperformancepeakingaroundÎ»=1.0.\n514\nAblation on and . Table 5 presents the results of CALL as well as the ablation study\n515 LCon LDiv\nwhere the consistency loss and the diversity loss are removed separately from CALL.\nCon Div\n516 L L\nSince and are responsible for allocating majority knowledge and minority expertise,\nCon Div\n517 L L\nrespectively,removingeitherleadstoasignificantdropinperformance,highlightingtheirnecessity.\n518\n519\n14\n520\n521 12\n522 10\n523 8\n524 [1,2,2] [1,2,2,2] [1,2,3,4]\n525 Arch\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n) (DIF â†“\nImb.CIFAR10,IR=100\nDDPM OC\nCBDM CALL 10\n8\n[1,2,2] [1,2,2,2] [1,2,3,4]\nArch\n) (DIF â†“\nAblation on network configura-\nImb.CIFAR100,IR=100\ntions. We conduct experiments\nDDPM OC\non UNet architectures with varying CBDM CALL widths and depths. Figure 5 shows\nFIDs on Imb. CIFAR-10 and Imb.\nCIFAR-100 with IR = 100. Differ-\nent widths and depths are achieved\nby setting the channel multipliers\nFigure 5: FIDs with various UNet configurations on Imb.\nparameter to [1,2,2], [1,2,2,2] (de-\nCIFAR-10andImb. CIFAR-100withIR=100.\nfault), and[1,2,3,4]. Asshown, our\nmethodconsistentlydemonstratesclearadvantagesacrossdifferentnetworkconfigurations.\n6 CONCLUSION\nIn this paper, we seek to improve the robustness of diffusion models to imbalanced data. Unlike\npreviousworkthatfocusesonimprovingobjectivefunctions,weaimtoprotectthegenerationper-\nformanceofminoritiesbyreservingandallocatingmodelcapacityforthem.Wefirstdecomposethe\nmodelparametersintopartsthatcapturegeneralandmajorityknowledge,andadedicatedpartfor\nminorityexpertiseusinglow-rankdecompositiontechniques. Byintroducingacapacityallocation\nloss, we successfully allocate the corresponding knowledge to the reserved model capacity during\ntraining. ExtensiveexperimentsandempiricalanalysesconfirmthatourmethodCALLeffectively\nprotectsminoritiesinimbalanceddiffusionmodelsviacapacityallocation.\n10\n\nUnderreviewasaconferencepaperatICLR2025\n540 ETHICS STATEMENT\n541\n542\nInthispaper,weproposeamethodtoenhancetherobustnessofgenerativediffusionmodelsagainst\n543 imbalanced data distributions. This advancement holds significant social implications, both pos-\n544 itive and negative. On the positive side, our approach could democratize access to high-quality\n545 datageneration, allowingmarginalizedcommunitiestobenefitfrommoreequitablerepresentation\n546 in AI applications. By improving the modelâ€™s performance on underrepresented classes, we can\n547 foster inclusivity in various fields, such as healthcare, finance, and education, where data-driven\n548 decisions can impact lives. Conversely, there are potential negative consequences to consider. As\ngenerativemodelsbecomemorepowerful,theymaybemisusedtocreatedeceptivecontent,leading\n549\nto misinformation and erosion of trust in digital media. Additionally, our methodâ€™s emphasis on\n550\nunderrepresentedsegmentsinthetrainingdataposesariskofdatapoisoningifsupervisionislack-\n551\ning. Malicious actors could exploit this focus to introduce biased or harmful data, compromising\n552\nthemodelâ€™sintegrity. Thisvulnerabilityunderscorestheneedforrobustmonitoringandvalidation\n553\nmechanismstoensuredatareliability,asanycompromisecouldleadtounintendednegativeconse-\n554 quences. Therefore,proactivedatagovernanceisessentialtomitigatetheseriskswhilemaximizing\n555 thebenefitsofourproposedmethod.\n556\n557\nREPRODUCIBILITY STATEMENT\n558\n559\nTo ensure the reproducibility of experimental results, we will provide a link for an anonymous\n560\nrepositoryaboutthesourcecodesofthispaperinthediscussionforumaccordingtotheICLR2025\n561\nAuthorGuide. AlltheexperimentsareconductedonNVIDIAA100swithPython3.8andPytorch\n562\n2.0.1. WeprovideexperimentalsetupsandimplementationdetailsinSection5.1andSection5.2.\n563\n564\n565\nREFERENCES\n566 Laith Alzubaidi, Jinglan Zhang, Amjad J. Humaidi, Ayad Q. Al-Dujaili, Ye Duan, Omran Al-\n567 Shamma, JoseÂ´ SantamarÂ´Ä±a, Mohammed A. Fadhel, Muthana Al-Amidie, and Laith Farhan. Re-\n568 view of deep learning: concepts, CNN architectures, challenges, applications, future directions.\n569 J.BigData,8(1):53,2021.\n570\nFanBao,ChongxuanLi,JunZhu,andBoZhang. Analytic-dpm:ananalyticestimateoftheoptimal\n571\nreversevarianceindiffusionprobabilisticmodels. InICLR.OpenReview.net,2022.\n572\n573 ShaneT.BarrattandRishiSharma. Anoteontheinceptionscore. CoRR,abs/1801.01973,2018.\n574\n575 MikolajBinkowski,DanicaJ.Sutherland,MichaelArbel,andArthurGretton. DemystifyingMMD\ngans. InICLR.OpenReview.net,2018.\n576\n577 Ali Borji. Pros and cons of GAN evaluation measures. Comput. Vis. Image Underst., 179:41â€“65,\n578 2019.\n579\n580 MateuszBuda,AtsutoMaki,andMaciejA.Mazurowski. Asystematicstudyoftheclassimbalance\n581 probleminconvolutionalneuralnetworks. NeuralNetworks,106:249â€“259,2018.\n582\nKaidi Cao, Colin Wei, Adrien Gaidon, Nikos AreÂ´chiga, and Tengyu Ma. Learning imbalanced\n583\ndatasetswithlabel-distribution-awaremarginloss. InHannaM.Wallach,HugoLarochelle,Alina\n584 Beygelzimer, Florence dâ€™AlcheÂ´-Buc, Emily B. Fox, and Roman Garnett (eds.), NeurIPS, pp.\n585 1565â€“1576,2019.\n586\n587 JiequanCui, ZhishengZhong, ShuLiu, BeiYu, andJiayaJia. Parametriccontrastivelearning. In\nICCV,pp.695â€“704.IEEE,2021.\n588\n589\nJiequan Cui, Shu Liu, Zhuotao Tian, Zhisheng Zhong, and Jiaya Jia. Reslt: Residual learning for\n590 long-tailedrecognition. IEEETrans.PatternAnal.Mach.Intell.,45(3):3695â€“3706,2023.\n591\n592 PrafullaDhariwalandAlexanderQuinnNichol. Diffusionmodelsbeatgansonimagesynthesis. In\n593 Marcâ€™AurelioRanzato,AlinaBeygelzimer,YannN.Dauphin,PercyLiang,andJenniferWortman\nVaughan(eds.),NeurIPS,pp.8780â€“8794,2021.\n11\n\nUnderreviewasaconferencepaperatICLR2025\n594\nIanJ.Goodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,\n595 AaronC.Courville,andYoshuaBengio. Generativeadversarialnetworks. CoRR,abs/1406.2661,\n596 2014.\n597\nSongHan,HuiziMao,andWilliamJDally. Deepcompression: Compressingdeepneuralnetworks\n598\nwithpruning,trainedquantizationandhuffmancoding. arXivpreprintarXiv:1510.00149,2015.\n599\n600 HaiboHeandEdwardoA.Garcia. Learningfromimbalanceddata. IEEETrans.Knowl.DataEng.,\n601 21(9):1263â€“1284,2009.\n602\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\n603\nGans trained by a two time-scale update rule converge to a local nash equilibrium. In Isabelle\n604\nGuyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vish-\n605\nwanathan,andRomanGarnett(eds.),NeurIPS,pp.6626â€“6637,2017.\n606\n607 JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. InNeurIPS,\n608 2020.\n609\nEdwardJ.Hu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,\n610\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR. OpenRe-\n611\nview.net,2022.\n612\n613 ZiyuJiang,TianlongChen,BobakJ.Mortazavi,andZhangyangWang. Self-damagingcontrastive\n614 learning. InMarinaMeilaandTongZhang(eds.),ICML,volume139ofProceedingsofMachine\n615\nLearningResearch,pp.4927â€“4939.PMLR,2021.\n616\nTeroKarras, TimoAila, SamuliLaine, andJaakkoLehtinen. Progressivegrowingofgansforim-\n617 provedquality,stability,andvariation. InICLR.OpenReview.net,2018.\n618\n619 TeroKarras,MiikaAittala,JanneHellsten,SamuliLaine,JaakkoLehtinen,andTimoAila.Training\ngenerative adversarial networks with limited data. In Hugo Larochelle, Marcâ€™Aurelio Ranzato,\n620\nRaiaHadsell,Maria-FlorinaBalcan,andHsuan-TienLin(eds.),NeurIPS,2020.\n621\n622 TeroKarras,MiikaAittala,TimoAila,andSamuliLaine. Elucidatingthedesignspaceofdiffusion-\n623 basedgenerativemodels.InSanmiKoyejo,S.Mohamed,A.Agarwal,DanielleBelgrave,K.Cho,\n624 andA.Oh(eds.),NeurIPS,2022.\n625\nDiederikP.KingmaandJimmyBa.Adam:Amethodforstochasticoptimization.InYoshuaBengio\n626\nandYannLeCun(eds.),ICLR,2015.\n627\n628 DiederikPKingma,TimSalimans,BenPoole,andJonathanHo. Ondensityestimationwithdiffu-\n629 sionmodels. InA.Beygelzimer,Y.Dauphin,P.Liang,andJ.WortmanVaughan(eds.),NeurIPS,\n630 2021.\n631\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n632\n2009.\n633\n634 TuomasKynkaÂ¨aÂ¨nniemi,TeroKarras,SamuliLaine,JaakkoLehtinen,andTimoAila. Improvedpre-\n635 cisionandrecallmetricforassessinggenerativemodels. InHannaM.Wallach,HugoLarochelle,\n636 Alina Beygelzimer, Florence dâ€™AlcheÂ´-Buc, Emily B. Fox, and Roman Garnett (eds.), NeurIPS,\n637 pp.3929â€“3938,2019.\n638\nHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for\n639 efficientconvnets. InICLR.OpenReview.net,2017.\n640\n641 PeiyuanLiao,XiuyuLi,XihuiLiu,andKurtKeutzer. Theartbenchdataset: Benchmarkinggenera-\n642 tivemodelswithartworks. CoRR,abs/2206.11404,2022.\n643\nTsung-YiLin, PriyaGoyal, RossB.Girshick, KaimingHe, andPiotrDollaÂ´r. Focallossfordense\n644 objectdetection. InICCV,pp.2999â€“3007.IEEEComputerSociety,2017.\n645\n646 Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International\n647\nConference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net,2019.\n12\n\nUnderreviewasaconferencepaperatICLR2025\n648\nDhruv Mahajan, Ross B. Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan\n649 Li, AshwinBharambe, andLaurensvanderMaaten. Exploringthelimitsofweaklysupervised\n650 pretraining. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.),\n651 ECCV,volume11206,pp.185â€“201.Springer,2018.\n652\nAdityaKrishnaMenon,SadeepJayasumana,AnkitSinghRawat,HimanshuJain,AndreasVeit,and\n653\nSanjivKumar. Long-taillearningvialogitadjustment. InICLR.OpenReview.net,2021.\n654\n655 Eric J. Nunn, Pejman Khadivi, and Shadrokh Samavi. Compound frechet inception distance for\n656 qualityassessmentofGANcreatedimages. CoRR,abs/2106.08575,2021.\n657\nYiming Qin, Huangjie Zheng, Jiangchao Yao, Mingyuan Zhou, and Ya Zhang. Class-balancing\n658\ndiffusionmodels. InCVPR,pp.18434â€“18443.IEEE,2023.\n659\n660 Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n661 conditionalimagegenerationwithCLIPlatents. CoRR,abs/2204.06125,2022.\n662\nHarsh Rangwani, Konda Reddy Mopuri, and R. Venkatesh Babu. Class balancing GAN with a\n663\nclassifier in the loop. In Cassio P. de Campos, Marloes H. Maathuis, and Erik Quaeghebeur\n664\n(eds.), UAI, volume 161 of Proceedings of Machine Learning Research, pp. 1618â€“1627. AUAI\n665\nPress,2021.\n666\n667 HarshRangwani,NamanJaswani,TejanKarmali,VarunJampani,andR.VenkateshBabu. Improv-\n668 ing gans for long-tailed data through group spectral regularization. In Shai Avidan, Gabriel J.\nBrostow, Moustapha CisseÂ´, Giovanni Maria Farinella, and Tal Hassner (eds.), ECCV, volume\n669\n13675ofLectureNotesinComputerScience,pp.426â€“442.Springer,2022.\n670\n671 William J Reed. The pareto, zipf and other power laws. Economics Letters, 74(1):15â€“19, 2001.\n672 ISSN0165-1765.\n673\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjoÂ¨rn Ommer. High-\n674\nresolutionimagesynthesiswithlatentdiffusionmodels.InCVPR,pp.10674â€“10685.IEEE,2022.\n675\n676 OlafRonneberger,PhilippFischer,andThomasBrox. U-net: Convolutionalnetworksforbiomedi-\n677 calimagesegmentation. InNassirNavab,JoachimHornegger,WilliamM.WellsIII,andAlejan-\n678 droF.Frangi(eds.),MICCAI,volume9351ofLectureNotesinComputerScience,pp.234â€“241.\n679 Springer,2015.\n680\nTim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\n681\nImprovedtechniquesfortraininggans.InDanielD.Lee,MasashiSugiyama,UlrikevonLuxburg,\n682 IsabelleGuyon,andRomanGarnett(eds.),NeurIPS,pp.2226â€“2234,2016.\n683\n684 Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsu-\npervised learning using nonequilibrium thermodynamics. In Francis R. Bach and David M.\n685\nBlei(eds.),ICML,volume37ofJMLRWorkshopandConferenceProceedings,pp.2256â€“2265.\n686\nJMLR.org,2015.\n687\n688 JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. InICLR,\n689 VirtualEvent,Austria,May3-7,2021.OpenReview.net,2021a.\n690\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and\n691\nBenPoole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. InICLR.\n692\nOpenReview.net,2021b.\n693\n694 Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Re-\n695 thinkingtheinceptionarchitectureforcomputervision.InCVPR,pp.2818â€“2826.IEEEComputer\n696 Society,2016.\n697\nJianfengWang,ThomasLukasiewicz,XiaolinHu,JianfeiCai,andZhenghuaXu. RSG:Asimple\n698 buteffectivemoduleforlearningimbalanceddatasets.InCVPR,pp.3784â€“3793.ComputerVision\n699 Foundation/IEEE,2021a.\n700\n701 XudongWang,LongLian,ZhongqiMiao,ZiweiLiu,andStellaX.Yu. Long-tailedrecognitionby\nroutingdiversedistribution-awareexperts. InICLR.OpenReview.net,2021b.\n13\n\nUnderreviewasaconferencepaperatICLR2025\n702\nDivinYan,LuQi,VincentTaoHu,Ming-HsuanYang,andMengTang. Trainingclass-imbalanced\n703 diffusionmodelviaoverlapoptimization,2024.\n704\n705 SergeyZagoruykoandNikosKomodakis. Wideresidualnetworks. InRichardC.Wilson,EdwinR.\nHancock,andWilliamA.P.Smith(eds.),BMVC.BMVAPress,2016.\n706\n707\nTianjiaoZhang,HuangjieZheng,JiangchaoYao,XiangfengWang,MingyuanZhou,YaZhang,and\n708 YanfengWang. Long-taileddiffusionmodelswithorientedcalibration. InICLR,2024.\n709\n710 YifanZhang,BryanHooi,LanqingHong,andJiashiFeng. Self-supervisedaggregationofdiverse\nexperts for test-agnostic long-tailed recognition. In Sanmi Koyejo, S. Mohamed, A. Agarwal,\n711\nDanielleBelgrave,K.Cho,andA.Oh(eds.),NeurIPS,2022.\n712\n713 YifanZhang,BingyiKang,BryanHooi,ShuichengYan,andJiashiFeng.Deeplong-tailedlearning:\n714 Asurvey. IEEETrans.PatternAnal.Mach.Intell.,2023.\n715\nZhihan Zhou, Jiangchao Yao, Feng Hong, Ya Zhang, Bo Han, and Yanfeng Wang. Combating\n716\nrepresentationlearningdisparitywithgeometricharmonization. InAliceOh,TristanNaumann,\n717\nAmirGloberson,KateSaenko,MoritzHardt,andSergeyLevine(eds.),NeurIPS,2023.\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n14\n\nUnderreviewasaconferencepaperatICLR2025\n756 Algorithm1AlgorithmofCALL\n757\nâ–·Training,takeDDPMasbase,sample-wise\n758\n759 Initialize: Î¸g = {W 1g,W 2g,... },Î¸e = {B 1eAe 1,B 2eAe 2,...\n}\nrepeat\n760\nSampledata(x,y)\n761 âˆˆD\nSampleatimestept Uniform( 1,...,T )\n762 âˆ¼ { }\nSampleanoiseÏµ (0,I)\n7 7 76 6 63 4\n5\nTB C aa a ks p ee ac glo i rts ays d: a ielL l nob tca dase eti so= câˆ¼ n enâˆ¥ lN o tÏµ sÎ¸ osg n:âŠ• LÎ¸e\nC\nÎ¸(\nA\ngâˆš\n,L\nÎ¸Î± eLÂ¯ (t =x b+ ( aÏ‰ se( Cy1 +onâˆ’ Î»âˆ’Î±Â¯ t CÏ‰) ADyÏµ, Livt L), âˆ¥y )Ïµ) Î¸âˆ’ gâŠ•Ïµ Î¸âˆ¥ e2 2 (x t,t,y) âˆ’Ïµ Î¸g(x t,t,y) âˆ¥2 2.\n766 untilconverged âˆ‡ L L\n767 â–·Sampling,takeDDPMforexample,sample-wise\n768 MergemodelparametersasÎ¸ =Î¸g Î¸e\n769 Samplex (0,I) âŠ•\nT\n770 fort=T,..âˆ¼ .,N 1do\n771 z (0,I)ift>1,elsez =0\nâˆ¼N\n7 77 72\n3\nenx dt fâˆ’ o1 r= âˆš1 Î±t(x t\nâˆ’\nâˆš 1Î² âˆ’t Î±Â¯tÏµ Î¸(x t,t,y))+Ïƒ tz\n774 return x\n0\n775\n776\n777\nDDPM\n778\n779\n780\nCALL\n781\n782\n783 Real\n784 Images\n785\n786\n787 Figure 6: The visualization of generated images on Imb. CelebA-HQ with imbalance ratio IR =\n788 100. Thefigureshowcasesthegeneratedoutputsfortheclassâ€œMaleâ€,whichistheminorityclass,\n789 frombothDDPMandCALL.Thelastrowdisplaysrealimagesfromthedatasetforreference. Itis\n790 evidentthatCALLgeneratesgeneratesmorerealisticanddiversefaces.\n791\n792\nA ALGORITHM\n793\n794\nWesummarizetheprocedureofourCALLinAlgorithm1,whereweuseDDPMasthebaseloss,\n795\nemployDDPMforsampling,andillustratetheprocessinasample-wisemannerasanexample.\n796\n797\n798 B MORE VISUALIZATION\n799\n800 Thegeneratedimagesforonethemediumclassesâ€œsurrealismâ€onImb.ArtBench-10withIR=100\n801 are shown in Figure 7. It is evident that the generated styles of CALL are much closer to the\n802 real images. More visualization of generation results with CALL are presented in Figure 8 (Imb.\nCIFAR-100,IR=100),Figure9(Imb. CelebA-HQ,IR=100),andFigure10(Imb. ArtBench-10,\n803\nIR=100).\n804\n805\n806\n807\n808\n809\n15\n\nUnderreviewasaconferencepaperatICLR2025\n810\n811\n812\nDDPM\n813\n814\n815\nCALL\n816\n817\n818 Real\nImages\n819\n820\n821\n822 Figure7: ThevisualizationofgeneratedimagesonImb. ArtBench-10withimbalanceratioIR =\n823 100. The figure showcases the generated outputs for the class â€œSurrealismâ€, which is one of the\n824 mediumclasses, frombothDDPMandCALL.Thelastrowdisplaysrealimagesfromthedataset\n825 forreference.ItisevidentthatCALLgeneratesresultsthataresignificantlymorestylisticallycloser\n826 totherealimagescomparedtoDDPM.Theimagesshownarerandomlyselected.\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\nFigure8: VisualizationofgenerationresultsonImb. CIFAR-100(IR=100)withCALL.\n863\n16\n\nUnderreviewasaconferencepaperatICLR2025\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907 Figure9: VisualizationofgenerationresultsonImb. CelebA-HQ(IR=100)withCALL.\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n17\n\nUnderreviewasaconferencepaperatICLR2025\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n948\n949\n950\n951\n952\n953\n954\n955\n956\n957\n958\n959\n960\n961 Figure10: VisualizationofgenerationresultsonImb. ArtBench-10(IR=100)withCALL.\n962\n963\n964\n965\n966\n967\n968\n969\n970\n971\n18\n\n",
    "total_pages": 18,
    "pages": [
      {
        "page": 1,
        "content": "UnderreviewasaconferencepaperatICLR2025\n000 PROTECTING MINORITIES IN DIFFUSION MODELS\n001\n002 VIA CAPACITY ALLOCATION\n003\n004\nAnonymousauthors\n005\nPaperunderdouble-blindreview\n006\n007\n008\n009 ABSTRACT\n010\n011 Diffusionmodelshaveadvancedquicklyinimagegeneration. However,theirper-\n012 formancedeclinessignificantlyontheimbalanceddatacommonlyencounteredin\n013 real-world scenarios. Current research on imbalanced diffusion models focuses\n014 onimprovingtheobjectivefunctiontofacilitateknowledgetransferbetweenma-\n015 joritiesandminorities,therebyenhancingthegenerationofminoritysamples. In\nthispaper,wemakethefirstattempttoaddresstheimbalanceddatachallengesin\n016\ndiffusionmodelsfromtheperspectiveofmodelcapacity. Specifically,majorities\n017\noccupy most of the model capacity because of their larger representation, con-\n018\nsequently restricting the capacity available for minority classes. To tackle this\n019\nchallenge, we propose Protecting Minorities via Capacity ALLocation (CALL).\n020\nWe reserve capacity for minority expertise by low-rank decomposing the model\n021 parameters and allocate the corresponding knowledge to the reserved model ca-\n022 pacitythroughacapacityallocationlossfunction. Extensiveexperimentsdemon-\n023 stratethatourmethod,whichisorthogonaltoexistingmethods,consistentlyand\n024 significantlyimprovestherobustnessofdiffusionmodelsonimbalanceddata.\n025\n026\n027\n1 INTRODUCTION\n028\n029 Inrecentyears, diffusionmodelshavedemonstratedexceptionalpotentialandeffectivenessinim-\n030 age generation, leading to increasing adoption by both industry and individuals (Ho et al., 2020;\nSong et al., 2021b; Dhariwal & Nichol, 2021). Diffusion model-based products such as DALL-E\n031\n2 (Ramesh et al., 2022) and the open-source Stable Diffusion (SD) (Rombach et al., 2022) have\n032\ndrawn millions of users, with numbers continuing to rise. However, recent studies reveal that dif-\n033\nfusion models suffer from significant performance degradation when trained on class-imbalanced\n034\ndatasets (Qinetal.,2023;Zhangetal.,2024),whichisparticularlyconcerninggiventheprevalence\n035\noftheimbalancenatureinthereal-worldscenarios(Reed,2001;Zhangetal.,2023).\n036\nCurrentresearchonimbalancedlearningprimarilyfocusesonimprovingtherobustnessofdiscrim-\n037\ninative models (Buda et al., 2018; He & Garcia, 2009; Wang et al., 2021a; Menon et al., 2021;\n038\nCuietal.,2021)orgenerativeadversarialnetworks(GANs)(Rangwanietal.,2021;2022)toclass\n039\nimbalance. However, most of them cannot be directly applied to diffusion models due to the sig-\n040\nnificantlydifferentmodelstructuresandtrainingandinferenceprocesses. Forimbalanceddiffusion\n041\nmodels,existingeffortsattempttoenhancetherobustnesstoimbalanceddistributionsbyimproving\n042 the objective function. Class Balancing Diffusion Models (CBDM) (Qin et al., 2023) introduced\n043 a loss function regularizer that implicitly encourages generated images to follow a balanced prior\n044 distributionateachsamplingstep. Yanetal.(2024)designedacontrastivelearningregularization\n045 to enhance inter-class separability. Oriented Calibration (OC) (Zhang et al., 2024) enhanced the\n046 generationqualityofminoritiesthroughknowledgetransferbetweenmajoritiesandminorities.\n047\nInthispaper, whileexistingeffortshaveprimarilyfocusedontheobjectivefunction, weapproach\n048 the challenges of class-imbalanced diffusion models from a new perspective: model capacity. In\n049 scenarios with significant class imbalance, majority classes dominate most of the model capacity\n050 duetotheirlargerrepresentation,squeezingthecapacityavailableforminorityclasses.Asshownin\n051 Figure1(a),minorityclassesexperienceamorepronouncedchangeinlossbeforeandafterpruning\n052 thetrainedmodel. Thisbehaviorindicatesthatminorityclassesutilizelessofthemodelâ€™scapacity,\n053 making them more vulnerable to pruning. We aim to enhance the robustness of diffusion models\nagainstimbalanceddatabysafeguardingthemodelcapacityforminorities.\n1"
      },
      {
        "page": 2,
        "content": "UnderreviewasaconferencepaperatICLR2025\n054\nToaddressthechallengeofmodelcapacityencroachment,weproposeanewmethodforimbalanced\n055 diffusionmodels: ProtectingMinoritiesviaCapacityALLocation(CALL).Ourcoreconceptisto\n056 allocatededicatedmodelcapacityforminorityexpertise,reservedinadvancetopreventencroach-\n057 mentbymajorities,therebysafeguardingthetrainingprocessofminoritysamples. Specifically,we\n058 firstdecomposethemodelparametersintotwopartsusinglow-ranktechniques:oneformajorityand\n059 generalknowledge, andtheotherreservedforminorityexpertise(Eq.(3)). Byintroducingtheca-\n060 pacityallocationloss(Eq.(4)),weeffectivelyallocatethecorrespondingknowledgetothereserved\nmodelcapacityduringtraining. Duetothenatureoflow-rankparameterdecompositionandaggre-\n061\ngation, thecapacityallocationdoesnotintroduceadditionalinferencelatency, whichiscrucialfor\n062\nreal-worlddeploymentofdiffusionmodels. Additionally,CALLisorthogonaltoexistingmethods\n063\nandcanbecombinedtoachievefurtherimprovements. Thecontributionsaresummarizedas:\n064\n065 â€¢ Weexplorethechallengeofimbalanceddiffusionmodelsfromanewperspective:modelcapacity.\n066 Wehighlightthatthekeyliesinprotectingthemodelcapacityallocatedtominorities, settingit\n067 apartfromexistingeffortsfocusingonimprovingtheobjectivefunctiontoenhanceminorities.\n068 â€¢ Totackletheissueofmajoritiesencroachingonthemodelcapacityrequiredforminorities,pro-\n069 poseanovelmethod,CALL,whichprotectsminoritiesbyreservingmodelcapacityforminority\n070 expertise and effectively allocating the corresponding knowledge during training. CALL is or-\n071 thogonaltoexistingmethods,allowingforcomplementaryintegration.\n072 â€¢ Weconductextensiveexperimentstoshowcasethesuperiorityofourmethod,CALL,inenhanc-\n073 ingtherobustnessofdiffusionmodelsagainstimbalanceddataacrossvarioussettings,including\n074 trainingdiffusionmodelsfromscratchandfine-tuningpre-trainedStableDiffusion.\n075\n076 2 RELATED WORK\n077\n078 Diffusion Models. Diffusion models, a powerful class of generative models, are originally in-\n079 spiredbynon-equilibriumthermodynamics(Sohl-Dicksteinetal.,2015)andarenowsuccessfully\n080 appliedtoimagegeneration(Hoetal.,2020),showingremarkablyeffectiveperformance(Dhariwal\n081 &Nichol,2021;Rombachetal.,2022). Hoetal.(2020)conductthetrainingofdiffusionmodels\n082 using a weighted variational bound. (Song et al., 2021b) propose an alternative method for con-\n083 structing diffusion models by using a stochastic differential equation (SDE). Karras et al. (2022)\n084 introduce a design space that clearly outlines the key design choices in previous works. Denois-\ningdiffusionimplicitmodels(DDIMs)(Songetal.,2021a)employsanalternativenon-Markovian\n085\ngenerationprocess,enablingfastersamplingfordiffusionmodels.\n086\n087 Imbalanced Generation. Several works have investigated imbalanced generation based on gen-\n088 erativeadversarialnetworks(GANs)(Goodfellowetal.,2014). CB-GAN(Rangwanietal.,2021)\n089 mitigates class imbalance during training by using a pre-trained classifier. Rangwani et al. (2022)\n090 notethatperformancedeclineinlong-tailedgenerationmainlyresultsfromclass-specificmodecol-\nlapse in minority classes, which is linked to the spectral explosion of the conditioning parameter\n091\nmatrix. To address this, they propose a corresponding group spectral regularizer. With diffusion\n092\nmodelsdemonstratingexceptionalgenerativecapabilities,recentworkhasbeguntoexploretraining\n093\na robust diffusion model from imbalanced data. CBDM (Qin et al., 2023) employs a distribution\n094\nadjustmentregularizerduringtrainingtoaugmenttheminorities. Yanetal.(2024)introduceacon-\n095\ntrastive learning regularization loss to strengthen the minorities. OC (Zhang et al., 2024) utilizes\n096 transferlearningbetweenmajoritiesandminoritiestoenhancethequalityofminoritygeneration.\n097\n098\n3 PRELIMINARIES\n099\n100\n3.1 PROBLEMFORMULATION\n101\n102 Let and = 1,2,...,C betheimagespaceandtheclassspace,whereC representstheclass\n103 numX ber. AY n imb{ alanced trai} ning set can be denoted as = (xn,yn) N ( , )N, where\nD { }n=1 âˆˆ X Y\n104 N is the size of the training set. The sample number N of each class c in the descending\nc\nâˆˆ Y\n105 orderexhibitsalong-taileddistribution. Thegoalistolearnagenerativediffusionmodelp Î¸(xy),\n|\n106 parameterizedbyÎ¸ fromtheimbalancedtrainingset ,capableofgeneratingrealisticanddiverse\nD\n107 samplesacrossallclasses. Forunconditionalgenerationusingp Î¸(xy), theclassconditioncanbe\n|\nsettoNull,resultinginp (x)=p (xNull).\nÎ¸ Î¸\n|\n2"
      },
      {
        "page": 3,
        "content": "UnderreviewasaconferencepaperatICLR2025\n108\n3.2 DIFFUSIONMODELS\n109\n110 Webrieflyreviewdiscrete-timediffusionmodels,specificallydenoisingdiffusionprobabilisticmod-\n111 els(DDPMs)(Hoetal.,2020). Givenarandomvariablex andaforwarddiffusionprocesson\n112 xdefinedasx := x ,...,x withT\nN+,theMarkovâˆˆ tX\nransitionprobabilityfromx tox\n1:T 1 T tâˆ’1 t\n113 is q(x t |x tâˆ’1) = N(x t;âˆš1 âˆ’Î² tx tâˆ’1,Î² tIâˆˆ ), where x 0 := x âˆ¼ q(x 0), and {Î² t }T t=1 is the variance\n114 schedule. Theforwardprocessallowsustosamplex t atanarbitrarytimesteptdirectlyfromx 0 in\n115 aclosedformq(x t |x 0)= N(x t;âˆšÎ±Â¯ tx 0,(1 âˆ’Î±Â¯ t)I),whereÎ± t :=1 âˆ’Î² t andÎ±Â¯ t\n:=(cid:81)t\ni=1Î± i. The\n116 variancescheduleisprescribedsuchthatx T isnearlyanisotropicGaussiandistribution.\n117 Training objective. The reverse process for DDPMs is defined as a Markov chain that aims\n118 to approximate q(x ) by gradually denoising from the standard Gaussian distribution p(x ) =\n0 T\n119 N(x T;0,I): p Î¸(x\ntâˆ’1\n|x t) = N(p Î¸(x tâˆ’1;Âµ Î¸(x t,t),Ïƒ t2I), where Âµ Î¸(x t,t) = âˆš1 Î±t(x\nt\nâˆ’\n1 12 20\n1\nâˆš 1Î² âˆ’t Î±Â¯tÏµ Î¸(x t,t)) is parameterized by a time-conditioned noise prediction network Ïµ Î¸(x t,t) and\nÏƒ ,...,Ïƒ are time dependent constants that can be predefined or analytically computed (Bao\n1 T\n122\net al., 2022). The reverse process can be learned by optimizing the variational lower bound on\n123 log-likelihoodas\n124\n125 logp (x)â‰¥E [âˆ’D (q(x |x )âˆ¥p(x ))+logp (x |x )âˆ’(cid:88) D (q(x |x ,x )âˆ¥p (x |x ))]\nÎ¸ q KL T 0 T Î¸ 0 1 KL tâˆ’1 t 0 Î¸ tâˆ’1 t\n126\nt>1\n127 =âˆ’E Ïµ,t[w tâˆ¥Ïµ Î¸(x t,t)âˆ’Ïµâˆ¥2 2]+C 1, (1)\n128\n129 where Ïµ (Ïµ;0,1), x = âˆšÎ±Â¯ x + âˆš1 Î±Â¯ Ïµ according to the forward process, w =\nt t 0 t t\n130 Î²2 âˆ¼ N âˆ’\n131 2Ïƒ t2Î±t(1t âˆ’Î±Â¯t)), and C 1 is typically small and can be dropped (Ho et al., 2020; Song et al., 2021b).\n132 Theterm LDiff(x,Î¸) = E Ïµ,t[w t âˆ¥Ïµ Î¸(x t,t) âˆ’Ïµ âˆ¥2 2]iscalledthediffusionloss(Kingmaetal.,2021).\nTobenefitsamplequality,Hoetal.(2020)applyasimplifiedtrainingobjectivebysettingw =1.\n133 t\n134 Class-conditional diffusion models. When the class labels of the training set are available, the\n135 class-conditional diffusion model p (xy) can be parameterized by Ïµ(x ,t,y). And the uncondi-\nÎ¸ t\n|\n136 tionaldiffusionmodelp Î¸(x)canbeviewedasaspecialcasewithanullconditionÏµ(x t,t,Null). A\n137 similarlowerboundontheclass-conditionallog-likelihoodtoEq.(1)is\n138\n139 logp Î¸(x |y) â‰¥âˆ’E Ïµ,t[w t âˆ¥Ïµ Î¸(x t,t,y) âˆ’Ïµ âˆ¥2 2]+C 2, (2)\n140\n141 whereC 2 isanothersmallconstantandcanbedropped(Hoetal.,2020;Songetal.,2021b). The\nclass-conditionaldiffusionlosscanbewrittenas (x,y,Î¸)=E [w Ïµ (x ,t,y) Ïµ 2].\n142 LDiff Ïµ,t t âˆ¥ Î¸ t âˆ’ âˆ¥2\n143\n144\n4 METHOD\n145\n146\n147 4.1 MOTIVATION\n148\n149 Although diffusion models have demonstrated significant advantages in terms of fidelity and di-\n150 versity in generation, most existing diffusion models implicitly assume that the training data is\napproximately uniformly distributed across classes. When training data exhibits real-world class\n151\nimbalance, diffusion models struggle to generate high-quality and diverse samples for the minori-\n152\nties (Qin et al., 2023; Yan et al., 2024; Zhang et al., 2024). Current efforts focus on adjusting the\n153\nobjectivetogivemoreattentiontominorityclasses,improvingtherobustnessofdiffusionmodelsto\n154\nimbalanceddistributions. Wetackletherobustnesschallengeofimbalanceddistributionsfromthe\n155 newperspectiveofmodelcapacity. Majoritiestakeupmostofthemodelcapacityduetoquantity\n156 dominance,leavingminoritieswithlimitedcapacityandpoorperformance. InFigure1(a),weshow\n157 thesamplesizeforeachclassandthelosschangeafterthemodelpruning(Hanetal.,2015;Lietal.,\n158 2017)operation. Itisclearthatpruninghasagreaterimpactontheoutputofminorities,indicating\n159 thatminorityclassesoccupylessmodelcapacityandarethereforelessrobusttopruning.Jiangetal.\n160 (2021)alsodiscussasimilarphenomenoninimbalanceddiscriminativemodels. Ifwecanreserve\n161 andallocateaportionofthemodelcapacityspecificallyforminorities,wecanpreventtheadverse\neffectsofcapacitydominationandimprovetherobustnesstoimbalanceddistributions.\n3"
      },
      {
        "page": 4,
        "content": "UnderreviewasaconferencepaperatICLR2025\n162\n(a)Motivation (b)CapacityReservation (c)CapacityAllocation\n163\n164\nğœ½ğ’ˆ\n165\n166\n167 â„’â€™()) C Don ivs eis rt se en ft of ror mm ina oj ro ir tii eti ses (â„’(â„’ $%! &\" )#)\n168\n169\nğœ½= ğœ½ğ’ˆ â¨ ğœ½ğ’†\n170\n171 â„’#$%&\n172\n173 Figure 1: (a) The class distribution of training data in Imb. CIFAR-100 with imbalance ratio of\n174 IR = 100,alongwiththeaveragelossvaluechangesperclassbeforeandafterpruningtheDDPM\n175 model trained on it. The x-axis shows classes arranged in descending order of sample size. The\n176 pruning rate is set to 0.1. The images are for illustration purposes only. (b) An illustration of\n177 thecapacityreservationpartofourmethod, CALL.(c)AnillustrationofhowCALLallocatesthe\ncorrespondingknowledgetothereservedmodelcapacityduringtraining.\n178\n179\n180\n4.2 PROTECTINGMINORITIESVIACAPACITYALLOCATION\n181\n182 4.2.1 CAPACITYRESERVATION\n183\nToallocatesufficientmodelcapacityforminorities,wefirstneedtoexplicitlypartitionthemodelca-\n184\npacity. HereweachievethispurposebyatechniquesimilartoLow-RankAdaptations(LoRAs)(Hu\n185\netal.,2022), whichhasdemonstratedexcellentperformanceandversatilityinthefieldofefficient\n186\nfine-tuning.Whileourtaskandgoaldiffer,weapplyitslow-rankdecompositionconcepttopartition\n187\nthemodelcapacity. ForadiffusionmodelparameterizedbyÎ¸ = W ,W ,... ,whereeachW Î¸\n1 2\n188 representsaparametermatrixinthenetwork,wedecomposeany{ W RdÃ—k a} s âˆˆ\n189 âˆˆ\n190 W =Wg+BA=Wg+We, W Î¸, (3)\nâˆ€ âˆˆ\n191 whereWg RdÃ—krepresentsthepartofW toberetainedformajoritiesandgeneralizedknowledge,\n192 We = BAâˆˆ RdÃ—k represents the part to be allocated to the expertise of minorities, B RdÃ—r,\n193 A RrÃ—k,aâˆˆ ndtherankr < min(d,k). FromEq.(3),wedecomposeÎ¸ intoÎ¸g = Wg,Wâˆˆg,...\n194 andâˆˆ Î¸e = We,We,... andmergethembyÎ¸ =Î¸g Î¸e,where meanstheelement{ -wi1 sead2 dition} .\n{ 1 2 } âŠ• âŠ•\n195 AnillustrationofCapacityReservationisshowninFigure1(b).\n196\n197 4.2.2 CAPACITYALLOCATION\n198\nWiththemodelparametersdecomposedasÎ¸ =Î¸g Î¸e,ourgoalduringtrainingistostoreminority\n199 expertise in Î¸e and general knowledge in Î¸g, ensuâŠ• ring protection for minorities through capacity\n200\nallocation. Toachievethis,thediffusionmodelp Î¸(xy) = p Î¸gâŠ•Î¸e(xy)shouldperformwellonall\n201 samples, both majorities and minorities. Meanwhile| , p Î¸g(xy) shou| ld perform well on majorities\n202 butpoorlyonminorities,asÎ¸g isnotintendedtolearnthemi| norityexpertise.\n203\nCapacity allocation loss. For Î¸ = Î¸g Î¸e, we use a loss function ( ,Î¸) that balances\n204 âŠ• Lbase D\nperformance across majorities and minorities. This is not our primary focus, so we directly adopt\n205\nthe loss functions from existing imbalanced diffusion models, e.g., Zhang et al. (2024); Qin et al.\n206 (2023),as . Forimbalanceddata,weproposeacapacityallocationloss,whichencouragesÎ¸e\nbase\n207 tolearnmiL norityexpertiseandÎ¸g tocapturegeneralknowledge:\n208\n209 Capacityallocationloss: CALL(x,y,Î¸g,Î¸e)= Con(x,y,Î¸g,Î¸e)+ Div(x,y,Î¸g,Î¸e),\nL L L\n210 Consistencyloss: LCon(x,y,Î¸g,Î¸e)=Ï‰ Cy onE\nt\nâˆ¥Ïµ Î¸gâŠ•Î¸e(x t,t,y) âˆ’Ïµ Î¸g(x t,t,y) âˆ¥2 2, (4)\n2 21 11\n2\nDiversityloss: LDiv(x,y,Î¸g,Î¸e)= âˆ’Ï‰ Dy ivE\nt\nâˆ¥Ïµ Î¸gâŠ•Î¸e(x t,t,y) âˆ’Ïµ Î¸g(x t,t,y) âˆ¥2 2.\n213 WevarytheconsistencyclassweightÏ‰ Con andthediversityclassweightÏ‰ Div appliedtodifferent\n214 classes. Forclassc withN c instances,alargerN c (majorities)resultsinahigherconsistency\n215 class weight Ï‰ Cc on, âˆˆ leaY ding to more consistent outputs between Ïµ Î¸gâŠ•Î¸e(x t,t,y) and Ïµ Î¸g(x t,t,y).\nConversely,forthediversityclassweight,asmallerN (minorities)resultsinahigherÏ‰c ,leading\nc Div\n4"
      },
      {
        "page": 5,
        "content": "UnderreviewasaconferencepaperatICLR2025\n216 tomorediverseoutputsbetweenÏµ Î¸gâŠ•Î¸e(x t,t,y)andÏµ Î¸g(x t,t,y).Thus,p Î¸g(xy)excelsonmajori-\n217 ties,asitsoutputalignswithÎ¸,butunderperformsonminoritiesduetothediv| ergencebetweenthe\n218 outputsofÎ¸g andÎ¸. Specifically,\n219\n220 Ï‰y = CN y , Ï‰y = C . (5)\n221 Con (cid:80)C N Div N (cid:80)C 1\n222\nc=1 c y c=1 Nc\n223 HereÏ‰ scaleslinearlywithclasssamplesize,whileÏ‰ isinverselyproportionaltoclasssample\nCon Div\n224 size,ensuringÏ‰ =Ï‰ =1, (x,y,Î¸g,Î¸e)=0forabalancedtrainingset.\nCon Div CALL\n225 L\nJointoptimization. ForÎ¸ = Î¸g Î¸e,weoptimizethebaseloss andthecapacityallocation\n226 âŠ• Lbase\nloss ,weightedbyhyperparameterÎ»:\n227 LCALL\n228\n(cid:88) 1\n229 min Total( ,Î¸)= base( ,Î¸)+Î» CALL(x,y,Î¸g,Î¸e), (6)\nÎ¸ L D L D NL\n230 (x,y)âˆˆD\n231\n232 where the base loss base optimizes Î¸ for both majorities and minorities, while the capacity allo-\nL\ncation loss acts as a regularizer to allocate capacity and protect minorities. This guides Î¸\n233 CALL\nL\ntoward more balanced and effective model weights. An illustration of the training process of our\n234\nCALLispresentedinFigure1(c).\n235\n236 Inference. For inference, we can explicitly compute and store Î¸ = Î¸g Î¸e, and sample images\nâŠ•\n237 fromp Î¸(xy). Thus,ourmethoddoesnotincreasemodelcapacity,ensuringnoadditionalinference\n|\n238\nlatencycomparedtoastandarddiffusionmodel,whichiscrucialasinferencespeedisakeybottle-\nneckinreal-worlddeployment(Songetal.,2021a). ThisadvantagecomesfromusingaLoRA-like\n239\nparameterdecompositioninEq.(3)andexplicitlyaggregatingtheparametersduringinference.\n240\n241\n242 4.3 DISSCUSSION\n243\n244 Comparisonwithexistingimbalanceddiffusionmodels. UnlikecurrentmethodssuchasCBDM\n245 andOC,whichprioritizedesigningmoresuitableobjectivefunctionsforimbalanceddata,ourCALL\n246 improves the robustness of diffusion models to imbalanced distributions from a new perspective:\n247 allocatingmodelcapacitytoprotectminorities. CALLisorthogonalandcanbenefitfromimproved\n248 objectivefunctionstoachievefurtherenhancements(asshownempiricallyinTable5).\n249 Comparison with LoRA. While the capacity reservation mechanism in CALL shares a similar\n250 structurewithLoRA,ourgoalistodecomposeandallocatemodelcapacitypriortotraining,whereas\n251 LoRA is aimed at efficiently fine-tuning pre-trained models. Additionally, our method involves a\n252 jointtrainingstrategy,whereasLoRAfocusessolelyonoptimizingthelow-rankcomponents.\n253\nComparisonwithensemble-basedimbalancedclassificationmethods. Severalensemble-based\n254\nmethods (Cui et al., 2023; Wang et al., 2021b; Zhang et al., 2022) leverage multiple experts to\n255 capturediverseknowledge,achievingstrongperformanceinclassificationtasksthroughprediction\n256 ensemble. However, most of these methods are tailored for classification networks in terms of ar-\n257 chitecture, training paradigm, and loss functions, making them unsuitable for direct application in\n258 diffusionmodels. Whiletheyalsoinvolveknowledgeallocation,theirgainmainlycomesfromin-\n259 creasedcapacityandensemblepredictions. Additionally,theyoftenrequirestructuralmodifications\n260 to the network and incur higher inference latency, further limiting applicability. In contrast, our\nmethodintroducesnochangestonetworkstructure, doesnotincreasemodelcapacityorinference\n261\nlatency,andenhancesimbalanceddiffusionmodelspurelythroughcapacityallocation.\n262\n263 ExtensiontoLoRA-finetuning. OurmethodcanbeseamlesslyextendedtoLoRA-finetuningsce-\n264 nariosbymodifyingEq.(3)totheform: W = Wf +BgAg +BeAe. Here,Î¸f = Wf,Wf,...\n265 representsthefrozenpre-trainedmodelparameters,Î¸g = BgAg,BgAg,... denot{ est1 hetra2 inabl} e\n2 26 66\n7\nrp ea sr pa om ne dt ser ts oa tl hl eoc tra ate ind af bo lerm paa rj ao mri eti te es rsa rn ed seg re vn ee dra foli rze md ink on ro iw tyl{ e ed xg p1 e e, rt1 a isn ed .2 Î¸ Fe o2 r= W{B} 1e RA de 1 Ã—,B k,2e BA ge 2,.. R. } dÃ—co rgr-\n,\n268 Ag RrgÃ—k, Be RdÃ—re , Ae RreÃ—k, we have re < rg < min(d,k).âˆˆ During inferâˆˆ ence, the\n269 modâˆˆ elparametersaâˆˆ remergedbyÎ¸âˆˆ =Î¸f Î¸g Î¸e. ThisextensionpreservesthestructureofLoRA\nâŠ• âŠ•\nwhileenhancingthefine-tuningprocessbycapacityallocationforimbalanceddata.\n5"
      },
      {
        "page": 6,
        "content": "UnderreviewasaconferencepaperatICLR2025\n270\nTable1:FIDs( ),KIDs( ),Recalls( ),andISs( )ofCALLandvariousbaselinemethodsonImb.\n271 â†“ â†“ â†‘ â†‘\nCIFAR-10 and Imb. CIFAR-100 with different imbalance ratios IR = 100,50 . All results are\n272 reportedasMean Std. Bestandsecond-bestresultsarehighlighted. { }\n273 Â±\n274 Imb.CIFAR-10,IR=100\n275 Method FID KID Recall IS\nâ†“ â†“ â†‘ â†‘\n276 DDPM(Hoetal.,2020) 10.697 0.079 0.0035 0.0008 0.47 0.01 9.39 0.12\nÂ± Â± Â± Â±\n277 +ADA(Karrasetal.,2020) 9.266 0.133 0.0029 0.0003 0.49 0.02 9.26 0.14\nÂ± Â± Â± Â±\n278 +RS(Mahajanetal.,2018) 12.332 0.064 0.0037 0.0003 0.45 0.02 9.25 0.08\nÂ± Â± Â± Â±\n+Focal(Linetal.,2017) 10.842 0.134 0.0034 0.0001 0.46 0.03 9.42 0.18\n279\nCBDM(Qinetal.,2023) 8.233\nÂ±\n0.152\n0.0026Â±\n0.0001\n0.53Â±0.02 9.23Â±\n0.11\n280 OC(Zhangetal.,2024) 8.390Â± 0.063 0.0027Â± 0.0002 0.52Â± 0.03 9.53Â±0.12\n281 CALL\n7.727Â±0.124 0.0023Â±0.0001 0.53Â±0.01 9.52Â±\n0.10\nÂ± Â± Â± Â±\n282 Imb.CIFAR-10,IR=50\n283\nMethod FID KID Recall IS\n284 â†“ â†“ â†‘ â†‘\nDDPM(Hoetal.,2020) 10.216 0.138 0.0035 0.0002 0.47 0.03 9.37 0.13\n285 Â± Â± Â± Â±\n+ADA(Karrasetal.,2020) 9.132 0.215 0.0030 0.0002 0.51 0.04 9.28 0.21\n286 +RS(Mahajanetal.,2018) 11.231Â± 0.177 0.0038Â± 0.0002 0.47Â± 0.02 9.31Â± 0.14\nÂ± Â± Â± Â±\n287 +Focal(Linetal.,2017) 10.315 0.263 0.0034 0.0003 0.48 0.01 9.38 0.23\nCBDM(Qinetal.,2023) 7.933\nÂ±\n0.082\n0.0026Â±\n0.0002\n0.54Â±0.02 9.42Â±\n0.14\n288 Â± Â± Â± Â±\nOC(Zhangetal.,2024) 8.034 0.225 0.0027 0.0001 0.53 0.01 9.65 0.09\n289 CALL 7.372Â±0.125 0.0024Â±0.0002 0.54Â±0.01 9.69Â±0.09\nÂ± Â± Â± Â±\n290\nImb.CIFAR-100,IR=100\n291\nMethod FID KID Recall IS\n292 â†“ â†“ â†‘ â†‘\n293 DDPM(Hoetal.,2020) 10.163 0.077 0.0029 0.0005 0.46 0.01 13.45 0.15\nÂ± Â± Â± Â±\n+ADA(Karrasetal.,2020) 9.482 0.125 0.0032 0.0002 0.51 0.01 12.44 0.16\n294 Â± Â± Â± Â±\n+RS(Mahajanetal.,2018) 11.432 0.287 0.0038 0.0007 0.44 0.03 12.12 0.18\n295 +Focal(Linetal.,2017) 10.212Â± 0.110 0.0032Â± 0.0004 0.47Â± 0.02 13.07Â± 0.26\nÂ± Â± Â± Â±\n296 CBDM(Qinetal.,2023) 10.051 0.391 0.0036 0.0003 0.51 0.01 12.35 0.12\n297 CO AC L(Z Lhangetal.,2024) 8 7. .3 50 19 9Â±Â± 0 0. .2 13 33\n2\n0 0. .0 00 02 16 7Â± Â±0 0. .0 00 00 02\n3\n0 0. .5 52 2Â± Â±0 0. .0 02\n2\n1 13 3. .4 44 5Â±Â± 0 0. .2 20\n3\n298 Â± Â± Â± Â±\n299 Imb.CIFAR-100,IR=50\n300 Method FID KID Recall IS\nâ†“ â†“ â†‘ â†‘\n301 DDPM(Hoetal.,2020) 9.363 0.069 0.0032 0.0002 0.47 0.02 14.27 0.22\nÂ± Â± Â± Â±\n302 +ADA(Karrasetal.,2020) 8.927 0.138 0.0033 0.0001 0.51 0.02 12.89 0.17\nÂ± Â± Â± Â±\n+RS(Mahajanetal.,2018) 10.259 0.217 0.0037 0.0003 0.47 0.03 12.38 0.23\n303 Â± Â± Â± Â±\n+Focal(Linetal.,2017) 9.477 0.114 0.0034 0.0002 0.49 0.03 13.31 0.15\n304\nCBDM(Qinetal.,2023)\n8.946Â±\n0.178\n0.0036Â±\n0.0003\n0.55Â±0.02 12.59Â±\n0.19\n305 OC(Zhangetal.,2024) 7.188Â± 0.274 0.0024Â± 0.0002 0.54Â± 0.01 13.99Â± 0.22\n306 CALL\n6.732Â±0.052 0.0021Â±0.0001 0.55Â±0.03 14.12Â±\n0.15\nÂ± Â± Â± Â±\n307\n308\n5 EXPERIMENTS\n309\n310\n311\n5.1 EXPERIMENTALSETUP\n312\nDatasets. We conduct experiments on the imbalanced versions of commonly used datasets in the\n313\nfield of image synthesis, including CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky\n314 etal.,2009),CelebA-HQ(Karrasetal.,2018),andArtBench-10(Liaoetal.,2022). CIFAR-10and\n315 CIFAR-100havearesolutionof32 32,whileforCelebA-HQ,weusethe64 64version,andfor\nÃ— Ã—\n316 ArtBench-10,weusetheoriginalresolutionof256 256. WefollowCaoetal.(2019)toconstruct\nÃ—\n317 imbalancedversionsofthesedatasetsbydownsampling,resultinginanexponentialdecreaseinthe\n318 samplesizeofeachclasswithitsindex.WerefertotheseimbalanceddatasetsasImb.dataset,e.g.,,\n319 Imb.CIFAR-10.Wecontrolthelevelofimbalanceinthedatasetbysettingdifferentimbalanceratios\nIR 50,100 ,whereIRistheratioofthenumberofsamplesinthemostpopulousclasstothat\n320\nâˆˆ { }\n321 intheleastpopulousclass,definedasIR= m ma inx cc âˆˆâˆˆ YY NN cc. ForImb. CIFAR-10andImb. ArtBench-10,\n322 wedividethedatasetintothreesplits: Many(classes0-2),Medium(classes3-5),andFew(classes\n323 6-9)basedonclasssizesindescendingorder. Similarly,forImb. CIFAR-100,thesplitsareMany\n(classes0-32),Medium(classes33-65),andFew(classes66-99).\n6"
      },
      {
        "page": 7,
        "content": "UnderreviewasaconferencepaperatICLR2025\n324\nTable 2: FIDs ( ), KIDs ( ), and per-class FIDs ( ) of CALL and baselines on Imb. CelebA-HQ\n325 â†“ â†“ â†“\nwithdifferentimbalanceratiosIR= 100,50 .\n326 { }\nImb.CelebA-HQ,IR=100\n327\n328 Method FemaleFID MaleFID OverallFID KID\nâ†“ â†“ â†“ â†“\n329 DDPM(Hoetal.,2020) 7.143 0.147 16.425 0.032 8.727 0.126 0.0037 0.0001\nÂ± Â± Â± Â±\n330 CBDM(Qinetal.,2023) 7.043 0.079 14.273 0.183 7.823 0.115 0.0043 0.0002\nÂ± Â± Â± Â±\n331 OC(Zhangetal.,2024) 7.092 0.323 13.962 0.221 7.871 0.237 0.0034 0.0002\nÂ± Â± Â± Â±\nCALL 6.815 0.241 12.788 0.316 7.538 0.201 0.0033 0.0002\n332 Â± Â± Â± Â±\n333 Imb.CelebA-HQ,IR=50\n334 Method FemaleFID MaleFID OverallFID KID\nâ†“ â†“ â†“ â†“\n335\nDDPM(Hoetal.,2020) 7.348 0.219 14.808 0.152 8.007 0.265 0.0034 0.0002\n336 CBDM(Qinetal.,2023) 7.317Â± 0.273 12.592Â± 0.181 7.423Â± 0.139 0.0042Â± 0.0001\nÂ± Â± Â± Â±\n337 OC(Zhangetal.,2024) 7.283 0.226 12.938 0.277 7.438 0.247 0.0034 0.0003\nÂ± Â± Â± Â±\n338 CALL 7.147 0.182 11.273 0.146 7.193 0.282 0.0033 0.0002\nÂ± Â± Â± Â±\n339\n340 Baselines. We consider baselines including: (1) the base denoising diffusion probabilistic model\n341 (DDPM);(2)methodsspecificallytargetingimbalanceddiffusionmodels: theclass-balancingdif-\n342 fusionmodel(CBDM)(Qinetal.,2023)andOrientedCalibration(OC)(Zhangetal.,2024);(3)ap-\n343 plyingimbalancelearningmethodsfromdiscriminativemodelsorgenerativeadversarialnetworks\n344 (GANs)todiffusionmodels: re-sampling(RS)(Mahajanetal.,2018),adaptivediscriminatoraug-\n345 mentation(ADA)(Karrasetal.,2020),andfocalloss(Linetal.,2017). Notethatmanyimbalanced\nlearningmethodsfordiscriminativemodelsandGANsheavilyrelyonspecificmodelarchitectures\n346\nortrainingparadigms,e.g.,Menonetal.(2021);Zhouetal.(2023);Rangwanietal.(2022),making\n347\nthemincompatiblewithimbalanceddiffusionmodels.\n348\n349 Implementationdetails. FollowingHoetal.(2020),weutilizeaU-Net(Ronnebergeretal.,2015)\n350 basedonaWideResNet(Zagoruyko&Komodakis,2016)asthenoisepredictionnetwork. Weset\n351 thehyperparametersforDDPMasÎ² 1 =10âˆ’4 andÎ² T =0.02,withmaximumtimestepT =1000.\nThe Adam optimizer (Kingma & Ba, 2015) is used with betas = (0.9,0.999) and a learning rate\n352\nof 2 10âˆ’4. The dropout rate is set to 0.1. We use a batch size of 64 and train the model for\n353 Ã—\n300,000steps,includingawarm-upperiodof5,000steps. FortherankofBAinEq.(3),wefixitat\n354\n1 min(d,k). Weonlyapplythelow-rankdecompositiontotheupsamplingpartoftheU-Net,i.e.,\n355 10\nthelatterhalfofthemodel,astheshallowlayerstendtocapturemoregeneralknowledge(Alzubaidi\n356\netal.,2021). ForthehyperparameterÎ»inEq.(6),wefixitasÎ»=1. ForthebaselossinEq.(6),we\n357 adopttheobjectivefunctionfromZhangetal.(2024),unlessotherwisespecified. Duringinference,\n358 newimagesaregeneratedutilizingthe50-stepDDIMsolver(Songetal.,2021a).\n359\nMetrics. The performance of our method and all baselines is evaluated using the metrics Frechet\n360\nInceptionDistance(FID)(Heuseletal.,2017),KernelInceptionDistance(KID)(Binkowskietal.,\n361\n2018), Recall (KynkaÂ¨aÂ¨nniemi et al., 2019), and Inception Score (IS) (Salimans et al., 2016). All\n362\nmetricsarecalculatedbasedonfeaturesextractedfromapre-trainedInception-V3(Szegedyetal.,\n363 2016)model1. Duringevaluation,themetricsarecalculatedusingabalancedsetofrealimagesand\n364 50,000generatedimages. Themetricsforeach many,medium,few splitarecomputedusingthe\n{ }\n365 correspondingsplitâ€™srealimagesand20,000generatedimages.\n366\n367 5.2 MAINRESULTS\n368\n369 Performance on Imb. CIFAR-10 and Imb. CIFAR-100. In Table 1, we summarize the FIDs,\n370 KIDs, Recalls, ISs of our CALL and all baseline methods on Imb. CIFAR-10 and Imb. CIFAR-\n100 with different imbalance ratios IR = 50,100 . Our CALL achieves the best results on 16\n371\n{ }\nmetricsacrossallfoursettings,exceptfortwoslightlylowerISs. NotethatIScannotdetectmode\n372\ncollapse (Barratt & Sharma, 2018), e.g., if the generated minority samples are overwhelmed by\n373\nmajority characteristics, such low-quality images would not lead to a drop in IS, which explains\n374\nwhy vanilla DDPM still performs well on some ISs. Additionally, IS lacks a reference to real\n375\nimages, makingitgenerallyconsideredalessreliablemetric(Borji,2019;Nunnetal.,2021). On\n376\n377 1https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/\nweights-inception-2015-12-05-6726825d.pth\n7"
      },
      {
        "page": 8,
        "content": "UnderreviewasaconferencepaperatICLR2025\n378\nTable3:FIDs( ),KIDs( ),Recalls( ),andISs( )ofCALLandvariousbaselinemethodsonImb.\n379 â†“ â†“ â†‘ â†‘\nArtBench-10(imbalanceratiosIR= 100,50 )usingLoRAtofine-tuneStableDiffusion.\n380 { }\n381\nImb.ArtBench-10,IR=100\n382 Method FID KID Recall IS\nâ†“ â†“ â†‘ â†‘\n383 DDPM(Hoetal.,2020) 27.083 0.438 0.0142 0.0003 0.39 0.01 8.47 0.19\nÂ± Â± Â± Â±\n384 CBDM(Qinetal.,2023) 25.723 0.263 0.0122 0.0002 0.43 0.01 7.97 0.22\nÂ± Â± Â± Â±\n385 OC(Zhangetal.,2024) 24.315 0.162 0.0106 0.0005 0.42 0.01 8.71 0.20\nÂ± Â± Â± Â±\nCALL 22.776 0.078 0.0087 0.0002 00.44 0.02 8.71 0.18\n386 Â± Â± Â± Â±\n387 Imb.ArtBench-10,IR=50\n388 Method FID KID Recall IS\nâ†“ â†“ â†‘ â†‘\n389\nDDPM(Hoetal.,2020) 25.557 0.082 0.0134 0.0004 0.39 0.02 8.41 0.15\n390 CBDM(Qinetal.,2023) 24.487Â± 0.153 0.0114Â± 0.0002 0.43Â± 0.02 8.03Â± 0.23\n391 OC(Zhangetal.,2024) 23.287Â± 0.232 0.0097Â± 0.0003 0.43Â± 0.02 8.48Â± 0.17\nÂ± Â± Â± Â±\n392 CALL 21.733 0.153 0.0080 0.0002 0.44 0.01 8.51 0.23\nÂ± Â± Â± Â±\n393\n394\nthemostwidelyusedmetricFID,CALLachievesignificantimprovementsoverDDPMwithgains\n395\nof2.725,2.844,2.644,and2.571,andconsistentimprovementsoverthebestbaselineineachsetting\n396\nby0.506,0.561,0.790,and0.456,respectively.Forbaselinemethods,CBDMperformswellonImb.\n397\nCIFAR-10, while OC shows better results on Imb. CIFAR-100. DDPM + RS generally performs\n398 worse than DDPM. DDPM + ADA, although still weaker than specialized methods like CBDM\n399 andOC,demonstratesstableimprovementsoverDDPM,suggestingthepotentialofexploringdata\n400 augmentationtoaddresschallengesofimbalanceddataindiffusionmodels.DDPM+Focalachieves\n401 comparableresultstoDDPM,likelybecausethelossdifferencesbetweenclassesindiffusionmodels\n402 arelessdistinct,makingFocallosslesseffectiveforloss-basedhardexamplemining.\n403\n404\n20\n405\n18 406\n407 16\n408 14\n409 12\n410 10\nMany Medium Few\n411 Split\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n) (DIF â†“\nImb.CIFAR10,IR=100\n22.5\nDDPM\nCBDM 20.0\nOC\nCALL 17.5\n15.0\n12.5\n10.0\nMany Medium Few\nSplit\n) (DIF â†“\nMany/Medium/Few analysis. In\nFigure 2, we show the fine-grained Imb.CIFAR100,IR=100\nmany, medium, few per-split DDPM\n{ } CBDM FIDs of different methods on Imb.\nOC\nCIFAR-10 and Imb. CIFAR-100 CALL\nwith imbalance ratio IR = 100.\nOur method achieves the best re-\nsultsacrossallthreesplits, withthe\nprimary improvements observed in\nthe Medium and Few classes. It is\nFigure 2: Per-split FIDs of CALL and baselines on Imb.\nnoteworthythatonImb. CIFAR-10,\nCIFAR-10(IR=100)andImb. CIFAR-100(IR=100).\nthe generation quality for Medium\nclasses is worse than for Few classes. Similar observations have been made on imbalanced con-\ntrastive learning (Zhou et al., 2023). This could be attributed to the inherent difficulty differences\nbetween classes, suggesting a promising direction of addressing imbalanced diffusion models by\ncombininginherentdifficultyimbalancewithquantityimbalance.\nPerformance on Imb. CelebA-HQ. In Table 2, we report the FIDs, KIDs, and per-class FIDs of\nCALLandbaselinemethodsonImb. CelebA-HQwithdifferentimbalanceratiosIR = 100,50 .\n{ }\nImb. CelebA-HQ contains two classes: Female and Male, with Female being the majority class.\nOur CALL achieves the best performance across all eight metrics in both settings. Specifically, it\nimprovestheOverallFIDby1.189and0.814comparedtoDDPMandby0.285and0.230compared\nto the best baselines in each setting. For the minority class (Male), our method enhances FID\nby 3.637 and 3.535 over DDPM and by 1.174 and 1.319 over the best baselines. In Figure 6 in\nAppendix,weshowcasethegeneratedresultsfortheâ€œMaleâ€classwithimbalanceratioIR = 100.\nItisevidentthatourmethodgeneratesmorerealisticanddiversefaces.\nPerformanceofFine-tuningStableDiffusiononImb. ArtBench-10. OnImb. ArtBench-10,we\nfine-tunetheStableDiffusionmodel2(Rombachetal.,2022)byLoRA(Huetal.,2022)witharank\nof128. AndforÎ¸e,therankissetto8. Wetrainthemodelinaclass-conditionalmannerwherethe\ntextualpromptissimplysetasâ€œa class paintingâ€suchasâ€œarenaissancepaintingâ€. Thedropout\n{ }\n2https://huggingface.co/lambdalabs/miniSD-diffusers\n8"
      },
      {
        "page": 9,
        "content": "UnderreviewasaconferencepaperatICLR2025\n432\n433\nDDPM\n434\n435\n436\nCALL\n437\n438\n439 Real\nImages\n440\n441\n442\nFigure 3: The visualization of generated images on Imb. ArtBench-10 with imbalance ratio\n443\nIR = 100. The figure showcases the generated outputs for the class â€œRealismâ€, which is one of\n444\nthefewclasses,frombothDDPMandCALL.Thelastrowdisplaysrealimagesfromthedatasetfor\n445 reference. ItisevidentthatCALLgeneratesresultsthataresignificantlymorediverseandstylisti-\n446 callyclosertotherealimagescomparedtoDDPM.Theimagesshownarerandomlyselected.\n447\n448\n449 8.5\n450 8.0\n451\n452 7.5\n453 0 0.5 0.81.01.2 1.5\nÎ»\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n)(DIF â†“\nIR=100\n8.00\n7.75\n7.50 CALL\nOC 7.25\n7.00\n0 0.5 0.81.01.2 1.5\nÎ»\n)(DIF â†“\nIR=50\n8.5\n8.0\nCALL\nOC 7.5\n0 0.5 0.81.01.2 1.5\nÎ»\n(a) Imb.CIFAR-10,IR={100,50}\n)(DIF â†“\nIR=100\n7.2\n7.0\nCALL 6.8\nOC 6.6\n0 0.5 0.81.01.2 1.5\nÎ»\n)(DIF â†“\nIR=50\nCALL\nOC\n(b) Imb.CIFAR-100,IR={100,50}\nFigure 4: Ablation study on the hyperparameter Î» in Eq. (6). We use OC as a reference because\nitshowsthebestoverallperformanceamongthebaselinesandservesasourbaseloss. Figures(a)\nand(b)showresultsonImb. CIFAR-10andImb. CIFAR-100, respectively, withimbalanceratios\nofIR=100andIR=50fromlefttoright. WereportFIDsforÎ»= 0.0,0.5,0.8,1.0,1.2,1.5 .\n{ }\nrateissetto0.1,andthemodelistrainedfor100epochswithabatchsizeof64,usingtheAdamW\noptimizer(Loshchilov&Hutter,2019)withaweightdecayof10âˆ’6 andaninitiallearningrateof\n3 10âˆ’4. During inference, we generate new images using a 50-step DDIM solver (Song et al.,\nÃ—\n2021a). InTable3,wecompareourCALLagainstDDPMandthetwostrongestbaselines,CBDM\nandOC,onImb. ArtBench-10withimbalanceratiosIR= 100,50 . OurCALLachievesthebest\n{ }\nresults across all eight metrics. Specifically, it outperforms DDPM in terms of FID by 4.307 and\n3.824,andthebestbaselineineachsettingby1.539and1.554,respectively. NotethatISshowsa\ndecreasingtrendastheimbalanceratiodecreasesfrom100to50,indicatingitsunreliabilityonImb.\nArtBench. ThisisbecausetheoutputsoftheImageNet-pretrainedInception-V3arelessreliablefor\nartwork images, and IS does not use real images as a reference. The generated images for one of\nthefewclassesâ€œRealismâ€onImb. ArtBench-10withIR=100areshowninFigure3. Ourmethod\ngeneratesmorediverseimages,andthegeneratedstylesareclosertotherealimages.\n5.3 FURTHERANALYSIS\nCALLasauniversalframework. Table5summarizestheperformanceofourCALLwheninte-\ngrated with DDPM, CBDM, and OC (i.e., using the corresponding objective function for in\nbase\nL\nEq.(6))onImb. CIFAR-100withIR = 100. Itcanbeobservedthatourmethodconsistentlyim-\nprovestheperformanceofimbalancedgenerationwhencombinedwithdifferentbaselines. Dueto\ntheorthogonalityofCALLtoexistingmethods,itcanconsistentlybenefitfromimprovedobjective\nfunctions,includingpotentialfutureadvancements.\nEffectofknowledgeallocationbetweenÎ¸g andÎ¸e. ToinvestigatetheeffectofCALLonknowl-\nedgeallocationbetweenÎ¸gandÎ¸e,wepresenttheresultsofgeneratingimagesusingonlyÎ¸g(CALL\n(Î¸g))andusingÎ¸ =Î¸g Î¸e(CALL)onImb. CIFAR-100withimbalanceratioIR=100inTable4.\nCALL(Î¸g)performswâŠ•\nellontheManyandMediumclassesbutstruggleswiththefewclasses. In\ncontrast,CALLshowsstrongperformanceacrossallsplits. ThisindicatesthatCALLsuccessfully\nallocatesminorityexpertisetoÎ¸e,whilereservingmajorityandgeneralknowledgeforÎ¸g.\n9"
      },
      {
        "page": 10,
        "content": "UnderreviewasaconferencepaperatICLR2025\n486\nTable4:Per-splitFIDsandoverallFIDs( ,Mean Std)ofDDPM,CALL(Î¸g),andCALLonImb.\n487 â†“ Â±\nCIFAR-100withimbalanceratioIR = 100. Many,Medium,andFewarethethreesplitsbasedon\n488 thetrainingimbalance. Bestresultsarehighlighted.\n489\nMethod ManyFID Med.FID FewFID OverallFID\n490 â†“ â†“ â†“ â†“\n491 DDPM(Hoetal.,2020) 14.068 0.193 15.660 0.047 22.188 0.241 10.163 0.077\nCALL(Î¸g) 11.923Â± 0.139 14.872Â± 0.157 29.357Â± 0.318 13.732Â± 0.240\n492 CALL(Î¸=Î¸g Î¸e) 11.732Â± 0.247 13.043Â± 0.138 18.729Â± 0.141 7.519 Â± 0.132\n493 âŠ• Â± Â± Â± Â±\n494\n495\nTable5: FIDs( ),KIDs( ),Recalls( ),andISs( )ofdifferentbaselinesonImb. CIFAR-100with\n496 imbalanceratioâ†“ IR=100â†“ andtheirreâ†‘ sultswhencâ†‘\nombinedwithCALL.Thelasttworowsshowthe\n497 resultsofCALLafterremoving and ,respectively.\nCon Div\nL L\n498\nMethod FID KID Recall IS\n499 â†“ â†“ â†‘ â†‘\n500 DDPM(Hoetal.,2020) 10.163 0.077 0.0029 0.0005 0.46 0.01 13.45 0.15\nÂ± Â± Â± Â±\n+CALL 9.281 0.251 0.0027 0.0002 0.49 0.01 13.37 0.19\n501 Â± Â± Â± Â±\nCBDM(Qinetal.,2023) 10.051 0.391 0.0036 0.0003 0.51 0.01 12.35 0.12\n502 +CALL 8.837 Â± 0.245 0.0029Â± 0.0001 0.51Â± 0.02 13.07Â± 0.16\nÂ± Â± Â± Â±\n503 OC(Zhangetal.,2024) 8.309 0.233 0.0026 0.0002 0.52 0.02 13.44 0.20\nÂ± Â± Â± Â±\n504 +CALL 7.519 0.132 0.0017 0.0003 0.52 0.02 13.45 0.23\nÂ± Â± Â± Â±\n505 +CALLw/o LCon 8.412 Â±0.227 0.0029 Â±0.0002 0.50 Â±0.01 13.23 Â±0.22\n+CALLw/o Div 8.073 0.174 0.0025 0.0001 0.51 0.01 13.42 0.16\n506 L Â± Â± Â± Â±\n507\n508\nAblationonthehyperparameterÎ»inEq.(6). ToinvestigatetheimpactofthehyperparameterÎ»,\n509\ntheweightoftheCALLlossinEq.(6),ontheperformanceofourmethod,weconductablationex-\n510\nperimentsonImb. CIFAR-10andImb. CIFAR-100withdifferentimbalanceratiosIR= 100,50 .\n511 Figure 4 illustrates how the FID of CALL changes with varying Î» values under differen{ t setting} s.\n512 WeobservethatCALLmaintainsaconsistentadvantageoverOCacrossawiderangeofÎ»values,\n513 withitsperformancepeakingaroundÎ»=1.0.\n514\nAblation on and . Table 5 presents the results of CALL as well as the ablation study\n515 LCon LDiv\nwhere the consistency loss and the diversity loss are removed separately from CALL.\nCon Div\n516 L L\nSince and are responsible for allocating majority knowledge and minority expertise,\nCon Div\n517 L L\nrespectively,removingeitherleadstoasignificantdropinperformance,highlightingtheirnecessity.\n518\n519\n14\n520\n521 12\n522 10\n523 8\n524 [1,2,2] [1,2,2,2] [1,2,3,4]\n525 Arch\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n) (DIF â†“\nImb.CIFAR10,IR=100\nDDPM OC\nCBDM CALL 10\n8\n[1,2,2] [1,2,2,2] [1,2,3,4]\nArch\n) (DIF â†“\nAblation on network configura-\nImb.CIFAR100,IR=100\ntions. We conduct experiments\nDDPM OC\non UNet architectures with varying CBDM CALL widths and depths. Figure 5 shows\nFIDs on Imb. CIFAR-10 and Imb.\nCIFAR-100 with IR = 100. Differ-\nent widths and depths are achieved\nby setting the channel multipliers\nFigure 5: FIDs with various UNet configurations on Imb.\nparameter to [1,2,2], [1,2,2,2] (de-\nCIFAR-10andImb. CIFAR-100withIR=100.\nfault), and[1,2,3,4]. Asshown, our\nmethodconsistentlydemonstratesclearadvantagesacrossdifferentnetworkconfigurations.\n6 CONCLUSION\nIn this paper, we seek to improve the robustness of diffusion models to imbalanced data. Unlike\npreviousworkthatfocusesonimprovingobjectivefunctions,weaimtoprotectthegenerationper-\nformanceofminoritiesbyreservingandallocatingmodelcapacityforthem.Wefirstdecomposethe\nmodelparametersintopartsthatcapturegeneralandmajorityknowledge,andadedicatedpartfor\nminorityexpertiseusinglow-rankdecompositiontechniques. Byintroducingacapacityallocation\nloss, we successfully allocate the corresponding knowledge to the reserved model capacity during\ntraining. ExtensiveexperimentsandempiricalanalysesconfirmthatourmethodCALLeffectively\nprotectsminoritiesinimbalanceddiffusionmodelsviacapacityallocation.\n10"
      },
      {
        "page": 11,
        "content": "UnderreviewasaconferencepaperatICLR2025\n540 ETHICS STATEMENT\n541\n542\nInthispaper,weproposeamethodtoenhancetherobustnessofgenerativediffusionmodelsagainst\n543 imbalanced data distributions. This advancement holds significant social implications, both pos-\n544 itive and negative. On the positive side, our approach could democratize access to high-quality\n545 datageneration, allowingmarginalizedcommunitiestobenefitfrommoreequitablerepresentation\n546 in AI applications. By improving the modelâ€™s performance on underrepresented classes, we can\n547 foster inclusivity in various fields, such as healthcare, finance, and education, where data-driven\n548 decisions can impact lives. Conversely, there are potential negative consequences to consider. As\ngenerativemodelsbecomemorepowerful,theymaybemisusedtocreatedeceptivecontent,leading\n549\nto misinformation and erosion of trust in digital media. Additionally, our methodâ€™s emphasis on\n550\nunderrepresentedsegmentsinthetrainingdataposesariskofdatapoisoningifsupervisionislack-\n551\ning. Malicious actors could exploit this focus to introduce biased or harmful data, compromising\n552\nthemodelâ€™sintegrity. Thisvulnerabilityunderscorestheneedforrobustmonitoringandvalidation\n553\nmechanismstoensuredatareliability,asanycompromisecouldleadtounintendednegativeconse-\n554 quences. Therefore,proactivedatagovernanceisessentialtomitigatetheseriskswhilemaximizing\n555 thebenefitsofourproposedmethod.\n556\n557\nREPRODUCIBILITY STATEMENT\n558\n559\nTo ensure the reproducibility of experimental results, we will provide a link for an anonymous\n560\nrepositoryaboutthesourcecodesofthispaperinthediscussionforumaccordingtotheICLR2025\n561\nAuthorGuide. AlltheexperimentsareconductedonNVIDIAA100swithPython3.8andPytorch\n562\n2.0.1. WeprovideexperimentalsetupsandimplementationdetailsinSection5.1andSection5.2.\n563\n564\n565\nREFERENCES\n566 Laith Alzubaidi, Jinglan Zhang, Amjad J. Humaidi, Ayad Q. Al-Dujaili, Ye Duan, Omran Al-\n567 Shamma, JoseÂ´ SantamarÂ´Ä±a, Mohammed A. Fadhel, Muthana Al-Amidie, and Laith Farhan. Re-\n568 view of deep learning: concepts, CNN architectures, challenges, applications, future directions.\n569 J.BigData,8(1):53,2021.\n570\nFanBao,ChongxuanLi,JunZhu,andBoZhang. Analytic-dpm:ananalyticestimateoftheoptimal\n571\nreversevarianceindiffusionprobabilisticmodels. InICLR.OpenReview.net,2022.\n572\n573 ShaneT.BarrattandRishiSharma. Anoteontheinceptionscore. CoRR,abs/1801.01973,2018.\n574\n575 MikolajBinkowski,DanicaJ.Sutherland,MichaelArbel,andArthurGretton. DemystifyingMMD\ngans. InICLR.OpenReview.net,2018.\n576\n577 Ali Borji. Pros and cons of GAN evaluation measures. Comput. Vis. Image Underst., 179:41â€“65,\n578 2019.\n579\n580 MateuszBuda,AtsutoMaki,andMaciejA.Mazurowski. Asystematicstudyoftheclassimbalance\n581 probleminconvolutionalneuralnetworks. NeuralNetworks,106:249â€“259,2018.\n582\nKaidi Cao, Colin Wei, Adrien Gaidon, Nikos AreÂ´chiga, and Tengyu Ma. Learning imbalanced\n583\ndatasetswithlabel-distribution-awaremarginloss. InHannaM.Wallach,HugoLarochelle,Alina\n584 Beygelzimer, Florence dâ€™AlcheÂ´-Buc, Emily B. Fox, and Roman Garnett (eds.), NeurIPS, pp.\n585 1565â€“1576,2019.\n586\n587 JiequanCui, ZhishengZhong, ShuLiu, BeiYu, andJiayaJia. Parametriccontrastivelearning. In\nICCV,pp.695â€“704.IEEE,2021.\n588\n589\nJiequan Cui, Shu Liu, Zhuotao Tian, Zhisheng Zhong, and Jiaya Jia. Reslt: Residual learning for\n590 long-tailedrecognition. IEEETrans.PatternAnal.Mach.Intell.,45(3):3695â€“3706,2023.\n591\n592 PrafullaDhariwalandAlexanderQuinnNichol. Diffusionmodelsbeatgansonimagesynthesis. In\n593 Marcâ€™AurelioRanzato,AlinaBeygelzimer,YannN.Dauphin,PercyLiang,andJenniferWortman\nVaughan(eds.),NeurIPS,pp.8780â€“8794,2021.\n11"
      },
      {
        "page": 12,
        "content": "UnderreviewasaconferencepaperatICLR2025\n594\nIanJ.Goodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,\n595 AaronC.Courville,andYoshuaBengio. Generativeadversarialnetworks. CoRR,abs/1406.2661,\n596 2014.\n597\nSongHan,HuiziMao,andWilliamJDally. Deepcompression: Compressingdeepneuralnetworks\n598\nwithpruning,trainedquantizationandhuffmancoding. arXivpreprintarXiv:1510.00149,2015.\n599\n600 HaiboHeandEdwardoA.Garcia. Learningfromimbalanceddata. IEEETrans.Knowl.DataEng.,\n601 21(9):1263â€“1284,2009.\n602\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\n603\nGans trained by a two time-scale update rule converge to a local nash equilibrium. In Isabelle\n604\nGuyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vish-\n605\nwanathan,andRomanGarnett(eds.),NeurIPS,pp.6626â€“6637,2017.\n606\n607 JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. InNeurIPS,\n608 2020.\n609\nEdwardJ.Hu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,\n610\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR. OpenRe-\n611\nview.net,2022.\n612\n613 ZiyuJiang,TianlongChen,BobakJ.Mortazavi,andZhangyangWang. Self-damagingcontrastive\n614 learning. InMarinaMeilaandTongZhang(eds.),ICML,volume139ofProceedingsofMachine\n615\nLearningResearch,pp.4927â€“4939.PMLR,2021.\n616\nTeroKarras, TimoAila, SamuliLaine, andJaakkoLehtinen. Progressivegrowingofgansforim-\n617 provedquality,stability,andvariation. InICLR.OpenReview.net,2018.\n618\n619 TeroKarras,MiikaAittala,JanneHellsten,SamuliLaine,JaakkoLehtinen,andTimoAila.Training\ngenerative adversarial networks with limited data. In Hugo Larochelle, Marcâ€™Aurelio Ranzato,\n620\nRaiaHadsell,Maria-FlorinaBalcan,andHsuan-TienLin(eds.),NeurIPS,2020.\n621\n622 TeroKarras,MiikaAittala,TimoAila,andSamuliLaine. Elucidatingthedesignspaceofdiffusion-\n623 basedgenerativemodels.InSanmiKoyejo,S.Mohamed,A.Agarwal,DanielleBelgrave,K.Cho,\n624 andA.Oh(eds.),NeurIPS,2022.\n625\nDiederikP.KingmaandJimmyBa.Adam:Amethodforstochasticoptimization.InYoshuaBengio\n626\nandYannLeCun(eds.),ICLR,2015.\n627\n628 DiederikPKingma,TimSalimans,BenPoole,andJonathanHo. Ondensityestimationwithdiffu-\n629 sionmodels. InA.Beygelzimer,Y.Dauphin,P.Liang,andJ.WortmanVaughan(eds.),NeurIPS,\n630 2021.\n631\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n632\n2009.\n633\n634 TuomasKynkaÂ¨aÂ¨nniemi,TeroKarras,SamuliLaine,JaakkoLehtinen,andTimoAila. Improvedpre-\n635 cisionandrecallmetricforassessinggenerativemodels. InHannaM.Wallach,HugoLarochelle,\n636 Alina Beygelzimer, Florence dâ€™AlcheÂ´-Buc, Emily B. Fox, and Roman Garnett (eds.), NeurIPS,\n637 pp.3929â€“3938,2019.\n638\nHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for\n639 efficientconvnets. InICLR.OpenReview.net,2017.\n640\n641 PeiyuanLiao,XiuyuLi,XihuiLiu,andKurtKeutzer. Theartbenchdataset: Benchmarkinggenera-\n642 tivemodelswithartworks. CoRR,abs/2206.11404,2022.\n643\nTsung-YiLin, PriyaGoyal, RossB.Girshick, KaimingHe, andPiotrDollaÂ´r. Focallossfordense\n644 objectdetection. InICCV,pp.2999â€“3007.IEEEComputerSociety,2017.\n645\n646 Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International\n647\nConference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net,2019.\n12"
      },
      {
        "page": 13,
        "content": "UnderreviewasaconferencepaperatICLR2025\n648\nDhruv Mahajan, Ross B. Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan\n649 Li, AshwinBharambe, andLaurensvanderMaaten. Exploringthelimitsofweaklysupervised\n650 pretraining. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.),\n651 ECCV,volume11206,pp.185â€“201.Springer,2018.\n652\nAdityaKrishnaMenon,SadeepJayasumana,AnkitSinghRawat,HimanshuJain,AndreasVeit,and\n653\nSanjivKumar. Long-taillearningvialogitadjustment. InICLR.OpenReview.net,2021.\n654\n655 Eric J. Nunn, Pejman Khadivi, and Shadrokh Samavi. Compound frechet inception distance for\n656 qualityassessmentofGANcreatedimages. CoRR,abs/2106.08575,2021.\n657\nYiming Qin, Huangjie Zheng, Jiangchao Yao, Mingyuan Zhou, and Ya Zhang. Class-balancing\n658\ndiffusionmodels. InCVPR,pp.18434â€“18443.IEEE,2023.\n659\n660 Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n661 conditionalimagegenerationwithCLIPlatents. CoRR,abs/2204.06125,2022.\n662\nHarsh Rangwani, Konda Reddy Mopuri, and R. Venkatesh Babu. Class balancing GAN with a\n663\nclassifier in the loop. In Cassio P. de Campos, Marloes H. Maathuis, and Erik Quaeghebeur\n664\n(eds.), UAI, volume 161 of Proceedings of Machine Learning Research, pp. 1618â€“1627. AUAI\n665\nPress,2021.\n666\n667 HarshRangwani,NamanJaswani,TejanKarmali,VarunJampani,andR.VenkateshBabu. Improv-\n668 ing gans for long-tailed data through group spectral regularization. In Shai Avidan, Gabriel J.\nBrostow, Moustapha CisseÂ´, Giovanni Maria Farinella, and Tal Hassner (eds.), ECCV, volume\n669\n13675ofLectureNotesinComputerScience,pp.426â€“442.Springer,2022.\n670\n671 William J Reed. The pareto, zipf and other power laws. Economics Letters, 74(1):15â€“19, 2001.\n672 ISSN0165-1765.\n673\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjoÂ¨rn Ommer. High-\n674\nresolutionimagesynthesiswithlatentdiffusionmodels.InCVPR,pp.10674â€“10685.IEEE,2022.\n675\n676 OlafRonneberger,PhilippFischer,andThomasBrox. U-net: Convolutionalnetworksforbiomedi-\n677 calimagesegmentation. InNassirNavab,JoachimHornegger,WilliamM.WellsIII,andAlejan-\n678 droF.Frangi(eds.),MICCAI,volume9351ofLectureNotesinComputerScience,pp.234â€“241.\n679 Springer,2015.\n680\nTim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\n681\nImprovedtechniquesfortraininggans.InDanielD.Lee,MasashiSugiyama,UlrikevonLuxburg,\n682 IsabelleGuyon,andRomanGarnett(eds.),NeurIPS,pp.2226â€“2234,2016.\n683\n684 Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsu-\npervised learning using nonequilibrium thermodynamics. In Francis R. Bach and David M.\n685\nBlei(eds.),ICML,volume37ofJMLRWorkshopandConferenceProceedings,pp.2256â€“2265.\n686\nJMLR.org,2015.\n687\n688 JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. InICLR,\n689 VirtualEvent,Austria,May3-7,2021.OpenReview.net,2021a.\n690\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and\n691\nBenPoole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. InICLR.\n692\nOpenReview.net,2021b.\n693\n694 Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Re-\n695 thinkingtheinceptionarchitectureforcomputervision.InCVPR,pp.2818â€“2826.IEEEComputer\n696 Society,2016.\n697\nJianfengWang,ThomasLukasiewicz,XiaolinHu,JianfeiCai,andZhenghuaXu. RSG:Asimple\n698 buteffectivemoduleforlearningimbalanceddatasets.InCVPR,pp.3784â€“3793.ComputerVision\n699 Foundation/IEEE,2021a.\n700\n701 XudongWang,LongLian,ZhongqiMiao,ZiweiLiu,andStellaX.Yu. Long-tailedrecognitionby\nroutingdiversedistribution-awareexperts. InICLR.OpenReview.net,2021b.\n13"
      },
      {
        "page": 14,
        "content": "UnderreviewasaconferencepaperatICLR2025\n702\nDivinYan,LuQi,VincentTaoHu,Ming-HsuanYang,andMengTang. Trainingclass-imbalanced\n703 diffusionmodelviaoverlapoptimization,2024.\n704\n705 SergeyZagoruykoandNikosKomodakis. Wideresidualnetworks. InRichardC.Wilson,EdwinR.\nHancock,andWilliamA.P.Smith(eds.),BMVC.BMVAPress,2016.\n706\n707\nTianjiaoZhang,HuangjieZheng,JiangchaoYao,XiangfengWang,MingyuanZhou,YaZhang,and\n708 YanfengWang. Long-taileddiffusionmodelswithorientedcalibration. InICLR,2024.\n709\n710 YifanZhang,BryanHooi,LanqingHong,andJiashiFeng. Self-supervisedaggregationofdiverse\nexperts for test-agnostic long-tailed recognition. In Sanmi Koyejo, S. Mohamed, A. Agarwal,\n711\nDanielleBelgrave,K.Cho,andA.Oh(eds.),NeurIPS,2022.\n712\n713 YifanZhang,BingyiKang,BryanHooi,ShuichengYan,andJiashiFeng.Deeplong-tailedlearning:\n714 Asurvey. IEEETrans.PatternAnal.Mach.Intell.,2023.\n715\nZhihan Zhou, Jiangchao Yao, Feng Hong, Ya Zhang, Bo Han, and Yanfeng Wang. Combating\n716\nrepresentationlearningdisparitywithgeometricharmonization. InAliceOh,TristanNaumann,\n717\nAmirGloberson,KateSaenko,MoritzHardt,andSergeyLevine(eds.),NeurIPS,2023.\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n14"
      },
      {
        "page": 15,
        "content": "UnderreviewasaconferencepaperatICLR2025\n756 Algorithm1AlgorithmofCALL\n757\nâ–·Training,takeDDPMasbase,sample-wise\n758\n759 Initialize: Î¸g = {W 1g,W 2g,... },Î¸e = {B 1eAe 1,B 2eAe 2,...\n}\nrepeat\n760\nSampledata(x,y)\n761 âˆˆD\nSampleatimestept Uniform( 1,...,T )\n762 âˆ¼ { }\nSampleanoiseÏµ (0,I)\n7 7 76 6 63 4\n5\nTB C aa a ks p ee ac glo i rts ays d: a ielL l nob tca dase eti so= câˆ¼ n enâˆ¥ lN o tÏµ sÎ¸ osg n:âŠ• LÎ¸e\nC\nÎ¸(\nA\ngâˆš\n,L\nÎ¸Î± eLÂ¯ (t =x b+ ( aÏ‰ se( Cy1 +onâˆ’ Î»âˆ’Î±Â¯ t CÏ‰) ADyÏµ, Livt L), âˆ¥y )Ïµ) Î¸âˆ’ gâŠ•Ïµ Î¸âˆ¥ e2 2 (x t,t,y) âˆ’Ïµ Î¸g(x t,t,y) âˆ¥2 2.\n766 untilconverged âˆ‡ L L\n767 â–·Sampling,takeDDPMforexample,sample-wise\n768 MergemodelparametersasÎ¸ =Î¸g Î¸e\n769 Samplex (0,I) âŠ•\nT\n770 fort=T,..âˆ¼ .,N 1do\n771 z (0,I)ift>1,elsez =0\nâˆ¼N\n7 77 72\n3\nenx dt fâˆ’ o1 r= âˆš1 Î±t(x t\nâˆ’\nâˆš 1Î² âˆ’t Î±Â¯tÏµ Î¸(x t,t,y))+Ïƒ tz\n774 return x\n0\n775\n776\n777\nDDPM\n778\n779\n780\nCALL\n781\n782\n783 Real\n784 Images\n785\n786\n787 Figure 6: The visualization of generated images on Imb. CelebA-HQ with imbalance ratio IR =\n788 100. Thefigureshowcasesthegeneratedoutputsfortheclassâ€œMaleâ€,whichistheminorityclass,\n789 frombothDDPMandCALL.Thelastrowdisplaysrealimagesfromthedatasetforreference. Itis\n790 evidentthatCALLgeneratesgeneratesmorerealisticanddiversefaces.\n791\n792\nA ALGORITHM\n793\n794\nWesummarizetheprocedureofourCALLinAlgorithm1,whereweuseDDPMasthebaseloss,\n795\nemployDDPMforsampling,andillustratetheprocessinasample-wisemannerasanexample.\n796\n797\n798 B MORE VISUALIZATION\n799\n800 Thegeneratedimagesforonethemediumclassesâ€œsurrealismâ€onImb.ArtBench-10withIR=100\n801 are shown in Figure 7. It is evident that the generated styles of CALL are much closer to the\n802 real images. More visualization of generation results with CALL are presented in Figure 8 (Imb.\nCIFAR-100,IR=100),Figure9(Imb. CelebA-HQ,IR=100),andFigure10(Imb. ArtBench-10,\n803\nIR=100).\n804\n805\n806\n807\n808\n809\n15"
      },
      {
        "page": 16,
        "content": "UnderreviewasaconferencepaperatICLR2025\n810\n811\n812\nDDPM\n813\n814\n815\nCALL\n816\n817\n818 Real\nImages\n819\n820\n821\n822 Figure7: ThevisualizationofgeneratedimagesonImb. ArtBench-10withimbalanceratioIR =\n823 100. The figure showcases the generated outputs for the class â€œSurrealismâ€, which is one of the\n824 mediumclasses, frombothDDPMandCALL.Thelastrowdisplaysrealimagesfromthedataset\n825 forreference.ItisevidentthatCALLgeneratesresultsthataresignificantlymorestylisticallycloser\n826 totherealimagescomparedtoDDPM.Theimagesshownarerandomlyselected.\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\nFigure8: VisualizationofgenerationresultsonImb. CIFAR-100(IR=100)withCALL.\n863\n16"
      },
      {
        "page": 17,
        "content": "UnderreviewasaconferencepaperatICLR2025\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907 Figure9: VisualizationofgenerationresultsonImb. CelebA-HQ(IR=100)withCALL.\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n17"
      },
      {
        "page": 18,
        "content": "UnderreviewasaconferencepaperatICLR2025\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n948\n949\n950\n951\n952\n953\n954\n955\n956\n957\n958\n959\n960\n961 Figure10: VisualizationofgenerationresultsonImb. ArtBench-10(IR=100)withCALL.\n962\n963\n964\n965\n966\n967\n968\n969\n970\n971\n18"
      }
    ]
  },
  "pdf_url": "/uploads/a97680971a026da8a31bc709cd9f2f7d.pdf"
}