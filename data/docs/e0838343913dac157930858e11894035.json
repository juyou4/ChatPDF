{
  "filename": "MEAL Multi-Model Ensemble via Adversarial Learning.pdf",
  "upload_time": "2026-02-03T20:04:02.802394",
  "data": {
    "full_text": "TheThirty-ThirdAAAIConferenceonArtificialIntelligence(AAAI-19)\nMEAL: Multi-Model Ensemble via Adversarial Learning\nZhiqiangShen,1,2∗ ZhankuiHe,3∗ XiangyangXue1\n1ShanghaiKeyLaboratoryofIntelligentInformationProcessing,\nSchoolofComputerScience,FudanUniversity,Shanghai,China\n2BeckmanInstitute,UniversityofIllinoisatUrbana-Champaign,IL,USA\n3SchoolofDataScience,FudanUniversity,Shanghai,China\nzhiqiangshen0214@gmail.com,{zkhe15,xyxue}@fudan.edu.cn\nAbstract\nOftenthebestperformingdeepneuralmodelsareensembles 6×\nofmultiplebase-levelnetworks.Unfortunately,thespacere-\nquired to store these many networks, and the time required 5×\nto execute them at test-time, prohibits their use in applica-\ntions where test sets are large (e.g., ImageNet). In this pa- 4×\nper, we present a method for compressing large, complex\ntrained ensembles into a single network, where knowledge 3×\nfrom a variety of trained deep neural networks (DNNs) is\ndistilled and transferred to a single DNN. In order to distill 2×\ndiverse knowledge from different trained (teacher) models,\nweproposetouseadversarial-basedlearningstrategywhere 1×\nwe define a block-wise training loss to guide and optimize\nthepredefinedstudentnetworktorecovertheknowledgein\n0×\nteachermodels,andtopromotethediscriminatornetworkto 1 2 3 4 5\n# of ensembles\ndistinguish teacher vs. student features simultaneously. The\nproposedensemblemethod(MEAL)oftransferringdistilled\nknowledgewithadversariallearningexhibitsthreeimportant\nadvantages: (1) the student network that learns the distilled\nknowledge with discriminators is optimized better than the\noriginalmodel;(2)fastinferenceisrealizedbyasinglefor-\nward pass, while the performance is even better than tra-\nditional ensembles from multi-original models; (3) the stu-\ndentnetworkcanlearnthedistilledknowledgefromateacher\nmodelthathasarbitrarystructures.Extensiveexperimentson\nCIFAR-10/100, SVHN and ImageNet datasets demonstrate\nthe effectiveness of our MEAL method. On ImageNet, our\nResNet-50basedMEALachievestop-1/521.79%/5.99%val\nerror,whichoutperformstheoriginalmodelby2.06%/1.14%.\n1. Introduction\nThe ensemble approach is a collection of neural networks\nwhose predictions are combined at test stage by weighted\naveraging or voting. It has been long observed that en-\nsembles of multiple networks are generally much more ro-\nbust and accurate than a single network. This benefit has\nalso been exploited indirectly when training a single net-\nwork through Dropout (Srivastava et al. 2014), Dropcon-\nnect(Wanetal.2013),StochasticDepth(Huangetal.2016),\nSwapout(Singh,Hoiem,andForsyth2016),etc.Weextend\nthis idea by forming ensemble predictions during training,\n∗equalcontribution.ThisworkwasdonewhenZhankuiHewas\naresearchinternatUniversityofIllinoisatUrbana-Champaign.\nCopyright(cid:13)c 2019,AssociationfortheAdvancementofArtificial\nIntelligence(www.aaai.org).Allrightsreserved.\nsPOLF\nFLOPs at Inference Time\nSnapshot Ensemble\n(Huang et al. 2017)\nOur FLOPs at Test Time\nFigure 1: Comparison of FLOPs at inference time. Huang\net al. (Huang et al. 2017a) employ models at different lo-\ncal minimum for ensembling, which enables no additional\ntrainingcost,butthecomputationalFLOPsattesttimelin-\nearlyincreasewithmoreensembles.Incontrast,ourmethod\nuseonlyonemodelduringinferencetimethroughout,sothe\ntestingcostisindependentof#ensembles.\nusingtheoutputsofdifferentnetworkarchitectureswithdif-\nferentoridenticalaugmentedinput.Ourtestingstilloperates\nonasinglenetwork,butthesupervisionlabelsmadeondif-\nferentpre-trainednetworkscorrespondtoanensemblepre-\ndictionofagroupofindividualreferencenetworks.\nThe traditional ensemble, or called true ensemble, has\nsome disadvantages that are often overlooked. 1) Redun-\ndancy: The information or knowledge contained in the\ntrainedneuralnetworksarealwaysredundantandhasover-\nlaps between with each other. Directly combining the pre-\ndictionsoftenrequiresextracomputationalcostbutthegain\nis limited. 2) Ensemble is always large and slow: Ensem-\nble requires more computing operations than an individual\nnetwork,whichmakesitunusableforapplicationswithlim-\nitedmemory,storagespace,orcomputationalpowersuchas\ndesktop,mobileandevenembeddeddevices,andforappli-\ncationsinwhichreal-timepredictionsareneeded.\nTo address the aforementioned shortcomings, in this pa-\nper we propose to use a learning-based ensemble method.\nOurgoalistolearnanensembleofmultipleneuralnetworks\n4886\n\nproving the generalization of an individual network. Re-\nlibrarytobaccoshop\ncently, Snapshot Ensembles (Huang et al. 2017a) is pro-\nbookshop toyshop posedtoaddressthecostoftrainingensembles.Incontrast\ntotheSnapshotEnsembles,herewefocusonthecostoftest-\nconfectionery ing ensembles. Our method is based on the recently raised\ngrocerystore\nknowledge distillation (Hinton, Vinyals, and Dean 2015;\nPapernot et al. 2017; Li et al. 2017; Yim et al. 2017) and\nadversariallearning(Goodfellowetal.2014),sowewillre-\nviewtheonesthataremostdirectlyconnectedtoourwork.\nFigure2:Leftisatrainingexampleofclass“tobaccoshop”\n“Implicit”Ensembling.Essentially,ourmethodisan“im-\nfrom ImageNet. Right are soft distributions from different\nplicit” ensemble which usually has high efficiency during\ntrained architectures. The soft labels are more informative\nboth training and testing. The typical “implicit” ensemble\nandcanprovidemorecoverageforvisually-relatedscenes.\nmethods include: Dropout (Srivastava et al. 2014), Drop-\nConnection (Wan et al. 2013), Stochastic Depth (Huang et\nal. 2016), Swapout (Singh, Hoiem, and Forsyth 2016), etc.\nwithout incurring any additional testing costs. We achieve\nThese methods generally create an exponential number of\nthis goal by leveraging the combination of diverse outputs\nnetworks with shared weights during training and then im-\nfromdifferentneuralnetworksassupervisionstoguidethe\nplicitlyensemblethemattesttime.Incontrast,ourmethod\ntarget network training. The reference networks are called\nfocusesonthesubtledifferencesoflabelswithidenticalin-\nTeachersandthetargetnetworksarecalledStudents.Instead\nput.Perhapsthemostsimilartoourworkistherecentpro-\nofusingthetraditionalone-hotvectorlabels,weusethesoft\nposedLabelRefinery(Bagherinezhadetal.2018),whofo-\nlabelsthatprovidemorecoverageforco-occurringandvisu-\ncusonthesinglemodelrefinementusingthesoftenedlabels\nallyrelatedobjectsandscenes.Wearguethatlabelsshould\nfrom the previous trained neural networks and iteratively\nbe informative for the specific image. In other words, the\nlearnanewandmoreaccuratenetwork.Ourmethoddiffers\nlabels should not be identical for all the given images with\nfromitinthatweintroduceadversarialmodulestoforcethe\nthesameclass.Morespecifically,asshowninFig.2,anim-\nmodeltolearnthedifferencebetweenteachersandstudents,\nage of “tobacco shop” has similar appearance to “library”\nwhichcanimprovemodelgeneralizationandcanbeusedin\nshould have a different label distribution than an image of\nconjunctionwithanyotherimplicitensemblingtechniques.\n“tobaccoshop”butismoresimilarto“grocerystore”.Itcan\nAdversarial Learning. Generative Adversarial Learn-\nalso be observed that soft labels can provide the additional\ning (Goodfellow et al. 2014) is proposed to generate\nintra-andinter-categoryrelationsofdatasets.\nrealistic-looking images from random noise using neural\nTo further improve the robustness of student networks,\nnetworks. It consists of two components. One serves as a\nwe introduce an adversarial learning strategy to force the\ngenerator and another one as a discriminator. The gener-\nstudent to generate similar outputs as teachers. Our exper-\nator is used to synthesize images to fool the discrimina-\niments show that MEAL consistently improves the accu-\ntor, meanwhile, the discriminator tries to distinguish real\nracy across a variety of popular network architectures on\nand fake images. Generally, the generator and discrimina-\ndifferent datasets. For instance, our shake-shake (Gastaldi\ntoraretrainedsimultaneouslythroughcompetingwitheach\n2017)basedMEALachieves2.54%testerroronCIFAR-10,\nwhichisarelative11.2%improvement1.OnImageNet,our other.Inthiswork,weemploygeneratorstosynthesizestu-\ndentfeaturesandusediscriminatortodiscriminatebetween\nResNet-50 based MEAL achieves 21.79%/5.99% val error,\nteacher and student outputs for the same input image. An\nwhichoutperformsthebaselinebyalargemargin.\nadvantage of adversarial learning is that the generator tries\nInsummary,ourcontributioninthispaperisthreefold.\nto produce similar features as a teacher that the discrimi-\n• Anend-to-endframeworkwithadversariallearningisde- nator cannot differentiate. This procedure improves the ro-\nsignedbasedontheteacher-studentlearningparadigmfor bustness of training for student network and has applied to\ndeepneuralnetworkensembling. manyfieldssuchasimagegeneration(Johnson,Gupta,and\n• Theproposedmethodcanachievethegoalofensembling Fei-Fei2018),detection(Baietal.2018),etc.\nmultipleneuralnetworkswithnoadditionaltestingcost. Knowledge Transfer. Distilling knowledge from trained\n• Theproposedmethodimprovesthestate-of-the-artaccu- neural networks and transferring it to another new network\nracyonCIFAR-10/100,SVHN,ImageNetforavarietyof hasbeenwellexploredin(Hinton,Vinyals,andDean2015;\nexistingnetworkarchitectures. Chen,Goodfellow,andShlens2016;Lietal.2017;Yimet\nal. 2017; Bagherinezhad et al. 2018; Anil et al. 2018). The\n2. RelatedWork typicalwayoftransferringknowledgeistheteacher-student\nThereisalargebodyofpreviouswork(HansenandSalamon learningparadigm,whichusesasofteneddistributionofthe\n1990;PerroneandCooper1995;KroghandVedelsby1995; final output of a teacher network to teach information to a\nDietterich 2000; Huang et al. 2017a; Lakshminarayanan, student network. With this teaching procedure, the student\nPritzel, and Blundell 2017) on ensembles with neural net- can learn how a teacher studied given tasks in a more effi-\nworks. However, most of these prior studies focus on im- cient form. Yim et al. (Yim et al. 2017) define the distilled\nknowledge to be transferred flows between different inter-\n1Shake-shakebaseline(Gastaldi2017)is2.86%. mediatelayersandcomputertheinnerproductbetweenpa-\n4887\n\nNrehcaeT\nTeacher\nSelection\nModule FcLayers\nBinaryCross-entropy\nLoss\nTeacherNet Alignment Alignment Alignment\nSimilarityLoss SimilarityLoss SimilarityLoss\nGenerator\nDiscriminator Discriminator Discriminator\nAlignment Alignment Alignment\nStudentNet\n…\nArehcaeT\nFigure3:Overviewofourproposedarchitecture.Weinputthesameimageintotheteacherandstudentnetworkstogenerate\nintermediate and final outputs for Similarity Loss and Discriminators. The model is trained adversarially against several dis-\ncriminator networks. During training the model observes supervisions from trained teacher networks instead of the one-hot\nground-truthlabels,andtheteacher’sparametersarefixedallthetime.\nrametersfromtwonetworks.Bagherinezhadetal.(Bagher- dent network S is trained over the same set of images,\nθ\ninezhadetal.2018)studiedtheeffectsofvariousproperties but uses labels generated by T . More formally, we can\nθ\noflabelsandintroducetheLabelRefinerymethodthatiter- viewthisprocedureastrainingS onanewlabeleddataset\nθ\natively updated the ground truth labels after examining the D˜ = (X ,T (X )).Oncetheteachernetworkistrained,we\ni θ i\nentiredatasetwiththeteacher-studentlearningparadigm. freezeitsparameterswhentrainingthestudentnetwork.\nWe train the student network S by minimizing the sim-\nθ\n3. Overview ilarity distance between its output and the soft label gener-\nSiamese-like Network Structure Our framework is a atedbytheteachernetwork.LettingpT cθ(X i) = T θ(X i)[c],\nsiamese-likearchitecturethatcontainstwo-streamnetworks pS cθ(X i)=S θ(X i)[c]betheprobabilitiesassignedtoclassc\nin teacher and student branches. The structures of two intheteachermodelT θandstudentmodelS θ.Thesimilarity\nstreams can be identical or different, but should have the metriccanbeformulatedas:\nsame number of blocks, in order to utilize the intermediate\nL =d(T (X ),S (X ))\noutputs. The whole framework of our method is shown in Sim θ i θ i\nFig. 3. It consists of a teacher network, a student network, =(cid:88) d(pT cθ(X i),pS cθ(X i)) (1)\nalignmentlayers,similaritylosslayersanddiscriminators.\nc\nTheteacherandstudentnetworksareprocessedtogener- We investigated three distance metrics in this work, in-\nateintermediateoutputsforalignment.Thealignmentlayer cluding(cid:96) ,(cid:96) andKL-divergence.Thedetailedexperimental\n1 2\nisanadaptivepoolingprocessthattakesthesameordiffer- comparisons are shown in Tab. 1. Here we formulate them\nent length feature vectors as input and output fixed-length asfollows.\nnewfeatures.Weforcethemodeltooutputsimilarfeatures (cid:96) distanceisusedtominimizetheabsolutedifferencesbe-\n1\nofstudentandteacherbytrainingstudentnetworkadversar- tweentheestimatedstudentprobabilityvaluesandtherefer-\nially against several discriminators. We will elaborate each enceteacherprobabilityvalues.Hereweformulateitas:\nofthesecomponentsinthefollowingsectionswithmorede-\nn\ntails. L\n(cid:96)1\nSim(S θ)= n1 (cid:88)(cid:88)(cid:12) (cid:12)pT cθ(X i)−pS cθ(X i)(cid:12) (cid:12)1 (2)\n4. AdversarialLearning(AL)forKnowledge c i=1\n(cid:96) distanceoreuclideandistanceisthestraight-linedistance\nDistillation 2\nineuclideanspace.Weuse(cid:96) lossfunctiontominimizethe\n2\n4.1 SimilarityMeasurement error which is the sum of all squared differences between\nGiven a dataset D = (X ,Y ), we pre-trained the teacher thestudentoutputprobabilitiesandtheteacherprobabilities.\ni i\nnetwork T\nθ\nover the dataset using the cross-entropy loss The(cid:96) 2canbeformulatedas:\nagainsttheone-hotimage-levellabels2 inadvance.Thestu- n\n2Ground-truthlabels\nL\n(cid:96)2\nSim(S θ)= n1 (cid:88)(cid:88)(cid:13) (cid:13)pT cθ(X i)−pS cθ(X i)(cid:13) (cid:13)2 (3)\nc i=1\n4888\n\n0.9 0.1 0.6 0.2 0.3 0.5 0.7 0.3 0 0.5 0 0 0 0.1\n!\n\" adaptivepooling\nTeacheroutputs 0 0 2 indices 0 0 2 indices\noutputsize=3\n! ! 0.9 0.6 0.7 values 0.3 0.5 0.1 gradients\n# Teacher?\nForward backward\nStudent?\nStudentoutputs\nFigure 5: The process of adaptive pooling in forward and\nbackwardstages.Weusemaxoperationforillustration.\nFigure4:Illustrationofourproposeddiscriminator.Wecon-\ncatenate the outputs of teacher and student as the inputs (cid:88)\nof a discriminator. The discriminator is a three-layer fully- L Sim = Lj Sim (7)\nconnectednetwork. j∈A\nwhereAisthesetoflayersthatwechoosetoproduceout-\nput.Inourexperiments,weusethelastlayerineachblock\nKL-divergenceisameasureofhowoneprobabilitydistri- ofanetwork(block-wise).\nbution is different from another reference probability dis-\ntribution. Here we train student network S by minimizing 4.3 StackedDiscriminators\nθ\ntheKL-divergencebetweenitsoutputpS cθ(X i)andthesoft\nWegeneratestudentoutputbytrainingthestudentnetwork\nlabels pT cθ(X i) generated by the teacher network. Our loss\nS\nθ\nand freezing the teacher parts adversarially against a\nfunctionis:\nseries of stacked discriminators D . A discriminator D at-\nj\nL KLSim(S θ)=− n1 (cid:88)\nc\n(cid:88) i=n 1pT cθ(X i)log(p pS c\nT\ncθθ (( XX ii )) ) t me Lm iz jp ints gt to h =ecl fa os ls l Eoif wy ii nt lgs ogi on b Dp ju ect (xtx iv )a e +s (Gte oa Ec oh de fer l lo l oor gws (t 1u etd −ae ln D.t 2b 0 (y x1 )4m )):a (x 8i-\n)\nn GAN j j\n=− n1 (cid:88)(cid:88) pT cθ(X i)logpS cθ(X i) (4)\nwhere\nxx ∼∼p pte sa tc uhe dr\nent\nare\noutputx s∼ fp rs otu mdent\ngeneration network\nc i=1 S .Atthesametime,S attemptstogeneratesimilarout-\nn\nθj θj\n+ n1 (cid:88)(cid:88) pT cθ(X i)logpT cθ(X i) put Ss inw ch eic thh ew pi all rafo mo el tt eh re sd oi fs oc uri rm tein aa ct ho er rb ay rem fiin xeim di dz uin rig nL gj G trA aiN n-.\nc i=1\ning,thefirsttermcanberemovedandourfinalobjectiveloss\nwhere the second term is the entropy of soft labels from is:\nrte ea mc oh ve ern ite at nw dor sk ima pn ld yi ms ic no imns it za ent thw ei cth ror se ss -p enec trt ot po yT lθ o. ssW ae sfc oa ln\n-\nLj\nGAN\n= x∼pE studentlog(1−D j(x)) (9)\nlows: In Eq. 10, x is the concatenation of teacher and student\noutputs. We feed x into the discriminator which is a three-\nn layerfully-connectednetwork.Thewholestructureofadis-\n1 (cid:88)(cid:88)\nL CESim(S θ)=−\nn\npT cθ(X i)logpS cθ(X i) (5) c Mri um ltin i-a St to ar gi ess Dh io sw crn imin inF aig to. r4 s.\n. Using multi-Stage discrimi-\nc i=1\nnatorscanrefinethestudentoutputsgradually.Asshownin\nFig. 3, the final adversarial loss is a sum of the individual\n4.2 IntermediateAlignment\nones:\nAdaptive Pooling. The purpose of the adaptive pooling L = (cid:88) Lj (10)\nlayer is to align the intermediate output from teacher net- GAN GAN\nj∈A\nwork and student network. This kind of layer is similar to\nLet|A|bethenumberofdiscriminators.Inourexperiments,\ntheordinarypoolinglayerlikeaverageormaxpooling,but\nweuse3forCIFAR(Krizhevsky2009)andSVHN(Netzer\ncangenerateapredefinedlengthofoutputwithdifferentin-\netal.2011),and5forImageNet(Dengetal.2009).\nputsize.Becauseofthisspecialty,wecanusethedifferent\nteachernetworksandpooltheoutputtothesamelengthof\n4.4 JointTrainingofSimilarityandDiscriminators\nstudentoutput.Poolinglayercanalsoachievespatialinvari-\nBased on above definition and analysis, we incorporate the\nancewhenreducingtheresolutionoffeaturemaps.Thus,for\nsimilarity loss in Eq. 7 and adversarial loss in Eq. 10 into\ntheintermediateoutput,ourlossfunctionis:\nourfinallossfunction.Ourwholeframeworkistrainedend-\nLj =d(f(T ),f(S )) (6) to-endbythefollowingobjectivefunction:\nSim θj θj\nL=αL +βL (11)\nwhere T and S are the outputs at j-th layer of the Sim GAN\nθj θj\nteacher and student, respectively. f is the adaptive pooling where α and β are trade-off weights. We set them as\nfunction that can be average or max. Fig. 5 illustrates the 1 in our experiments by cross validation. We also use the\nprocessofadaptivepooling.Becauseweadoptmultiplein- weightedcoefficientstobalancethecontributionsofdiffer-\ntermediate layers, our final similarity loss is a sum of indi- entblocks.For3-blocknetworks,weues[0.01,0.05,1],and\nvidualone: [0.001,0.01,0.05,0.1,1]for5-blockones.\n4889\n\n5. Multi-ModelEnsembleviaAdversarial tion scheme3 (Lee et al. 2015; Romero et al. 2015; Lars-\nLearning(MEAL) son, Maire, and Shakhnarovich 2016; Huang et al. 2017a;\nLiuetal.2017)isused.Wereportthetesterrorsinthissec-\nWe achieve ensemble with a training method that is sim- tionwithtrainingonthewholetrainingset.\nple and straight-forward to implement. As different net- SVHN. The Street View House Number (SVHN)\nworkstructurescanobtaindifferentdistributionsofoutputs, dataset (Netzer et al. 2011) consists of 32×32 colored\nwhich can be viewed as soft labels (knowledge), we adopt digit images, with one class for each digit. The train\nthese soft labels to train our student, in order to compress and test sets contain 604,388 and 26,032 images, respec-\nknowledge of different architectures into a single network. tively. Following previous works (Goodfellow et al. 2013;\nThuswecanobtaintheseeminglycontradictorygoalofen- Huangetal.2016;2017a;Liuetal.2017),wesplitasubset\nsembling multiple neural networks at no additional testing of 6,000 images for validation, and train on the remaining\ncost. imageswithoutdataaugmentation.\nImageNet.TheILSVRC2012classificationdataset(Deng\n5.1 LearningProcedure et al. 2009) consists of 1000 classes, with a number of\n1.2 million training images and 50,000 validation im-\nToclearlyunderstandwhatthestudentlearnedinourwork,\nages. We adopt the the data augmentation scheme follow-\nwe define two conditions. First, the student has the same\ning(Krizhevsky,Sutskever,andHinton2012)andapplythe\nstructure as the teacher network. Second, we choose one\nsameoperationas (Huangetal.2017a)attesttime.\nstructure for student and randomly select a structure for\nteacherineachiterationasourensemblelearningprocedure.\n6.2 Networks\nThelearningprocedurecontainstwostages.First,wepre-\ntrain the teachers to produce a model zoo. Because we use We adopt several popular network architectures as our\ntheclassificationtasktotrainthesemodels,wecanusethe teachermodelzoo,includingVGGNet(SimonyanandZis-\nsoftmax cross entropy loss as the main training loss in this serman 2015), ResNet (He et al. 2016), DenseNet (Huang\nstage.Second,weminimizethelossfunctionLinEq.11to et al. 2017b), MobileNet (Howard et al. 2017), shake-\nmakethestudentoutputsimilartothatoftheteacheroutput. shake (Gastaldi 2017), etc. For VGGNet, we use 19-layer\nThelearningprocedureisexplainedbelowinAlgorithm1. with Batch Normalization (Ioffe and Szegedy 2015). For\nResNet,weuse18-layernetworkforCIFARandSVHNand\n50-layerforImagNet.ForDenseNet,weusetheBC struc-\nAlgorithm1Multi-ModelEnsembleviaAdversarialLearn-\nture with depth L=100, and growth rate k=24. For shake-\ning(MEAL).\nshake,weuse26-layer2×96dversion.Notethatduetothe\nStage1: highcomputingcosts,weuseshake-shakeasateacheronly\nBuilding and Pre-training the Teacher Model Zoo T = whenthestudentisshake-shakenetwork.\n{T1,T2,...Ti},including:VGGNet(SimonyanandZisserman\nθ θ θ\n2015), ResNet (He et al. 2016), DenseNet (Huang et al. 2017b),\nMobileNet(Howardetal.2017),Shake-Shake(Gastaldi2017),etc. Table 1: Ablation study on CIFAR-10 using VGGNet-19\nStage2:\nw/BN.PleaserefertoSection6.3formoredetails.\n1: functionTSM(T)\n2: T ←RS(T) (cid:46)RandomSelection\n(cid:96)1dis. (cid:96)2dis. Cross-Entropy Intermediate Adversarial TestErrors(%)\nθ BaseModel(VGG-19w/BN)(SimonyanandZisserman2015) 6.34\n3: returnT\nθ (cid:33) 6.97\n4: endfunction (cid:33) 6.22\n5: foreachiterationdo: (cid:33) 6.18\n6: T θ ←TSM(T) (cid:46)RandomlySelectaTeacherModel (cid:33) (cid:33) 6.10\n7: StuS deθ n=\nt\nargmin Sθ L(T θ,S θ) (cid:46)AdversarialLearningfora (cid:33)\n(cid:33)\n(cid:33)\n(cid:33) (cid:33)\n6 5. .1 87\n3\n8: endfor (cid:33) (cid:33) (cid:33) 7.57\n6.3 AblationStudies\n6. ExperimentsandAnalysis\nWe first investigate each design principle of our MEAL\nWe empirically demonstrate the effectiveness of MEAL on framework. We design several controlled experiments on\nseveral benchmark datasets. We implement our method on CIFAR-10withVGGNet-19w/BN(bothtoteacherandstu-\nthePyTorch(Paszkeetal.2017)platform. dent)forthisablationstudy.Aconsistentsettingisimposed\non all the experiments, unless when some components or\n6.1. Datasets structuresareexamined.\nThe results are mainly summarized in Table 1. The first\nCIFAR. The two CIFAR datasets (Krizhevsky 2009) con-\nthreerowsindicatethatweonlyuse(cid:96) ,(cid:96) orcross-entropy\n1 2\nsistofcolorednaturalimageswithasizeof32×32.CIFAR-\n10 is drawn from 10 and CIFAR-100 is drawn from 100 3zero-padded with 4 pixels on both sides, randomly cropped\nclasses.Ineachdataset,thetrainandtestsetscontain50,000 toproduce32x32images,andhorizontallymirrorwithprobability\nand10,000images,respectively.Astandarddataaugmenta- 0.5.\n4890\n\n3.80\n3.75 3.70\n3.65\n3.60\n3.55\n3.50\nCIFAR-10\n)%( rorrE\ntseT\n3.76 3.74 3.73 S S Tri in n ug g el l e e E nA sL . 20 Our Ens. 19\n18\n3.56 17\nCIFAR-100\n)%( rorrE\ntseT\n1.80\n20.23 S Si in ng gl le e AL T Or uu re E E nn ss .. 1.75 19.04\n1.70\n17.73\n1.65 17.21\n1.60\nSVHN\n)%( rorrE\ntseT\nSingle 24.0 1.77 Single AL True Ens. 23.5 Our Ens. 23.0\n1.69 22.5\n1.66 22.0\n1.64 21.5\n21.0\nImageNet\n)%( rorrE\ntseT\n1-poT\n23.85 Single 23.71 Single AL True Ens. Our Ens. 22.76\n21.69\nFigure6:Errorrates(%)onCIFAR-10andCIFAR-100,SVHNandImageNetdatasets.Ineachfigure,theresultsfromleftto\nrightare1)basemodel;2)basemodelwithadversariallearning;3)trueensemble/traditionalensemble;and4)ourensemble\nresults.Forthefirstthreedatasets,weemployDenseNetasstudent,andResNetforthelastone(ImageNet).\nloss from the last layer of a network. It’s similar to the\nTable3:Comparisonoferrorrate(%)withDropout(Srivas-\nKnowledge Distillation method. We can observe that use\ntavaetal.2014)baselineonCIFAR-10.\ncross-entropy achieve the best accuracy. Then we employ\nmoreintermediateoutputstocalculatetheloss,asshownin Network Dropout(%) OurEns.(%)\nVGG-19w/BN(SimonyanandZisserman2015) 6.89 5.55\nrows 4 and 5. It’s obvious that including more layers im-\nGoogLeNet(Szegedyetal.2015) 5.37 4.83\nprovestheperformance.Finally,weinvolvethediscrimina-\nResNet-18(Heetal.2016) 4.69 4.35\ntorstoexamtheeffectivenessofadversariallearning.Using\nDenseNet-BC(k=24)(Huangetal.2017b) 3.75 3.54\ncross-entropy, intermediate layers and adversarial learning\nachieve the best result. Additionally, we use average based\nadaptivepoolingforalignment.Wealsotriedmaxoperation,\nditional ensemble are the sum of individual ones. There-\ntheaccuracyismuchworse(6.32%).\nfore, our method has both better performance and higher\n6.4 Results efficiency. Most notably, our MEAL Plus4 yields an error\nrate of Top-1 21.79%, Top-5 5.99% on ImageNet, far out-\nComparison with Traditional Ensemble. The results are\nperforming the original ResNet-50 23.85%/7.13% and the\nsummarized in Figure 6 and Table 2. In Figure 6, we com-\ntraditional ensemble 22.76%/6.49%. This shows great po-\npare the error rate using the same architecture on a vari-\ntentialonlarge-scalereal-sizedatasets.\nety of datasets (except ImageNet). It can be observed that\nourresultsconsistentlyoutperformthesingleandtraditional\nmethods on these datasets. The traditional ensembles are Table4:Val.error(%)onImageNetdataset.\nobtained through averaging the final predictions across all\nMethod Top-1(%) Top-5(%) #FLOPs InferenceTime(per/image)\nteachermodels.InTable2,wecompareerrorrateusingdif- TeacherNetworks:\nferent architectures on the same dataset. In most cases, our VGG-19w/BN 25.76 8.15 19.52B 5.70×10−3s\nensemblemethodachieveslowererrorthananyofthebase- ResNet-50 23.85 7.13 4.09B 1.10×10−2s\nlines,includingthesinglemodelandtraditionalensemble. Ours(ResNet-50) 23.58 6.86 4.09B 1.10×10−2s\nTraditionalEns. 22.76 6.49 23.61B 1.67×10−2s\nOursPlus(ResNet-50) 21.79 5.99 4.09B 1.10×10−2s\nTable2:Errorrate(%)usingdifferentnetworkarchitectures\nonCIFAR-10dataset.\nNetwork Single(%) TraditionalEns.(%) OurEns.(%)\nMobileNet(Howardetal.2017) 10.70 – 8.09\nVGG-19w/BN(SimonyanandZisserman2015) 6.34 – 5.55\nDenseNet-BC(k=24)(Huangetal.2017b) 3.76 3.73 3.54\nShake-Shake-262x96d(Gastaldi2017) 2.86 2.79 2.54\nComparison with Dropout. We compare MEAL with the\n“Implicit”methodDropout(Srivastavaetal.2014).There-\nsults are shown in Table 3, we employ several network ar-\nchitectures in this comparison. All models are trained with\nthesameepochs.Weuseaprobabilityof0.2fordropnodes\nduringtraining.Itcanbeobservedthatourmethodachieves\nbetterperformancethanDropoutonallthesenetworks.\nOurLearning-BasedEnsembleResultsonImageNet.As Figure7:Accuracyofourensemblemethodunderdifferent\nshowninTable4,wecompareourensemblemethodwiththe trainingbudgetsonCIFAR-10.\noriginalmodelandthetraditionalensemble.WeuseVGG-\n19 w/BN and ResNet-50 as our teachers, and use ResNet-\n50 as the student. The #FLOPs and inference time for tra- 4denotesusingmorepowerfulteacherslikeResNet-101/152.\n4891\n\n10.0\n9.5\n9.0\n8.5\n8.0\n7.5\n1 2 3 4 5\n# of ensembles\n)%(\nrorrE\ntseT\nMobileNet\n6.7\n6.2 MobileNet baseline: 10.70% 5.9\n(Howard et al. 2017) 5.8\n5.7\n5.6\n5.5\n1 2 3 4 5\n# of ensembles\n)%(\nrorrE\ntseT\nVGG19-BN\n3.75 VGG19-BN baseline: 6.34%\n(Simonyan et al. 2015) 3.70\n3.65\n3.60\n3.55\n3.50\n1 2 3 4 5\n# of ensembles\n)%(\nrorrE\ntseT\nDenseNet\nDenseNet baseline: 3.76%\n(Huang et al. 2017)\nFigure8:Errorrate(%)onCIFAR-10withMobileNet,VGG-19w/BNandDenseNet.\nFigure10:VisualizationsofvalidationimagesfromtheIma-\ngeNetdatasetbyt-SNE(MaatenandHinton2008).Weran-\ndomlysample10classeswithin1000classes.Leftisthesin-\nFigure 9: Probability Distributions between four net-\ngle model result using the standard training strategy. Right\nworks. Left: SequeezeNet vs. VGGNet. Right: ResNet vs.\nisourensemblemodelresult.\nDenseNet.\n1,2,...,1000,andtherightfigureis(pk ,pk ).\n6.5 Analysis ResNet DenseNet\nIfthelabeldistributionsareidenticalfromtwonetworks,the\nEffectivenessofEnsembleSize.Figure8displaystheper- bubbleswillbeplacedonthemasterdiagonal.It’sveryin-\nterestingtoobservethattheleft(weakernetworkpairs)has\nformanceofthreearchitecturesonCIFAR-10astheensem-\nbigger diversity than the right (stronger network pairs). It\nblesizeisvaried.Althoughensemblingmoremodelsgener-\nmakes sense because the stronger models generally tend to\nally gives better accuracy, we have two important observa-\ngeneratepredictionsclosetotheground-truth.Inbrief,these\ntions.First,weobservethatoursinglemodel“ensemble”al-\ndifferencesinpredictionscanbeexploitedtocreateeffective\nreadyoutputsthebaselinemodelwitharemarkablemargin,\nensemblesandourmethodiscapableofimprovingthecom-\nwhich demonstrates the effectiveness of adversarial learn-\npetitivebaselinesusingthiskindofdiversesupervisions.\ning. Second, we observe some drops in accuracy using the\nVGGNetandDenseNetnetworkswhenincludingtoomany\n6.6 VisualizationoftheLearnedFeatures\nensembles for training. In most case, an ensemble of four\nmodelsobtainsthebestperformance. To further explore what our model actually learned, we vi-\nBudget for Training. On CIFAR datasets, the standard sualize the embedded features from the single model and\ntraining budget is 300 epochs. Intuitively, our ensemble our ensembling model. The visualization is plotted by t-\nmethod can benefit from more training budget, since we SNEtool(MaatenandHinton2008)withthelastconv-layer\nusethediversesoftdistributionsaslabels.Figure7displays features (2048 dimensions) from ResNet-50. We randomly\ntherelationbetweenperformanceandtrainingbudget.Itap- sample 10 classes on ImageNet, results are shown in Fig-\npears that more than 400 epochs is the optimal choice and ure10,it’sobviousthatourmodelhasbetterfeatureembed-\nourmodelwillfullyconvergeatabout500epochs. dingresult.\nDiversityofSupervision.Wehypothesizethatdifferentar-\n7. Conclusion\nchitecturescreatesoftlabelswhicharenotonlyinformative\nbutalsodiversewithrespecttoobjectcategories.Wequalita- We have presented MEAL, a learning-based ensemble\ntivelymeasurethisdiversitybyvisualizingthepairwisecor- method that can compress multi-model knowledge into a\nrelationofsoftmaxoutputsfromtwodifferentnetworks.To single network with adversarial learning. Our experimental\ndoso,wecomputethesoftmaxpredictionsforeachtraining evaluationonthreebenchmarksCIFAR-10/100,SVHNand\nimageinImageNetdatasetandvisualizeeachpairofthecor- ImageNetverifiedtheeffectivenessofourproposedmethod,\nrespondingones.Figure9displaysthebubblemapsoffour whichachievedthestate-of-the-artaccuracyforavarietyof\narchitectures.Intheleftfigure,thecoordinateofeachbubble networkarchitectures.Ourfurtherworkwillfocusonadopt-\nis a pair of k-th predictions (pk ,pk ), k = ingMEALforcross-domainensembleandadaption.\nSequeezeNet VGGNet\n4892\n\nAcknowledgementsThisworkwassupportedinpartbyNa- Larsson, G.; Maire, M.; and Shakhnarovich, G. 2016. Fractal-\ntionalKeyR&DProgramofChina(No.2017YFC0803700), net:Ultra-deepneuralnetworkswithoutresiduals. arXivpreprint\nNSFC under Grant (No.61572138 & No.U1611461) and arXiv:1605.07648.\nSTCSMProjectunderGrantNo.16JC1420400.\nLee,C.-Y.;Xie,S.;Gallagher,P.W.;etal.2015.Deeply-supervised\nnets. InAISTATS.\nReferences\nLi, Y.; Yang, J.; Song, Y.; Cao, L.; Luo, J.; and Li, L.-J. 2017.\nAnil, R.; Pereyra, G.; Passos, A.; Ormandi, R.; Dahl, G. E.; and Learningfromnoisylabelswithdistillation. InICCV.\nHinton,G.E.2018.Largescaledistributedneuralnetworktraining\nLiu,Z.;Li,J.;Shen,Z.;Huang,G.;Yan,S.;andZhang,C. 2017.\nthroughonlinedistillation. InICLR.\nLearning efficient convolutional networks through network slim-\nBagherinezhad, H.; Horton, M.; Rastegari, M.; and Farhadi, A. ming. InICCV.\n2018. Label refinery: Improving imagenet classification through\nMaaten,L.v.d.,andHinton,G.2008.Visualizingdatausingt-sne.\nlabelprogression. InECCV.\nJournalofmachinelearningresearch9(Nov):2579–2605.\nBai,Y.;Zhang,Y.;Ding,M.;andGhanem,B. 2018. Findingtiny\nNetzer, Y.; Wang, T.; Coates, A.; Bissacco, A.; Wu, B.; and Ng,\nfacesinthewildwithgenerativeadversarialnetwork.\nA.Y. 2011. Readingdigitsinnaturalimageswithunsupervised\nChen,T.;Goodfellow,I.;andShlens,J. 2016. Net2net:Accelerat- featurelearning.InNIPSworkshopondeeplearningandunsuper-\ninglearningviaknowledgetransfer. InICLR. visedfeaturelearning,volume2011, 5.\nDeng,J.;Dong,W.;Socher,R.;Li,L.-J.;etal. 2009. Imagenet:A Papernot,N.;Abadi,M.;Erlingsson,U.;Goodfellow,I.;andTal-\nlarge-scalehierarchicalimagedatabase. InCVPR. war,K. 2017. Semi-supervisedknowledgetransferfordeeplearn-\nDietterich,T.G. 2000. Ensemblemethodsinmachinelearning. In ingfromprivatetrainingdata. InICLR.\nInternationalworkshoponmultipleclassifiersystems,1–15. Paszke,A.;Gross,S.;Chintala,S.;Chanan,G.;Yang,E.;DeVito,\nGastaldi, X. 2017. Shake-shake regularization. arXiv preprint Z.;Lin,Z.;Desmaison,A.;Antiga,L.;andLerer,A. 2017. Auto-\narXiv:1705.07485. maticdifferentiationinpytorch.\nGoodfellow,I.J.;Warde-Farley,D.;Mirza,M.;Courville,A.;and Perrone,M.P.,andCooper,L.N. 1995. Whennetworksdisagree:\nBengio,Y. 2013. Maxoutnetworks. InICML. Ensemblemethodsforhybridneuralnetworks. InHowWeLearn;\nHowWeRemember:TowardanUnderstandingofBrainandNeu-\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-\nralSystems:SelectedPapersofLeonNCooper.WorldScientific.\nFarley, D.; Ozair, S.; Courville, A.; and Bengio, Y. 2014. Gen-\n342–358.\nerativeadversarialnets. InNIPS.\nRomero,A.;Ballas,N.;Kahou,S.E.;Chassang,A.;Gatta,C.;and\nHansen, L. K., and Salamon, P. 1990. Neural network ensem-\nBengio,Y. 2015. Fitnets:Hintsforthindeepnets. InICLR.\nbles. IEEEtransactionsonpatternanalysisandmachineintelli-\ngence12(10):993–1001. Simonyan,K.,andZisserman,A. 2015. Verydeepconvolutional\nnetworksforlarge-scaleimagerecognition. InICLR.\nHe,K.;Zhang,X.;Ren,S.;andSun,J. 2016. Deepresiduallearn-\ningforimagerecognition. InCVPR. Singh,S.;Hoiem,D.;andForsyth,D.2016.Swapout:Learningan\nensembleofdeeparchitectures.InAdvancesinneuralinformation\nHinton,G.;Vinyals,O.;andDean,J. 2015. Distillingtheknowl-\nedgeinaneuralnetwork. arXivpreprintarXiv:1503.02531.\nprocessingsystems,28–36.\nHoward,A.G.;Zhu,M.;Chen,B.;Kalenichenko,D.;Wang,W.; Srivastava,N.;Hinton,G.E.;Krizhevsky,A.;etal.2014.Dropout:\nWeyand,T.;Andreetto,M.;andAdam,H. 2017. Mobilenets:Effi- asimplewaytopreventneuralnetworksfromoverfitting. JMLR.\ncientconvolutionalneuralnetworksformobilevisionapplications. Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P.; et al. 2015. Going\narXivpreprintarXiv:1704.04861. deeperwithconvolutions. InCVPR.\nHuang,G.;Sun,Y.;Liu,Z.;Sedra,D.;andWeinberger,K.Q.2016. Wan,L.;Zeiler,M.;Zhang,S.;LeCun,Y.;andFergus,R. 2013.\nDeepnetworkswithstochasticdepth. InECCV. Regularizationofneuralnetworksusingdropconnect. InICML.\nHuang, G.; Li, Y.; Pleiss, G.; Liu, Z.; Hopcroft, J. E.; and Wein- Yim,J.;Joo,D.;Bae,J.;andKim,J. 2017. Agiftfromknowledge\nberger,K.Q. 2017a. Snapshotensembles:Train1,getmforfree. distillation:Fastoptimization,networkminimizationandtransfer\nInICLR. learning. InCVPR.\nHuang, G.; Liu, Z.; Weinberger, K. Q.; and van der Maaten, L.\n2017b. Denselyconnectedconvolutionalnetworks. InCVPR.\nIoffe,S.,andSzegedy,C.2015.Batchnormalization:Accelerating\ndeepnetworktrainingbyreducinginternalcovariateshift. arXiv\npreprintarXiv:1502.03167.\nJohnson, J.; Gupta, A.; and Fei-Fei, L. 2018. Image generation\nfromscenegraphs. InCVPR.\nKrizhevsky,A.;Sutskever,I.;andHinton,G.2012.Imagenetclas-\nsificationwithdeepconvolutionalneuralnetworks. InNIPS.\nKrizhevsky, A. 2009. Learning multiple layers of features from\ntinyimages. Technicalreport.\nKrogh,A.,andVedelsby,J.1995.Neuralnetworkensembles,cross\nvalidation,andactivelearning. InNIPS.\nLakshminarayanan,B.;Pritzel,A.;andBlundell,C. 2017. Simple\nand scalable predictive uncertainty estimation using deep ensem-\nbles. InNIPS.\n4893\n\n",
    "total_pages": 8,
    "pages": [
      {
        "page": 1,
        "content": "TheThirty-ThirdAAAIConferenceonArtificialIntelligence(AAAI-19)\nMEAL: Multi-Model Ensemble via Adversarial Learning\nZhiqiangShen,1,2∗ ZhankuiHe,3∗ XiangyangXue1\n1ShanghaiKeyLaboratoryofIntelligentInformationProcessing,\nSchoolofComputerScience,FudanUniversity,Shanghai,China\n2BeckmanInstitute,UniversityofIllinoisatUrbana-Champaign,IL,USA\n3SchoolofDataScience,FudanUniversity,Shanghai,China\nzhiqiangshen0214@gmail.com,{zkhe15,xyxue}@fudan.edu.cn\nAbstract\nOftenthebestperformingdeepneuralmodelsareensembles 6×\nofmultiplebase-levelnetworks.Unfortunately,thespacere-\nquired to store these many networks, and the time required 5×\nto execute them at test-time, prohibits their use in applica-\ntions where test sets are large (e.g., ImageNet). In this pa- 4×\nper, we present a method for compressing large, complex\ntrained ensembles into a single network, where knowledge 3×\nfrom a variety of trained deep neural networks (DNNs) is\ndistilled and transferred to a single DNN. In order to distill 2×\ndiverse knowledge from different trained (teacher) models,\nweproposetouseadversarial-basedlearningstrategywhere 1×\nwe define a block-wise training loss to guide and optimize\nthepredefinedstudentnetworktorecovertheknowledgein\n0×\nteachermodels,andtopromotethediscriminatornetworkto 1 2 3 4 5\n# of ensembles\ndistinguish teacher vs. student features simultaneously. The\nproposedensemblemethod(MEAL)oftransferringdistilled\nknowledgewithadversariallearningexhibitsthreeimportant\nadvantages: (1) the student network that learns the distilled\nknowledge with discriminators is optimized better than the\noriginalmodel;(2)fastinferenceisrealizedbyasinglefor-\nward pass, while the performance is even better than tra-\nditional ensembles from multi-original models; (3) the stu-\ndentnetworkcanlearnthedistilledknowledgefromateacher\nmodelthathasarbitrarystructures.Extensiveexperimentson\nCIFAR-10/100, SVHN and ImageNet datasets demonstrate\nthe effectiveness of our MEAL method. On ImageNet, our\nResNet-50basedMEALachievestop-1/521.79%/5.99%val\nerror,whichoutperformstheoriginalmodelby2.06%/1.14%.\n1. Introduction\nThe ensemble approach is a collection of neural networks\nwhose predictions are combined at test stage by weighted\naveraging or voting. It has been long observed that en-\nsembles of multiple networks are generally much more ro-\nbust and accurate than a single network. This benefit has\nalso been exploited indirectly when training a single net-\nwork through Dropout (Srivastava et al. 2014), Dropcon-\nnect(Wanetal.2013),StochasticDepth(Huangetal.2016),\nSwapout(Singh,Hoiem,andForsyth2016),etc.Weextend\nthis idea by forming ensemble predictions during training,\n∗equalcontribution.ThisworkwasdonewhenZhankuiHewas\naresearchinternatUniversityofIllinoisatUrbana-Champaign.\nCopyright(cid:13)c 2019,AssociationfortheAdvancementofArtificial\nIntelligence(www.aaai.org).Allrightsreserved.\nsPOLF\nFLOPs at Inference Time\nSnapshot Ensemble\n(Huang et al. 2017)\nOur FLOPs at Test Time\nFigure 1: Comparison of FLOPs at inference time. Huang\net al. (Huang et al. 2017a) employ models at different lo-\ncal minimum for ensembling, which enables no additional\ntrainingcost,butthecomputationalFLOPsattesttimelin-\nearlyincreasewithmoreensembles.Incontrast,ourmethod\nuseonlyonemodelduringinferencetimethroughout,sothe\ntestingcostisindependentof#ensembles.\nusingtheoutputsofdifferentnetworkarchitectureswithdif-\nferentoridenticalaugmentedinput.Ourtestingstilloperates\nonasinglenetwork,butthesupervisionlabelsmadeondif-\nferentpre-trainednetworkscorrespondtoanensemblepre-\ndictionofagroupofindividualreferencenetworks.\nThe traditional ensemble, or called true ensemble, has\nsome disadvantages that are often overlooked. 1) Redun-\ndancy: The information or knowledge contained in the\ntrainedneuralnetworksarealwaysredundantandhasover-\nlaps between with each other. Directly combining the pre-\ndictionsoftenrequiresextracomputationalcostbutthegain\nis limited. 2) Ensemble is always large and slow: Ensem-\nble requires more computing operations than an individual\nnetwork,whichmakesitunusableforapplicationswithlim-\nitedmemory,storagespace,orcomputationalpowersuchas\ndesktop,mobileandevenembeddeddevices,andforappli-\ncationsinwhichreal-timepredictionsareneeded.\nTo address the aforementioned shortcomings, in this pa-\nper we propose to use a learning-based ensemble method.\nOurgoalistolearnanensembleofmultipleneuralnetworks\n4886"
      },
      {
        "page": 2,
        "content": "proving the generalization of an individual network. Re-\nlibrarytobaccoshop\ncently, Snapshot Ensembles (Huang et al. 2017a) is pro-\nbookshop toyshop posedtoaddressthecostoftrainingensembles.Incontrast\ntotheSnapshotEnsembles,herewefocusonthecostoftest-\nconfectionery ing ensembles. Our method is based on the recently raised\ngrocerystore\nknowledge distillation (Hinton, Vinyals, and Dean 2015;\nPapernot et al. 2017; Li et al. 2017; Yim et al. 2017) and\nadversariallearning(Goodfellowetal.2014),sowewillre-\nviewtheonesthataremostdirectlyconnectedtoourwork.\nFigure2:Leftisatrainingexampleofclass“tobaccoshop”\n“Implicit”Ensembling.Essentially,ourmethodisan“im-\nfrom ImageNet. Right are soft distributions from different\nplicit” ensemble which usually has high efficiency during\ntrained architectures. The soft labels are more informative\nboth training and testing. The typical “implicit” ensemble\nandcanprovidemorecoverageforvisually-relatedscenes.\nmethods include: Dropout (Srivastava et al. 2014), Drop-\nConnection (Wan et al. 2013), Stochastic Depth (Huang et\nal. 2016), Swapout (Singh, Hoiem, and Forsyth 2016), etc.\nwithout incurring any additional testing costs. We achieve\nThese methods generally create an exponential number of\nthis goal by leveraging the combination of diverse outputs\nnetworks with shared weights during training and then im-\nfromdifferentneuralnetworksassupervisionstoguidethe\nplicitlyensemblethemattesttime.Incontrast,ourmethod\ntarget network training. The reference networks are called\nfocusesonthesubtledifferencesoflabelswithidenticalin-\nTeachersandthetargetnetworksarecalledStudents.Instead\nput.Perhapsthemostsimilartoourworkistherecentpro-\nofusingthetraditionalone-hotvectorlabels,weusethesoft\nposedLabelRefinery(Bagherinezhadetal.2018),whofo-\nlabelsthatprovidemorecoverageforco-occurringandvisu-\ncusonthesinglemodelrefinementusingthesoftenedlabels\nallyrelatedobjectsandscenes.Wearguethatlabelsshould\nfrom the previous trained neural networks and iteratively\nbe informative for the specific image. In other words, the\nlearnanewandmoreaccuratenetwork.Ourmethoddiffers\nlabels should not be identical for all the given images with\nfromitinthatweintroduceadversarialmodulestoforcethe\nthesameclass.Morespecifically,asshowninFig.2,anim-\nmodeltolearnthedifferencebetweenteachersandstudents,\nage of “tobacco shop” has similar appearance to “library”\nwhichcanimprovemodelgeneralizationandcanbeusedin\nshould have a different label distribution than an image of\nconjunctionwithanyotherimplicitensemblingtechniques.\n“tobaccoshop”butismoresimilarto“grocerystore”.Itcan\nAdversarial Learning. Generative Adversarial Learn-\nalso be observed that soft labels can provide the additional\ning (Goodfellow et al. 2014) is proposed to generate\nintra-andinter-categoryrelationsofdatasets.\nrealistic-looking images from random noise using neural\nTo further improve the robustness of student networks,\nnetworks. It consists of two components. One serves as a\nwe introduce an adversarial learning strategy to force the\ngenerator and another one as a discriminator. The gener-\nstudent to generate similar outputs as teachers. Our exper-\nator is used to synthesize images to fool the discrimina-\niments show that MEAL consistently improves the accu-\ntor, meanwhile, the discriminator tries to distinguish real\nracy across a variety of popular network architectures on\nand fake images. Generally, the generator and discrimina-\ndifferent datasets. For instance, our shake-shake (Gastaldi\ntoraretrainedsimultaneouslythroughcompetingwitheach\n2017)basedMEALachieves2.54%testerroronCIFAR-10,\nwhichisarelative11.2%improvement1.OnImageNet,our other.Inthiswork,weemploygeneratorstosynthesizestu-\ndentfeaturesandusediscriminatortodiscriminatebetween\nResNet-50 based MEAL achieves 21.79%/5.99% val error,\nteacher and student outputs for the same input image. An\nwhichoutperformsthebaselinebyalargemargin.\nadvantage of adversarial learning is that the generator tries\nInsummary,ourcontributioninthispaperisthreefold.\nto produce similar features as a teacher that the discrimi-\n• Anend-to-endframeworkwithadversariallearningisde- nator cannot differentiate. This procedure improves the ro-\nsignedbasedontheteacher-studentlearningparadigmfor bustness of training for student network and has applied to\ndeepneuralnetworkensembling. manyfieldssuchasimagegeneration(Johnson,Gupta,and\n• Theproposedmethodcanachievethegoalofensembling Fei-Fei2018),detection(Baietal.2018),etc.\nmultipleneuralnetworkswithnoadditionaltestingcost. Knowledge Transfer. Distilling knowledge from trained\n• Theproposedmethodimprovesthestate-of-the-artaccu- neural networks and transferring it to another new network\nracyonCIFAR-10/100,SVHN,ImageNetforavarietyof hasbeenwellexploredin(Hinton,Vinyals,andDean2015;\nexistingnetworkarchitectures. Chen,Goodfellow,andShlens2016;Lietal.2017;Yimet\nal. 2017; Bagherinezhad et al. 2018; Anil et al. 2018). The\n2. RelatedWork typicalwayoftransferringknowledgeistheteacher-student\nThereisalargebodyofpreviouswork(HansenandSalamon learningparadigm,whichusesasofteneddistributionofthe\n1990;PerroneandCooper1995;KroghandVedelsby1995; final output of a teacher network to teach information to a\nDietterich 2000; Huang et al. 2017a; Lakshminarayanan, student network. With this teaching procedure, the student\nPritzel, and Blundell 2017) on ensembles with neural net- can learn how a teacher studied given tasks in a more effi-\nworks. However, most of these prior studies focus on im- cient form. Yim et al. (Yim et al. 2017) define the distilled\nknowledge to be transferred flows between different inter-\n1Shake-shakebaseline(Gastaldi2017)is2.86%. mediatelayersandcomputertheinnerproductbetweenpa-\n4887"
      },
      {
        "page": 3,
        "content": "NrehcaeT\nTeacher\nSelection\nModule FcLayers\nBinaryCross-entropy\nLoss\nTeacherNet Alignment Alignment Alignment\nSimilarityLoss SimilarityLoss SimilarityLoss\nGenerator\nDiscriminator Discriminator Discriminator\nAlignment Alignment Alignment\nStudentNet\n…\nArehcaeT\nFigure3:Overviewofourproposedarchitecture.Weinputthesameimageintotheteacherandstudentnetworkstogenerate\nintermediate and final outputs for Similarity Loss and Discriminators. The model is trained adversarially against several dis-\ncriminator networks. During training the model observes supervisions from trained teacher networks instead of the one-hot\nground-truthlabels,andtheteacher’sparametersarefixedallthetime.\nrametersfromtwonetworks.Bagherinezhadetal.(Bagher- dent network S is trained over the same set of images,\nθ\ninezhadetal.2018)studiedtheeffectsofvariousproperties but uses labels generated by T . More formally, we can\nθ\noflabelsandintroducetheLabelRefinerymethodthatiter- viewthisprocedureastrainingS onanewlabeleddataset\nθ\natively updated the ground truth labels after examining the D˜ = (X ,T (X )).Oncetheteachernetworkistrained,we\ni θ i\nentiredatasetwiththeteacher-studentlearningparadigm. freezeitsparameterswhentrainingthestudentnetwork.\nWe train the student network S by minimizing the sim-\nθ\n3. Overview ilarity distance between its output and the soft label gener-\nSiamese-like Network Structure Our framework is a atedbytheteachernetwork.LettingpT cθ(X i) = T θ(X i)[c],\nsiamese-likearchitecturethatcontainstwo-streamnetworks pS cθ(X i)=S θ(X i)[c]betheprobabilitiesassignedtoclassc\nin teacher and student branches. The structures of two intheteachermodelT θandstudentmodelS θ.Thesimilarity\nstreams can be identical or different, but should have the metriccanbeformulatedas:\nsame number of blocks, in order to utilize the intermediate\nL =d(T (X ),S (X ))\noutputs. The whole framework of our method is shown in Sim θ i θ i\nFig. 3. It consists of a teacher network, a student network, =(cid:88) d(pT cθ(X i),pS cθ(X i)) (1)\nalignmentlayers,similaritylosslayersanddiscriminators.\nc\nTheteacherandstudentnetworksareprocessedtogener- We investigated three distance metrics in this work, in-\nateintermediateoutputsforalignment.Thealignmentlayer cluding(cid:96) ,(cid:96) andKL-divergence.Thedetailedexperimental\n1 2\nisanadaptivepoolingprocessthattakesthesameordiffer- comparisons are shown in Tab. 1. Here we formulate them\nent length feature vectors as input and output fixed-length asfollows.\nnewfeatures.Weforcethemodeltooutputsimilarfeatures (cid:96) distanceisusedtominimizetheabsolutedifferencesbe-\n1\nofstudentandteacherbytrainingstudentnetworkadversar- tweentheestimatedstudentprobabilityvaluesandtherefer-\nially against several discriminators. We will elaborate each enceteacherprobabilityvalues.Hereweformulateitas:\nofthesecomponentsinthefollowingsectionswithmorede-\nn\ntails. L\n(cid:96)1\nSim(S θ)= n1 (cid:88)(cid:88)(cid:12) (cid:12)pT cθ(X i)−pS cθ(X i)(cid:12) (cid:12)1 (2)\n4. AdversarialLearning(AL)forKnowledge c i=1\n(cid:96) distanceoreuclideandistanceisthestraight-linedistance\nDistillation 2\nineuclideanspace.Weuse(cid:96) lossfunctiontominimizethe\n2\n4.1 SimilarityMeasurement error which is the sum of all squared differences between\nGiven a dataset D = (X ,Y ), we pre-trained the teacher thestudentoutputprobabilitiesandtheteacherprobabilities.\ni i\nnetwork T\nθ\nover the dataset using the cross-entropy loss The(cid:96) 2canbeformulatedas:\nagainsttheone-hotimage-levellabels2 inadvance.Thestu- n\n2Ground-truthlabels\nL\n(cid:96)2\nSim(S θ)= n1 (cid:88)(cid:88)(cid:13) (cid:13)pT cθ(X i)−pS cθ(X i)(cid:13) (cid:13)2 (3)\nc i=1\n4888"
      },
      {
        "page": 4,
        "content": "0.9 0.1 0.6 0.2 0.3 0.5 0.7 0.3 0 0.5 0 0 0 0.1\n!\n\" adaptivepooling\nTeacheroutputs 0 0 2 indices 0 0 2 indices\noutputsize=3\n! ! 0.9 0.6 0.7 values 0.3 0.5 0.1 gradients\n# Teacher?\nForward backward\nStudent?\nStudentoutputs\nFigure 5: The process of adaptive pooling in forward and\nbackwardstages.Weusemaxoperationforillustration.\nFigure4:Illustrationofourproposeddiscriminator.Wecon-\ncatenate the outputs of teacher and student as the inputs (cid:88)\nof a discriminator. The discriminator is a three-layer fully- L Sim = Lj Sim (7)\nconnectednetwork. j∈A\nwhereAisthesetoflayersthatwechoosetoproduceout-\nput.Inourexperiments,weusethelastlayerineachblock\nKL-divergenceisameasureofhowoneprobabilitydistri- ofanetwork(block-wise).\nbution is different from another reference probability dis-\ntribution. Here we train student network S by minimizing 4.3 StackedDiscriminators\nθ\ntheKL-divergencebetweenitsoutputpS cθ(X i)andthesoft\nWegeneratestudentoutputbytrainingthestudentnetwork\nlabels pT cθ(X i) generated by the teacher network. Our loss\nS\nθ\nand freezing the teacher parts adversarially against a\nfunctionis:\nseries of stacked discriminators D . A discriminator D at-\nj\nL KLSim(S θ)=− n1 (cid:88)\nc\n(cid:88) i=n 1pT cθ(X i)log(p pS c\nT\ncθθ (( XX ii )) ) t me Lm iz jp ints gt to h =ecl fa os ls l Eoif wy ii nt lgs ogi on b Dp ju ect (xtx iv )a e +s (Gte oa Ec oh de fer l lo l oor gws (t 1u etd −ae ln D.t 2b 0 (y x1 )4m )):a (x 8i-\n)\nn GAN j j\n=− n1 (cid:88)(cid:88) pT cθ(X i)logpS cθ(X i) (4)\nwhere\nxx ∼∼p pte sa tc uhe dr\nent\nare\noutputx s∼ fp rs otu mdent\ngeneration network\nc i=1 S .Atthesametime,S attemptstogeneratesimilarout-\nn\nθj θj\n+ n1 (cid:88)(cid:88) pT cθ(X i)logpT cθ(X i) put Ss inw ch eic thh ew pi all rafo mo el tt eh re sd oi fs oc uri rm tein aa ct ho er rb ay rem fiin xeim di dz uin rig nL gj G trA aiN n-.\nc i=1\ning,thefirsttermcanberemovedandourfinalobjectiveloss\nwhere the second term is the entropy of soft labels from is:\nrte ea mc oh ve ern ite at nw dor sk ima pn ld yi ms ic no imns it za ent thw ei cth ror se ss -p enec trt ot po yT lθ o. ssW ae sfc oa ln\n-\nLj\nGAN\n= x∼pE studentlog(1−D j(x)) (9)\nlows: In Eq. 10, x is the concatenation of teacher and student\noutputs. We feed x into the discriminator which is a three-\nn layerfully-connectednetwork.Thewholestructureofadis-\n1 (cid:88)(cid:88)\nL CESim(S θ)=−\nn\npT cθ(X i)logpS cθ(X i) (5) c Mri um ltin i-a St to ar gi ess Dh io sw crn imin inF aig to. r4 s.\n. Using multi-Stage discrimi-\nc i=1\nnatorscanrefinethestudentoutputsgradually.Asshownin\nFig. 3, the final adversarial loss is a sum of the individual\n4.2 IntermediateAlignment\nones:\nAdaptive Pooling. The purpose of the adaptive pooling L = (cid:88) Lj (10)\nlayer is to align the intermediate output from teacher net- GAN GAN\nj∈A\nwork and student network. This kind of layer is similar to\nLet|A|bethenumberofdiscriminators.Inourexperiments,\ntheordinarypoolinglayerlikeaverageormaxpooling,but\nweuse3forCIFAR(Krizhevsky2009)andSVHN(Netzer\ncangenerateapredefinedlengthofoutputwithdifferentin-\netal.2011),and5forImageNet(Dengetal.2009).\nputsize.Becauseofthisspecialty,wecanusethedifferent\nteachernetworksandpooltheoutputtothesamelengthof\n4.4 JointTrainingofSimilarityandDiscriminators\nstudentoutput.Poolinglayercanalsoachievespatialinvari-\nBased on above definition and analysis, we incorporate the\nancewhenreducingtheresolutionoffeaturemaps.Thus,for\nsimilarity loss in Eq. 7 and adversarial loss in Eq. 10 into\ntheintermediateoutput,ourlossfunctionis:\nourfinallossfunction.Ourwholeframeworkistrainedend-\nLj =d(f(T ),f(S )) (6) to-endbythefollowingobjectivefunction:\nSim θj θj\nL=αL +βL (11)\nwhere T and S are the outputs at j-th layer of the Sim GAN\nθj θj\nteacher and student, respectively. f is the adaptive pooling where α and β are trade-off weights. We set them as\nfunction that can be average or max. Fig. 5 illustrates the 1 in our experiments by cross validation. We also use the\nprocessofadaptivepooling.Becauseweadoptmultiplein- weightedcoefficientstobalancethecontributionsofdiffer-\ntermediate layers, our final similarity loss is a sum of indi- entblocks.For3-blocknetworks,weues[0.01,0.05,1],and\nvidualone: [0.001,0.01,0.05,0.1,1]for5-blockones.\n4889"
      },
      {
        "page": 5,
        "content": "5. Multi-ModelEnsembleviaAdversarial tion scheme3 (Lee et al. 2015; Romero et al. 2015; Lars-\nLearning(MEAL) son, Maire, and Shakhnarovich 2016; Huang et al. 2017a;\nLiuetal.2017)isused.Wereportthetesterrorsinthissec-\nWe achieve ensemble with a training method that is sim- tionwithtrainingonthewholetrainingset.\nple and straight-forward to implement. As different net- SVHN. The Street View House Number (SVHN)\nworkstructurescanobtaindifferentdistributionsofoutputs, dataset (Netzer et al. 2011) consists of 32×32 colored\nwhich can be viewed as soft labels (knowledge), we adopt digit images, with one class for each digit. The train\nthese soft labels to train our student, in order to compress and test sets contain 604,388 and 26,032 images, respec-\nknowledge of different architectures into a single network. tively. Following previous works (Goodfellow et al. 2013;\nThuswecanobtaintheseeminglycontradictorygoalofen- Huangetal.2016;2017a;Liuetal.2017),wesplitasubset\nsembling multiple neural networks at no additional testing of 6,000 images for validation, and train on the remaining\ncost. imageswithoutdataaugmentation.\nImageNet.TheILSVRC2012classificationdataset(Deng\n5.1 LearningProcedure et al. 2009) consists of 1000 classes, with a number of\n1.2 million training images and 50,000 validation im-\nToclearlyunderstandwhatthestudentlearnedinourwork,\nages. We adopt the the data augmentation scheme follow-\nwe define two conditions. First, the student has the same\ning(Krizhevsky,Sutskever,andHinton2012)andapplythe\nstructure as the teacher network. Second, we choose one\nsameoperationas (Huangetal.2017a)attesttime.\nstructure for student and randomly select a structure for\nteacherineachiterationasourensemblelearningprocedure.\n6.2 Networks\nThelearningprocedurecontainstwostages.First,wepre-\ntrain the teachers to produce a model zoo. Because we use We adopt several popular network architectures as our\ntheclassificationtasktotrainthesemodels,wecanusethe teachermodelzoo,includingVGGNet(SimonyanandZis-\nsoftmax cross entropy loss as the main training loss in this serman 2015), ResNet (He et al. 2016), DenseNet (Huang\nstage.Second,weminimizethelossfunctionLinEq.11to et al. 2017b), MobileNet (Howard et al. 2017), shake-\nmakethestudentoutputsimilartothatoftheteacheroutput. shake (Gastaldi 2017), etc. For VGGNet, we use 19-layer\nThelearningprocedureisexplainedbelowinAlgorithm1. with Batch Normalization (Ioffe and Szegedy 2015). For\nResNet,weuse18-layernetworkforCIFARandSVHNand\n50-layerforImagNet.ForDenseNet,weusetheBC struc-\nAlgorithm1Multi-ModelEnsembleviaAdversarialLearn-\nture with depth L=100, and growth rate k=24. For shake-\ning(MEAL).\nshake,weuse26-layer2×96dversion.Notethatduetothe\nStage1: highcomputingcosts,weuseshake-shakeasateacheronly\nBuilding and Pre-training the Teacher Model Zoo T = whenthestudentisshake-shakenetwork.\n{T1,T2,...Ti},including:VGGNet(SimonyanandZisserman\nθ θ θ\n2015), ResNet (He et al. 2016), DenseNet (Huang et al. 2017b),\nMobileNet(Howardetal.2017),Shake-Shake(Gastaldi2017),etc. Table 1: Ablation study on CIFAR-10 using VGGNet-19\nStage2:\nw/BN.PleaserefertoSection6.3formoredetails.\n1: functionTSM(T)\n2: T ←RS(T) (cid:46)RandomSelection\n(cid:96)1dis. (cid:96)2dis. Cross-Entropy Intermediate Adversarial TestErrors(%)\nθ BaseModel(VGG-19w/BN)(SimonyanandZisserman2015) 6.34\n3: returnT\nθ (cid:33) 6.97\n4: endfunction (cid:33) 6.22\n5: foreachiterationdo: (cid:33) 6.18\n6: T θ ←TSM(T) (cid:46)RandomlySelectaTeacherModel (cid:33) (cid:33) 6.10\n7: StuS deθ n=\nt\nargmin Sθ L(T θ,S θ) (cid:46)AdversarialLearningfora (cid:33)\n(cid:33)\n(cid:33)\n(cid:33) (cid:33)\n6 5. .1 87\n3\n8: endfor (cid:33) (cid:33) (cid:33) 7.57\n6.3 AblationStudies\n6. ExperimentsandAnalysis\nWe first investigate each design principle of our MEAL\nWe empirically demonstrate the effectiveness of MEAL on framework. We design several controlled experiments on\nseveral benchmark datasets. We implement our method on CIFAR-10withVGGNet-19w/BN(bothtoteacherandstu-\nthePyTorch(Paszkeetal.2017)platform. dent)forthisablationstudy.Aconsistentsettingisimposed\non all the experiments, unless when some components or\n6.1. Datasets structuresareexamined.\nThe results are mainly summarized in Table 1. The first\nCIFAR. The two CIFAR datasets (Krizhevsky 2009) con-\nthreerowsindicatethatweonlyuse(cid:96) ,(cid:96) orcross-entropy\n1 2\nsistofcolorednaturalimageswithasizeof32×32.CIFAR-\n10 is drawn from 10 and CIFAR-100 is drawn from 100 3zero-padded with 4 pixels on both sides, randomly cropped\nclasses.Ineachdataset,thetrainandtestsetscontain50,000 toproduce32x32images,andhorizontallymirrorwithprobability\nand10,000images,respectively.Astandarddataaugmenta- 0.5.\n4890"
      },
      {
        "page": 6,
        "content": "3.80\n3.75 3.70\n3.65\n3.60\n3.55\n3.50\nCIFAR-10\n)%( rorrE\ntseT\n3.76 3.74 3.73 S S Tri in n ug g el l e e E nA sL . 20 Our Ens. 19\n18\n3.56 17\nCIFAR-100\n)%( rorrE\ntseT\n1.80\n20.23 S Si in ng gl le e AL T Or uu re E E nn ss .. 1.75 19.04\n1.70\n17.73\n1.65 17.21\n1.60\nSVHN\n)%( rorrE\ntseT\nSingle 24.0 1.77 Single AL True Ens. 23.5 Our Ens. 23.0\n1.69 22.5\n1.66 22.0\n1.64 21.5\n21.0\nImageNet\n)%( rorrE\ntseT\n1-poT\n23.85 Single 23.71 Single AL True Ens. Our Ens. 22.76\n21.69\nFigure6:Errorrates(%)onCIFAR-10andCIFAR-100,SVHNandImageNetdatasets.Ineachfigure,theresultsfromleftto\nrightare1)basemodel;2)basemodelwithadversariallearning;3)trueensemble/traditionalensemble;and4)ourensemble\nresults.Forthefirstthreedatasets,weemployDenseNetasstudent,andResNetforthelastone(ImageNet).\nloss from the last layer of a network. It’s similar to the\nTable3:Comparisonoferrorrate(%)withDropout(Srivas-\nKnowledge Distillation method. We can observe that use\ntavaetal.2014)baselineonCIFAR-10.\ncross-entropy achieve the best accuracy. Then we employ\nmoreintermediateoutputstocalculatetheloss,asshownin Network Dropout(%) OurEns.(%)\nVGG-19w/BN(SimonyanandZisserman2015) 6.89 5.55\nrows 4 and 5. It’s obvious that including more layers im-\nGoogLeNet(Szegedyetal.2015) 5.37 4.83\nprovestheperformance.Finally,weinvolvethediscrimina-\nResNet-18(Heetal.2016) 4.69 4.35\ntorstoexamtheeffectivenessofadversariallearning.Using\nDenseNet-BC(k=24)(Huangetal.2017b) 3.75 3.54\ncross-entropy, intermediate layers and adversarial learning\nachieve the best result. Additionally, we use average based\nadaptivepoolingforalignment.Wealsotriedmaxoperation,\nditional ensemble are the sum of individual ones. There-\ntheaccuracyismuchworse(6.32%).\nfore, our method has both better performance and higher\n6.4 Results efficiency. Most notably, our MEAL Plus4 yields an error\nrate of Top-1 21.79%, Top-5 5.99% on ImageNet, far out-\nComparison with Traditional Ensemble. The results are\nperforming the original ResNet-50 23.85%/7.13% and the\nsummarized in Figure 6 and Table 2. In Figure 6, we com-\ntraditional ensemble 22.76%/6.49%. This shows great po-\npare the error rate using the same architecture on a vari-\ntentialonlarge-scalereal-sizedatasets.\nety of datasets (except ImageNet). It can be observed that\nourresultsconsistentlyoutperformthesingleandtraditional\nmethods on these datasets. The traditional ensembles are Table4:Val.error(%)onImageNetdataset.\nobtained through averaging the final predictions across all\nMethod Top-1(%) Top-5(%) #FLOPs InferenceTime(per/image)\nteachermodels.InTable2,wecompareerrorrateusingdif- TeacherNetworks:\nferent architectures on the same dataset. In most cases, our VGG-19w/BN 25.76 8.15 19.52B 5.70×10−3s\nensemblemethodachieveslowererrorthananyofthebase- ResNet-50 23.85 7.13 4.09B 1.10×10−2s\nlines,includingthesinglemodelandtraditionalensemble. Ours(ResNet-50) 23.58 6.86 4.09B 1.10×10−2s\nTraditionalEns. 22.76 6.49 23.61B 1.67×10−2s\nOursPlus(ResNet-50) 21.79 5.99 4.09B 1.10×10−2s\nTable2:Errorrate(%)usingdifferentnetworkarchitectures\nonCIFAR-10dataset.\nNetwork Single(%) TraditionalEns.(%) OurEns.(%)\nMobileNet(Howardetal.2017) 10.70 – 8.09\nVGG-19w/BN(SimonyanandZisserman2015) 6.34 – 5.55\nDenseNet-BC(k=24)(Huangetal.2017b) 3.76 3.73 3.54\nShake-Shake-262x96d(Gastaldi2017) 2.86 2.79 2.54\nComparison with Dropout. We compare MEAL with the\n“Implicit”methodDropout(Srivastavaetal.2014).There-\nsults are shown in Table 3, we employ several network ar-\nchitectures in this comparison. All models are trained with\nthesameepochs.Weuseaprobabilityof0.2fordropnodes\nduringtraining.Itcanbeobservedthatourmethodachieves\nbetterperformancethanDropoutonallthesenetworks.\nOurLearning-BasedEnsembleResultsonImageNet.As Figure7:Accuracyofourensemblemethodunderdifferent\nshowninTable4,wecompareourensemblemethodwiththe trainingbudgetsonCIFAR-10.\noriginalmodelandthetraditionalensemble.WeuseVGG-\n19 w/BN and ResNet-50 as our teachers, and use ResNet-\n50 as the student. The #FLOPs and inference time for tra- 4denotesusingmorepowerfulteacherslikeResNet-101/152.\n4891"
      },
      {
        "page": 7,
        "content": "10.0\n9.5\n9.0\n8.5\n8.0\n7.5\n1 2 3 4 5\n# of ensembles\n)%(\nrorrE\ntseT\nMobileNet\n6.7\n6.2 MobileNet baseline: 10.70% 5.9\n(Howard et al. 2017) 5.8\n5.7\n5.6\n5.5\n1 2 3 4 5\n# of ensembles\n)%(\nrorrE\ntseT\nVGG19-BN\n3.75 VGG19-BN baseline: 6.34%\n(Simonyan et al. 2015) 3.70\n3.65\n3.60\n3.55\n3.50\n1 2 3 4 5\n# of ensembles\n)%(\nrorrE\ntseT\nDenseNet\nDenseNet baseline: 3.76%\n(Huang et al. 2017)\nFigure8:Errorrate(%)onCIFAR-10withMobileNet,VGG-19w/BNandDenseNet.\nFigure10:VisualizationsofvalidationimagesfromtheIma-\ngeNetdatasetbyt-SNE(MaatenandHinton2008).Weran-\ndomlysample10classeswithin1000classes.Leftisthesin-\nFigure 9: Probability Distributions between four net-\ngle model result using the standard training strategy. Right\nworks. Left: SequeezeNet vs. VGGNet. Right: ResNet vs.\nisourensemblemodelresult.\nDenseNet.\n1,2,...,1000,andtherightfigureis(pk ,pk ).\n6.5 Analysis ResNet DenseNet\nIfthelabeldistributionsareidenticalfromtwonetworks,the\nEffectivenessofEnsembleSize.Figure8displaystheper- bubbleswillbeplacedonthemasterdiagonal.It’sveryin-\nterestingtoobservethattheleft(weakernetworkpairs)has\nformanceofthreearchitecturesonCIFAR-10astheensem-\nbigger diversity than the right (stronger network pairs). It\nblesizeisvaried.Althoughensemblingmoremodelsgener-\nmakes sense because the stronger models generally tend to\nally gives better accuracy, we have two important observa-\ngeneratepredictionsclosetotheground-truth.Inbrief,these\ntions.First,weobservethatoursinglemodel“ensemble”al-\ndifferencesinpredictionscanbeexploitedtocreateeffective\nreadyoutputsthebaselinemodelwitharemarkablemargin,\nensemblesandourmethodiscapableofimprovingthecom-\nwhich demonstrates the effectiveness of adversarial learn-\npetitivebaselinesusingthiskindofdiversesupervisions.\ning. Second, we observe some drops in accuracy using the\nVGGNetandDenseNetnetworkswhenincludingtoomany\n6.6 VisualizationoftheLearnedFeatures\nensembles for training. In most case, an ensemble of four\nmodelsobtainsthebestperformance. To further explore what our model actually learned, we vi-\nBudget for Training. On CIFAR datasets, the standard sualize the embedded features from the single model and\ntraining budget is 300 epochs. Intuitively, our ensemble our ensembling model. The visualization is plotted by t-\nmethod can benefit from more training budget, since we SNEtool(MaatenandHinton2008)withthelastconv-layer\nusethediversesoftdistributionsaslabels.Figure7displays features (2048 dimensions) from ResNet-50. We randomly\ntherelationbetweenperformanceandtrainingbudget.Itap- sample 10 classes on ImageNet, results are shown in Fig-\npears that more than 400 epochs is the optimal choice and ure10,it’sobviousthatourmodelhasbetterfeatureembed-\nourmodelwillfullyconvergeatabout500epochs. dingresult.\nDiversityofSupervision.Wehypothesizethatdifferentar-\n7. Conclusion\nchitecturescreatesoftlabelswhicharenotonlyinformative\nbutalsodiversewithrespecttoobjectcategories.Wequalita- We have presented MEAL, a learning-based ensemble\ntivelymeasurethisdiversitybyvisualizingthepairwisecor- method that can compress multi-model knowledge into a\nrelationofsoftmaxoutputsfromtwodifferentnetworks.To single network with adversarial learning. Our experimental\ndoso,wecomputethesoftmaxpredictionsforeachtraining evaluationonthreebenchmarksCIFAR-10/100,SVHNand\nimageinImageNetdatasetandvisualizeeachpairofthecor- ImageNetverifiedtheeffectivenessofourproposedmethod,\nrespondingones.Figure9displaysthebubblemapsoffour whichachievedthestate-of-the-artaccuracyforavarietyof\narchitectures.Intheleftfigure,thecoordinateofeachbubble networkarchitectures.Ourfurtherworkwillfocusonadopt-\nis a pair of k-th predictions (pk ,pk ), k = ingMEALforcross-domainensembleandadaption.\nSequeezeNet VGGNet\n4892"
      },
      {
        "page": 8,
        "content": "AcknowledgementsThisworkwassupportedinpartbyNa- Larsson, G.; Maire, M.; and Shakhnarovich, G. 2016. Fractal-\ntionalKeyR&DProgramofChina(No.2017YFC0803700), net:Ultra-deepneuralnetworkswithoutresiduals. arXivpreprint\nNSFC under Grant (No.61572138 & No.U1611461) and arXiv:1605.07648.\nSTCSMProjectunderGrantNo.16JC1420400.\nLee,C.-Y.;Xie,S.;Gallagher,P.W.;etal.2015.Deeply-supervised\nnets. InAISTATS.\nReferences\nLi, Y.; Yang, J.; Song, Y.; Cao, L.; Luo, J.; and Li, L.-J. 2017.\nAnil, R.; Pereyra, G.; Passos, A.; Ormandi, R.; Dahl, G. E.; and Learningfromnoisylabelswithdistillation. InICCV.\nHinton,G.E.2018.Largescaledistributedneuralnetworktraining\nLiu,Z.;Li,J.;Shen,Z.;Huang,G.;Yan,S.;andZhang,C. 2017.\nthroughonlinedistillation. InICLR.\nLearning efficient convolutional networks through network slim-\nBagherinezhad, H.; Horton, M.; Rastegari, M.; and Farhadi, A. ming. InICCV.\n2018. Label refinery: Improving imagenet classification through\nMaaten,L.v.d.,andHinton,G.2008.Visualizingdatausingt-sne.\nlabelprogression. InECCV.\nJournalofmachinelearningresearch9(Nov):2579–2605.\nBai,Y.;Zhang,Y.;Ding,M.;andGhanem,B. 2018. Findingtiny\nNetzer, Y.; Wang, T.; Coates, A.; Bissacco, A.; Wu, B.; and Ng,\nfacesinthewildwithgenerativeadversarialnetwork.\nA.Y. 2011. Readingdigitsinnaturalimageswithunsupervised\nChen,T.;Goodfellow,I.;andShlens,J. 2016. Net2net:Accelerat- featurelearning.InNIPSworkshopondeeplearningandunsuper-\ninglearningviaknowledgetransfer. InICLR. visedfeaturelearning,volume2011, 5.\nDeng,J.;Dong,W.;Socher,R.;Li,L.-J.;etal. 2009. Imagenet:A Papernot,N.;Abadi,M.;Erlingsson,U.;Goodfellow,I.;andTal-\nlarge-scalehierarchicalimagedatabase. InCVPR. war,K. 2017. Semi-supervisedknowledgetransferfordeeplearn-\nDietterich,T.G. 2000. Ensemblemethodsinmachinelearning. In ingfromprivatetrainingdata. InICLR.\nInternationalworkshoponmultipleclassifiersystems,1–15. Paszke,A.;Gross,S.;Chintala,S.;Chanan,G.;Yang,E.;DeVito,\nGastaldi, X. 2017. Shake-shake regularization. arXiv preprint Z.;Lin,Z.;Desmaison,A.;Antiga,L.;andLerer,A. 2017. Auto-\narXiv:1705.07485. maticdifferentiationinpytorch.\nGoodfellow,I.J.;Warde-Farley,D.;Mirza,M.;Courville,A.;and Perrone,M.P.,andCooper,L.N. 1995. Whennetworksdisagree:\nBengio,Y. 2013. Maxoutnetworks. InICML. Ensemblemethodsforhybridneuralnetworks. InHowWeLearn;\nHowWeRemember:TowardanUnderstandingofBrainandNeu-\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-\nralSystems:SelectedPapersofLeonNCooper.WorldScientific.\nFarley, D.; Ozair, S.; Courville, A.; and Bengio, Y. 2014. Gen-\n342–358.\nerativeadversarialnets. InNIPS.\nRomero,A.;Ballas,N.;Kahou,S.E.;Chassang,A.;Gatta,C.;and\nHansen, L. K., and Salamon, P. 1990. Neural network ensem-\nBengio,Y. 2015. Fitnets:Hintsforthindeepnets. InICLR.\nbles. IEEEtransactionsonpatternanalysisandmachineintelli-\ngence12(10):993–1001. Simonyan,K.,andZisserman,A. 2015. Verydeepconvolutional\nnetworksforlarge-scaleimagerecognition. InICLR.\nHe,K.;Zhang,X.;Ren,S.;andSun,J. 2016. Deepresiduallearn-\ningforimagerecognition. InCVPR. Singh,S.;Hoiem,D.;andForsyth,D.2016.Swapout:Learningan\nensembleofdeeparchitectures.InAdvancesinneuralinformation\nHinton,G.;Vinyals,O.;andDean,J. 2015. Distillingtheknowl-\nedgeinaneuralnetwork. arXivpreprintarXiv:1503.02531.\nprocessingsystems,28–36.\nHoward,A.G.;Zhu,M.;Chen,B.;Kalenichenko,D.;Wang,W.; Srivastava,N.;Hinton,G.E.;Krizhevsky,A.;etal.2014.Dropout:\nWeyand,T.;Andreetto,M.;andAdam,H. 2017. Mobilenets:Effi- asimplewaytopreventneuralnetworksfromoverfitting. JMLR.\ncientconvolutionalneuralnetworksformobilevisionapplications. Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P.; et al. 2015. Going\narXivpreprintarXiv:1704.04861. deeperwithconvolutions. InCVPR.\nHuang,G.;Sun,Y.;Liu,Z.;Sedra,D.;andWeinberger,K.Q.2016. Wan,L.;Zeiler,M.;Zhang,S.;LeCun,Y.;andFergus,R. 2013.\nDeepnetworkswithstochasticdepth. InECCV. Regularizationofneuralnetworksusingdropconnect. InICML.\nHuang, G.; Li, Y.; Pleiss, G.; Liu, Z.; Hopcroft, J. E.; and Wein- Yim,J.;Joo,D.;Bae,J.;andKim,J. 2017. Agiftfromknowledge\nberger,K.Q. 2017a. Snapshotensembles:Train1,getmforfree. distillation:Fastoptimization,networkminimizationandtransfer\nInICLR. learning. InCVPR.\nHuang, G.; Liu, Z.; Weinberger, K. Q.; and van der Maaten, L.\n2017b. Denselyconnectedconvolutionalnetworks. InCVPR.\nIoffe,S.,andSzegedy,C.2015.Batchnormalization:Accelerating\ndeepnetworktrainingbyreducinginternalcovariateshift. arXiv\npreprintarXiv:1502.03167.\nJohnson, J.; Gupta, A.; and Fei-Fei, L. 2018. Image generation\nfromscenegraphs. InCVPR.\nKrizhevsky,A.;Sutskever,I.;andHinton,G.2012.Imagenetclas-\nsificationwithdeepconvolutionalneuralnetworks. InNIPS.\nKrizhevsky, A. 2009. Learning multiple layers of features from\ntinyimages. Technicalreport.\nKrogh,A.,andVedelsby,J.1995.Neuralnetworkensembles,cross\nvalidation,andactivelearning. InNIPS.\nLakshminarayanan,B.;Pritzel,A.;andBlundell,C. 2017. Simple\nand scalable predictive uncertainty estimation using deep ensem-\nbles. InNIPS.\n4893"
      }
    ],
    "ocr_used": false,
    "ocr_backend": null,
    "extraction_quality": "good",
    "quality_reason": "文本提取质量正常"
  },
  "pdf_url": "/uploads/e0838343913dac157930858e11894035.pdf"
}