{
  "filename": "Katsumata_Label_Augmentation_As_Inter-Class_Data_Augmentation_for_Conditional_Image_Synthesis_WACV_2024_paper.pdf#___text=into the discriminator of a,com_raven38.pdf",
  "upload_time": "2025-12-12T21:06:33.125945",
  "data": {
    "full_text": "Label Augmentation as Inter-class Data Augmentation\nfor Conditional Image Synthesis with Imbalanced Data\nKaiKatsumata DucMinhVo HidekiNakayama\nTheUniversityofTokyo,Japan\n{katsumata,vmduc,nakayama}@nlab.ci.i.u-tokyo.ac.jp\nAbstract\nConditional image synthesis performs admirably when\ntrained on well-constructed and balanced datasets. How-\never, in practice, training datasets frequently contain mi-\nnorities (i.e., a class with a few samples), known as im-\nbalanced data, which causes difficulties in learning gen-\nerative models. To address conditional image synthesis\nwithimbalanceddata,weanalyzeadiversityissueoflabel-\npreservingdataaugmentationandanaffinityissueofnon-\nlabel-preservingdataaugmentation.Fromthisobservation,\nwepresentlabelaugmentation,whichworksasinter-class\ndata augmentation that effectively augments data by pre-\ndicting a new label for a given image using the prediction\nof a pretrained image classification model (i.e., probabili-\ntiesforeachclass). Weincorporateourlabelaugmentation\ninto the discriminator of a seminal conditional generative\nadversarial network (GAN) model, proposing Softlabel-\nGAN.Usingclassprobabilitiesextractsclass-invariantand\nshared features between similar classes, achieving data\naugmentation with high affinity and diversity. Our exper-\niments on imbalanced datasets show that Softlabel-GAN\nproduces images with high quality and diversity while be-\ning hardly affected by the number of samples in each\nclass. Code: https://github.com/raven38/\nsoftlabel-gan.\n1.Introduction\nThe impressive success of conditional deep generative\nmodels[12,24,37,45,48,49,62]hasbeenlargelyaidedbya\nlargeamountofwell-collected,balanced,anddiversedata,\ntypically consisting of not only a large number of images\nin total but also a certain number of images in each class.\nDespite the enormous number of images available online,\ncollectingspecialorrareobjectsisnotalwaysfeasibleow-\ning to annotation costs, data constraints (e.g., paintings of\na specific artist), unauthorized data, and privacy concerns.\nAsaresult,imbalanceddata[20]isinevitableinreal-world\nscenarios, leading to the failure of generative models and\nlaeR\nguAffiDhtoomS\nsruO\nThisWACVpaperistheOpenAccessversion,providedbytheComputerVisionFoundation.\nExceptforthiswatermark,itisidenticaltotheacceptedversion;\nthefinalpublishedversionoftheproceedingsisavailableonIEEEXplore.\nFigure 1. Generated examples on AnimeFace [1]. DiffAug-\nGAN(DiffuAug)generatesimageswithlowdiversity(redrectan-\ngles). Smooth-GAN(Smooth)generatesimagesthatdifferfrom\nthegivenclass(dashedblackrectangles). Incontrast, Softlabel-\nGAN(Ours)canavoidoverlappedimagesandimageswithoutre-\nspecttothegivenclass.Eachclasshasfivesamples.\namplifyingbiases. Thepotentialriskofthelatterhasbeen\ndiscussedintermsofAIethics[2,35,50]. Traininggener-\nativemodelsintheimbalanceddataregimeisthusconsid-\nered necessary, potentially broadening generative models’\nreal-worldapplicationsandprovidingsafetytothem.\nData augmentation is a straightforward solution for im-\nage generation with limited data. According to [13], two\nimportantaspectsofdataaugmentationareaffinityanddi-\nversity, with affinity indicating how small the distribution\nshiftsbetweentheaugmentedandtrainingdatadistributions\nand diversity indicating the complexity of the augmented\ndata. Low-affinitydatacausesthemodeltolearnincorrect\nfeatures, whereas low-diversity data causes the model to\neasilymemorizetrainingdata,degradingthequalityanddi-\nversityofgeneratedimages. Achievingaugmentationwith\nhigh affinity and diversity is difficult for imbalanced data\nbecauseoftworeasons: (i)thenumberoftrainingsamples\nisinsufficientinminorclassesand(ii)theminorclasssam-\nples are not diverse. As a result, data augmentation with\nhigh affinity and diversity is vital for successfully condi-\ntionalimagesynthesiswithimbalanceddata.\nData augmentation consists of two parts: label-\npreserving data augmentation [11,25,56,64,65] and non-\nlabel-preserving data augmentation [54,61]. The former\npart directly modifies only image inputs, whereas the lat-\n4944\n\nteronedoesthelabelsofinputs. Thelabel-preservingdata • We propose Softlabel-GAN, which uses our novel la-\naugmentation for images mostly employs geometric trans- bel augmentation, for conditional image synthesis in\nformations, image filters, and color intensity transforma- animbalanceddataregime. Tothebestofourknowl-\ntions,andtheiroperationsarelimitedtothosethatmaintain edge, thisisthefirststudythatinvestigateslabelaug-\ninput labels. On the other hand, non-label-preserving data mentationinlearningcGANswithimbalanceddata.\naugmentationallowsarbitraryoperations. Label-preserving • We demonstrate, on several imbalanced datasets, that\ndataaugmentationiswidelyusedingenerativeadversarial our method outperforms the other methods. Further-\nnetworks (GANs) [25,56,64,65], with notable results on more,theexperimentsshowouradvantageswithintra-\nlimitedandbalanceddata. However,inourcase,wherethe classfidelityanddiversity.\ndata is not only limited but also imbalanced (i.e., the ap-\npearance of minor classes), the current data augmentation 2.RelatedWork\nisinsufficienttoexpandthedatadistribution.Themainrea-\nImage generation with limited data aims to improve\nson is that such data augmentation creates additional data\nthe training stability and generation quality without the\nbysolelyreusingthesampleswithineachclass,leadingto\nimmense amount of data. Collecting large high-quality\naugmenteddatawithhighaffinityyetlowdiversity. Forin-\ndatasets is not always possible because of the tremendous\nstance,simplyapplyinggeometric,color,corruption,and/or\nannotation cost, data constraints, and privacy. In some\nfilteringtransformationstothetrainingdata(e.g.,DiffAug-\ncases, we only collect a few examples for each class, e.g.,\nment [64]) leads to a rapid imbalance between a generator\nphotos of a specific landmark or illustrations of a specific\nandadiscriminator,yieldingimageswithlowdiversity(i.e.,\nartist. SincetrainingGANswithoutahugeamountofdata\nrepeatalmostthesameimages)(Fig.1).\nis crucial, several studies [16,21,25,51,64] paid attention\nOur main idea is to predict a new label for a given im-\nto the data efficiency aspect. Some approaches [25,64]\nage by assigning probabilities to all classes to which the\nare able to learn from limited data by using data augmen-\nimagebelongs. Therefore,ournovelaugmentationisatype\ntation (i.e., label-preserving data augmentation). In con-\nof non-label-preserving data augmentation, which we call\ntrast to data augmentation-based approaches, another line\nlabelaugmentation. Weapproachourproblembyincorpo-\nofresearch[7,26,36,53]employssemi-andself-supervised\nrating our simple yet effective label augmentation into the\nlearning to reduce the cost of human annotation. Recent\ndiscriminator of a seminal cGAN model. More precisely,\nworks[42,44]designarchitecture-specificmethodsforim-\nwefirstprepareanimageclassificationmodelpretrainedon\nagegenerationwithlimiteddata. Weintroduceadataaug-\ntheimbalanceddata.Then,weusetheoutputofthesoftmax\nmentationapproach,whichachieveshigh-affinityandhigh-\nfunction(i.e.,probabilitiesforeachclass)obtainedthrough\ndiversityaugmentationonimbalanceddata.\nthepretrainedclassifierastheclassconditioninthediscrim-\nNon-label-preserving augmentation is a set of data aug-\ninator. Using class probabilities enables our model to take\nmentation. While the label-preserving augmentation di-\nintoaccountsemanticsimilaritiesbetweenclasseswithre-\nrectly modifies an input image, the non-label-preserving\nspecttotheperceptionofpretrainedclassifiers. Naivelabel\naugmentation modifies an input label. Of which, label\naugmentationmethods[54,61]blindlydistributeclassprob-\nsmoothing [54] and Mixup [61] are popularly used. Label\nabilities to the data, resulting in augmented data with high\nsmoothing [54] replaces hard targets with soft targets by\ndiversitybutlowaffinity,aswellasthegenerationofimages\ntaking the weighted average of the original targets, avoid-\nthat are irrelevant to the given class (Fig. 1). By contrast,\ning overconfidence on several tasks [9,57]. Online label\nour augmentation precisely distributes class probabilities,\nsmoothing [60] quantifies class similarities and then as-\nresulting in augmented images with both high affinity and\nsigns class-wise soft labels, unlike our method, which as-\ndiversity. Consequently,ourmethodhindersthetrainingof\nsigns instance-wise soft labels. Mixup [61] trains a net-\nthe discriminator and balances the generator and the dis-\nwork on convex combinations of the samples and their la-\ncriminator, yielding better-generated images (Fig. 1). Our\nbels to improve accuracy and robustness to hyperparame-\ncontributionscanbesummarizedasfollows:\nters. Thesemethods[54,61]oftenprovideincorrectinfor-\n• We observe that existing data augmentation ap- mationtocGANsowingtoblindlydistributingclassprob-\nproachesprovideeitherdiversityoraffinityforimbal- abilities, resulting in the high diversity yet low affinity of\nanceddata. augmented data. In this work, we present content-aware\n• We find that assigning classifier output with suffi- labelaugmentationforimbalanceddata,whichbuildsaug-\ncient entropy to samples can be interpreted as inter- menteddatawithhighaffinityanddiversity.\nclassdataaugmentationthatincreasesthediversityper Pretrained recognition models have been widely used in\nclass.Wethusproposeasimpleyeteffectivelabelaug- trainingGANs. Togenerateimagesconsidering their con-\nmentationmethodthatproducesaugmenteddatawith tents, the high-level features extracted from a pretrained\nhighaffinityanddiversityusingapretrainedclassifier. modelasanalternativetothehumanvisualperceptionare\n4945\n\nTable 1. Comparison of balanced and imbalanced datasets. For 3.2.ConditionalGANs\neach dataset, we indicate the range of the number of images in\neachclass, aratio(largestclasssamples/smallestclasssamples), Conditional GANs aim to model the conditional distri-\noverallsamples,andthenumberofimagesperclassandtheratio butionofatargetdatasetusingageneratorG:Rdz×Rk →\nafterourlabelaugmentation. Thenumberofsamplesperclasson Rd andadiscriminatorD :Rd×Rk →R,wheredandd\nz\nthe imbalanced dataset is various and significantly less than that arethedimensionsofanimageandalatentvariable,respec-\non the balanced one. A higher ratio indicates more imbalance. tively.Here,theclasslabely ∈Rkwithkbeingthenumber\nOuraugmentationactuallyincreasestheminorclasssamples(last\nofclassesindicatestheprobabilitiesofaninstancebelong-\ncolumn).\ningtoeachclass,includingone-hotvectors. Thegenerator\nDataset #Samples Ratio#Samples#AugmentedAugmented Gmapsconditionyf ∈Rk andlatentvectorz ∈Rdz from\nperclass samples ratio\na prior distribution p(z) to output xf = G(z,yf) ∈ Rd.\nBalanceddatasets\nThediscriminatorDlearnstodistinguishbetweenthegen-\nCIFAR-10[28] 5000 1× 50000 — —\nCIFAR-100[28] 500 1× 50000 — — erateddistributionpandthetargetdistributionq. Thedis-\nImbalanceddatasets criminator receives either a pair of a real sample xr ∈ Rd\nAnimeFace[1] 17–161 9.5× 14490 109–792 7.2×\nOxford-102Flowers[41] 40–258 6.5× 8189 50–315 6.3×\nandacorrespondinglabelyr ∈Rk orafakepair(xf,yf).\nImbalancedCIFAR-10 65–172026.5× 3208 149–2232 14.9× TheobjectivefunctionsofcGANsare\nImbalancedCIFAR-100 6–36 6× 2993 35–176 5×\nImbalancedTinyImageNet 9–586 65.1× 47602 60–680 11.3×\nStanfordCars[27] 24–68 2.8× 8144 73–263 3.6× L D =E yr∼q(y),xr∼q(x|y)[f D(−D(xr,yr))]\n+E (cid:2) f (D(G(z,yf),yf))(cid:3) , (1)\nyf∼p(y),z∼p(z) D\nL =E (cid:2) −D(G(z,yf),yf)(cid:3) , (2)\nused [8,22,31]. The feature distance between two images G yf∼p(y),z∼p(z)\nwiththepretrainedVGG[52]hasbeenwidelyusedinstyle\nwheref (·) = max(0,1+·)isthehingeloss[33,37,55].\ntransfer [22] and super-resolution [31]. Knowledge distil- D\nConventional cGANs optimize the above functions, lead-\nlation [15] transfers knowledge from a teacher model to\ningtothegenerateddistributionbeingclosetoq(x|y)ona\na student model that solves the same task as the teacher\nwell-constructeddataset. Incontrast,weaimtolearnadis-\nsolves for model compression [6,32,47,58]. A concur-\ntributionthatisclosetoq(x|y)onanimbalanceddataset.\nrentwork[10]proposesaCLIP[43]-basedknowledgedis-\ntillation method and exploits huge external knowledge for\n4.ProposedMethod\nimage generation. In contrast, we aim to share knowledge\namongclassestoenhanceminorityclasses.\n4.1.Labelaugmentationforimbalanceddata\nImbalanced data possesses a special property, whereas\n3.PreliminaryKnowledge\nsome classes have a certain number of samples (i.e., ma-\n3.1.Imbalanceddataset jor classes) while others do not (i.e., minor classes). The\nexistence of minor classes leads to the failure of straight-\nDependingonthenumberofsamplesineachclass,any forwarddataaugmentation,whichdirectlyappliestransfor-\ndatasetcanbeclassifiedaseitherabalancedorimbalanced mations to training images. The reason can be explained\ndataset [5,20,34]. While a balanced dataset possesses as follows. Label-preserving data augmentation uses sam-\nclasses with roughly the same number of samples in each, pleswithineachclasstoaugmentdata. Thisstrategyworks\nan imbalanced dataset possesses some classes with a few well for major classes while it cannot provide enough di-\nsamples. As we can see in Tab. 1, the ratios between the versity given minor classes. Namely, the augmented data\nmajor and minor classes of imbalanced datasets are much havehighaffinityyetlowdiversity. Obviously, theyeasily\nhigherthanthoseofbalanceddatasets. triggerlearningshort-cutfeatures.\nOnthebasisoftheabovedefinition[34],wecollectsome Toaddresstheinadequacyoflabel-preservingdataaug-\nimbalanceddatasetstoverifyourmethod. WeuseAnime- mentation,wefocusonincreasingtheaffinityanddiversity\nFace[1],Oxford-102Flowers[41],imbalancedCIFAR-10, ofaugmenteddata. Needlesstosay,alltrainingdatawould\nimbalanced CIFAR-100, imbalanced Tiny ImageNet, and sharesomefeaturessuchascolorandshape.Inspiredbythe\nStanford Cars [27] in our experiments. Note that we con- aboveobservation,ourmethodisdesignedtoallowaclass\nstructimbalancedCIFAR-10/100andTinyImageNetfrom to implicitly borrow samples from other similar classes as\ntheoriginalones[28,59](seeSec.5.1formoredetails).For augmentedsamplesratherthanreusingsampleswithinthe\ncomparison, we list the number of classes and the number class. Anappropriatewayisnon-label-preservingaugmen-\nof samples per class for balanced and imbalanced datasets tation,inwhichasingleimageissharedbymultipleclasses.\ninTab.1,showingthattheimbalanceddatasetsusedinour However, naive label augmentation methods automatically\nexperimentsaremorechallenging. distributetheprobability. Labelsmoothing[54]makesnew\n4946\n\nAdversarial Adversarial\nloss loss\nLabel-preservingdataaugmentation[64] Inner Inner\nproduct product\nEmbedding Linear\nLabelsmoothing[54]\nOurlabelaugmentation\nFigure 2. Training samples from the same class after applying\neachaugmentationmethod(i.e.,imagesthatassignprobabilityto\ntheclass). Thebluerectanglesmeanaugmentedsamples,andthe (a)Projection. (b)Ours.\nothers are original samples of the class. Unlike other methods, Figure3.DiscriminatorarchitecturesforcGANs:aprojectiondis-\nourlabelaugmentationimportssimilarimagesfromotherclasses, criminator and a discriminator with our label augmentation. (a)\nresultinginaugmenteddatawithhighaffinityanddiversity. Thetypicalconditionaldiscriminatorreceivesapairofanimage\nandalabel: (xf,yf)or(xr,yr). (b)Ourdiscriminatorreceives\napairofanimageandaprobabilityvectorobtainedfromapre-\nlabels dissociate from the image content. Mixup [61] ex- trainedclassifieryˆr =C(xr):(xf,yf)or(xr,yˆr).\npands the class distribution by using convex combinations\nevenfordissimilarclasspairs. Labelaugmentationdefinition. LetT:Rd×Rk→Rd×Rk\nWe thus develop a distribution manner that distributes\nbe a label augmentation function. We define T(x,y) =\nprobabilitiesassociatedwitheachinputimage. Namely,we\n(T (·),T (·)) where T is an image prediction function\nx y x\nemployapretrainedimageclassifierforourlabelaugmen-\n(i.e.,predictingamodifiedimage)andT isalabelpredic-\ny\ntation.Assigningthepredictionsoftheclassifiertosamples\ntionfunction(i.e.,predictinganewlabelforgivenimage).\nasnewlabelsenablesthediscriminatortoconsidertherela-\nOurT istheidentityfunction. UnlikeT usedin[54,61],\nx y\ntionshipsbetweenclassesintraining. Ourlabelaugmenta-\nwhichdirectlypredictsanewlabelfromagivenlabely,our\ntion,therefore,facilitateslearningwiththeproperinforma-\nT predictsanewlabelyˆfromagivenimagex.\ny\ntionandbalancingthegeneratorandthediscriminator. As\nAs discussed above, we aim to distribute class proba-\nshown in Fig. 2, while typical data augmentation methods\nbilities to the given image x precisely, indicating that yˆ is\ncomplete data augmentation within each class, label aug-\nassignedtomultipleclasses. Therefore,weuseapretrained\nmentation does classes by importing instances from other classifierC :Rd →Rk asT . Thenewlabelyˆisobtained\ny\nclasses. Therefore, label augmentation can be interpreted\nusingC asyˆ=C(x). Inotherwords,T(x,y)=(x,yˆ).\nasinter-classdataaugmentation.\nIntegration of label augmentation and discriminator.\nWecompareaugmenteddataobtainedbydifferentaug-\nWe now integrate yˆ obtained by our augmentation into\nmentationmethods(Fig.2).Label-preservingdataaugmen-\nEq.(1),definingtheobjectiveofSoftlabel-GANas\ntation [64] only reproduces images similar to original im-\nages,resultinginaugmenteddatawithhighaffinityandlow L =E [f (−D(xr,yˆr)))]\nD yr∼q(y),xr∼q(x|y) D\ndiversity. Label smoothing [54] imports images different +E (cid:2) D(G(z,yf),yf)(cid:3) . (3)\nfromtheoriginalimages,resultinginlowaffinityandhigh yf∼p(y),z∼p(z)\ndiversity. In contrast to the above methods, our label aug- WealsoemployEq.(2)astheobjectivefunctionofthegen-\nmentationimportsimages(fromotherclasses)thataresim- erator for our training scheme. The generator only takes\nilartotheoriginalclassimages(e.g., thecharacteristicsof one-hot inputs yf in both the training and testing phases.\nthesamehaircolorandsimilarpaintingstyle),resultingin Ourdiscriminatortakesyˆforrealsamplesandyf forfake\naugmenteddatawithhighaffinityanddiversity. samples,whicharesampledfromtheuniformdistribution.\nWe apply our label augmentation to only real samples be-\n4.2.Softlabel-GAN\ncauseouraugmentationaimstocorrecttheclassimbalance\nWe propose Softlabel-GAN by incorporating our label in the dataset. Figure 3 illustrates the difference between\naugmentation into the discriminator of a cGAN model. our discriminator and the widely used projection discrim-\nMoreprecisely,wefeedtheprobabilityvectorsofinputim- inator [38]. Note that in Fig. 3, we omit the generator of\nages to the discriminator instead of one-hot ground truths. Softlabel-GANforsimplicitybecausewemaintainthegen-\nInwhatfollows,wewillformallypresentourlabelaugmen- eratorofprojection-basedGANs.\ntationaswellashowtocombineitwiththediscriminator. Our method increases the diversity of the augmented\n4947\n\ndata while maintaining high affinity by importing simi- 128andalearningrateof2×10−4forboththegeneratorand\nlar images from other classes. This is because it assigns discriminator. For the experiments with higher resolution,\nhigherprobabilitiestoproperclasses(e.g.,correctorsimilar weusethehierarchicallatentspacewith20dimensionsfor\nclasses)andlowerprobabilitiestoimproperclasses(e.g.,ir- eachlatentvariableandthesharedembeddingwith128di-\nrelevantordissimilarclasses)accordingtotheperceptionof mensions. We use minibatch sizes of 512 and 32 for the\napretrainedclassifier. Asopposedtoourmethod, thedata resolutionsof64×64and128×128, respectively. Weuse\naugmentation-basedmethods[25,64]limitthediversitydue thelearningratesof1×10−4and4×10−4forthegenerator\nto only reusing a few samples inside each class. By aug- anddiscriminator,respectively.\nmenting data with high affinity and diversity, our method\npreventsjustmemorizingtrainingdata. 5.Experiments\nGenerally, we can use another function as T. Label\n5.1.Datasets\nsmoothing[54](i.e.,T(x,y)=(x,y(1−α)+α1/k)and\nMixup[61](i.e.,T(x,y)=(λx+(1−λ)x′,λy+(1−λ)y′)\nAnimeFace [1] is constructed by extracting face regions\nhavelabelpredictionfunctionsT y(y)=y(1−α)+α1/k fromtheimagesofanimecharactersobtainedfromtheweb.\nand T y(y) = λy + (1 − λ)y′, respectively. Unlike our Itconsistsof176characters(i.e.,classes),whereeachclass\nmethod taking T y : Rd → Rk, naive label augmentations containsbetween17and161images(128×128).\ntake T y : Rk → Rk (i.e., predicting a new label from a Oxford-102 Flowers [41] consists of 102 flower classes.\ngivenlabel).Augmentingaclasswithoutconsideringactual The smallest class has 40 images and the largest one has\nimagecontentsresultsinthelow-affinityaugmenteddata. 258images. Allimagesareresizedto128×128.\nWe checked that our label augmentation works as data ImbalancedCIFAR-10isanimbalancedsubsetofCIFAR-\naugmentation.Thenumberofsamplesthatassignedaprob- 10 [28]. The original CIFAR-10 contains 50,000 32×32\nabilityabovea threshold of0.01 toaclasswas countedas imagesas thetraining set. Thebuilding procedurefor this\nthe number of samples belonging to the class. Our label dataset consists of three steps. First, we shuffle the order\naugmentationindeedincreasestheminorclasssize(Tab.1, ofclasslabels. Then,todefinethefrequencyofeachclass,\nlastcolumn). we consider a histogram that has the same number of bins\nasthenumberofclassesandapproximatestherange[0,3)\n4.3.Implementationdetails\nin a lognormal distribution with a standard deviation of 3.\nWe use BigGAN [4] to examine Softlabel-GAN. We Eachsortedclasscorrespondstoonebin,andwefinallyran-\nbuild Softlabel-GAN upon BigGAN [4] by integrating our domly pick up samples so that the overall class frequency\nlabelaugmentationandDiffAugment[64]withthreetrans- followsthishistogram. Thedatasetcontains3208colorim-\nformations: translation (within [−1,1] of the image size), ages,with65–1720imagesperclass.\n8 8\ncolor(includingrandombrightnesswithin[−0.5,0.5],con- ImbalancedCIFAR-100isanimbalancedsubsetof32×32\ntrast within [0.5,1.5], and saturation within [0,2]), and CIFAR-100. Webuilditinthesamemannerasimbalanced\ncutout(maskingwitharandomsquareofhalfimagesize).\nCIFAR-10anduseaχ2-distributionwith3degreesoffree-\nWe use SpinalNet [23] as pretrained classifier C be- dom instead of a lognormal distribution. The dataset con-\ncauseitachievesstat-of-the-artperformanceonfine-grained sistsof2993imageswith6–36imagesperclass.\ndatasets and empirically works well on imbalanced data. ImbalancedTinyImageNetisasubsetofTinyImageNet\nWetrainSpinalNetonatargetdatasetwiththeentropyreg- 128×128 [59] (200 classes). We take it in the same man-\nularizationterm. Empirically, theweightofthetermisset nerasimbalancedCIFAR-10/100withtherange[1,4)ina\nto 0.3 for all datasets. We then use the trained classifier Paretodistributionwithashapeparameter, α = 2. Itcon-\nto realize our label augmentation. The classifier feeds on tains47602imageswith41–483imagesperclass.\ninputbeforeapplyingDiffAugment. Notethatourmethod Stanford Cars Dataset [27] consists of 196 classes with\ndoes not require a perfect classifier because we aim to as- 24–68imagesperclass.Allimagesareresizedto128×128.\nsignprobabilitiestosimilarclasses. Thedatasetprovides8144images.\nWeconvertinputprobabilityvectorsbyafullyconnected\n5.2.Comparedmethodsandevaluationmetrics\nlayer without a bias term instead of an embedding layer\nto accept yˆ ∈ Rk (i.e., a non-one-hot vector). Then, we Comparedmethods. WeuseBigGAN[4]asabasemodel\nuse the discriminator D(x,y)=ϕ(x)Vy+ψ(ϕ(x)) with and carefully integrate data and label augmentations into\nthe feature extractor ϕ : Rd→Rl, the discriminator head it. Since BigGAN [4] cannot work properly without data\nψ : Rl→R, and the weights of the linear layer V∈Rl×k augmentationasseenlaterinSec.5.4,weemployDiffAug-\n(Fig.3). ment[64]forallthecomparedmethods.\nFortheexperimentswiththeresolutionof32×32,weset For comparison (Sec. 5.3), we compare Softlabel-GAN\nthelatentdimensiond =128. Weuseaminibatchsizeof with DiffAug-GAN (i.e., BigGAN [4] with DiffAug-\nz\n4948\n\nlaeR\nguAffiD\nlebaltfoS Figure5.Asthenumberofsamplesintheminorclassesdecreases,\nthe FIDs obtained by DiffAug-GAN and Smooth-GAN increase\nconsiderably, implying that those methods worsen. In contrast,\nFigure 4. Generated examples on Stanford Cars [27]. DiffAug- Softlabel-GAN achieves a relatively consistent performance re-\nGANcannotgeneratethecolorvariationsoftargetclasses. gardlessoftheminimumnumberofsamplesforeachclass.\nment [64]). We also use Smooth-GAN (i.e., BigGAN [4] ing the superiority of Softlabel-GAN in overall and per-\nwithDiffAugment[64]andlabelsmoothing[54]withαof class performance. Table 2 also shows that our method\n0.1)asastrongbaselinewithnaivelabelaugmentation. achieves higher LPIPS and intra-LPIPS scores than other\nFordetailedanalysis(Sec.5.4),weusetwoablatedmod- methods in most cases, indicating that our generated im-\nels: Softlabel-GAN−−, which is our model without both ages are more diverse. Tight standard deviations of intra-\naugmentations (i.e., BigGAN [4]), and Softlabel-GAN−, FIDandintra-LPIPSofourmethodindicateconsistentper-\nwhich is our model without data augmentation (i.e., Big- formance in each class. Unlike in the case of FID, which\nGAN [4] with our label augmentation). Additionally, neglectstheminorclasses, Tab.2demonstratesthesuperi-\nwe prepare Smooth-GAN− (i.e., BigGAN [4] with label ority of our method in terms of intra-class metrics, which\nsmoothing[54]). considersthequalityofminorclasses.Smooth-GANsome-\nEvaluationmetrics. WeemployInceptionScore(IS)[46], times achieves the highest diversity score because images\nFre´chetInceptionDistance(FID)[14], andLPIPS[63]di- that ignore a given class condition result in a higher di-\nversity score. In addition, we use Precision, Recall, Den- versity score, but not actual diversity. Unlike Smooth-\nsity, and Coverage [30,39]. We also calculate intra-class GAN, Softlabel-GAN generates images always consistent\nmetrics: intra-FID[38],intra-LPIPS,intra-Precision,intra- with a given class (see Figs. 1 and 7). As shown in Fig. 1\nRecall, intra-Density, and intra-Coverage to more exten- andTab.2,ourmethodachievesdiverseimagegeneration.\nsively evaluate the quality within each class. Intra-FID, WenotethatKD-DLGAN[10]achievesaFIDof11.63on\nintra-LPIPS, intra-Precision, intra-Recall, intra-Density, StanfordCars.\nandintra-CoveragearetheaveragesoftheFID,LPIPS,Pre- Next, we evaluated the effects of the number of minor\ncision, Recall, Density, and Coverage calculated for each class samples on the performance of the compared meth-\nclass,respectively. ForFIDandintra-FID,wesample10K ods. To this end, we train the compared models on imbal-\ngenerated images. For LPIPS and intra-LPIPS, we sample anced CIFAR-10 using variable sizes of minor class sam-\n100generatedimagesforeachclass. ples(i.e., 120, 90, and65samples, respectively). Figure5\nindicates that while DiffAug-GAN and Smooth-GAN sig-\n5.3.Experimentalresults nificantly deteriorate their performance as the number of\nsamplesdecreases,ourmethodachievesstableperformance\nQualitative comparison. Figures 1 and 4 provide the\nevenwiththelimitedamountofdata. Whencomparedwith\nexamples generated by Softlabel-GAN and the baselines,\nDiffAug-GAN, our method reduces performance degrada-\nshowing that our method succeeds in the plausible and di-\ntionby70%. Theseobservationsclearlyshowthebenefits\nverse image generation on imbalanced training data. On\nofourmethodinboostingtherobustnesstominorclasses.\nAnimeFace(Fig.1),allmethodscanproduceplausibleim-\nages. However, from the perspective of the distribution\n5.4.Detailedanalysis\nof generated images, DiffAug-GAN produces only a few\nmodesregardlessoflatentvariables,resultinginlessdiverse WeevaluatedthenecessityofDiffAugment[64];seethe\nimages, and Smooth-GAN generates images with a wrong first three rows of Tab. 3. For this ablation study, we use\nclass. On Stanford Cars (Fig. 4), DiffAug-GAN generates AnimeFace and Oxford-102 at the resolution of 64 × 64.\nimageswithlowdiversity. Bycontrast, ourmethodgener- Softlabel-GANachievesbetterFIDovertheablationmod-\nates the color variations of car models. We provide more els(Tab.4). WecanseethenecessityoftheDiffuAugment\nexamplesinSupplementaryMaterial. baseline. Inaddition,ourlabelaugmentationfurtherbrings\nQuantitative comparison. Table 2 provides quantitative aperformancegainoverthebaseline.\nresults on the six datasets. Our method outperforms the To confirm the contribution of each augmentation, we\nothers in FID and intra-FID on all datasets, demonstrat- explore the FID and LPIPS (computed on each class) ob-\n4949\n\nTable2.Quantitativeresultsonthesixbenchmarkdatasets.\nMethod AnimeFace\nFID↓ LPIPS↑ intra-FID↓ intra-LPIPS↑ Precision↑ Recall↑ Intra-Prec↑ Intra-Rec↑ Density↑ Coverage↑ Intra-Dens↑ Intra-Cov↑\nDiffAug-GAN 25.09 0.5049 66.25±17.00 0.4018±0.0266 0.877 0.231 0.967 0.027 1.220 0.538 1.437 0.975\nSmooth-GAN 22.46 0.5111 64.40±15.87 0.4211±0.0226 0.885 0.319 0.946 0.077 1.271 0.589 1.372 0.985\nSoftlabel-GAN 19.14 0.5183 57.43±16.56 0.4627±0.0314 0.890 0.454 0.952 0.225 1.442 0.659 1.365 0.988\nOxford-102Flowers\nFID↓ LPIPS↑ intra-FID↓ intra-LPIPS↑ Precision↑ Recall↑ Intra-Prec↑ Intra-Rec↑ Density↑ Coverage↑ Intra-Dens↑ Intra-Cov↑\nDiffAug-GAN 28.70 0.5826 159.51±57.66 0.3964±0.0566 0.795 0.435 0.848 0.051 0.484 0.408 0.527 0.850\nSmooth-GAN 23.36 0.5961 141.47±47.02 0.4313±0.0463 0.798 0.547 0.798 0.205 0.567 0.478 0.571 0.924\nSoftlabel-GAN 20.97 0.5793 126.32±42.69 0.4696±0.0407 0.815 0.585 0.796 0.300 0.613 0.513 0.612 0.927\nImbalancedCIFAR-10\nFID↓ LPIPS↑ intra-FID↓ intra-LPIPS↑ Precision↑ Recall↑ Intra-Prec↑ Intra-Rec↑ Density↑ Coverage↑ Intra-Dens↑ Intra-Cov↑\nDiffAug-GAN 57.24 0.2090 112.96±26.62 0.1647±0.0154 0.742 0.468 0.557 0.343 0.435 0.393 0.277 0.569\nSmooth-GAN 66.75 0.2017 125.31±35.36 0.1751±0.0197 0.697 0.399 0.502 0.317 0.392 0.382 0.238 0.567\nSoftlabel-GAN 54.59 0.2058 109.79±24.72 0.1773±0.0167 0.756 0.408 0.590 0.337 0.477 0.438 0.308 0.590\nImbalancedCIFAR-100\nFID↓ LPIPS↑ intra-FID↓ intra-LPIPS↑ Precision↑ Recall↑ Intra-Prec↑ Intra-Rec↑ Density↑ Coverage↑ Intra-Dens↑ Intra-Cov↑\nDiffAug-GAN 37.70 0.2774 209.54±37.19 0.1543±0.0381 0.830 0.416 0.742 0.080 0.703 0.680 0.502 0.902\nSmooth-GAN 34.36 0.2570 205.59±34.82 0.1704±0.0382 0.772 0.500 0.714 0.112 0.619 0.701 0.476 0.914\nSoftlabel-GAN 32.70 0.2438 201.61±32.50 0.1948±0.0352 0.770 0.577 0.699 0.262 0.635 0.712 0.446 0.948\nStanfordCars\nFID↓ LPIPS↑ intra-FID↓ intra-LPIPS↑ Precision↑ Recall↑ Intra-Prec↑ Intra-Rec↑ Density↑ Coverage↑ Intra-Dens↑ Intra-Cov↑\nDiffAug-GAN 8.99 0.5664 95.00±12.62 0.5437±0.0207 0.869 0.616 0.921 0.310 1.389 0.856 1.247 1.000\nSmooth-GAN 7.91 0.5863 103.55±12.56 0.5436±0.0171 0.857 0.657 0.894 0.436 1.217 0.851 0.990 0.999\nSoftlabel-GAN 7.35 0.5855 89.04±11.64 0.5452±0.0168 0.884 0.664 0.927 0.455 1.228 0.851 0.972 1.000\nImbalancedTinyImagenet\nIS↑ FID↓ LPIPS↑ intra-FID↓ intra-LPIPS↑ Precision↑ Recall↑ Intra-Prec↑ Intra-Rec↑ Density↑ Coverage↑ Intra-Dens↑ Intra-Cov↑\nDiffAug-GAN 4.33 159.74 0.5496 332.61±34.55 0.2630±0.0319 0.341 0.000 0.139 0.000 0.080 0.017 0.022 0.049\nSmooth-GAN 4.48 151.10 0.5573 334.39±33.84 0.3991±0.0544 0.352 0.003 0.128 0.001 0.075 0.020 0.016 0.070\nSoftlabel-GAN 14.58 53.22 0.6631 238.83±53.74 0.5755±0.0501 0.648 0.647 0.475 0.270 0.357 0.264 0.212 0.634\nTable3.AblationstudyontheimbalanceddatasetsusingFID.\nMethod AnimeFace Oxford-102 CIFAR-10 CIFAR-100\nSoftlabel-GAN 17.26 46.32 36.72 49.99\nSoftlabel-GAN−− 36.16 84.36 79.16 80.99\nSoftlabel-GAN− 32.89 60.31 72.81 73.94\nSmooth-GAN− 40.45 78.20 83.68 72.80\n(a)ClassFIDscores(↓)ofallclasses.\nTable4.ComparisonwithMixupandOversampleusingFID.\nMethod AnimeFace Cars Oxford102 CIFAR-10 CIFAR-100\nSoftlabel-GAN 19.14 7.35 20.97 54.59 32.70\nMixup 24.87 14.53 39.13 56.36 36.32\nOversample 26.07 9.83 29.36 59.03 40.83\n(b)ClassLPIPSscores(↑)ofallclasses.\nTable5.FIDscoresintheexperimentsat256×256resolution.\nFigure6. HistogramofclassFIDandclassLPIPSscoresonAni-\nDiffAug-GAN Smooth-GAN Softlabel-GAN\nmeFace. ThemodetotheleftinFIDandthemodetotherightin\nAnimeFace 20.5066 20.6889 17.2493 LPIPSindicategoodperformance.Thenarrowdistributionmeans\nStanfordCars 10.1542 23.3695 9.8605\nachievingconsistentperformance. TheperformanceofDiffAug-\nGANvariesbyclassmorethanSoftlabel-GAN.\ntained by both methods in depth. Figure 6 shows the\nhistograms of FID and LPIPS for each class on Anime-\nFace. DiffAug-GANhasalargervarianceofclassFIDand\nLPIPS, varying the performance by each class data. It has\nthe distributions of FID and LPIPS with longer right and\nlefttailsthanSoftlabel-GAN,respectively. Softlabel-GAN\nwins 152 classes out of 176 against DiffAug-GAN in FID\nand wins 175 classes in LPIPS. To sum up, the proposed\nmethod reduces standard deviation in addition to improv-\ning performance, indicating that label augmentation and\ndataaugmentationmakedifferentcontributions.Alongwith\nqualitativeandquantitativeresults(Sec.5.3),ourmethodis\npromisingindealingwithimbalanceddata.\nFigure7showstheexamplesgeneratedbycGANswith\nlabelaugmentation,i.e.,Softlabel-GANandSmooth-GAN.\nhtoomSlebaltfoS\nFigure7. ComparisonofGANswithlabelaugmentation. Allim-\nagesaregeneratedwiththesameclass,butsomeimagescontain\nadifferentclassfromthetheotherimagesinSmooth-GAN.Each\nclasshasfoursamples.\nNote that we use the same class to generate images (i.e.,\ntwofirstrowsbelongtoaclassandtwolastrowsbelongto\nanother class). Unlike Softlabel-GAN, Smooth-GAN gen-\nerates images of a different class from a conditional class.\n4950\n\nTable6.FIDscoresinexperimentswiththeADC-GAN[19]base- GAN [29] baseline. Vision-aided GAN achieves FIDs of\nline. 15.77 on AnimeFace, 14.48 on Stanford Cars, and 26.88\nDiffAug-GAN Smooth-GAN Softlabel-GAN on Oxford102, and Vision-aided GAN with our label aug-\nAnimeFace 35.0864 23.1834 21.0494 mentation achieves FIDs of 13.61 on AnimeFace, 11.01\nStanfordCars 20.6485 23.4987 14.5688\non Stanford Cars, and 16.84 on Oxford102. We also re-\nportfine-grainedmetricsfortheexperimentsinSupplemen-\nTable7. Comparisonwiththediffusionmodelwithclassifier-free\ntary Material. For the applicability of our method to an-\nguidance(CFG)onTinyImageNetinFID,intra-KID,intra-LPIPS,\nothergenerativemodel,weintegrateourlabelaugmentation\nintra-Precision (i-P), intra-Recall (i-R), intra-Density (i-D), and\nintodiffusionmodels. Wecompareclassifier-freeguidance\nintra-Coverage(i-C).\n(CFG) [17,18,40] and CFG with our label augmentation\nMethod FID intra-KID intra-LPIPS i-P i-R i-D i-C\non the imbalanced TinyImageNet dataset. In this experi-\nCFG[18] 21.92 0.065 0.271 0.783 0.419 0.732 0.867\nw/ourlabelaugmentation 22.04 0.064 0.268 0.795 0.426 0.773 0.880 ment,weuseintra-KernelInceptionDistance(KID)[3]in-\nsteadofintra-FIDduetothehugeinferencecostsofdiffu-\nTable8.QuantitativeresultsonImageNetLT[34]. sion models. In Tab. 7, our method shows that marginal\nMethod FID LPIPS intra-FID intra-LPIPS i-P i-R i-D i-C improvements over a diffusion model with CFG in fine-\nDiffAug-GAN 48.19 0.6641 371.1±29.20 0.615±0.015 0.232 0.632 0.045 0.600 grainedmetrics. Table8showstheexperimentalresultson\nSoftlabel-GAN 49.17 0.6601 352.0±29.05 0.643±0.011 0.251 0.723 0.055 0.707 ImageNetLT [34]. Our method demonstrates the improve-\nmentsinfine-grainedmetricsonthelarge-scaledataset.The\ncomparisonofper-classperformanceinFig.8clearlyshows\nourmethodprovidesperformancegainsinminorclasses.\nWediscusstheimpactofclassificationaccuracyongen-\nerationquality. Theaccuraciesofthepretrainedclassifiers\nintheexperimentsforAnimeFace,Oxford102,imbalanced\nTiny ImageNet, and Cars are 81.2%, 99.9%, 84.4%, and\n99.9%, respectively. An accuracy of less than 90% is not\nsophisticatedbutusefulfortrainingthecGANs. Wefurther\nanalyzetheinsensitivityoftheclassifierperformancetothe\ngenerationqualityinSupplementaryMaterial.\nFigure8. DifferencesbetweenclassFIDscoresofDiffAug-GAN\nand Softlabel-GAN. The large gains in minor classes and small\n6.Conclusion\ngainsinmajor(rightmost)classesshowthatourmethodeffectively\ncontributestominorclasses.\nWe investigated the problem of image generation in an\nimbalanced data regime. To prevent overlooking minor\nWecountthegeneratedimageswithaclassdifferentfroma classes in conventional approaches, we introduced label\nconditional class. First, we generate 50 images per class augmentationtoincreasediversitywhilemaintainingaffin-\non AnimeFace and compute the color histogram of each ity. Furthermore,weproposedSoftlabel-GANbyincorpo-\ngenerated image. The color histogram describes the num- ratingourlabelaugmentationintothediscriminator.Owing\nber of pixels in each color range over all pixels of an im- totheuseofclassifierpredictionsasadiscriminator’sclass\nage. Next, we calculate the average of the histogram cor- condition,Softlabel-GANenablesustoextractthefeatures\nrelations among images with the same class and use the from other class samples, resulting in more focus on mi-\ninter-quartile range rules to detect images with a differ- nor classes. Comprehensive benchmarking on imbalanced\nent class from a given condition. DiffAug-GAN, Smooth- datasetsshowsthatourmethodoutperformsothermethods\nGAN, and Softlabel-GAN generate 11, 52, and 12 images andislessaffectedbythenumberofsamplesofeachclass.\nthatdifferfromthegivenconditions,respectively.DiffAug- Ourlimitationisthatourmethodoutperformsconventional\nGANandourmethodcangeneratethecorrectclass,while methodsononlyimbalanceddatasetsanddoesnotoutper-\nSmooth-GAN fails. This is because Smooth-GAN blindly form them on balanced datasets, as conventional methods\ndistributes probabilities without considering the given im- performwellwithsufficienttrainingdata.\nages. Thisobservationclearlyconfirmstheoutperformance Acknowledgements.ThisworkwassupportedbytheInsti-\nofourlabelaugmentationagainstothermethods. Intheex- tuteforAIandBeyondoftheUniversityofTokyo,thecom-\nperimentswith256resolution, wealsoobservethesuperi- missioned research (No. 225) by National Institute of In-\nority of our Softlabel-GANover the baselines as shown in formationandCommunicationsTechnology(NICT),ROIS\nTab. 5. The experimentswith an ADC-GAN[19] baseline NII Open Collaborative Research 2023 23FC01, JSPS\n(Tab. 6) show that our method works on another architec- KAKENHI Grant Numbers JP22K17947, JP23KJ0381,\nture. We also conduct the experiment with a Vision-aided JP23H03449,andJP22H05015.\n4951\n\nReferences [17] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdif-\nfusionprobabilisticmodels. InNeurIPS,volume33,pages\n[1] Animeface dataset. www.nurs.or.jp/˜nagadomi/\n6840–6851,2020. 8\nanimeface-character-dataset/. 1,3,5\n[18] Jonathan Ho and Tim Salimans. Classifier-free diffusion\n[2] Federico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal\nguidance. arXivpreprintarXiv:2207.12598,2022. 8\nLadhak,MyraCheng,DeboraNozza,TatsunoriHashimoto,\n[19] LiangHou,QiCao,HuaweiShen,SiyuanPan,Xiaoshuang\nDanJurafsky,JamesZou,andAylinCaliskan. Easilyacces-\nLi,andXueqiCheng. ConditionalGANswithauxiliarydis-\nsibletext-to-imagegenerationamplifiesdemographicstereo-\ncriminativeclassifier. InICML,pages8888–8902,2022. 8\ntypesatlargescale.InProceedingsofthe2023ACMConfer-\n[20] NathalieJapkowicz. Theclassimbalanceproblem: Signif-\nence on Fairness, Accountability, and Transparency, pages\nicance and strategies. In Proceedings of the International\n1493–1504,2023. 1\nConferenceonArtificialIntelligence(ICAI),pages111–117,\n[3] MikołajBin´kowski,DanicaJSutherland,MichaelArbel,and\n2000. 1,3\nArthurGretton.DemystifyingMMDGANs.InICLR,2018.\n[21] Liming Jiang, Bo Dai, Wayne Wu, and Chen Change Loy.\n8\nDeceiveD:adaptivepseudoaugmentationforGANtraining\n[4] AndrewBrock,JeffDonahue,andKarenSimonyan. Large\nwithlimiteddata. InNeurIPS,pages21655–21667,2021. 2\nscaleGANtrainingforhighfidelitynaturalimagesynthesis.\n[22] JustinJohnson,AlexandreAlahi,andLiFei-Fei. Perceptual\nInICLR,2018. 5,6\nlosses for real-time style transfer and super-resolution. In\n[5] NiteshV.Chawla,KevinW.Bowyer,LawrenceO.Hall,and\nECCV,pages694–711,2016. 3\nW. Philip Kegelmeyer. Smote: Synthetic minority over-\n[23] HM Kabir, Moloud Abdar, Seyed Mohammad Jafar Jalali,\nsampling technique. Journal of Artificial Intelligence Re-\nAbbasKhosravi,AmirFAtiya,SaeidNahavandi,andDipti\nsearch,16(1):321–357,2002. 3\nSrinivasan. Spinalnet: Deep neural network with gradual\n[6] GuobinChen,WongunChoi,XiangYu,TonyHan,andMan-\ninput. arXivpreprintarXiv:2007.03347,2020. 5\nmohanChandraker.Learningefficientobjectdetectionmod-\nelswithknowledgedistillation. InNeurIPS,pages742–751, [24] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,\nEliShechtman,SylvainParis,andTaesungPark. Scalingup\n2017. 3\nGANsfortext-to-imagesynthesis. InCVPR,pages10124–\n[7] TingChen,XiaohuaZhai,MarvinRitter,MarioLucic,and\n10134,2023. 1\nNeilHoulsby. Self-supervisedGANsviaauxiliaryrotation\nloss. InCVPR,pages12146–12155,2019. 2 [25] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine,\nJaakkoLehtinen,andTimoAila. Traininggenerativeadver-\n[8] AnoopCherianandAlanSullivan.Sem-GAN:Semantically-\nsarialnetworkswithlimiteddata.InNeurIPS,pages12104–\nconsistent image-to-image translation. In WACV, pages\n12114,2020. 1,2,5\n1797–1806,2019. 3\n[9] Jan Chorowski and Navdeep Jaitly. Towards better decod- [26] KaiKatsumata,DucMinhVo,andHidekiNakayama. OS-\ningandlanguagemodelintegrationinsequencetosequence SGAN: Open-set semi-supervised image generation. In\nmodels. In Proceedings of Interspeech, pages 523–527, CVPR,pages11185–11193,2022. 2\n2017. 2 [27] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.\n[10] KaiwenCui,YingchenYu,FangnengZhan,ShengcaiLiao, 3Dobjectrepresentationsforfine-grainedcategorization. In\nShijianLu,andEricPXing. KD-DLGAN:Datalimitedim- 4thInternationalIEEEWorkshopon3DRepresentationand\nagegenerationviaknowledgedistillation. InCVPR,pages Recognition(3dRR-13),2013. 3,5,6\n3872–3882,2023. 3,6 [28] Alex Krizhevsky and Geoffrey Hinton. Learning multiple\n[11] TerranceDevriesandGrahamW.Taylor. Improvedregular- layersoffeaturesfromtinyimages. 2009. 3,5\nizationofconvolutionalneuralnetworkswithcutout. arXiv [29] NupurKumari,RichardZhang,EliShechtman,andJun-Yan\npreprintarXiv:1708.04552,2017. 1 Zhu. Ensemblingoff-the-shelfmodelsforgantraining. In\n[12] PrafullaDhariwalandAlexanderNichol. Diffusionmodels CVPR,pages10651–10662,2022. 8\nbeatgansonimagesynthesis.InNeurIPS,pages8780–8794, [30] TuomasKynka¨a¨nniemi, TeroKarras, SamuliLaine, Jaakko\n2021. 1 Lehtinen, and Timo Aila. Improved precision and recall\n[13] RaphaelGontijo-Lopes,SylviaSmullin,EkinDogusCubuk, metricforassessinggenerativemodels. InNeurIPS,pages\nandEthanDyer. Tradeoffsindataaugmentation:Anempir- 3927—-3936,2019. 6\nicalstudy. InICLR,2021. 1 [31] ChristianLedig,LucasTheis,FerencHusza´r,JoseCaballero,\n[14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Andrew Cunningham, Alejandro Acosta, Andrew Aitken,\nBernhard Nessler, and Sepp Hochreiter. GANs trained by AlykhanTejani, JohannesTotz, ZehanWang, etal. Photo-\natwotime-scaleupdateruleconvergetoalocalnashequi- realisticsingleimagesuper-resolutionusingagenerativead-\nlibrium. InNeurIPS,pages6626–6637,2017. 6 versarialnetwork. InCVPR,pages4681–4690,2017. 3\n[15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill- [32] MuyangLi,JiLin,YaoyaoDing,ZhijianLiu,Jun-YanZhu,\ning the knowledge in a neural network. arXiv preprint andSongHan.GANcompression:Efficientarchitecturesfor\narXiv:1503.02531,2015. 3 interactiveconditionalGANs. IEEETPAMI,44(12):9331–\n[16] Tobias Hinz, Matthew Fisher, Oliver Wang, and Stefan 9346,2022. 3\nWermter. Improved techniques for training single-image [33] JaeHyunLimandJongChulYe. GeometricGAN. arXiv\nGANs. InWACV,pages1300–1309,2021. 2 preprintarXiv:1705.02894,2017. 3\n4952\n\n[34] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, [50] ArtemSevastopolskiy,YuryMalkov,NikitaDurasov,Luisa\nBoqing Gong, and Stella X Yu. Large-scale long-tailed Verdoliva,andMatthiasNießner. Howtoboostfacerecog-\nrecognitioninanopenworld. InCVPR,pages2537–2546, nitionwithstylegan? InICCV,pages20924–20934,2023.\n2019. 3,8 1\n[35] Alexandra Sasha Luccioni, Christopher Akiki, Margaret [51] TamarRottShaham,TaliDekel,andTomerMichaeli. Sin-\nMitchell, and Yacine Jernite. Stable bias: Analyzing so- GAN:Learningagenerativemodelfromasinglenaturalim-\ncietal representations in diffusion models. arXiv preprint age. InCVPR,pages4570–4580,2019. 2\narXiv:2303.11408,2023. 1 [52] KarenSimonyanandAndrewZisserman. Verydeepconvo-\n[36] Mario Lucˇic´, Michael Tschannen, Marvin Ritter, Xiaohua lutionalnetworksforlarge-scaleimagerecognition.InICLR,\nZhai,OlivierBachem,andSylvainGelly. High-fidelityim- 2015. 3\nage generation with fewer labels. In ICML, pages 4183– [53] JiazeSun,BinodBhattarai,andTae-KyunKim.MatchGAN:\n4192,2019. 2 aself-supervisedsemi-supervisedconditionalgenerativead-\n[37] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and versarialnetwork. InACCV,2020. 2\nYuichi Yoshida. Spectral normalization for generative ad- [54] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nversarialnetworks. InICLR,2018. 1,3 Shlens, andZbigniewWojna. Rethinkingtheinceptionar-\n[38] TakeruMiyatoandMasanoriKoyama. cGANswithprojec- chitectureforcomputervision. InCVPR,pages2818–2826,\ntiondiscriminator. InICLR,2018. 4,6 2016. 1,2,3,4,5,6\n[39] MuhammadFerjadNaeem,SeongJoonOh,YoungjungUh, [55] DustinTran,RajeshRanganath,andDavidBlei. Hierarchi-\nYunjeyChoi,andJaejunYoo. Reliablefidelityanddiversity calimplicitmodelsandlikelihood-freevariationalinference.\nmetricsforgenerativemodels. InICML,pages7176–7185, InNeurIPS,pages5523–5533,2017. 3\n2020. 6 [56] Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen,\n[40] Alexander Quinn Nichol and Prafulla Dhariwal. Improved Trung-KienNguyen,andNgai-ManCheung. Ondataaug-\ndenoising diffusion probabilistic models. In ICML, pages mentation for GAN training. IEEE TIP, 30:1882–1897,\n8162–8171,2021. 8 2021. 1,2\n[41] Maria-Elena Nilsback and Andrew Zisserman. Automated [57] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-\nflower classification over a large number of classes. In reit,LlionJones,AidanNGomez,LukaszKaiser,andIllia\nProceedingsoftheIndianConferenceonComputerVision, Polosukhin. Attention is all you need. In NeurIPS, pages\nGraphics&ImageProcessing,pages722–729,2008. 3,5 6000–6010,2017. 2\n[42] Yiming Qin, Huangjie Zheng, Jiangchao Yao, Mingyuan [58] Duc Minh Vo, Akihiro Sugimoto, and Hideki Nakayama.\nZhou,andYaZhang. Class-balancingdiffusionmodels. In PPCD-GAN: Progressive pruning and class-aware distilla-\nCVPR,pages18434–18443,2023. 2 tion for large-scale conditional GANs compression. In\n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya WACV,pages2436–2444,2022. 3\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, [59] Jiayu Wu, Qixiang Zhang, and Guoxi Xu. Tiny ImageNet\nAmandaAskell,PamelaMishkin,JackClark,etal. Learn- challenge. 3,5\ningtransferablevisualmodelsfromnaturallanguagesuper- [60] Chang-Bin Zhang, Peng-Tao Jiang, Qibin Hou, Yunchao\nvision. InICML,pages8748–8763,2021. 3 Wei,QiHan,ZhenLi,andMing-MingCheng.Delvingdeep\n[44] Harsh Rangwani, Naman Jaswani, Tejan Karmali, Varun intolabelsmoothing. IEEETIP,30:5984–5996,2021. 2\nJampani,andRVenkateshBabu.ImprovingGANsforlong- [61] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\ntaileddatathroughgroupspectralregularization. InECCV, DavidLopez-Paz. Mixup:Beyondempiricalriskminimiza-\npages426–442,2022. 2 tion. InICLR,2018. 1,2,4,5\n[45] AliRazavi, AaronVandenOord, andOriolVinyals. Gen- [62] HanZhang,IanGoodfellow,DimitrisMetaxas,andAugus-\nerating diverse high-fidelity images with VQ-VAE-2. In tusOdena.Self-attentiongenerativeadversarialnetworks.In\nNeurIPS,pages14866—-14876,2019. 1 ICML,pages7354–7363,2019. 1\n[46] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki [63] R.Zhang,P.Isola,A.A.Efros,E.Shechtman,andO.Wang.\nCheung,AlecRadford,andXiChen. Improvedtechniques Theunreasonableeffectivenessofdeepfeaturesasapercep-\nfortrainingGANs. InNeurIPS,pages2234–2242,2016. 6 tualmetric. InCVPR,pages586–595,2018. 6\n[47] Victor Sanh, Lysandre Debut, Julien Chaumond, and [64] ShengyuZhao,ZhijianLiu,JiLin,Jun-YanZhu,andSong\nThomas Wolf. DistilBERT, a distilled version of bert: Han. Differentiable augmentation for data-efficient GAN\nsmaller, faster, cheaper and lighter. arXiv preprint training. In NeurIPS, pages 7559–7570, 2020. 1, 2, 4, 5,\narXiv:1910.01108,2019. 3 6\n[48] Axel Sauer, Kashyap Chitta, Jens Mu¨ller, and Andreas [65] ZhengliZhao,ZizhaoZhang,TingChen,SameerSingh,and\nGeiger. ProjectedGANsconvergefaster. InNeurIPS,pages HanZhang. ImageaugmentationsforGANtraining. arXiv\n17480–17492,2021. 1 preprintarXiv:2006.02595,2020. 1,2\n[49] AxelSauer,KatjaSchwarz,andAndreasGeiger.StyleGAN-\nXL: Scaling StyleGAN to large diverse datasets. In Inter-\nnationalConferenceonComputerGraphicsandInteractive\nTechniques(SIGGRAPH),2022. 1\n4953\n\n",
    "total_pages": 10,
    "pages": [
      {
        "page": 1,
        "content": "Label Augmentation as Inter-class Data Augmentation\nfor Conditional Image Synthesis with Imbalanced Data\nKaiKatsumata DucMinhVo HidekiNakayama\nTheUniversityofTokyo,Japan\n{katsumata,vmduc,nakayama}@nlab.ci.i.u-tokyo.ac.jp\nAbstract\nConditional image synthesis performs admirably when\ntrained on well-constructed and balanced datasets. How-\never, in practice, training datasets frequently contain mi-\nnorities (i.e., a class with a few samples), known as im-\nbalanced data, which causes difficulties in learning gen-\nerative models. To address conditional image synthesis\nwithimbalanceddata,weanalyzeadiversityissueoflabel-\npreservingdataaugmentationandanaffinityissueofnon-\nlabel-preservingdataaugmentation.Fromthisobservation,\nwepresentlabelaugmentation,whichworksasinter-class\ndata augmentation that effectively augments data by pre-\ndicting a new label for a given image using the prediction\nof a pretrained image classification model (i.e., probabili-\ntiesforeachclass). Weincorporateourlabelaugmentation\ninto the discriminator of a seminal conditional generative\nadversarial network (GAN) model, proposing Softlabel-\nGAN.Usingclassprobabilitiesextractsclass-invariantand\nshared features between similar classes, achieving data\naugmentation with high affinity and diversity. Our exper-\niments on imbalanced datasets show that Softlabel-GAN\nproduces images with high quality and diversity while be-\ning hardly affected by the number of samples in each\nclass. Code: https://github.com/raven38/\nsoftlabel-gan.\n1.Introduction\nThe impressive success of conditional deep generative\nmodels[12,24,37,45,48,49,62]hasbeenlargelyaidedbya\nlargeamountofwell-collected,balanced,anddiversedata,\ntypically consisting of not only a large number of images\nin total but also a certain number of images in each class.\nDespite the enormous number of images available online,\ncollectingspecialorrareobjectsisnotalwaysfeasibleow-\ning to annotation costs, data constraints (e.g., paintings of\na specific artist), unauthorized data, and privacy concerns.\nAsaresult,imbalanceddata[20]isinevitableinreal-world\nscenarios, leading to the failure of generative models and\nlaeR\nguAffiDhtoomS\nsruO\nThisWACVpaperistheOpenAccessversion,providedbytheComputerVisionFoundation.\nExceptforthiswatermark,itisidenticaltotheacceptedversion;\nthefinalpublishedversionoftheproceedingsisavailableonIEEEXplore.\nFigure 1. Generated examples on AnimeFace [1]. DiffAug-\nGAN(DiffuAug)generatesimageswithlowdiversity(redrectan-\ngles). Smooth-GAN(Smooth)generatesimagesthatdifferfrom\nthegivenclass(dashedblackrectangles). Incontrast, Softlabel-\nGAN(Ours)canavoidoverlappedimagesandimageswithoutre-\nspecttothegivenclass.Eachclasshasfivesamples.\namplifyingbiases. Thepotentialriskofthelatterhasbeen\ndiscussedintermsofAIethics[2,35,50]. Traininggener-\nativemodelsintheimbalanceddataregimeisthusconsid-\nered necessary, potentially broadening generative models’\nreal-worldapplicationsandprovidingsafetytothem.\nData augmentation is a straightforward solution for im-\nage generation with limited data. According to [13], two\nimportantaspectsofdataaugmentationareaffinityanddi-\nversity, with affinity indicating how small the distribution\nshiftsbetweentheaugmentedandtrainingdatadistributions\nand diversity indicating the complexity of the augmented\ndata. Low-affinitydatacausesthemodeltolearnincorrect\nfeatures, whereas low-diversity data causes the model to\neasilymemorizetrainingdata,degradingthequalityanddi-\nversityofgeneratedimages. Achievingaugmentationwith\nhigh affinity and diversity is difficult for imbalanced data\nbecauseoftworeasons: (i)thenumberoftrainingsamples\nisinsufficientinminorclassesand(ii)theminorclasssam-\nples are not diverse. As a result, data augmentation with\nhigh affinity and diversity is vital for successfully condi-\ntionalimagesynthesiswithimbalanceddata.\nData augmentation consists of two parts: label-\npreserving data augmentation [11,25,56,64,65] and non-\nlabel-preserving data augmentation [54,61]. The former\npart directly modifies only image inputs, whereas the lat-\n4944"
      },
      {
        "page": 2,
        "content": "teronedoesthelabelsofinputs. Thelabel-preservingdata • We propose Softlabel-GAN, which uses our novel la-\naugmentation for images mostly employs geometric trans- bel augmentation, for conditional image synthesis in\nformations, image filters, and color intensity transforma- animbalanceddataregime. Tothebestofourknowl-\ntions,andtheiroperationsarelimitedtothosethatmaintain edge, thisisthefirststudythatinvestigateslabelaug-\ninput labels. On the other hand, non-label-preserving data mentationinlearningcGANswithimbalanceddata.\naugmentationallowsarbitraryoperations. Label-preserving • We demonstrate, on several imbalanced datasets, that\ndataaugmentationiswidelyusedingenerativeadversarial our method outperforms the other methods. Further-\nnetworks (GANs) [25,56,64,65], with notable results on more,theexperimentsshowouradvantageswithintra-\nlimitedandbalanceddata. However,inourcase,wherethe classfidelityanddiversity.\ndata is not only limited but also imbalanced (i.e., the ap-\npearance of minor classes), the current data augmentation 2.RelatedWork\nisinsufficienttoexpandthedatadistribution.Themainrea-\nImage generation with limited data aims to improve\nson is that such data augmentation creates additional data\nthe training stability and generation quality without the\nbysolelyreusingthesampleswithineachclass,leadingto\nimmense amount of data. Collecting large high-quality\naugmenteddatawithhighaffinityyetlowdiversity. Forin-\ndatasets is not always possible because of the tremendous\nstance,simplyapplyinggeometric,color,corruption,and/or\nannotation cost, data constraints, and privacy. In some\nfilteringtransformationstothetrainingdata(e.g.,DiffAug-\ncases, we only collect a few examples for each class, e.g.,\nment [64]) leads to a rapid imbalance between a generator\nphotos of a specific landmark or illustrations of a specific\nandadiscriminator,yieldingimageswithlowdiversity(i.e.,\nartist. SincetrainingGANswithoutahugeamountofdata\nrepeatalmostthesameimages)(Fig.1).\nis crucial, several studies [16,21,25,51,64] paid attention\nOur main idea is to predict a new label for a given im-\nto the data efficiency aspect. Some approaches [25,64]\nage by assigning probabilities to all classes to which the\nare able to learn from limited data by using data augmen-\nimagebelongs. Therefore,ournovelaugmentationisatype\ntation (i.e., label-preserving data augmentation). In con-\nof non-label-preserving data augmentation, which we call\ntrast to data augmentation-based approaches, another line\nlabelaugmentation. Weapproachourproblembyincorpo-\nofresearch[7,26,36,53]employssemi-andself-supervised\nrating our simple yet effective label augmentation into the\nlearning to reduce the cost of human annotation. Recent\ndiscriminator of a seminal cGAN model. More precisely,\nworks[42,44]designarchitecture-specificmethodsforim-\nwefirstprepareanimageclassificationmodelpretrainedon\nagegenerationwithlimiteddata. Weintroduceadataaug-\ntheimbalanceddata.Then,weusetheoutputofthesoftmax\nmentationapproach,whichachieveshigh-affinityandhigh-\nfunction(i.e.,probabilitiesforeachclass)obtainedthrough\ndiversityaugmentationonimbalanceddata.\nthepretrainedclassifierastheclassconditioninthediscrim-\nNon-label-preserving augmentation is a set of data aug-\ninator. Using class probabilities enables our model to take\nmentation. While the label-preserving augmentation di-\nintoaccountsemanticsimilaritiesbetweenclasseswithre-\nrectly modifies an input image, the non-label-preserving\nspecttotheperceptionofpretrainedclassifiers. Naivelabel\naugmentation modifies an input label. Of which, label\naugmentationmethods[54,61]blindlydistributeclassprob-\nsmoothing [54] and Mixup [61] are popularly used. Label\nabilities to the data, resulting in augmented data with high\nsmoothing [54] replaces hard targets with soft targets by\ndiversitybutlowaffinity,aswellasthegenerationofimages\ntaking the weighted average of the original targets, avoid-\nthat are irrelevant to the given class (Fig. 1). By contrast,\ning overconfidence on several tasks [9,57]. Online label\nour augmentation precisely distributes class probabilities,\nsmoothing [60] quantifies class similarities and then as-\nresulting in augmented images with both high affinity and\nsigns class-wise soft labels, unlike our method, which as-\ndiversity. Consequently,ourmethodhindersthetrainingof\nsigns instance-wise soft labels. Mixup [61] trains a net-\nthe discriminator and balances the generator and the dis-\nwork on convex combinations of the samples and their la-\ncriminator, yielding better-generated images (Fig. 1). Our\nbels to improve accuracy and robustness to hyperparame-\ncontributionscanbesummarizedasfollows:\nters. Thesemethods[54,61]oftenprovideincorrectinfor-\n• We observe that existing data augmentation ap- mationtocGANsowingtoblindlydistributingclassprob-\nproachesprovideeitherdiversityoraffinityforimbal- abilities, resulting in the high diversity yet low affinity of\nanceddata. augmented data. In this work, we present content-aware\n• We find that assigning classifier output with suffi- labelaugmentationforimbalanceddata,whichbuildsaug-\ncient entropy to samples can be interpreted as inter- menteddatawithhighaffinityanddiversity.\nclassdataaugmentationthatincreasesthediversityper Pretrained recognition models have been widely used in\nclass.Wethusproposeasimpleyeteffectivelabelaug- trainingGANs. Togenerateimagesconsidering their con-\nmentationmethodthatproducesaugmenteddatawith tents, the high-level features extracted from a pretrained\nhighaffinityanddiversityusingapretrainedclassifier. modelasanalternativetothehumanvisualperceptionare\n4945"
      },
      {
        "page": 3,
        "content": "Table 1. Comparison of balanced and imbalanced datasets. For 3.2.ConditionalGANs\neach dataset, we indicate the range of the number of images in\neachclass, aratio(largestclasssamples/smallestclasssamples), Conditional GANs aim to model the conditional distri-\noverallsamples,andthenumberofimagesperclassandtheratio butionofatargetdatasetusingageneratorG:Rdz×Rk →\nafterourlabelaugmentation. Thenumberofsamplesperclasson Rd andadiscriminatorD :Rd×Rk →R,wheredandd\nz\nthe imbalanced dataset is various and significantly less than that arethedimensionsofanimageandalatentvariable,respec-\non the balanced one. A higher ratio indicates more imbalance. tively.Here,theclasslabely ∈Rkwithkbeingthenumber\nOuraugmentationactuallyincreasestheminorclasssamples(last\nofclassesindicatestheprobabilitiesofaninstancebelong-\ncolumn).\ningtoeachclass,includingone-hotvectors. Thegenerator\nDataset #Samples Ratio#Samples#AugmentedAugmented Gmapsconditionyf ∈Rk andlatentvectorz ∈Rdz from\nperclass samples ratio\na prior distribution p(z) to output xf = G(z,yf) ∈ Rd.\nBalanceddatasets\nThediscriminatorDlearnstodistinguishbetweenthegen-\nCIFAR-10[28] 5000 1× 50000 — —\nCIFAR-100[28] 500 1× 50000 — — erateddistributionpandthetargetdistributionq. Thedis-\nImbalanceddatasets criminator receives either a pair of a real sample xr ∈ Rd\nAnimeFace[1] 17–161 9.5× 14490 109–792 7.2×\nOxford-102Flowers[41] 40–258 6.5× 8189 50–315 6.3×\nandacorrespondinglabelyr ∈Rk orafakepair(xf,yf).\nImbalancedCIFAR-10 65–172026.5× 3208 149–2232 14.9× TheobjectivefunctionsofcGANsare\nImbalancedCIFAR-100 6–36 6× 2993 35–176 5×\nImbalancedTinyImageNet 9–586 65.1× 47602 60–680 11.3×\nStanfordCars[27] 24–68 2.8× 8144 73–263 3.6× L D =E yr∼q(y),xr∼q(x|y)[f D(−D(xr,yr))]\n+E (cid:2) f (D(G(z,yf),yf))(cid:3) , (1)\nyf∼p(y),z∼p(z) D\nL =E (cid:2) −D(G(z,yf),yf)(cid:3) , (2)\nused [8,22,31]. The feature distance between two images G yf∼p(y),z∼p(z)\nwiththepretrainedVGG[52]hasbeenwidelyusedinstyle\nwheref (·) = max(0,1+·)isthehingeloss[33,37,55].\ntransfer [22] and super-resolution [31]. Knowledge distil- D\nConventional cGANs optimize the above functions, lead-\nlation [15] transfers knowledge from a teacher model to\ningtothegenerateddistributionbeingclosetoq(x|y)ona\na student model that solves the same task as the teacher\nwell-constructeddataset. Incontrast,weaimtolearnadis-\nsolves for model compression [6,32,47,58]. A concur-\ntributionthatisclosetoq(x|y)onanimbalanceddataset.\nrentwork[10]proposesaCLIP[43]-basedknowledgedis-\ntillation method and exploits huge external knowledge for\n4.ProposedMethod\nimage generation. In contrast, we aim to share knowledge\namongclassestoenhanceminorityclasses.\n4.1.Labelaugmentationforimbalanceddata\nImbalanced data possesses a special property, whereas\n3.PreliminaryKnowledge\nsome classes have a certain number of samples (i.e., ma-\n3.1.Imbalanceddataset jor classes) while others do not (i.e., minor classes). The\nexistence of minor classes leads to the failure of straight-\nDependingonthenumberofsamplesineachclass,any forwarddataaugmentation,whichdirectlyappliestransfor-\ndatasetcanbeclassifiedaseitherabalancedorimbalanced mations to training images. The reason can be explained\ndataset [5,20,34]. While a balanced dataset possesses as follows. Label-preserving data augmentation uses sam-\nclasses with roughly the same number of samples in each, pleswithineachclasstoaugmentdata. Thisstrategyworks\nan imbalanced dataset possesses some classes with a few well for major classes while it cannot provide enough di-\nsamples. As we can see in Tab. 1, the ratios between the versity given minor classes. Namely, the augmented data\nmajor and minor classes of imbalanced datasets are much havehighaffinityyetlowdiversity. Obviously, theyeasily\nhigherthanthoseofbalanceddatasets. triggerlearningshort-cutfeatures.\nOnthebasisoftheabovedefinition[34],wecollectsome Toaddresstheinadequacyoflabel-preservingdataaug-\nimbalanceddatasetstoverifyourmethod. WeuseAnime- mentation,wefocusonincreasingtheaffinityanddiversity\nFace[1],Oxford-102Flowers[41],imbalancedCIFAR-10, ofaugmenteddata. Needlesstosay,alltrainingdatawould\nimbalanced CIFAR-100, imbalanced Tiny ImageNet, and sharesomefeaturessuchascolorandshape.Inspiredbythe\nStanford Cars [27] in our experiments. Note that we con- aboveobservation,ourmethodisdesignedtoallowaclass\nstructimbalancedCIFAR-10/100andTinyImageNetfrom to implicitly borrow samples from other similar classes as\ntheoriginalones[28,59](seeSec.5.1formoredetails).For augmentedsamplesratherthanreusingsampleswithinthe\ncomparison, we list the number of classes and the number class. Anappropriatewayisnon-label-preservingaugmen-\nof samples per class for balanced and imbalanced datasets tation,inwhichasingleimageissharedbymultipleclasses.\ninTab.1,showingthattheimbalanceddatasetsusedinour However, naive label augmentation methods automatically\nexperimentsaremorechallenging. distributetheprobability. Labelsmoothing[54]makesnew\n4946"
      },
      {
        "page": 4,
        "content": "Adversarial Adversarial\nloss loss\nLabel-preservingdataaugmentation[64] Inner Inner\nproduct product\nEmbedding Linear\nLabelsmoothing[54]\nOurlabelaugmentation\nFigure 2. Training samples from the same class after applying\neachaugmentationmethod(i.e.,imagesthatassignprobabilityto\ntheclass). Thebluerectanglesmeanaugmentedsamples,andthe (a)Projection. (b)Ours.\nothers are original samples of the class. Unlike other methods, Figure3.DiscriminatorarchitecturesforcGANs:aprojectiondis-\nourlabelaugmentationimportssimilarimagesfromotherclasses, criminator and a discriminator with our label augmentation. (a)\nresultinginaugmenteddatawithhighaffinityanddiversity. Thetypicalconditionaldiscriminatorreceivesapairofanimage\nandalabel: (xf,yf)or(xr,yr). (b)Ourdiscriminatorreceives\napairofanimageandaprobabilityvectorobtainedfromapre-\nlabels dissociate from the image content. Mixup [61] ex- trainedclassifieryˆr =C(xr):(xf,yf)or(xr,yˆr).\npands the class distribution by using convex combinations\nevenfordissimilarclasspairs. Labelaugmentationdefinition. LetT:Rd×Rk→Rd×Rk\nWe thus develop a distribution manner that distributes\nbe a label augmentation function. We define T(x,y) =\nprobabilitiesassociatedwitheachinputimage. Namely,we\n(T (·),T (·)) where T is an image prediction function\nx y x\nemployapretrainedimageclassifierforourlabelaugmen-\n(i.e.,predictingamodifiedimage)andT isalabelpredic-\ny\ntation.Assigningthepredictionsoftheclassifiertosamples\ntionfunction(i.e.,predictinganewlabelforgivenimage).\nasnewlabelsenablesthediscriminatortoconsidertherela-\nOurT istheidentityfunction. UnlikeT usedin[54,61],\nx y\ntionshipsbetweenclassesintraining. Ourlabelaugmenta-\nwhichdirectlypredictsanewlabelfromagivenlabely,our\ntion,therefore,facilitateslearningwiththeproperinforma-\nT predictsanewlabelyˆfromagivenimagex.\ny\ntionandbalancingthegeneratorandthediscriminator. As\nAs discussed above, we aim to distribute class proba-\nshown in Fig. 2, while typical data augmentation methods\nbilities to the given image x precisely, indicating that yˆ is\ncomplete data augmentation within each class, label aug-\nassignedtomultipleclasses. Therefore,weuseapretrained\nmentation does classes by importing instances from other classifierC :Rd →Rk asT . Thenewlabelyˆisobtained\ny\nclasses. Therefore, label augmentation can be interpreted\nusingC asyˆ=C(x). Inotherwords,T(x,y)=(x,yˆ).\nasinter-classdataaugmentation.\nIntegration of label augmentation and discriminator.\nWecompareaugmenteddataobtainedbydifferentaug-\nWe now integrate yˆ obtained by our augmentation into\nmentationmethods(Fig.2).Label-preservingdataaugmen-\nEq.(1),definingtheobjectiveofSoftlabel-GANas\ntation [64] only reproduces images similar to original im-\nages,resultinginaugmenteddatawithhighaffinityandlow L =E [f (−D(xr,yˆr)))]\nD yr∼q(y),xr∼q(x|y) D\ndiversity. Label smoothing [54] imports images different +E (cid:2) D(G(z,yf),yf)(cid:3) . (3)\nfromtheoriginalimages,resultinginlowaffinityandhigh yf∼p(y),z∼p(z)\ndiversity. In contrast to the above methods, our label aug- WealsoemployEq.(2)astheobjectivefunctionofthegen-\nmentationimportsimages(fromotherclasses)thataresim- erator for our training scheme. The generator only takes\nilartotheoriginalclassimages(e.g., thecharacteristicsof one-hot inputs yf in both the training and testing phases.\nthesamehaircolorandsimilarpaintingstyle),resultingin Ourdiscriminatortakesyˆforrealsamplesandyf forfake\naugmenteddatawithhighaffinityanddiversity. samples,whicharesampledfromtheuniformdistribution.\nWe apply our label augmentation to only real samples be-\n4.2.Softlabel-GAN\ncauseouraugmentationaimstocorrecttheclassimbalance\nWe propose Softlabel-GAN by incorporating our label in the dataset. Figure 3 illustrates the difference between\naugmentation into the discriminator of a cGAN model. our discriminator and the widely used projection discrim-\nMoreprecisely,wefeedtheprobabilityvectorsofinputim- inator [38]. Note that in Fig. 3, we omit the generator of\nages to the discriminator instead of one-hot ground truths. Softlabel-GANforsimplicitybecausewemaintainthegen-\nInwhatfollows,wewillformallypresentourlabelaugmen- eratorofprojection-basedGANs.\ntationaswellashowtocombineitwiththediscriminator. Our method increases the diversity of the augmented\n4947"
      },
      {
        "page": 5,
        "content": "data while maintaining high affinity by importing simi- 128andalearningrateof2×10−4forboththegeneratorand\nlar images from other classes. This is because it assigns discriminator. For the experiments with higher resolution,\nhigherprobabilitiestoproperclasses(e.g.,correctorsimilar weusethehierarchicallatentspacewith20dimensionsfor\nclasses)andlowerprobabilitiestoimproperclasses(e.g.,ir- eachlatentvariableandthesharedembeddingwith128di-\nrelevantordissimilarclasses)accordingtotheperceptionof mensions. We use minibatch sizes of 512 and 32 for the\napretrainedclassifier. Asopposedtoourmethod, thedata resolutionsof64×64and128×128, respectively. Weuse\naugmentation-basedmethods[25,64]limitthediversitydue thelearningratesof1×10−4and4×10−4forthegenerator\nto only reusing a few samples inside each class. By aug- anddiscriminator,respectively.\nmenting data with high affinity and diversity, our method\npreventsjustmemorizingtrainingdata. 5.Experiments\nGenerally, we can use another function as T. Label\n5.1.Datasets\nsmoothing[54](i.e.,T(x,y)=(x,y(1−α)+α1/k)and\nMixup[61](i.e.,T(x,y)=(λx+(1−λ)x′,λy+(1−λ)y′)\nAnimeFace [1] is constructed by extracting face regions\nhavelabelpredictionfunctionsT y(y)=y(1−α)+α1/k fromtheimagesofanimecharactersobtainedfromtheweb.\nand T y(y) = λy + (1 − λ)y′, respectively. Unlike our Itconsistsof176characters(i.e.,classes),whereeachclass\nmethod taking T y : Rd → Rk, naive label augmentations containsbetween17and161images(128×128).\ntake T y : Rk → Rk (i.e., predicting a new label from a Oxford-102 Flowers [41] consists of 102 flower classes.\ngivenlabel).Augmentingaclasswithoutconsideringactual The smallest class has 40 images and the largest one has\nimagecontentsresultsinthelow-affinityaugmenteddata. 258images. Allimagesareresizedto128×128.\nWe checked that our label augmentation works as data ImbalancedCIFAR-10isanimbalancedsubsetofCIFAR-\naugmentation.Thenumberofsamplesthatassignedaprob- 10 [28]. The original CIFAR-10 contains 50,000 32×32\nabilityabovea threshold of0.01 toaclasswas countedas imagesas thetraining set. Thebuilding procedurefor this\nthe number of samples belonging to the class. Our label dataset consists of three steps. First, we shuffle the order\naugmentationindeedincreasestheminorclasssize(Tab.1, ofclasslabels. Then,todefinethefrequencyofeachclass,\nlastcolumn). we consider a histogram that has the same number of bins\nasthenumberofclassesandapproximatestherange[0,3)\n4.3.Implementationdetails\nin a lognormal distribution with a standard deviation of 3.\nWe use BigGAN [4] to examine Softlabel-GAN. We Eachsortedclasscorrespondstoonebin,andwefinallyran-\nbuild Softlabel-GAN upon BigGAN [4] by integrating our domly pick up samples so that the overall class frequency\nlabelaugmentationandDiffAugment[64]withthreetrans- followsthishistogram. Thedatasetcontains3208colorim-\nformations: translation (within [−1,1] of the image size), ages,with65–1720imagesperclass.\n8 8\ncolor(includingrandombrightnesswithin[−0.5,0.5],con- ImbalancedCIFAR-100isanimbalancedsubsetof32×32\ntrast within [0.5,1.5], and saturation within [0,2]), and CIFAR-100. Webuilditinthesamemannerasimbalanced\ncutout(maskingwitharandomsquareofhalfimagesize).\nCIFAR-10anduseaχ2-distributionwith3degreesoffree-\nWe use SpinalNet [23] as pretrained classifier C be- dom instead of a lognormal distribution. The dataset con-\ncauseitachievesstat-of-the-artperformanceonfine-grained sistsof2993imageswith6–36imagesperclass.\ndatasets and empirically works well on imbalanced data. ImbalancedTinyImageNetisasubsetofTinyImageNet\nWetrainSpinalNetonatargetdatasetwiththeentropyreg- 128×128 [59] (200 classes). We take it in the same man-\nularizationterm. Empirically, theweightofthetermisset nerasimbalancedCIFAR-10/100withtherange[1,4)ina\nto 0.3 for all datasets. We then use the trained classifier Paretodistributionwithashapeparameter, α = 2. Itcon-\nto realize our label augmentation. The classifier feeds on tains47602imageswith41–483imagesperclass.\ninputbeforeapplyingDiffAugment. Notethatourmethod Stanford Cars Dataset [27] consists of 196 classes with\ndoes not require a perfect classifier because we aim to as- 24–68imagesperclass.Allimagesareresizedto128×128.\nsignprobabilitiestosimilarclasses. Thedatasetprovides8144images.\nWeconvertinputprobabilityvectorsbyafullyconnected\n5.2.Comparedmethodsandevaluationmetrics\nlayer without a bias term instead of an embedding layer\nto accept yˆ ∈ Rk (i.e., a non-one-hot vector). Then, we Comparedmethods. WeuseBigGAN[4]asabasemodel\nuse the discriminator D(x,y)=ϕ(x)Vy+ψ(ϕ(x)) with and carefully integrate data and label augmentations into\nthe feature extractor ϕ : Rd→Rl, the discriminator head it. Since BigGAN [4] cannot work properly without data\nψ : Rl→R, and the weights of the linear layer V∈Rl×k augmentationasseenlaterinSec.5.4,weemployDiffAug-\n(Fig.3). ment[64]forallthecomparedmethods.\nFortheexperimentswiththeresolutionof32×32,weset For comparison (Sec. 5.3), we compare Softlabel-GAN\nthelatentdimensiond =128. Weuseaminibatchsizeof with DiffAug-GAN (i.e., BigGAN [4] with DiffAug-\nz\n4948"
      },
      {
        "page": 6,
        "content": "laeR\nguAffiD\nlebaltfoS Figure5.Asthenumberofsamplesintheminorclassesdecreases,\nthe FIDs obtained by DiffAug-GAN and Smooth-GAN increase\nconsiderably, implying that those methods worsen. In contrast,\nFigure 4. Generated examples on Stanford Cars [27]. DiffAug- Softlabel-GAN achieves a relatively consistent performance re-\nGANcannotgeneratethecolorvariationsoftargetclasses. gardlessoftheminimumnumberofsamplesforeachclass.\nment [64]). We also use Smooth-GAN (i.e., BigGAN [4] ing the superiority of Softlabel-GAN in overall and per-\nwithDiffAugment[64]andlabelsmoothing[54]withαof class performance. Table 2 also shows that our method\n0.1)asastrongbaselinewithnaivelabelaugmentation. achieves higher LPIPS and intra-LPIPS scores than other\nFordetailedanalysis(Sec.5.4),weusetwoablatedmod- methods in most cases, indicating that our generated im-\nels: Softlabel-GAN−−, which is our model without both ages are more diverse. Tight standard deviations of intra-\naugmentations (i.e., BigGAN [4]), and Softlabel-GAN−, FIDandintra-LPIPSofourmethodindicateconsistentper-\nwhich is our model without data augmentation (i.e., Big- formance in each class. Unlike in the case of FID, which\nGAN [4] with our label augmentation). Additionally, neglectstheminorclasses, Tab.2demonstratesthesuperi-\nwe prepare Smooth-GAN− (i.e., BigGAN [4] with label ority of our method in terms of intra-class metrics, which\nsmoothing[54]). considersthequalityofminorclasses.Smooth-GANsome-\nEvaluationmetrics. WeemployInceptionScore(IS)[46], times achieves the highest diversity score because images\nFre´chetInceptionDistance(FID)[14], andLPIPS[63]di- that ignore a given class condition result in a higher di-\nversity score. In addition, we use Precision, Recall, Den- versity score, but not actual diversity. Unlike Smooth-\nsity, and Coverage [30,39]. We also calculate intra-class GAN, Softlabel-GAN generates images always consistent\nmetrics: intra-FID[38],intra-LPIPS,intra-Precision,intra- with a given class (see Figs. 1 and 7). As shown in Fig. 1\nRecall, intra-Density, and intra-Coverage to more exten- andTab.2,ourmethodachievesdiverseimagegeneration.\nsively evaluate the quality within each class. Intra-FID, WenotethatKD-DLGAN[10]achievesaFIDof11.63on\nintra-LPIPS, intra-Precision, intra-Recall, intra-Density, StanfordCars.\nandintra-CoveragearetheaveragesoftheFID,LPIPS,Pre- Next, we evaluated the effects of the number of minor\ncision, Recall, Density, and Coverage calculated for each class samples on the performance of the compared meth-\nclass,respectively. ForFIDandintra-FID,wesample10K ods. To this end, we train the compared models on imbal-\ngenerated images. For LPIPS and intra-LPIPS, we sample anced CIFAR-10 using variable sizes of minor class sam-\n100generatedimagesforeachclass. ples(i.e., 120, 90, and65samples, respectively). Figure5\nindicates that while DiffAug-GAN and Smooth-GAN sig-\n5.3.Experimentalresults nificantly deteriorate their performance as the number of\nsamplesdecreases,ourmethodachievesstableperformance\nQualitative comparison. Figures 1 and 4 provide the\nevenwiththelimitedamountofdata. Whencomparedwith\nexamples generated by Softlabel-GAN and the baselines,\nDiffAug-GAN, our method reduces performance degrada-\nshowing that our method succeeds in the plausible and di-\ntionby70%. Theseobservationsclearlyshowthebenefits\nverse image generation on imbalanced training data. On\nofourmethodinboostingtherobustnesstominorclasses.\nAnimeFace(Fig.1),allmethodscanproduceplausibleim-\nages. However, from the perspective of the distribution\n5.4.Detailedanalysis\nof generated images, DiffAug-GAN produces only a few\nmodesregardlessoflatentvariables,resultinginlessdiverse WeevaluatedthenecessityofDiffAugment[64];seethe\nimages, and Smooth-GAN generates images with a wrong first three rows of Tab. 3. For this ablation study, we use\nclass. On Stanford Cars (Fig. 4), DiffAug-GAN generates AnimeFace and Oxford-102 at the resolution of 64 × 64.\nimageswithlowdiversity. Bycontrast, ourmethodgener- Softlabel-GANachievesbetterFIDovertheablationmod-\nates the color variations of car models. We provide more els(Tab.4). WecanseethenecessityoftheDiffuAugment\nexamplesinSupplementaryMaterial. baseline. Inaddition,ourlabelaugmentationfurtherbrings\nQuantitative comparison. Table 2 provides quantitative aperformancegainoverthebaseline.\nresults on the six datasets. Our method outperforms the To confirm the contribution of each augmentation, we\nothers in FID and intra-FID on all datasets, demonstrat- explore the FID and LPIPS (computed on each class) ob-\n4949"
      },
      {
        "page": 7,
        "content": "Table2.Quantitativeresultsonthesixbenchmarkdatasets.\nMethod AnimeFace\nFID↓ LPIPS↑ intra-FID↓ intra-LPIPS↑ Precision↑ Recall↑ Intra-Prec↑ Intra-Rec↑ Density↑ Coverage↑ Intra-Dens↑ Intra-Cov↑\nDiffAug-GAN 25.09 0.5049 66.25±17.00 0.4018±0.0266 0.877 0.231 0.967 0.027 1.220 0.538 1.437 0.975\nSmooth-GAN 22.46 0.5111 64.40±15.87 0.4211±0.0226 0.885 0.319 0.946 0.077 1.271 0.589 1.372 0.985\nSoftlabel-GAN 19.14 0.5183 57.43±16.56 0.4627±0.0314 0.890 0.454 0.952 0.225 1.442 0.659 1.365 0.988\nOxford-102Flowers\nFID↓ LPIPS↑ intra-FID↓ intra-LPIPS↑ Precision↑ Recall↑ Intra-Prec↑ Intra-Rec↑ Density↑ Coverage↑ Intra-Dens↑ Intra-Cov↑\nDiffAug-GAN 28.70 0.5826 159.51±57.66 0.3964±0.0566 0.795 0.435 0.848 0.051 0.484 0.408 0.527 0.850\nSmooth-GAN 23.36 0.5961 141.47±47.02 0.4313±0.0463 0.798 0.547 0.798 0.205 0.567 0.478 0.571 0.924\nSoftlabel-GAN 20.97 0.5793 126.32±42.69 0.4696±0.0407 0.815 0.585 0.796 0.300 0.613 0.513 0.612 0.927\nImbalancedCIFAR-10\nFID↓ LPIPS↑ intra-FID↓ intra-LPIPS↑ Precision↑ Recall↑ Intra-Prec↑ Intra-Rec↑ Density↑ Coverage↑ Intra-Dens↑ Intra-Cov↑\nDiffAug-GAN 57.24 0.2090 112.96±26.62 0.1647±0.0154 0.742 0.468 0.557 0.343 0.435 0.393 0.277 0.569\nSmooth-GAN 66.75 0.2017 125.31±35.36 0.1751±0.0197 0.697 0.399 0.502 0.317 0.392 0.382 0.238 0.567\nSoftlabel-GAN 54.59 0.2058 109.79±24.72 0.1773±0.0167 0.756 0.408 0.590 0.337 0.477 0.438 0.308 0.590\nImbalancedCIFAR-100\nFID↓ LPIPS↑ intra-FID↓ intra-LPIPS↑ Precision↑ Recall↑ Intra-Prec↑ Intra-Rec↑ Density↑ Coverage↑ Intra-Dens↑ Intra-Cov↑\nDiffAug-GAN 37.70 0.2774 209.54±37.19 0.1543±0.0381 0.830 0.416 0.742 0.080 0.703 0.680 0.502 0.902\nSmooth-GAN 34.36 0.2570 205.59±34.82 0.1704±0.0382 0.772 0.500 0.714 0.112 0.619 0.701 0.476 0.914\nSoftlabel-GAN 32.70 0.2438 201.61±32.50 0.1948±0.0352 0.770 0.577 0.699 0.262 0.635 0.712 0.446 0.948\nStanfordCars\nFID↓ LPIPS↑ intra-FID↓ intra-LPIPS↑ Precision↑ Recall↑ Intra-Prec↑ Intra-Rec↑ Density↑ Coverage↑ Intra-Dens↑ Intra-Cov↑\nDiffAug-GAN 8.99 0.5664 95.00±12.62 0.5437±0.0207 0.869 0.616 0.921 0.310 1.389 0.856 1.247 1.000\nSmooth-GAN 7.91 0.5863 103.55±12.56 0.5436±0.0171 0.857 0.657 0.894 0.436 1.217 0.851 0.990 0.999\nSoftlabel-GAN 7.35 0.5855 89.04±11.64 0.5452±0.0168 0.884 0.664 0.927 0.455 1.228 0.851 0.972 1.000\nImbalancedTinyImagenet\nIS↑ FID↓ LPIPS↑ intra-FID↓ intra-LPIPS↑ Precision↑ Recall↑ Intra-Prec↑ Intra-Rec↑ Density↑ Coverage↑ Intra-Dens↑ Intra-Cov↑\nDiffAug-GAN 4.33 159.74 0.5496 332.61±34.55 0.2630±0.0319 0.341 0.000 0.139 0.000 0.080 0.017 0.022 0.049\nSmooth-GAN 4.48 151.10 0.5573 334.39±33.84 0.3991±0.0544 0.352 0.003 0.128 0.001 0.075 0.020 0.016 0.070\nSoftlabel-GAN 14.58 53.22 0.6631 238.83±53.74 0.5755±0.0501 0.648 0.647 0.475 0.270 0.357 0.264 0.212 0.634\nTable3.AblationstudyontheimbalanceddatasetsusingFID.\nMethod AnimeFace Oxford-102 CIFAR-10 CIFAR-100\nSoftlabel-GAN 17.26 46.32 36.72 49.99\nSoftlabel-GAN−− 36.16 84.36 79.16 80.99\nSoftlabel-GAN− 32.89 60.31 72.81 73.94\nSmooth-GAN− 40.45 78.20 83.68 72.80\n(a)ClassFIDscores(↓)ofallclasses.\nTable4.ComparisonwithMixupandOversampleusingFID.\nMethod AnimeFace Cars Oxford102 CIFAR-10 CIFAR-100\nSoftlabel-GAN 19.14 7.35 20.97 54.59 32.70\nMixup 24.87 14.53 39.13 56.36 36.32\nOversample 26.07 9.83 29.36 59.03 40.83\n(b)ClassLPIPSscores(↑)ofallclasses.\nTable5.FIDscoresintheexperimentsat256×256resolution.\nFigure6. HistogramofclassFIDandclassLPIPSscoresonAni-\nDiffAug-GAN Smooth-GAN Softlabel-GAN\nmeFace. ThemodetotheleftinFIDandthemodetotherightin\nAnimeFace 20.5066 20.6889 17.2493 LPIPSindicategoodperformance.Thenarrowdistributionmeans\nStanfordCars 10.1542 23.3695 9.8605\nachievingconsistentperformance. TheperformanceofDiffAug-\nGANvariesbyclassmorethanSoftlabel-GAN.\ntained by both methods in depth. Figure 6 shows the\nhistograms of FID and LPIPS for each class on Anime-\nFace. DiffAug-GANhasalargervarianceofclassFIDand\nLPIPS, varying the performance by each class data. It has\nthe distributions of FID and LPIPS with longer right and\nlefttailsthanSoftlabel-GAN,respectively. Softlabel-GAN\nwins 152 classes out of 176 against DiffAug-GAN in FID\nand wins 175 classes in LPIPS. To sum up, the proposed\nmethod reduces standard deviation in addition to improv-\ning performance, indicating that label augmentation and\ndataaugmentationmakedifferentcontributions.Alongwith\nqualitativeandquantitativeresults(Sec.5.3),ourmethodis\npromisingindealingwithimbalanceddata.\nFigure7showstheexamplesgeneratedbycGANswith\nlabelaugmentation,i.e.,Softlabel-GANandSmooth-GAN.\nhtoomSlebaltfoS\nFigure7. ComparisonofGANswithlabelaugmentation. Allim-\nagesaregeneratedwiththesameclass,butsomeimagescontain\nadifferentclassfromthetheotherimagesinSmooth-GAN.Each\nclasshasfoursamples.\nNote that we use the same class to generate images (i.e.,\ntwofirstrowsbelongtoaclassandtwolastrowsbelongto\nanother class). Unlike Softlabel-GAN, Smooth-GAN gen-\nerates images of a different class from a conditional class.\n4950"
      },
      {
        "page": 8,
        "content": "Table6.FIDscoresinexperimentswiththeADC-GAN[19]base- GAN [29] baseline. Vision-aided GAN achieves FIDs of\nline. 15.77 on AnimeFace, 14.48 on Stanford Cars, and 26.88\nDiffAug-GAN Smooth-GAN Softlabel-GAN on Oxford102, and Vision-aided GAN with our label aug-\nAnimeFace 35.0864 23.1834 21.0494 mentation achieves FIDs of 13.61 on AnimeFace, 11.01\nStanfordCars 20.6485 23.4987 14.5688\non Stanford Cars, and 16.84 on Oxford102. We also re-\nportfine-grainedmetricsfortheexperimentsinSupplemen-\nTable7. Comparisonwiththediffusionmodelwithclassifier-free\ntary Material. For the applicability of our method to an-\nguidance(CFG)onTinyImageNetinFID,intra-KID,intra-LPIPS,\nothergenerativemodel,weintegrateourlabelaugmentation\nintra-Precision (i-P), intra-Recall (i-R), intra-Density (i-D), and\nintodiffusionmodels. Wecompareclassifier-freeguidance\nintra-Coverage(i-C).\n(CFG) [17,18,40] and CFG with our label augmentation\nMethod FID intra-KID intra-LPIPS i-P i-R i-D i-C\non the imbalanced TinyImageNet dataset. In this experi-\nCFG[18] 21.92 0.065 0.271 0.783 0.419 0.732 0.867\nw/ourlabelaugmentation 22.04 0.064 0.268 0.795 0.426 0.773 0.880 ment,weuseintra-KernelInceptionDistance(KID)[3]in-\nsteadofintra-FIDduetothehugeinferencecostsofdiffu-\nTable8.QuantitativeresultsonImageNetLT[34]. sion models. In Tab. 7, our method shows that marginal\nMethod FID LPIPS intra-FID intra-LPIPS i-P i-R i-D i-C improvements over a diffusion model with CFG in fine-\nDiffAug-GAN 48.19 0.6641 371.1±29.20 0.615±0.015 0.232 0.632 0.045 0.600 grainedmetrics. Table8showstheexperimentalresultson\nSoftlabel-GAN 49.17 0.6601 352.0±29.05 0.643±0.011 0.251 0.723 0.055 0.707 ImageNetLT [34]. Our method demonstrates the improve-\nmentsinfine-grainedmetricsonthelarge-scaledataset.The\ncomparisonofper-classperformanceinFig.8clearlyshows\nourmethodprovidesperformancegainsinminorclasses.\nWediscusstheimpactofclassificationaccuracyongen-\nerationquality. Theaccuraciesofthepretrainedclassifiers\nintheexperimentsforAnimeFace,Oxford102,imbalanced\nTiny ImageNet, and Cars are 81.2%, 99.9%, 84.4%, and\n99.9%, respectively. An accuracy of less than 90% is not\nsophisticatedbutusefulfortrainingthecGANs. Wefurther\nanalyzetheinsensitivityoftheclassifierperformancetothe\ngenerationqualityinSupplementaryMaterial.\nFigure8. DifferencesbetweenclassFIDscoresofDiffAug-GAN\nand Softlabel-GAN. The large gains in minor classes and small\n6.Conclusion\ngainsinmajor(rightmost)classesshowthatourmethodeffectively\ncontributestominorclasses.\nWe investigated the problem of image generation in an\nimbalanced data regime. To prevent overlooking minor\nWecountthegeneratedimageswithaclassdifferentfroma classes in conventional approaches, we introduced label\nconditional class. First, we generate 50 images per class augmentationtoincreasediversitywhilemaintainingaffin-\non AnimeFace and compute the color histogram of each ity. Furthermore,weproposedSoftlabel-GANbyincorpo-\ngenerated image. The color histogram describes the num- ratingourlabelaugmentationintothediscriminator.Owing\nber of pixels in each color range over all pixels of an im- totheuseofclassifierpredictionsasadiscriminator’sclass\nage. Next, we calculate the average of the histogram cor- condition,Softlabel-GANenablesustoextractthefeatures\nrelations among images with the same class and use the from other class samples, resulting in more focus on mi-\ninter-quartile range rules to detect images with a differ- nor classes. Comprehensive benchmarking on imbalanced\nent class from a given condition. DiffAug-GAN, Smooth- datasetsshowsthatourmethodoutperformsothermethods\nGAN, and Softlabel-GAN generate 11, 52, and 12 images andislessaffectedbythenumberofsamplesofeachclass.\nthatdifferfromthegivenconditions,respectively.DiffAug- Ourlimitationisthatourmethodoutperformsconventional\nGANandourmethodcangeneratethecorrectclass,while methodsononlyimbalanceddatasetsanddoesnotoutper-\nSmooth-GAN fails. This is because Smooth-GAN blindly form them on balanced datasets, as conventional methods\ndistributes probabilities without considering the given im- performwellwithsufficienttrainingdata.\nages. Thisobservationclearlyconfirmstheoutperformance Acknowledgements.ThisworkwassupportedbytheInsti-\nofourlabelaugmentationagainstothermethods. Intheex- tuteforAIandBeyondoftheUniversityofTokyo,thecom-\nperimentswith256resolution, wealsoobservethesuperi- missioned research (No. 225) by National Institute of In-\nority of our Softlabel-GANover the baselines as shown in formationandCommunicationsTechnology(NICT),ROIS\nTab. 5. The experimentswith an ADC-GAN[19] baseline NII Open Collaborative Research 2023 23FC01, JSPS\n(Tab. 6) show that our method works on another architec- KAKENHI Grant Numbers JP22K17947, JP23KJ0381,\nture. We also conduct the experiment with a Vision-aided JP23H03449,andJP22H05015.\n4951"
      },
      {
        "page": 9,
        "content": "References [17] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdif-\nfusionprobabilisticmodels. InNeurIPS,volume33,pages\n[1] Animeface dataset. www.nurs.or.jp/˜nagadomi/\n6840–6851,2020. 8\nanimeface-character-dataset/. 1,3,5\n[18] Jonathan Ho and Tim Salimans. Classifier-free diffusion\n[2] Federico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal\nguidance. arXivpreprintarXiv:2207.12598,2022. 8\nLadhak,MyraCheng,DeboraNozza,TatsunoriHashimoto,\n[19] LiangHou,QiCao,HuaweiShen,SiyuanPan,Xiaoshuang\nDanJurafsky,JamesZou,andAylinCaliskan. Easilyacces-\nLi,andXueqiCheng. ConditionalGANswithauxiliarydis-\nsibletext-to-imagegenerationamplifiesdemographicstereo-\ncriminativeclassifier. InICML,pages8888–8902,2022. 8\ntypesatlargescale.InProceedingsofthe2023ACMConfer-\n[20] NathalieJapkowicz. Theclassimbalanceproblem: Signif-\nence on Fairness, Accountability, and Transparency, pages\nicance and strategies. In Proceedings of the International\n1493–1504,2023. 1\nConferenceonArtificialIntelligence(ICAI),pages111–117,\n[3] MikołajBin´kowski,DanicaJSutherland,MichaelArbel,and\n2000. 1,3\nArthurGretton.DemystifyingMMDGANs.InICLR,2018.\n[21] Liming Jiang, Bo Dai, Wayne Wu, and Chen Change Loy.\n8\nDeceiveD:adaptivepseudoaugmentationforGANtraining\n[4] AndrewBrock,JeffDonahue,andKarenSimonyan. Large\nwithlimiteddata. InNeurIPS,pages21655–21667,2021. 2\nscaleGANtrainingforhighfidelitynaturalimagesynthesis.\n[22] JustinJohnson,AlexandreAlahi,andLiFei-Fei. Perceptual\nInICLR,2018. 5,6\nlosses for real-time style transfer and super-resolution. In\n[5] NiteshV.Chawla,KevinW.Bowyer,LawrenceO.Hall,and\nECCV,pages694–711,2016. 3\nW. Philip Kegelmeyer. Smote: Synthetic minority over-\n[23] HM Kabir, Moloud Abdar, Seyed Mohammad Jafar Jalali,\nsampling technique. Journal of Artificial Intelligence Re-\nAbbasKhosravi,AmirFAtiya,SaeidNahavandi,andDipti\nsearch,16(1):321–357,2002. 3\nSrinivasan. Spinalnet: Deep neural network with gradual\n[6] GuobinChen,WongunChoi,XiangYu,TonyHan,andMan-\ninput. arXivpreprintarXiv:2007.03347,2020. 5\nmohanChandraker.Learningefficientobjectdetectionmod-\nelswithknowledgedistillation. InNeurIPS,pages742–751, [24] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,\nEliShechtman,SylvainParis,andTaesungPark. Scalingup\n2017. 3\nGANsfortext-to-imagesynthesis. InCVPR,pages10124–\n[7] TingChen,XiaohuaZhai,MarvinRitter,MarioLucic,and\n10134,2023. 1\nNeilHoulsby. Self-supervisedGANsviaauxiliaryrotation\nloss. InCVPR,pages12146–12155,2019. 2 [25] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine,\nJaakkoLehtinen,andTimoAila. Traininggenerativeadver-\n[8] AnoopCherianandAlanSullivan.Sem-GAN:Semantically-\nsarialnetworkswithlimiteddata.InNeurIPS,pages12104–\nconsistent image-to-image translation. In WACV, pages\n12114,2020. 1,2,5\n1797–1806,2019. 3\n[9] Jan Chorowski and Navdeep Jaitly. Towards better decod- [26] KaiKatsumata,DucMinhVo,andHidekiNakayama. OS-\ningandlanguagemodelintegrationinsequencetosequence SGAN: Open-set semi-supervised image generation. In\nmodels. In Proceedings of Interspeech, pages 523–527, CVPR,pages11185–11193,2022. 2\n2017. 2 [27] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.\n[10] KaiwenCui,YingchenYu,FangnengZhan,ShengcaiLiao, 3Dobjectrepresentationsforfine-grainedcategorization. In\nShijianLu,andEricPXing. KD-DLGAN:Datalimitedim- 4thInternationalIEEEWorkshopon3DRepresentationand\nagegenerationviaknowledgedistillation. InCVPR,pages Recognition(3dRR-13),2013. 3,5,6\n3872–3882,2023. 3,6 [28] Alex Krizhevsky and Geoffrey Hinton. Learning multiple\n[11] TerranceDevriesandGrahamW.Taylor. Improvedregular- layersoffeaturesfromtinyimages. 2009. 3,5\nizationofconvolutionalneuralnetworkswithcutout. arXiv [29] NupurKumari,RichardZhang,EliShechtman,andJun-Yan\npreprintarXiv:1708.04552,2017. 1 Zhu. Ensemblingoff-the-shelfmodelsforgantraining. In\n[12] PrafullaDhariwalandAlexanderNichol. Diffusionmodels CVPR,pages10651–10662,2022. 8\nbeatgansonimagesynthesis.InNeurIPS,pages8780–8794, [30] TuomasKynka¨a¨nniemi, TeroKarras, SamuliLaine, Jaakko\n2021. 1 Lehtinen, and Timo Aila. Improved precision and recall\n[13] RaphaelGontijo-Lopes,SylviaSmullin,EkinDogusCubuk, metricforassessinggenerativemodels. InNeurIPS,pages\nandEthanDyer. Tradeoffsindataaugmentation:Anempir- 3927—-3936,2019. 6\nicalstudy. InICLR,2021. 1 [31] ChristianLedig,LucasTheis,FerencHusza´r,JoseCaballero,\n[14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Andrew Cunningham, Alejandro Acosta, Andrew Aitken,\nBernhard Nessler, and Sepp Hochreiter. GANs trained by AlykhanTejani, JohannesTotz, ZehanWang, etal. Photo-\natwotime-scaleupdateruleconvergetoalocalnashequi- realisticsingleimagesuper-resolutionusingagenerativead-\nlibrium. InNeurIPS,pages6626–6637,2017. 6 versarialnetwork. InCVPR,pages4681–4690,2017. 3\n[15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill- [32] MuyangLi,JiLin,YaoyaoDing,ZhijianLiu,Jun-YanZhu,\ning the knowledge in a neural network. arXiv preprint andSongHan.GANcompression:Efficientarchitecturesfor\narXiv:1503.02531,2015. 3 interactiveconditionalGANs. IEEETPAMI,44(12):9331–\n[16] Tobias Hinz, Matthew Fisher, Oliver Wang, and Stefan 9346,2022. 3\nWermter. Improved techniques for training single-image [33] JaeHyunLimandJongChulYe. GeometricGAN. arXiv\nGANs. InWACV,pages1300–1309,2021. 2 preprintarXiv:1705.02894,2017. 3\n4952"
      },
      {
        "page": 10,
        "content": "[34] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, [50] ArtemSevastopolskiy,YuryMalkov,NikitaDurasov,Luisa\nBoqing Gong, and Stella X Yu. Large-scale long-tailed Verdoliva,andMatthiasNießner. Howtoboostfacerecog-\nrecognitioninanopenworld. InCVPR,pages2537–2546, nitionwithstylegan? InICCV,pages20924–20934,2023.\n2019. 3,8 1\n[35] Alexandra Sasha Luccioni, Christopher Akiki, Margaret [51] TamarRottShaham,TaliDekel,andTomerMichaeli. Sin-\nMitchell, and Yacine Jernite. Stable bias: Analyzing so- GAN:Learningagenerativemodelfromasinglenaturalim-\ncietal representations in diffusion models. arXiv preprint age. InCVPR,pages4570–4580,2019. 2\narXiv:2303.11408,2023. 1 [52] KarenSimonyanandAndrewZisserman. Verydeepconvo-\n[36] Mario Lucˇic´, Michael Tschannen, Marvin Ritter, Xiaohua lutionalnetworksforlarge-scaleimagerecognition.InICLR,\nZhai,OlivierBachem,andSylvainGelly. High-fidelityim- 2015. 3\nage generation with fewer labels. In ICML, pages 4183– [53] JiazeSun,BinodBhattarai,andTae-KyunKim.MatchGAN:\n4192,2019. 2 aself-supervisedsemi-supervisedconditionalgenerativead-\n[37] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and versarialnetwork. InACCV,2020. 2\nYuichi Yoshida. Spectral normalization for generative ad- [54] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nversarialnetworks. InICLR,2018. 1,3 Shlens, andZbigniewWojna. Rethinkingtheinceptionar-\n[38] TakeruMiyatoandMasanoriKoyama. cGANswithprojec- chitectureforcomputervision. InCVPR,pages2818–2826,\ntiondiscriminator. InICLR,2018. 4,6 2016. 1,2,3,4,5,6\n[39] MuhammadFerjadNaeem,SeongJoonOh,YoungjungUh, [55] DustinTran,RajeshRanganath,andDavidBlei. Hierarchi-\nYunjeyChoi,andJaejunYoo. Reliablefidelityanddiversity calimplicitmodelsandlikelihood-freevariationalinference.\nmetricsforgenerativemodels. InICML,pages7176–7185, InNeurIPS,pages5523–5533,2017. 3\n2020. 6 [56] Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen,\n[40] Alexander Quinn Nichol and Prafulla Dhariwal. Improved Trung-KienNguyen,andNgai-ManCheung. Ondataaug-\ndenoising diffusion probabilistic models. In ICML, pages mentation for GAN training. IEEE TIP, 30:1882–1897,\n8162–8171,2021. 8 2021. 1,2\n[41] Maria-Elena Nilsback and Andrew Zisserman. Automated [57] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-\nflower classification over a large number of classes. In reit,LlionJones,AidanNGomez,LukaszKaiser,andIllia\nProceedingsoftheIndianConferenceonComputerVision, Polosukhin. Attention is all you need. In NeurIPS, pages\nGraphics&ImageProcessing,pages722–729,2008. 3,5 6000–6010,2017. 2\n[42] Yiming Qin, Huangjie Zheng, Jiangchao Yao, Mingyuan [58] Duc Minh Vo, Akihiro Sugimoto, and Hideki Nakayama.\nZhou,andYaZhang. Class-balancingdiffusionmodels. In PPCD-GAN: Progressive pruning and class-aware distilla-\nCVPR,pages18434–18443,2023. 2 tion for large-scale conditional GANs compression. In\n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya WACV,pages2436–2444,2022. 3\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, [59] Jiayu Wu, Qixiang Zhang, and Guoxi Xu. Tiny ImageNet\nAmandaAskell,PamelaMishkin,JackClark,etal. Learn- challenge. 3,5\ningtransferablevisualmodelsfromnaturallanguagesuper- [60] Chang-Bin Zhang, Peng-Tao Jiang, Qibin Hou, Yunchao\nvision. InICML,pages8748–8763,2021. 3 Wei,QiHan,ZhenLi,andMing-MingCheng.Delvingdeep\n[44] Harsh Rangwani, Naman Jaswani, Tejan Karmali, Varun intolabelsmoothing. IEEETIP,30:5984–5996,2021. 2\nJampani,andRVenkateshBabu.ImprovingGANsforlong- [61] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\ntaileddatathroughgroupspectralregularization. InECCV, DavidLopez-Paz. Mixup:Beyondempiricalriskminimiza-\npages426–442,2022. 2 tion. InICLR,2018. 1,2,4,5\n[45] AliRazavi, AaronVandenOord, andOriolVinyals. Gen- [62] HanZhang,IanGoodfellow,DimitrisMetaxas,andAugus-\nerating diverse high-fidelity images with VQ-VAE-2. In tusOdena.Self-attentiongenerativeadversarialnetworks.In\nNeurIPS,pages14866—-14876,2019. 1 ICML,pages7354–7363,2019. 1\n[46] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki [63] R.Zhang,P.Isola,A.A.Efros,E.Shechtman,andO.Wang.\nCheung,AlecRadford,andXiChen. Improvedtechniques Theunreasonableeffectivenessofdeepfeaturesasapercep-\nfortrainingGANs. InNeurIPS,pages2234–2242,2016. 6 tualmetric. InCVPR,pages586–595,2018. 6\n[47] Victor Sanh, Lysandre Debut, Julien Chaumond, and [64] ShengyuZhao,ZhijianLiu,JiLin,Jun-YanZhu,andSong\nThomas Wolf. DistilBERT, a distilled version of bert: Han. Differentiable augmentation for data-efficient GAN\nsmaller, faster, cheaper and lighter. arXiv preprint training. In NeurIPS, pages 7559–7570, 2020. 1, 2, 4, 5,\narXiv:1910.01108,2019. 3 6\n[48] Axel Sauer, Kashyap Chitta, Jens Mu¨ller, and Andreas [65] ZhengliZhao,ZizhaoZhang,TingChen,SameerSingh,and\nGeiger. ProjectedGANsconvergefaster. InNeurIPS,pages HanZhang. ImageaugmentationsforGANtraining. arXiv\n17480–17492,2021. 1 preprintarXiv:2006.02595,2020. 1,2\n[49] AxelSauer,KatjaSchwarz,andAndreasGeiger.StyleGAN-\nXL: Scaling StyleGAN to large diverse datasets. In Inter-\nnationalConferenceonComputerGraphicsandInteractive\nTechniques(SIGGRAPH),2022. 1\n4953"
      }
    ]
  },
  "pdf_url": "/uploads/28e5bf668c718233be0973c24f6b9b37.pdf"
}