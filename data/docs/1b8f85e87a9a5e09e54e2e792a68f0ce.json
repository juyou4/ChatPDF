{
  "filename": "InstanceAssemble_compressed.pdf",
  "upload_time": "2025-12-07T17:20:05.072236",
  "data": {
    "full_text": "InstanceAssemble: Layout-Aware Image Generation\nvia Instance Assembling Attention\nQiangXiang1,2,ShuangSun2,BingleiLi1,3,\nDejiaSong2,HuaxiaLi2,YiboChen2, XuTang2,YaoHu2, JunpingZhang1∗\n1ShanghaiKeyLaboratoryofIntelligentInformationProcessing,\nCollegeofComputerScienceandArtificialIntelligence,FudanUniversity\n2XiaohongshuInc. 3ShanghaiInnovationInstitute\n{qxiang24, blli24}@m.fudan.edu.cn, jpzhang@fudan.edu.cn,\n{sunshuang1, dejiasong, lihuaxia, zhaohaibo, tangshen, xiahou}@xiaohongshu.com\nFigure1:Layout-awareimagegenerationresultbyInstanceAssemble.Weshowimagegeneration\nresultunderpreciselayoutcontrol,rangingfromsimpletointricate,sparsetodenselayouts.\nAbstract\nDiffusionmodelshavedemonstratedremarkablecapabilitiesingeneratinghigh-\nqualityimages. RecentadvancementsinLayout-to-Image(L2I)generationhave\nleveragedpositionalconditionsandtextualdescriptionstofacilitatepreciseand\ncontrollableimagesynthesis. Despiteoverallprogress,currentL2Imethodsstill\nexhibitsuboptimalperformance. Therefore,weproposeInstanceAssemble,anovel\narchitecturethatincorporateslayoutconditionsviainstance-assemblingattention,\nenabling position control with bounding boxes (bbox) and multimodal content\ncontrolincludingtextsandadditionalvisualcontent. Ourmethodachievesflexible\nadaptiontoexistingDiT-basedT2Imodelsthroughlight-weightedLoRAmodules.\nAdditionally,weproposeaLayout-to-Imagebenchmark,Denselayout,acompre-\nhensivebenchmarkforlayout-to-imagegeneration,containing5kimageswith90k\ninstancesintotal. WefurtherintroduceLayoutGroundingScore(LGS),aninter-\npretableevaluationmetrictomorepreciselyassesstheaccuracyofL2Igeneration.\nExperimentsdemonstratethatourInstanceAssemblemethodachievesstate-of-the-\nartperformanceundercomplexlayoutconditions,whileexhibitingstrongcompati-\nbilitywithdiversestyleLoRAmodules. Thecodeandpretrainedmodelsarepub-\nliclyavailableathttps://github.com/FireRedTeam/InstanceAssemble.\n∗Correspondingauthor.\n39thConferenceonNeuralInformationProcessingSystems(NeurIPS2025).\n5202\ntcO\n82\n]VC.sc[\n2v19661.9052:viXra\n\n1 Introduction\nDiffusionmodels[22]haverevolutionizedimagegenerationtask,witharchitectureslikeDiffusion\nTransformer(DiT)[40]offeringsuperiorqualityovertraditionalUNet-basedframeworks. Recent\nimplementationssuchasStableDiffusion3/3.5[15,49]andFlux.1[4]furtherenhancetext-to-image\nalignment, paving the way for advancements in layout-controlled generation. Layout-to-Image\n(L2I)generationisataskthatfocusesoncreatingimagesunderlayoutconditions,allowingusers\ntodefinespatialpositionsandsemanticcontentofeachinstanceexplicitly. Thistaskfacesseveral\nsignificantchallenges:(i)ensuringpreciselayoutalignmentwhilemaintaininghighimagequality,(ii)\npreservingobjectpositionsandsemanticattributesaccuratelyduringtheiterativedenoisingprocess\nofdiffusionmodels,and(iii)supportingvarioustypesofreferenceconditions,suchastexts,images\nandstructureinformation. Thesechallengeshighlightthecomplexityofachievingrobustandflexible\nlayout-controlledimagegeneration.\nExistingL2Imethodscanbebroadlycategorizedintotraining-freeandtraining-basedapproaches,\nbothpossessingdistinctadvantagesandlimitations. Training-freemethods[55,7,13,8,5,27]rely\nonheuristictechniqueswithoutmodifyingthebasemodel. However,thesemethodsoftenexhibit\ndegradedperformanceincomplexlayouts,demonstratehighsensitivitytohyperparametertuning,\nandsufferfromslowinferencespeed,whichmakethemlesspracticalforreal-worldapplications.\nIncontrast,training-basedmethods[63,53,64,29,61]involvetrainingspecificlayoutmodulesto\nimprovelayoutalignment,whichintroducesasignificantamountofextraparametersandincreases\ntrainingcomplexityandresourcerequirements. Additionally, existingL2Ievaluationmetricsex-\nhibitinaccuracies,suchasfalseacceptanceandlocalizationerrors. Theseidentifiedshortcomings\nnecessitatealgorithminnovationforeffectiveandefficientlayout-controlledimagegeneration.\nTherefore, we propose InstanceAssemble, a novel framework that systematically tackles these\nissuesthroughinnovativedesignandefficientimplementation. Ourapproachintroducesacascaded\nInstanceAssemblestructure,whichemploysamultimodalinteractionparadigmtoprocessglobal\nprompts and instance-wise layout conditions sequentially. By leveraging the Assemble-MMDiT\narchitecture,weapplyanindependentattentionmechanismtothesemanticcontentofeachinstance,\nthusenablingeffectivehandlingofdenseandcomplexlayouts. Furthermore,weadoptLoRA[23]\nforlightweightadaptation,addingonly71MparameterstoSD3-Medium(2B)and102MtoFlux.1\n(11.8B).Ourmethodenablespositioncontrolwithboundingboxesandmultimodalcontentcontrol\nincludingtextsandadditionalvisualcontent. Thislightweightdesignpreservesthecapabilitiesof\nthebasemodelwhileenhancingflexibilityandefficiency. Wealsointroduceanovelmetriccalled\nLayoutGroundingScore(LGS)toensureaccurateevaluationforL2Igeneration,alongsideatest\ndatasetDenseLayout. Thismetricprovidesaconsistentbenchmarkforassessinglayoutalignment.\nOurmethodachievesstate-of-the-artperformanceacrossbenchmarksanddemonstratesrobustlayout\nalignmentunderawidevarietyofscenarios,rangingfromsimpletointricate,sparsetodenselayouts.\nNotably,despitebeingtrainedonsparselayouts(≤ 10instances),ourapproachmaintainsrobust\ngeneralization capability on dense layouts (≥ 10 instances), confirming the effectiveness of our\nproposedInstanceAssemble. Themaincontributionsarelistedbelow.\n1. WeproposeacascadedInstanceAssemblestructurethatprocessesglobaltextpromptsand\nlayoutconditionssequentially,enablingrobusthandlingofcomplexlayoutsthroughanindependent\nattentionmechanism.\n2. ByleveragingLoRA[23],weachieveefficientadaptationwithminimalextraparameters\n(3.46%onSD3-Mediumand0.84%onFlux.1),supportingpositioncontrolwithmultimodalcontent\ncontrolwhilepreservingcapabilitiesofbasemodel.\n3. WeproposeanewtestdatasetDenseLayoutandanovelmetricLayoutGroundingScore\n(LGS)forLayout-to-Imageevaluation. Experimentalresultsdemonstratethatourapproachachieves\nstate-of-the-artperformanceandrobustcapabilityundercomplexanddenselayoutconditions.\n2 RelatedWork\nText-to-ImageGenerationText-to-imagesynthesis[47,45,42,40,30,17,6,1,44,3]haswitnessed\nrapidprogresswiththedevelopmentofdiffusionmodels.Initialworks[44,3,47,45]utilizeUNet[46]\nas the denoising architecture, leveraging cross-attention mechanisms to inject text-conditioning\n2\n\nGlobal Prompt 1. Global Prompt – Image 2. Instances - Image\nGlobal Prompt Global Prompt\n“\na\no\nVa\nf\ni\nsh\naq\nsi\nu\nus kat\nnr\nao\nie\nlr g.i\nh\nc\ntT\nIa ,hl\ne\nn\nr\nsNm\nim\ntoo don\nin\naiu\nnu\nnsm gme\ne\ncen\non\net\nnt\nl\nai\nCo\ns\noc\nh\na\noa\nnt\nr\nte ssd\net\ne\n.a\nni ”tn\nu\nt\ne\nredocnEredocnE\nEAVtxeT\nL Enay co odut\ner\nxN\nImage MMDiT\n1In\n2I sm taag\nn\n3e\nces\n4\nAs Ms Mem Dib Tle- 1In\n2I sm taag\nn\n3e\nces\n4\nredoceD\nEAV\nTextual Instance Content\nAssemble-MMDiT\n“A tall, pointed white structure\nw s “ s d “ hi p e B itAt h t r sa h e a o ttma r i n ouh i l z rejo c s e i e Sr a cosi l a e aft pz n q l i aco f d u an i e f, tt n a s i h ia i t gw ol a r r ue r\na\nl i i ra s ltb d a ee a a e n sh n t r e w Ld s dr i as t a t ee t h t a pd h\ny\na e t i on t u cb od p o e tr r\nu\ne p , eo n ta a . dn a k ” .z te. ”e” redocnE txeT FM oL uP\nrier\negamI mroNreyaLadA latent 1 tacno&C .joV rKQ P\n.\nnttA\n1 elbmessA FF&mroNreyaL\nPointe hd\no\nrr so eof DenseSample secnatsnI 1234 mroNreyaLadA 1234 2 tacno&C .jV oK rQ P\n.\n2\nAssemble-Attn\n1234 FF&mroNreyaL 1234\nHistorical monument Legend Concatenation Element-wise Addition Frozen Trainable(LoRA)\nFigure2: TheproposedInstanceAssemblepipeline. Variouslayoutconditionsareprocessedbythe\nLayoutEncodertoobtaininstancetokens,whichguidetheimagegenerationviaAssemble-MMDiT.\nInAssemble-MMDiT,theinstancetokensinteractwithimagetokensthroughtheAssembling-Attn.\nsignals. Recently, researches [6, 15, 49, 4, 34] have used the Multimodal Diffusion Transformer\n(MMDiT)architecture,markingasignificantimprovement.\nLayout-to-ImageGenerationLayout-to-Imagegenerationenablesimagegenerationunderlayout\nconditions,whichisdefinedasspatialpositionswithtextualdescriptions. Existingapproachescanbe\nbroadlycategorizedintotraining-freeandtraining-basedparadigms.\nTraining-freemethodsleveragepretrainedtext-to-imagediffusionmodelswithoutadditionaltrain-\ning. Acommonstrategyinvolvesgradient-basedguidanceduringdenoisingtoalignwithlayout\nconditions[55,12,41,13,18,50]. Also,therearemethodsthatdirectlymanipulatelatentsthrough\nwell-defined replacing or merging operations [8, 48, 2] or enforce layout alignment via spatially\nconstrainedattentionmasks[5,20]. GrounDiT[27]exploitssemanticsharinginDiT:acroppednoisy\npatchandthefullimagebecomesemanticcloneswhendenoisedtogether,enablinglayout-to-image\ngeneration by jointly denoising instance regions with their corresponding image context. Other\napproachesgenerateeachinstanceseparatelyandemployinpaintingtechniquestocomposethefinal\nimage[37]. However,thesemethodsdemonstratedecentperformanceprimarilyonsimpleandsparse\nlayouts,whiletheiraccuracydecreasesinmorecomplexlayouts. Somemethodsrequirehyperparam-\netertuningspecifictodifferentlayoutconditions,reducingtheiradaptability. Furthermore,additional\ngradient computations or latent manipulations result in slow inference speed, thus limiting their\napplicabilityinreal-worldscenarios.\nTraining-basedmethodsexplicitlyincorporatelayoutconditioningthrougharchitecturalmodifica-\ntions. Mostapproachesinjectspatialconstraintsviacross-attention[63,57,36,25,59,16,54]or\nself-attention[10,53,28].Someworksproposededicatedlayoutencodingmodules[9,64,65,58,62]\noradoptatwo-stagepipelinethatgeneratesimagesafterpredictingadepthmapwithlayoutcondi-\ntions[14,66]. Otherworksleverageautoregressiveimagegenerationmodels[19]. Thesemethods\nsufferhighcomputationalcostsduetoexcessiveparameters.\n3 Method\nPreliminariesRecentstate-of-the-arttext-to-imagemodelssuchasSD3[15]andFlux[4]adoptthe\nMultimodalDiffusionTransformer(MMDiT)asthebackboneforgeneration. Unliketraditional\nUNet-basedcross-attentionapproaches,MMDiTstreatimageandtextmodalitiesinasymmetric\nmanner, which leads to stronger prompt alignment and controllability. These models are trained\nunder the flow matching framework [33], which formulates generation as learning a continuous\nvelocityfieldthattransportsnoisetodata. GivenacleanlatentxandGaussiannoiseϵ∼N(0,I),an\n3\n\ninterpolatedlatentisdefinedas\nz =(1−t)x+tϵ, t∈[0,1]. (1)\nt\nThe training objective minimizes the squared error between the predicted velocity and the target\nvelocity(ϵ−x):\nh i\nL =E ∥v (z ,t,y)−(ϵ−x)∥2 , (2)\nFM ϵ∼N(0,I),x,t θ t 2\nwherev isimplementedwithanMMDiTbackbone.\nθ\nProblem Definition Layout-to-Image generation aims to synthesize images with precise control\nthroughaglobalpromptpandinstance-wiselayoutconditionsL. ThelayoutconditionscompriseN\ninstances{l }N ,whereeachinstancel isdefinedbyitsspatialpositionb andcontentc :\ni i=1 i i i\nL={l ,...,l }, wherel =(c ,b ). (3)\n1 N i i i\nInourframework,spatialpositionsarerepresentedasboundingboxes,whileinstancecontentcanbe\nspecifiedthroughmultiplemodalities: textualinstancecontentandadditionalvisualinstancecontent,\nincludingreferenceimages,depthmapsandedgemaps.\nWeproposeInstanceAssemble,aframeworkwithaLayoutEncodertoencodethelayoutconditions\nandAssemble-MMDiTtoeffectivelyintegratetheencodedlayoutconditionswithimagefeatures.\n3.1 LayoutEncoder\nWeuseaLayoutEncoder(Fig.2left-bottompanel)toencodeeachinstancel ,andthetokensare\ni\ndenotedashL =[hl1,...,hlN]whichrepresentsthelayoutinformationofeachinstance. Giventhe\nspatialpositionoftheinstance(boundingbox),wefirstenhancethespatialrepresentationthrough\nDenseSample. Givenaboundingboxb =(x ,y ,w,h)∈[0,1]4withtop-leftcoordinates(x ,y )\ni 1 1 1 1\nandsize(w,h),wegenerateK2uniformlyspacedpoints:\n(cid:26)(cid:18) (cid:19)(cid:12) (cid:27)\nw h (cid:12)\nP i = x 1+k x· K,y 1+k y· K (cid:12) (cid:12)k x,k y ∈{0,...,K−1} (4)\nThen,followingGLIGEN[28],wecomputethetextualinstancetokensas:\nhi =MLP([τ(c ),Fourier(P )]), (5)\nl i i\nwhere τ represents the text encoder, Fourier(·) denotes Fourier embedding [51], [·, ·] denotes\nconcatenationalongthefeaturedimension,andMLPisamulti-layerperception.\nAdditionally,wecanuseadditionalvisualinstancecontenttobetterimproveperformance. Given\nthevisualinstancecontent,wefirstextractfeaturesusingtheVAEencoderofthebasemodel,then\nprojectthemtotheunifiedinstancetokenspacethroughaMLP:\nhi =MLP(VAE(c )). (6)\nl i\n3.2 Assemble-MMDiT\nWeobservethatapplyingattentionbetweenallimagetokensandinstancetokensresultsinsuboptimal\nperformanceundercomplexlayoutconditions(e.g.,overlapping,tinyobjects). Toaddressthis,we\nintroduce Assemble-MMDiT (Fig. 2, right-bottom panel), which enhances the location of each\ninstancewhilemaintainingcompositionalcoherencewithotherinstances. Ourmethodprocesses\neachinstanceindependentlythroughattentionmoduleswithitsassociatedimagetokens,followedby\nweightedfeatureassembling.\nFormally,givenimagetokensh∈RC×W×H (whereC denotesthelatentchannelsizeand[W,H]\nthelatentsize)andinstancetokenshl ∈ RC×N,weapplyAdaLayerNorm[56],followedbyour\nproposedAssembling-Attn,asshowninFig.2(right-bottompanel). Wecroptheimagetokenshz by\nthebboxb ofeachinstanceandgethz =hz[b ]∈RC×w×h. Then,weprojectthecroppedimage\ni li i\ntokenshz\nli\nandtheircorrespondinginstancetokensl iintoqueries(Qzli,Qli),keys(Kzli,Kli),and\nvalues(Vzli,Vli),andthenapplyattention:\nhz li′ ,hl i′ =Attention(cid:0) [Qzli,Qli],[Kzli,Kli],[Vzli,Vli](cid:1) . (7)\n4\n\nwhere[·,·]denotesconcatenationalongthetokendimension. Theupdatedtokensareassembled\nacross instances. Let M ∈ NW×H represent the instance density map, calculating the counts of\ninstances.\nTheassembledimagetokenshz′ andinstancetokenshl′\narecomputedas:\nN\nhz′ :hz′\n[:,i,j]=\n1 X hz′\n[:,i,j], wherei∈[0,W −1],j ∈[0,H −1]\nM[i,j] lk (8)\nk=1\nhl′ :hl′ [:,k] =hl k′ .\nAs illustrated in Fig. 3, the top row demonstrates that\nourassemblingmechanismensuresinstancetokensattend\no l\ne\nc\nee\nxf\non f\nf\nprl t ey\nr\nlci\ne\nint\nct\nco\ni iv\ntb tr\ne\nsl le a\nal\npl yc ye akv\notg\ni.a uaun T tlit\nd\nch pi\ne\noem\nos\nnsma\ntg\ni\nrg\ntl\noi ie od\no\nlbd nr (e al bseg\nl\n.\noi pr tIo to nrn ow os mcm, orw re\np\nn\nov th\nt\nwree\nt\naoar )se l\nk\nts r,eu et\nn\ng\nsn h\nus\ner ae lntt tl\no\nsea th rt\nif\nae ne\no\ntd\nc\ni\nlm\no\nor ue\nn\nce sg ac woi lho\nin\nza in\nt\nan\nt\nhs\nh\nti\no\nia s\ne\noumr\ni\nne\nr\nt\ntuoyal\n/w\ntpmos re pc n la at bs on lI\nG\ne inrr co or ns s( is\" tB er ni cti is eh sS (\"h do or gth \"a mir\" isi sn inw g)r .onglocation)orsemantic tuoyal\no/w\nBritish ShorthairAmerican robin Maltipoo dog water\nFurthermore,topreservethegenerationcapabilityofthe Figure3: (Top)instance-imageattention\noriginalmodelandmitigateconditionalconflictsbetween mapw/layout. (Middle)globalprompt-\nglobal prompt and layout conditions, we employ a cas- imageattentionmapw/layout.(Bottom)\ncadedmechanismasshowninFig.2(right-abovepanel). globalprompt-imageattentionmapw/o\nIn our design, the global text prompt and image latents layout.\narepassedthroughoriginalMMDiTfirst,thentheimage\ntokens along with instance tokens are processed by our\nAssemble-MMDiTmodule. Thefirststepcapturesglobalcontextandensuresgenerationquality,\nwhilethesecondstepensuresinstancelayoutalignment. Besides,wetrainAssemble-MMDiTwith\nLoRA,significantlyreducingboththetrainingcostandinferencecosts.\n‘cRoanldld iye\nt\nt iCe oac nrt\n:’\nSAMbox co‘nTdoiwteiro’n: detect\ndetect co‘nWdinitdioown’:\n‘Blaccko nsdhiotlidoenr: bag’ detect\n(a) Rally Car? (b) Black shoulder bag? (c) Window? (d) Tower?\nCropVQA: Yes –> 1 CropVQA: No –> 0 SAMIoU: 0.77 BinaryIoU: 1\nDetectIoU: 0.60 DetectIoU: 0.54 DetectIoU: 0.00 DetectIoU: 0.81\nFigure4: Failurecasesofothermetrics. (a)falseacceptanceinCropVQA,(b)falserejectionin\nCropVQA,(c)localizationerrorinSAMIoU,and(d)discontinuousinBinaryIoU.\n3.3 Benchmark: DenseLayoutandLayoutGroundingScore\nTheLayout-to-Imagetaskaimstogenerateimagesthatalignpreciselywithprovidedlayouts,eval-\nuatingbothspatialaccuracyandsemanticconsistency(e.g.,color,texture,andshape,ifprovided).\nTheexistingmetrics(AP/AR)forobjectdetection[10,28,55,53]aresuboptimal. Theyassumea\nfixedcategorysetandrelyoninappropriateprecision/recallforbinarylayoutoutcomes. VLM-based\ncroppedVQAmethods[61]sufferfalseacceptance(Fig.4(a))andfalserejection(Fig.4(b)). While\nspatial-onlymetricslikeSAMIoU[11]ignoreappearanceconsistency(Fig.4(c)),GroundingDINO-\nbased[35]binaryIoUthresholds[64,54]failtocapturecontinuouslayoutprecision(Fig.4(d)). Thus,\nweproposeLayoutGroundingScore(LGS),whichintegratesbothspatialaccuracyandsemantic\naccuracy:\n1. SpatialAccuracy(DetectIoU):wedetectallinstancesviaanoff-the-shelfdetector[35],\ncomputetheIoUagainstconditionbbox, andreporttheglobalmeanIoUacrossallinstancesfor\nequalweighting.\n2. SemanticAccuracy: forinstanceswithIoU>0.5,wecropthepredictedregionandassessthe\nsemanticaccuracybyitsattributeconsistency(color,texture,shape)viaVLM-basedVQA[60].\nLGSsupportsopen-setevaluation,usesDetectIoUtoevaluatespatialaccuracyanddecouplesthe\nspatial and semantic check to avoid CropVQA [61] failures (shown in Fig. 4). Furthermore, we\n5\n\nTable1: QuantitativecomparisonbetweenourSD3-basedInstanceAssembleandotherL2I\nmethodsonLayoutSAM-Eval. ⋆ TheCropVQAscoreisproposedinCreatilayout[61]andthe\nscoreofInstanceDiff,MIGCandCreatiLayoutisborrowedfromCreatiLayout[61].\nCropVQA⋆ LayoutGroundingScore GlobalQuality\nLayoutSAM-Eval\nspatial↑color↑ texture↑shape↑ mIoU↑ color↑ texture↑shape↑ VQA↑ Pick↑ CLIP↑\nRealImages(UpperBound) 98.95 98.45 98.90 98.80 88.85 88.07 88.71 88.62\nInstanceDiff(SD1.5) 87.99 69.16 72.78 71.08 78.16 63.14 66.82 65.86 86.42 21.16 11.73\nMIGC(SD1.4) 85.66 66.97 71.24 69.06 62.87 50.70 52.99 51.77 88.97 20.69 12.56\nHICO(realisticVisionV51) 90.92 69.82 73.25 71.69 70.68 53.16 55.71 54.61 86.53 21.77 9.47\nCreatiLayout(SD3-M) 92.67 74.45 77.21 75.93 45.82 38.44 39.68 39.24 92.74 21.71 13.82\nInstanceAssemble(ours)(SD3-M) 94.97 77.53 80.72 80.11 78.88 63.89 66.27 65.86 93.12 21.79 12.76\nTable2: QuantitativecomparisonbetweenourInstanceAssembleandotherL2Imethodson\nDenseLayout.\nLayoutGroundingScore GlobalQuality\nDenseLayout\nmIoU↑ color↑ texture↑shape↑ VQA↑ Pick↑ CLIP↑\nRealImages(UpperBound) 92.35 76.52 80.78 79.78\nInstanceDiff(SD1.5) 47.31 29.48 33.36 32.43 88.79 20.87 11.73\nMIGC(SD1.4) 34.39 22.10 23.99 23.45 91.18 20.74 12.81\nHICO(realisticVisionV51) 22.42 10.52 11.69 11.46 74.42 20.51 8.16\nCreatiLayout(SD3-Medium) 15.54 11.69 12.34 12.17 93.42 21.88 12.89\nInstanceAssemble(ours)(SD3-Medium) 52.07 33.77 36.21 35.81 93.54 21.68 12.58\nRegional-Flux(Flux.1-Dev) 14.06 11.34 11.91 11.84 92.94 22.67 10.66\nRAG(Flux.1-Dev) 17.23 14.22 14.62 14.55 92.16 22.28 11.01\nInstanceAssemble(ours)(Flux.1-Dev) 43.42 27.60 29.50 29.14 93.36 21.98 11.38\nInstanceAssemble(ours)(Flux.1-Schnell) 45.33 27.73 30.06 29.62 93.52 21.72 10.78\nintroduceDenseLayout,adenseevaluationdatasetforL2I,whichconsistsof5kimageswith90k\ninstances(18.1perimage). TheimagesinDenseLayoutaregeneratedbyFlux.1-Dev, taggedby\nRAM++[24],detectedbyGroundingDINO[35],recaptionedbyQwen2.5-VL[43],andfilteredto\nretainthosewith≥15instances,thusprovidingdenselayoutconditions.\n3.4 TrainingandInference\nDuringtraining,wefreezetheparametersofthebasemodelandonlyupdatetheproposedLayout\nEncoder and Assemble-MMDiT module. We denote the adding parameters by θ′. The training\nobjectiveisgivenby\nL=E ϵ∼N(0,I),x,t,p,Lh(cid:13) (cid:13)v {θ,θ′}(cid:0) z t,t,p,L(cid:1) −(ϵ−x)(cid:13) (cid:13)2 2i , (9)\nwherez =(1−t)x+tϵ. Duringinference,layout-conditioneddenoisingisappliedduringthefirst\nt\n30%ofdiffusionsteps,asthelayoutprimarilyformsinearlystages[28,64].\n4 Experiments\n4.1 ExperimentalSetup\nImplementationDetailsThetextual-onlyInstanceAssembleistrainedonSD3-Medium[15]and\nFlux.1-Dev[4]andtheversionwithadditionalvisualinstancecontentisonlytrainedonSD3-Medium.\nWefreezethepretrainedMMDiTbackboneandonlyadapttheLayoutEncoderandLoRAmodules\nofAssemble-MMDiT.Assemble-MMDiTisinitializedfrompretrainedweights, andLoRAwith\nrank=4isapplied. IntheSD3-basedmodelallAssemble-MMDiTblocksareadapted,whileinthe\nFlux-basedmodelweadapteightblocks(sevendouble-blocksandonesingleblock)duetoresource\nconstraints. Duringinference,theLoRA-basedAssemble-MMDiTisactivatedforthefirst30%of\ndenoisingsteps,whiletheglobalprompt–imagephaseusesthefrozenbackbone. Thisdesignyields\n71M(SD3-M)and102M(Flux.1-Dev)additionalparametersforthetextual-onlysetting;thevariant\nwithadditionalvisualinstancecontent(SD3-M)introduces85Mparameters. Allmodelsaretrained\nonLayoutSAM[61]at1024×1024withProdigy,for380Kiterations(batchsize2)onSD3-Mand\n6\n\nTable4: Parameteradditionandtimeefficiencyundersparseanddenselayoutconditions. We\nevaluatedon10%oftheLayoutSAM-EvalandDenseLayoutdatasetsat1024×1024resolution. ⋆ are\noptimizedfor512×512resolution,sotheirresultsarereportedatthisscale.\nParameterAddition TimeEfficiency(s)(relativeruntimeincrease(%))\n(relativeparameteraddition(%)) SparseLayout DenseLayout\nInstanceDiff⋆(SD1.5) 369M(43%) 14.37(+771%) 44.81(+2754%)\nMIGC(SD1.4) 57M(6.64%) 14.41(+25.4%) 21.58(+87.5%)\nHICO⋆(realisticVisionV51) 361M(33.9%) 4.11(+92.9%) 9.93(+320%)\nCreatiLayout(SD3-M) 1.2B(64.0%) 4.37(+14.4%) 4.42(+14.8%)\nRegional-Flux(Flux.1-Dev) - 15.29(+113%) 37.47(+418%)\nRAG(Flux.1-Dev) - 15.69(+119%) 21.14(+192%)\nInstanceAssemble(ours)(SD3-M) 71M(3.46%) 7.19(+88.2%) 13.38(+248%)\nInstanceAssemble(ours)(Flux.1-Dev) 102M(0.84%) 8.21(+14.3%) 10.28(+41.9%)\nInstanceAssemble(ours)(Flux.1-Schnell) 102M(0.84%) 1.41(+8.46%) 1.70(+28.8%)\n300Kiterations(batchsize1)onFlux.1-Dev,using8×H800GPUs(7daysforSD3-M;5daysfor\nFlux.1-Dev).\nEvaluationDatasetWeuseLayoutSAM-Eval[61]toevaluateperformanceonfine-grainedopen-set\nsparseL2Idataset,containing5kimagesand19kinstancesintotal(3.8instancesperimage). To\nassess performance on fine-grained open-set dense L2I evaluation dataset, we use the proposed\nDenseLayout, which consists of 5kimages and 90k instancesin total (18.1 instancesper image).\nFollowingconventionalpractice,wealsoevaluateoncoarse-grainedclose-setL2Ievaluationdataset\nCOCO[31]. WecombineCOCO-StuffandCOCO-InstanceannotationstocreateourCOCO-Layout\nevaluationdataset,containing5kimagesand57kinstancesintotal(11.5instancesperimage).\nEvaluationMetricWeevaluatetheaccuracyofL2IgenerationusingourproposedLGSmetricalong\nwithCropVQAproposedbyCreatiLayout[61],measuringspatialandsemanticaccuracy. Wealso\nemploymultipleestablishedmetricstomeasureoverallimagequalityandglobalpromptalignment,\nincludingVQAScore[32],PickScore[26]andCLIPScore[21].\n4.2 EvaluationonL2IwithTextual-OnlyContent\nFine-GrainedOpen-SetSparseL2IGenerationTab.1presentsthequantitativeresultsofInstance-\nAssemble on LayoutSAM-Eval [61], reporting results using our proposed LGS, CropVQA [61]\nandglobalqualitymetrics. OurproposedInstanceAssemblenotonlyachievesSOTAinspatialand\nsemanticaccuracyofeachinstance,butalsodemonstratessuperiorglobalquality.\nFine-GrainedOpen-SetDenseL2IGener-\nation Tab. 2 presents results on DenseLay- Table 3: Comparison between our SD3-based\nout. WiththesameSD3-Mediumbackbone, InstanceAssemble and other L2I methods on\nInstanceAssemble significantly outperforms COCO-Layout. SinceCOCOdon\"thavedetailed\nCreatiLayout(mIoU:52.07vs. 15.54)while descriptionforeachinstance,wecannotevaluatethe\nmaintaining comparable global quality. On attributeaccuracyandonlyreportthespatialaccu-\nFlux.1,italsoyieldslargegainsoverRegional- racy-mIoU.\nFluxandRAG(e.g.,mIoU:43.42vs.17.23for\nRAG),showingthatourcascadedAssemble- LGS GlobalQuality\nCOCO-Layout\nAttndesigngeneralizeswellacrossbackbones. mIoU↑ VQA↑ Pick↑ CLIP↑\nComparedtoearlierUNet-basedapproaches RealImages(UpperBound) 49.14\nsuchasInstanceDiffandMIGC,ourmethod\nInstanceDiff(SD1.5) 30.39 75.77 20.75 24.41\nachieveshigherspatialandsemanticaccuracy MIGC(SD1.4) 27.36 70.32 20.20 23.58\n(mIoU:52.07vs.47.31)withoutsacrificingre- HICO(realisticVisionV51) 18.88 50.61 20.38 20.72\nCreatiLayout(SD3-M) 7.12 87.79 21.22 25.59\nalism. Overall,InstanceAssembleestablishes\nInstanceAssemble(ours)(SD3-M) 27.85 89.06 21.58 25.68\nconsistentimprovementsinlayoutalignment\nwhileensuringhighimagequalityunderchal-\nlengingdenselayouts.\nCoarse-GrainedClosed-SetL2IGenerationTab.3presentsthequantitativeresultofInstance-\nAssembleonCOCO-Layout. OurproposedInstanceAssemblesurpassespreviousmethodsinoverall\nimagequalitybutlagsslightlybehindInstanceDiff[53]inlayoutprecision. Weattributethisgapto:\n7\n\nLayout InstanceDiff MIGC HICO CreatiLayout Regional-Flux RAG Ours\nFigure5: QualitativecomparisonofInstanceAssemblewithothermethods.\n(i)InstanceDiff’sfine-grainedCOCOtrainingdatawithper-entityattributeannotations,and(ii)its\nentity-wisegenerationstrategy,whichimprovesprecisionatsignificantcomputationalcost(Tab.4).\nQualitativeComparisonThecomparativeresultsinFig.5\ndemonstrate that our proposed InstanceAssemble method\ntext\nachieves superior spatial precision and instance-caption 4te 6.x 1ture text+image\ntext+depth\nalignment compared to baseline methods. For example, 4 3 . 7color text+edge\n34.6\ninthethirdrow,bothInstanceDiff[53]andMIGC[64]gen- 32.8\neratemorethanoneshoes; HICO[9]failstogeneratethe shape 23.0 21.9\n45.8\nspecifiedNewBalanceshoe;Regional-Flux[5]doesnotad- 34.4 22.9 11.5 10.9\nhere to the layout conditions; and the shoe generated by 11.5 25.2 35.6 46.0 5 6 m.4 IoU\nRAG[8]isnotproperlyfusedwiththebackground. Incon-\n84.9\n10.7\ntrast,ourmethodgeneratesthecorrectinstance,accurately 87.9 20.0\n90.8 11.7\nplacedandseamlesslyintegratedwiththescene. VQA\n93.7 20.7 12.7\nTime Efficiency and Parameter Addition We compare 21.4 13.7\nCLIP\ntimeefficiencyandparameteradditionwithotherL2Imeth-\nPick2 2 .1\nods,asshowninTab.4. OurmethodachievesSOTAperfor-\nmanceonlayoutalignmentwithacceptabletimeefficiency Figure6: QuantitativeresultsofIn-\nandminimalparameteraddition. stanceAssemble with additional vi-\nsualinstancecontent.\n4.3 EvaluationonL2IwithAdditionalVisualContent\nWeevaluatethreeadditionalvisualinstancecontent: image,\ndepth,andedge(seeFig.6). Unsurprisingly,usingimage\nasadditionalinstancecontentyieldsthebestperformance, asitprovidesrichvisualinformation.\nAlthough depth and edge capture texture and shape features, their performance remains inferior\ncomparedtoimageinstancecontent. Nevertheless,visualmodalitiesoutperformtextual-onlyinstance\n8\n\nTable5: AblationstudyonourproposedcomponentsonDenseLayout. \"Assemble\"referstothe\npresenceoftheAssemble-Attnmodule(architecturaldesign). \"Cascaded\"indicatestheinteraction\norder: (✔)meansglobalprompt–imageinteractionfollowedbyinstance–imageinteraction(cascaded\nstructure),while(✘)meansbothareappliedinparallel. \"LoRA\"specifiesthetrainingstrategyforthe\nAssemble-MMDiTmodule: (✔)indicatestrainingwithLoRA,while(✘)indicatesfullfine-tuning.\n\"DenseSample\"denoteswhethertheDenseSamplespatialencodingisused.\nLayoutGroundingScore\nAssemble Cascaded LoRA DenseSample\nmIoU↑ color↑ texture↑shape↑ VQA↑\n✘ ✘ ✘ ✘ 11.69 9.16 9.68 9.56 93.75\n✔ ✘ ✘ ✘ 43.98 24.19 26.95 26.75 84.57\n✔ ✔ ✘ ✘ 45.96 29.61 31.50 31.09 92.71\n✔ ✔ ✔ ✘ 51.28 32.68 34.94 34.58 93.33\n✔ ✔ ✔ ✔ 52.07 33.77 36.21 35.81 93.54\ncontent. Qualitativecomparisons(Fig.7)furtherdemonstratethatvisualinstancecontentleadsto\nsuperiortextureandshapealignmentcomparedtotext.\nLayout Text+Image Text+Depth Text+Edge Text\nFigure7: QualitativeresultsofInstanceAssemblewithadditionalvisualinstancecontent.\n4.4 AblationStudy\nWe evaluated the contribution of each proposed component on SD3-M based InstanceAssemble\ninTab.5. Thebasemodel(SD3-Mediumwithoutadditionalmodules)yieldsaverylowLayout\nGrounding Score, indicating poor layout and content control when instance information is not\nexplicitlymodeled. TheintroductionofAssemble-Attnmoduleelevatesspatialaccuracy(mIoUto\n43.98)andboostssemanticmetrics(color/texture/shapeto24.19/26.95/26.75). Thecascadeddesign\n(✔:prompt–imagefollowedbyinstance–image;✘:parallel)resolvesglobalqualitydegradationwhile\nmaintaininglayoutalignment.UsingLoRAtotrainAssemble-MMDiTimprovesperformancefortwo\nreasons: (1)itretainsthebasemodel’scapabilitiescomparedtothefullyfine-tunedversion,and(2)\nitenableseffectivelayoutcontrolwithfarfewertrainableparametersbyintroducingonlylightweight\nlow-rankmatricesonattentionprojections. Finally,DenseSamplefurtherenhancesspatialaccuracy,\ninstancesemanticaccuracyandimagequality. Together,theserefinementsprogressivelycollectively\noptimizelayouttoimagemodelingwithoutcompromisinggenerationability.\n4.5 Applications\nWedemonstratethatInstanceAssembleisversatileandapplicabletovarioustasks. Itseamlessly\nintegrateswithdomain-specificLoRAmodulesformulti-domainstyletransferwhilemaintaining\nlayoutconsistency,asshowninFig.8. OurproposedInstanceAssemblecancooperatewithdistilled\nmodelssuchasFlux.1-Schnell[4],asillustratedinTab.2,achievinggeometriclayoutcontroland\ndetailedsynthesis. Ourapproachdemonstratesbothstyleadaptabilityandcomputationalefficiency,\nmakingitwell-suitedforcontrollablegenerativedesignapplications.\n9\n\nD3etUC\ngnitniap\nliO\nilbihG\nFigure8: TheadaptionofCute3D[52]/OilPainting[39]/Ghibli[38]LoRAwithourmethods.\nThe proposed InstanceAssemble successfully adapts diverse style lora and maintaining superior\nlayoutalignment.\n5 Conclusion\nWe present InstanceAssemble, a novel approach for Layout-to-Image generation. Our method\nachievesstate-of-the-artlayoutalignmentwhilemaintaininghigh-qualitygenerationcapabilitiesof\nDiT-basedarchitectures. WevalidateInstanceAssembleacrosstextualinstancecontentandadditional\nvisualinstancecontent,demonstratingitsversatilityandrobustness. Ourlayoutcontrolschemealso\nsuccessfullyadaptsdiversestyleLoRAswhilemaintainingsuperiorlayoutalignment,demonstrating\ncross-domaingeneralizationcapability. Futhermore,weintroduceLayoutGroundingScoremetric\nandaDenseLayoutevaluationdatasettovalidateperformanceundercomplexlayoutconditions.\nLimitationsandFutureWorkWhileourworkadvancescontrollablegenerationbyunifyingprecise\nlayoutcontrolwiththeexpressivepowerofdiffusionmodels,severallimitationsremain. First,our\ndesigncurrentlyrequiressequentialAssemble-MMDiTcalls,whichmayincurinefficiency;exploring\nparallelizationstrategiesisanimportantdirection. Second,althoughourapproachiseffectiveundera\nwiderangeoflayouts,imagefidelitycandegradeinextremelydenseorhighlycomplexcases.\nBroaderImpactsInstanceAssembleexpandsthefrontierofstructuredvisualsynthesisbyproviding\nfine-grainedlayoutcontrolandhigh-qualitymultimodalgeneration. However,itspowerfulgenerative\ncapabilities may also introduce risks. In particular, malicious use could enable the creation of\nmisleadingordeceptivelayouts,exacerbatingthespreadofdisinformation. Themodelmayalso\nraiseprivacyconcernsifappliedtosensitivedata,andlikemanygenerativesystems,itinheritsand\nmayamplifysocietalbiasespresentintrainingcorpora. Weencourageresponsibledeploymentand\ncontinuedinvestigationintosafeguardsthatmitigatetheseriskswhileenablingbeneficialapplications\nindesign,education,andaccessibility.\n10\n\nAcknowledgmentsandDisclosureofFunding\nThisresearchwassupportedbytheNationalNaturalScienceFoundationofChina(NSFC62576103,\n62176059). ThecomputationswereconductedusingtheCFFFplatformatFudanUniversity. Partof\nthisworkwascarriedoutduringaninternshipatXiaohongshu.\nReferences\n[1] F.Bao,S.Nie,K.Xue,Y.Cao,C.Li,H.Su,andJ.Zhu. AllareWorthWords: AViTBackbone\nforDiffusionModels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand\nPatternRecognition,pages22669–22679.IEEE,2023.\n[2] O.Bar-Tal, L.Yariv, Y.Lipman, andT.Dekel. MultiDiffusion: FusingDiffusionPathsfor\nControlled Image Generation. In Proceedings of the International Conference on Machine\nLearning,volume202ofProceedingsofMachineLearningResearch,pages1737–1752.PMLR,\n2023.\n[3] J.Betker,G.Goh,L.Jing,T.Brooks,J.Wang,L.Li,L.Ouyang,J.Zhuang,J.Lee,Y.Guo,\netal. Improvingimagegenerationwithbettercaptions,2023.\n[4] BlackForestLabs. Flux. https://github.com/black-forest-labs/flux,2024.\n[5] A.Chen,J.Xu,W.Zheng,G.Dai,Y.Wang,R.Zhang,H.Wang,andS.Zhang. Training-free\nRegionalPromptingforDiffusionTransformers,2024.URLhttps://arxiv.org/abs/2411.\n02395.\n[6] J.Chen,J.Yu,C.Ge,L.Yao,E.Xie,Z.Wang,J.T.Kwok,P.Luo,H.Lu,andZ.Li. PixArt-\nα: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis. In\nProceedingsoftheInternationalConferenceonLearningRepresentations.OpenReview.net,\n2024.\n[7] M.Chen,I.Laina,andA.Vedaldi.Training-FreeLayoutControlwithCross-AttentionGuidance.\nInWinterConferenceonApplicationsofComputerVision,pages5331–5341.IEEE,2024.\n[8] Z. Chen, Y. Li, H. Wang, Z. Chen, Z. Jiang, J. Li, Q. Wang, J. Yang, and Y. Tai. Region-\nAwareText-to-ImageGenerationviaHardBindingandSoftRefinement,2024. URLhttps:\n//arxiv.org/abs/2411.06558.\n[9] B.Cheng,Y.Ma,L.Wu,S.Liu,A.Ma,X.Wu,D.Leng,andY.Yin. HiCo: HierarchicalCon-\ntrollableDiffusionModelforLayout-to-imageGeneration. InAdvancesinNeuralInformation\nProcessingSystems,2024.\n[10] J.Cheng,X.Liang,X.Shi,T.He,T.Xiao,andM.Li. LayoutDiffuse: AdaptingFoundational\nDiffusionModelsforLayout-to-ImageGeneration,2023. URLhttps://arxiv.org/abs/\n2302.08908.\n[11] J. Cheng, Z. Zhao, T. He, T. Xiao, Z. Zhang, and Y. Zhou. Rethinking The Training And\nEvaluationofRich-ContextLayout-to-ImageGeneration. InAdvancesinNeuralInformation\nProcessingSystems,2024.\n[12] G. Couairon, M. Careil, M. Cord, S. Lathuilière, and J. Verbeek. Zero-shot spatial layout\nconditioningfortext-to-imagediffusionmodels. InProceedingsoftheIEEE/CVFInternational\nConferenceonComputerVision,pages2174–2183.IEEE,2023.\n[13] O. Dahary, O. Patashnik, K. Aberman, and D. Cohen-Or. Be Yourself: Bounded Attention\nfor Multi-subject Text-to-Image Generation. In Proceedings of the European Conference\nonComputerVision, volume15072ofLectureNotesinComputerScience, pages432–448.\nSpringer,2024.\n[14] deweiZhou,J.Xie,Z.Yang,andY.Yang. 3DIS:Depth-DrivenDecoupledImageSynthesis\nforUniversalMulti-InstanceGeneration. InProceedingsoftheInternationalConferenceon\nLearningRepresentations,2025. URLhttps://openreview.net/forum?id=MagmwodCAB.\n11\n\n[15] P.Esser,S.Kulal,A.Blattmann,R.Entezari,J.Müller,H.Saini,Y.Levi,D.Lorenz,A.Sauer,\nF.Boesel,D.Podell,T.Dockhorn,Z.English,andR.Rombach. ScalingRectifiedFlowTrans-\nformersforHigh-ResolutionImageSynthesis. InProceedingsoftheInternationalConference\nonMachineLearning.OpenReview.net,2024.\n[16] Y. Feng, B. Gong, D. Chen, Y. Shen, Y. Liu, and J. Zhou. Ranni: Taming Text-to-Image\nDiffusionforAccurateInstructionFollowing. InProceedingsoftheIEEE/CVFConferenceon\nComputerVisionandPatternRecognition,pages4744–4753.IEEE,2024.\n[17] P.Gao,L.Zhuo,D.Liu,R.Du,X.Luo,L.Qiu,Y.Zhang,C.Lin,R.Huang,S.Geng,R.Zhang,\nJ.Xi,W.Shao,Z.Jiang,T.Yang,W.Ye,H.Tong,J.He,Y.Qiao,andH.Li. Lumina-T2X:\nTransformingTextintoAnyModality,Resolution,andDurationviaFlow-basedLargeDiffusion\nTransformers,2024. URLhttps://arxiv.org/abs/2405.05945.\n[18] B.Gong,S.Huang,Y.Feng,S.Zhang,Y.Li,andY.Liu. Check,Locate,Rectify: ATraining-\nFreeLayoutCalibrationSystemforText-to-ImageGeneration.InProceedingsoftheIEEE/CVF\nConferenceonComputerVisionandPatternRecognition,pages6624–6634.IEEE,2024.\n[19] R.He,B.Cheng,Y.Ma,Q.Jia,S.Liu,A.Ma,X.Wu,L.Wu,D.Leng,andY.Yin. PlanGen:\nTowardsUnifiedLayoutPlanningandImageGenerationinAuto-RegressiveVisionLanguage\nModels,2025. URLhttps://arxiv.org/abs/2503.10127.\n[20] Y.He,R.Salakhutdinov,andJ.Z.Kolter. Localizedtext-to-imagegenerationforfreeviacross\nattentioncontrol,2023. URLhttps://arxiv.org/abs/2306.14636.\n[21] J.Hessel,A.Holtzman,M.Forbes,R.L.Bras,andY.Choi. CLIPScore: AReference-free\nEvaluationMetricforImageCaptioning. InProceedingsoftheEmpiricalMethodsinNatural\nLanguageProcessing,pages7514–7528.AssociationforComputationalLinguistics,2021.\n[22] J.Ho,A.Jain,andP.Abbeel. DenoisingDiffusionProbabilisticModels. InNeurIPS,2020.\n[23] E.J.Hu,Y.Shen,P.Wallis,Z.Allen-Zhu,Y.Li,S.Wang,L.Wang,andW.Chen. LoRA:Low-\nRankAdaptationofLargeLanguageModels. InProceedingsoftheInternationalConference\nonLearningRepresentations.OpenReview.net,2022.\n[24] X.Huang,Y.-J.Huang,Y.Zhang,W.Tian,R.Feng,Y.Zhang,Y.Xie,Y.Li,andL.Zhang.\nOpen-SetImageTaggingwithMulti-GrainedTextSupervision,2023. URLhttps://arxiv.\norg/abs/2310.15200.\n[25] C.Jia,M.Luo,Z.Dang,G.Dai,X.Chang,M.Wang,andJ.Wang. SSMG:Spatial-Semantic\nMapGuidedDiffusionModelforFree-FormLayout-to-ImageGeneration. InProceedingsof\ntheAAAIConferenceonArtificialIntelligence,pages2480–2488.AAAIPress,2024.\n[26] Y.Kirstain,A.Polyak,U.Singer,S.Matiana,J.Penna,andO.Levy. Pick-a-Pic: AnOpen\nDatasetofUserPreferencesforText-to-ImageGeneration. InAdvancesinNeuralInformation\nProcessingSystems,2023.\n[27] Y.Lee,T.Yoon,andM.Sung. GrounDiT:GroundingDiffusionTransformersviaNoisyPatch\nTransplantation. InAdvancesinNeuralInformationProcessingSystems,2024.\n[28] Y.Li,H.Liu,Q.Wu,F.Mu,J.Yang,J.Gao,C.Li,andY.J.Lee. GLIGEN:Open-SetGrounded\nText-to-ImageGeneration. InProceedingsoftheIEEE/CVFConferenceonComputerVision\nandPatternRecognition,pages22511–22521.IEEE,2023.\n[29] Y. Li, M. Keuper, D. Zhang, and A. Khoreva. Adversarial Supervision Makes Layout-to-\nImageDiffusionModelsThrive. InProceedingsoftheInternationalConferenceonLearning\nRepresentations.OpenReview.net,2024.\n[30] Z.Li, J.Zhang, Q.Lin, J.Xiong, Y.Long, X.Deng, Y.Zhang, X.Liu, M.Huang, Z.Xiao,\nD.Chen,J.He,J.Li,W.Li,C.Zhang,R.Quan,J.Lu,J.Huang,X.Yuan,X.Zheng,Y.Li,\nJ.Zhang,C.Zhang,M.Chen,J.Liu,Z.Fang,W.Wang,J.Xue,Y.Tao,J.Zhu,K.Liu,S.Lin,\nY.Sun,Y.Li,D.Wang,M.Chen,Z.Hu,X.Xiao,Y.Chen,Y.Liu,W.Liu,D.Wang,Y.Yang,\nJ.Jiang,andQ.Lu. Hunyuan-DiT:APowerfulMulti-ResolutionDiffusionTransformerwith\nFine-GrainedChineseUnderstanding,2024. URLhttps://arxiv.org/abs/2405.08748.\n12\n\n[31] T.Lin,M.Maire,S.J.Belongie,J.Hays,P.Perona,D.Ramanan,P.Dollár,andC.L.Zitnick.\nMicrosoftCOCO:CommonObjectsinContext. InProceedingsoftheEuropeanConferenceon\nComputerVision,volume8693ofLectureNotesinComputerScience,pages740–755.Springer,\n2014.\n[32] Z. Lin, D. Pathak, B. Li, J. Li, X. Xia, G. Neubig, P. Zhang, and D. Ramanan. Evaluating\nText-to-VisualGenerationwithImage-to-TextGeneration. InProceedingsoftheEuropean\nConferenceonComputerVision,volume15067ofLectureNotesinComputerScience,pages\n366–384.Springer,2024.\n[33] Y.Lipman,R.T.Q.Chen,H.Ben-Hamu,M.Nickel,andM.Le. Flowmatchingforgenera-\ntivemodeling. InProceedingsoftheInternationalConferenceonLearningRepresentations.\nOpenReview.net,2023.\n[34] B.Liu,E.Akhgari,A.Visheratin,A.Kamko,L.Xu,S.Shrirao,C.Lambert,J.Souza,S.Doshi,\nand D. Li. Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large\nLanguageModels,2024. URLhttps://arxiv.org/abs/2409.10695.\n[35] S.Liu,Z.Zeng,T.Ren,F.Li,H.Zhang,J.Yang,Q.Jiang,C.Li,J.Yang,H.Su,J.Zhu,and\nL.Zhang. GroundingDINO:MarryingDINOwithGroundedPre-trainingforOpen-SetObject\nDetection. InProceedingsoftheEuropeanConferenceonComputerVision,volume15105of\nLectureNotesinComputerScience,pages38–55.Springer,2024.\n[36] Z. Lv, Y. Wei, W. Zuo, and K. K. Wong. PLACE: Adaptive Layout-Semantic Fusion for\nSemanticImageSynthesis. InProceedingsoftheIEEE/CVFConferenceonComputerVision\nandPatternRecognition,pages9264–9274.IEEE,2024.\n[37] M.Ohanyan,H.Manukyan,Z.Wang,S.Navasardyan,andH.Shi. Zero-Painter: Training-Free\nLayoutControlforText-to-ImageSynthesis. InProceedingsoftheIEEE/CVFConferenceon\nComputerVisionandPatternRecognition,pages8764–8774.IEEE,2024.\n[38] openfree. flux-chatgpt-ghibli-lora. https://huggingface.co/openfree/\nflux-chatgpt-ghibli-lora,2025.\n[39] PatrickStarrrr. FLUX - Oil painting. https://civitai.com/models/1455014/\nchatgpt-4o-renderer?modelVersionId=1697982,2024.\n[40] W.PeeblesandS.Xie. ScalableDiffusionModelswithTransformers. InProceedingsofthe\nIEEE/CVFInternationalConferenceonComputerVision,pages4172–4182.IEEE,2023.\n[41] Q.Phung,S.Ge,andJ.Huang. GroundedText-to-ImageSynthesiswithAttentionRefocusing.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages7932–7942.IEEE,2024.\n[42] D.Podell,Z.English,K.Lacey,A.Blattmann,T.Dockhorn,J.Müller,J.Penna,andR.Rom-\nbach. SDXL:ImprovingLatentDiffusionModelsforHigh-ResolutionImageSynthesis. In\nProceedingsoftheInternationalConferenceonLearningRepresentations.OpenReview.net,\n2024.\n[43] QwenTeam. Qwen2.5-VL,January2025. URLhttps://qwenlm.github.io/blog/qwen2.\n5-vl/.\n[44] A.Ramesh,P.Dhariwal,A.Nichol,C.Chu,andM.Chen. HierarchicalText-ConditionalImage\nGenerationwithCLIPLatents,2022. URLhttps://arxiv.org/abs/2204.06125.\n[45] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-Resolution Image\nSynthesis with Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on\nComputerVisionandPatternRecognition,pages10674–10685.IEEE,2022.\n[46] O.Ronneberger,P.Fischer,andT.Brox. U-Net: ConvolutionalNetworksforBiomedicalImage\nSegmentation. InMedicalImageComputingandComputer-AssistedIntervention,volume9351\nofLectureNotesinComputerScience,pages234–241.Springer,2015.\n13\n\n[47] C.Saharia,W.Chan,S.Saxena,L.Li,J.Whang,E.L.Denton,S.K.S.Ghasemipour,R.G.\nLopes,B.K.Ayan,T.Salimans,J.Ho,D.J.Fleet,andM.Norouzi.PhotorealisticText-to-Image\nDiffusionModelswithDeepLanguageUnderstanding. InAdvancesinNeuralInformation\nProcessingSystems,2022.\n[48] T.ShirakawaandS.Uchida. NoiseCollage: ALayout-AwareText-to-ImageDiffusionModel\nBased on Noise Cropping and Merging. In Proceedings of the IEEE/CVF Conference on\nComputerVisionandPatternRecognition,pages8921–8930.IEEE,2024.\n[49] stability.ai. Stable Diffusion 3.5. https://stability.ai/news/\nintroducing-stable-diffusion-3-5,Nov2024.\n[50] A.Taghipour,M.Ghahremani,M.Bennamoun,A.M.Rekavandi,H.Laga,andF.Boussaid.\nBoxIttoBindIt: UnifiedLayoutControlandAttributeBindinginT2IDiffusionModels,2024.\nURLhttps://arxiv.org/abs/2402.17910.\n[51] M.Tancik,P.P.Srinivasan,B.Mildenhall,S.Fridovich-Keil,N.Raghavan,U.Singhal,R.Ra-\nmamoorthi,J.T.Barron,andR.Ng. FourierFeaturesLetNetworksLearnHighFrequency\nFunctions in Low Dimensional Domains. In Advances in Neural Information Processing\nSystems,2020.\n[52] vjleoliu. ChatGPT-4o Renderer. https://civitai.com/models/1455014/\nchatgpt-4o-renderer?modelVersionId=1697982,2025.\n[53] X.Wang,T.Darrell,S.S.Rambhatla,R.Girdhar,andI.Misra. InstanceDiffusion: Instance-\nLevelControlforImageGeneration. InProceedingsoftheIEEE/CVFConferenceonComputer\nVisionandPatternRecognition,pages6232–6242.IEEE,2024.\n[54] Y.Wu,X.Zhou,B.Ma,X.Su,K.Ma,andX.Wang. IFAdapter: InstanceFeatureControlfor\nGroundedText-to-ImageGeneration,2024. URLhttps://arxiv.org/abs/2409.08240.\n[55] J.Xie,Y.Li,Y.Huang,H.Liu,W.Zhang,Y.Zheng,andM.Z.Shou. BoxDiff: Text-to-Image\nSynthesiswithTraining-FreeBox-ConstrainedDiffusion. InProceedingsoftheIEEE/CVF\nInternationalConferenceonComputerVision,pages7418–7427.IEEE,2023.\n[56] J.Xu,X.Sun,Z.Zhang,G.Zhao,andJ.Lin.UnderstandingandImprovingLayerNormalization.\nInAdvancesinNeuralInformationProcessingSystems,pages4383–4393,2019.\n[57] H.Xue,Z.Huang,Q.Sun,L.Song,andW.Zhang. FreestyleLayout-to-ImageSynthesis. In\nProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages\n14256–14266.IEEE,2023.\n[58] B.Yang,Y.Luo,Z.Chen,G.Wang,X.Liang,andL.Lin.LAW-Diffusion:ComplexSceneGen-\nerationbyDiffusionwithLayouts. InProceedingsoftheIEEE/CVFInternationalConference\nonComputerVision,pages22612–22622.IEEE,2023.\n[59] Z. Yang, J. Wang, Z. Gan, L. Li, K. Lin, C. Wu, N. Duan, Z. Liu, C. Liu, M. Zeng, and\nL.Wang.ReCo:Region-ControlledText-to-ImageGeneration.InProceedingsoftheIEEE/CVF\nConferenceonComputerVisionandPatternRecognition,pages14246–14255.IEEE,2023.\n[60] Y.Yao, T.Yu, A.Zhang, C.Wang, J.Cui, H.Zhu, T.Cai, H.Li, W.Zhao, Z.He, Q.Chen,\nH. Zhou, Z. Zou, H. Zhang, S. Hu, Z. Zheng, J. Zhou, J. Cai, X. Han, G. Zeng, D. Li,\nZ. Liu, and M. Sun. MiniCPM-V: A GPT-4V Level MLLM on Your Phone, 2024. URL\nhttps://arxiv.org/abs/2408.01800.\n[61] H.Zhang,D.Hong,Y.Wang,J.Shao,X.Wu,Z.Wu,andY.-G.Jiang. CreatiLayout: Siamese\nMultimodal Diffusion Transformer for Creative Layout-to-Image Generation, 2025. URL\nhttps://arxiv.org/abs/2412.03859.\n[62] L.Zhang,A.Rao,andM.Agrawala. Addingconditionalcontroltotext-to-imagediffusion\nmodels. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages\n3813–3824.IEEE,2023.\n14\n\n[63] G.Zheng,X.Zhou,X.Li,Z.Qi,Y.Shan,andX.Li. LayoutDiffusion: ControllableDiffusion\nModel for Layout-to-Image Generation. In Proceedings of the IEEE/CVF Conference on\nComputerVisionandPatternRecognition,pages22490–22499.IEEE,2023.\n[64] D.Zhou,Y.Li,F.Ma,X.Zhang,andY.Yang. MIGC:Multi-InstanceGenerationControllerfor\nText-to-ImageSynthesis. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand\nPatternRecognition,pages6818–6828.IEEE,2024.\n[65] D. Zhou, Y. Li, F. Ma, Z. Yang, and Y. Yang. MIGC++: Advanced Multi-Instance Gener-\nationControllerforImageSynthesis. IEEETransactionsonPatternAnalysisandMachine\nIntelligence,47(3):1714–1728,2025.\n[66] D. Zhou, J. Xie, Z. Yang, and Y. Yang. 3DIS-FLUX: simple and efficient multi-instance\ngenerationwithDiTrendering,2025. URLhttps://arxiv.org/abs/2501.05131.\n15\n\nSupplementary Material\nA Early-StageLayoutControlinAssemble-MMDiT\nWeapplytheAssemble-MMDiTlayoutcontrolmoduleexclusivelyduringtheinitial30%ofthe\ndenoisingtrajectoryanddeactivateitfortheremaining70%. Thistwo-stagestrategyprovidesrobust\nlow-frequencystructuralguidanceintheearlystagestofacilitatelayoutalignment,whileallowing\nsubsequentunconstrainedrefinementofhigh-frequencydetailsduringlaterdenoisingphases.\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.0 0.2 0.4 0.6 0.8 1.0\nControl Ratio\nerocS\n3.0\n:detceleS\nAblation on Control Ratio\n10\n8\n6\nmIoU (LGS) 4\nColor (LGS)\nTexture (LGS)\nShape (LGS)\nVQA\n)s(\nemiT\nTime (s)\nFigure9: Impactoftheproportionofdiffusionstepsincorporatinglayoutconditioningongeneration\nquality.\nAsillustratedinFigure9,restrictinglayoutcontroltolessthan30%ofthediffusionprocessresults\nininsufficientlayoutalignmentwiththetargetboundingboxes. Incontrast,extendingcontrolbeyond\nthisoptimalthresholdleadstoadeclineinoutputquality. Furthermore,increasingtheproportionof\nlayout-guidedstepsresultsinsignificantadditionalcomputationalcost.\nB AdditionalAblationStudies\nB.1 EffectofBboxEncodingandDenseSample\nTo clarify the role of bounding box encoding and DenseSample, we further ablated the SD3-M\nbasedInstanceAssemblemodelonDenseLayout. Boundingboxembeddingsguidecorrectobject\nplacement,whileDenseSampleprovidesadditionalimprovementsinspatialaccuracyandinstance-\nlevelsemantics. TheresultsinTable6demonstratethatbothcomponentscontributetotheoverall\nperformance.\nTable6: AblationonboundingboxencodingandDenseSample.\nSetting mIoU↑ color↑ texture↑ shape↑ VQA↑\nw/obboxencoding,w/oDenseSample 51.22 32.15 34.04 33.53 93.30\nw/bboxencoding,w/oDenseSample 51.28 32.68 34.94 34.58 93.33\nw/bboxencoding,w/DenseSample 52.07 33.77 36.21 35.81 93.54\nB.2 ComparisonwithAttentionMask-basedRegionInjection\nWealsocompareourAssemble-Attndesignwithattentionmask-basedregioninjection. Whileboth\ncanbeviewedasregion-wiseattentionmechanisms,attentionmasksoperategloballyandmaycause\n16\n\nsemanticleakageinoverlappingregions. Ourmethodinsteadappliesinstance-wiseself-attention\noncroppedlatentregionsandthenfusestheupdatedfeaturesviatheAssemblestep,whichismore\neffective in dense layouts. As shown in Table 7, our design achieves superior instance attribute\nconsistencyandahigherVQAscorecomparedtotheattentionmaskbaseline.\nTable7: Comparisonbetweenattentionmask-basedinjectionandourAssemble-Attn.\nMethod spatial↑ color↑ texture↑ shape↑ VQA↑\nSD3-Medium(basemodel) 77.49 60.28 62.55 60.38 93.30\nAttentionmask(SD3-M) 94.11 74.28 77.58 76.54 91.53\nInstanceAssemble(ours,SD3-M) 94.97 77.53 80.72 80.11 93.12\nC Underlyingdataforradar-chartvisualizations\nInSections4.3,weutilizedaradarcharttodepicteachquantitativevariablealongequi-angularaxes,\nprovidinganintuitivecomparison. Thisvisualizationhighlightsthemultifacetedsuperiorityofour\nmethod. Here, wepresentthecorrespondingrawevaluationresultsintabularform. Specifically,\nTab.8correspondstoFig.6,thusensuringaclearmappingbetweeneachradar-chartsubfigureand\nitsunderlyingdata.\nTable8: QuantitativeresultsofadditionalvisualcontentonDenseLayout.\nLayoutGroundingScore GlobalQuality\nDenseLayout\nmIoU↑ color↑ texture↑shape↑ VQA↑ Pick↑ CLIP↑\nRealImages(UpperBound) 92.35 76.52 80.78 79.78\ntext 43.72 26.57 28.56 28.39 93.37 21.63 12.45\ntext+image 55.29 42.15 44.50 44.24 91.66 22.05 12.95\ntext+depth 49.64 28.25 31.82 31.62 92.83 21.28 13.25\ntext+edge 50.73 29.45 33.92 33.84 90.13 21.26 13.55\nD MoreDetailsonDenseLayoutEvaluationDataset\nD.1 ConstructionPipelineofDenseLayoutDataset\nThe DenseLayout dataset is constructed through a multi-stage pipeline designed to extract high-\ndensity and semantically-rich layout information from synthetic images. The pipeline includes\nfollowingsteps:\n1. ImageGenerationusingFlux.1-Dev[4]\nA diverse set of synthetic images is generated using Flux.1-Dev, a text-to-image model.\nTheinputpromptsaregenerictextualdescriptions,sampledfromtheLayoutSAMdataset,\nwhichisbasedonSA-1B.Theimagesareresizedtomaintainthesameaspectratioasthe\noriginalSA-1Bimages,withthelongeredgesetto1024pixels. Thisstepprovidesavisually\ncomplexbaseforextractinglayoutstructures.\n2. Multi-labelTaggingusingRAM++[24]\nThegeneratedimagesaretaggedusingRAM++,thenext-generationmodelofRAM,which\nsupportsopen-setrecognition. Thesetagsofferhigh-levelsemanticguidanceforsubsequent\ngrounding.\n3. ObjectDetectionviaGroundingDINO[35]\nUsingtheimageanditspredictedtagsasinput,GroundingDINOperformsopen-setobject\ndetection.Itoutputsboundingboxesandclasslabelsforalldetectedentities.Thedetectionis\nconfiguredwithabox_thresholdof0.35andatext_thresholdof0.25. Eachdetected\nboundingboxistreatedasaninstanceboundingbox, andthecorrespondingpredicted\nlabelisrecordedastheinstancedescription.\n4. DetailedCaptioningwithQwen2.5-VL[43]\nEachboundingboxregioniscroppedfromtheoriginalimageandfedintoQwen2.5-VL\n17\n\ntogenerateafine-grainedcaption. Theseregion-levelcaptionsarestoredasthedetailed\ndescriptionforeachinstance,enrichingthesemanticinformationbeyondcategorylabels.\n5. DensityFiltering\nToensurehighlayoutcomplexity,onlyimageswith15ormoredetectedinstances(asoutput\nbyGroundingDINO)areretained. Thisresultsinadenselayoutdistributionsuitablefor\nlayout-conditionedgenerationtasks. ThedistributionofinstancecountisshowninFig.10.\nFinally,theDenseLayoutdatasetcontains5,000imagesand90,339instances,withanaverageof\n18.1instancesperimage.\n1200\n1000\n800\n600\n400\n200\n0 15 20 25 30 35 40 45 50\nNumber of Instances per Image\ntnuoC\negamI\nDenseLayout: Instance Count Distribution\nMean: 18.1\nMedian: 17.0 Std: 3.7\nFigure10: InstancecountdistributionperimageinDenseLayout.\nAnnotationFormat. Theannotationforeachimageconsistsof:\n• global_caption: theoriginalpromptusedforimagegeneration.\n• image_info: metadataoftheimage,includingheightandwidth.\n• instance_info: alistofinstances,eachwith:\n– bbox: theboundingboxoftheinstance,formattingas[x ,y ,x ,y ].\n1 1 2 2\n– description: thecategorylabelpredictedbyGroundingDINO.\n– detail_description: a fine-grained caption generated by Qwen2.5-VL for the\ncroppedregion.\nD.2 SamplesofDenseLayoutDataset\n\"instance_info\": [\n{\n\"bbox\": [129,489,283,642],\n\"description\": \"nightstand\",\n\"detail_description\": \"The nightstand is dark brown,\ncompact, with a drawer.\"\n},\n{\n\"bbox\": [306,170,430,339],\n\"description\": \"picture frame\",\n\"detail_description\": \"Brown wooden frame containing a\nwatercolor painting of green leaves on a white\nbackground.\"\n},\n{\n\"bbox\": [603,170,727,340],\n\"description\": \"picture frame\",\n\"detail_description\": \"A simple brown wooden frame holds\na botanical print with detailed leaves and stems.\"\n},\n...\n]\nFigure11: AsampleofDenseLayoutanditsannotation.\n18\n\nThis is a photo showcasing a traditional Chinese-style pavilion and This is a photo depicting a traditional Asian floating market scene. In This is a photo showcasing a Chinese-style building and a statue. The\nboats on the river. The pavilion is located on the right side of the the picture, two women are sitting in their respective boats, each building is brightly colored, with a red roof and a yellow door, and the\npicture, with a beautifully decorated roof and a golden plaque hanging wearing a traditional conical hat, and dressed in brightly colored statue is located in front of the building, standing on a pedestal. The\nin the middle. In front of the pavilion is a row of wooden boats, each traditional clothing. The woman on the left is wearing a blue top and a surrounding environment is a spacious square, with several pedestrians\nwith a red flag hanging on it, and people are gathered on the boat dock. dark skirt, while the woman on the right is wearing a yellow top and a walking around the building. The background is a clear blue sky and lush\nThe river is calm, and the sky is clear, with a few clouds leisurely light-colored skirt. Their boats are filled with a variety of goods, trees.\ndrifting by. including fresh flowers, fruits, and other food items. The boats are\nadorned with bright colored cloths, adding a festive atmosphere to the\nscene. The entire scene is captured under natural light, presenting a\ntranquil and vibrant market atmosphere.\nThis is a photo showcasing a modern interior design style, with the This is a realistic-style photograph depicting a city street scene after This is a photograph showcasing a famous archway in a city. The archway\nfocus on a spacious and bright room. The room is furnished with wooden a flood. In the photo, vehicles and pedestrians are struggling to is a reddish-brown structure, adorned with golden decorations and\nfurniture, including a long wooden table and a matching wooden bench. On navigate through the flooded streets. A yellow tricycle is parked in the sculptures, and its design is very intricate. The archway is located on\nthe table, there are some books and a laptop, while on the bench, there middle of the street, surrounded by vehicles covered with blue a wide street, with pedestrians and cyclists weaving through the road.\nis a black desk lamp. The room's lighting comes from several minimalist waterproof cloth. Pedestrians are using umbrellas to shield themselves The sky is clear, with a few clouds scattered in the blue sky. The\nchandeliers and a large window, through which you can see the green from the rain, some are pushing bicycles, while others are walking. The buildings in the background have a Mediterranean style, with the domes\nplants outside. The floor is covered with a red carpet, and there are buildings on both sides of the street are submerged in water, and the of some buildings visible in the distance.\nseveral decorations on the floor, including a red vase and a decorative wires and poles above the street are also submerged, adding to the\nsculpture. The entire scene is illuminated by natural light, creating a severity of the flooding. The entire scene is shrouded in a gloomy\nwarm and comfortable atmosphere. atmosphere, with no sunlight piercing through the clouds.\nT f t b r a a ph oo h a r ri or e r f e es fe n e s sg g w c ei .r r a o ns o a n c v t Iu s d a e ia nn s r r n d l a s e gp f, a d h r n s p ao oi d m a w t nt , a r i to ts l k t r t l e h as os h e d nh fu e r d qo r r i e uw tf e h n n ic ha o s la ec a u f e s e r s r ai b e e o t nn up , n r dg ir s t e le e b e pa ds v o o s e ie e t f , ar nn r h cu gt a t a er si l w h n fa ,n i e d ul g w t l t o h f t l ha o e h ra e d t n e un rv e r c rd ei n a e s as v d . k lc ii b i y a sd u t I ap i i n i te ag l o s m, r d n t o re i a h f sw oe n l e i pi wn g l ht s w b l eh oa , o a e r fn o c d ea d i d k . w n e g wg oy c n r ir oe l o ta dl u s u hs el d t n s no i r d wl w n u , ha f g c in ec t t td no a u h e cl r e i eo l e cn sr a s h l ,. r i ot g a l uh wO e n l de in r d s s t ,h T m d e d a p sx th a e e ap ai g p c f tr is n i o i te n i c r g es ei f t a u rs ds i s t r ni c i e so ga e a v n l n e i a. ap t f n n sh i e dT so s g l a h t c u e fe wo u r m h l i l e e e os ns p n l rc dh t i t m au oo u n s e ll ww r , t p sc e a . dt ,a i eu s l l n T sr ti o o c h ie hn c n l e g rg a g u ni o t d a ss ut e r i l , gh d o n t s he b g a au i e r nr wi n , a dr hn i o it t h s s tu ce h o t hn hr e l a d ed i d t e e to c i u c wd hr e n e o a e n g r lb o t o a ly lf e a f t s i r e e ga c a d ax h o r rq tc f o f w eu h s i i i fu t s g t as rr h , u h di oc e r ot mh w e c re , a i o n o l t h m es uw t h o p dt ti a l l a st r a d e wt ih . i x iu d s n te et T o g g hs h h l e se e e a o ca h m m on if s n c e ld no c r t o ec u o r r su l s i f s p s c u i t l no u a ,n r n e da T w g s s i a be eh i l u n v ni t a r c fe cs h s r l er h s o u wa ei a u d l ss a n i p , m n d n es a o d e g de e d d et np e m r ss jh r e b e t oo n t y d ro yt a i- f io l s b as n e r nt s gs s v i sy t h t e c l a to r r k oe i hw u a n r ec c l b b s a t u tu ss u b i hi a ui r u l el n nn e i d d d sg s l i si h . d n qn b ia i g ug e n T n s a n em h g ri c .o e s a en h d n , e Te s o d t s hr q f sh en u m oe f a d o m o su r i d ec r kr e f e e yb f r an p a i e n rt e in s r ee o s e g r p s p n l w, l cq a t a a e lu v s li ea e a s kt t ar d r is o re c c n ,. w h u ge r i i r ,x e wT t t t t s ih h e a se t te c i or . h g t n mi s r u eo T aq e r w r h u y a a a e fa l l rc er s l eo s we l s s m q a t . sp u wi b y io a hs s l T ts r i , e h te e ts s e id ep w , r n i a i e go sc t f i h a o o r nu e s ,\nadding a warm and sacred atmosphere to the interior. clouds scattered in the blue sky.\nFigure12: RepresentativesamplesfromDenseLayoutdatasetdemonstrating: (a)High-densityscene\nwith≥15instances,(b)Complexinstancerelationshipswithpreciseattributespecifications.\n19\n\nE Moreresultswithtextual-onlycontent\nE.1 InstanceAssemblebasedonSD3-Medium\nx\nFigure13: MoreresultsofInstanceAssemblebasedonSD3-Mediumwithtextual-onlycontent.\nE.2 InstanceAssemblebasedonFlux.1-Dev\nFigure14: MoreresultsofInstanceAssemblebasedonFlux.1-Devwithtextual-onlycontent.\nE.3 InstanceAssemblebasedonFlux.1-Schnell\nFigure15: MoreresultsofInstanceAssemblebasedonFlux.1-Schnellwithtextual-onlycontent.\n20\n\nF Moreresultswithadditionalvisualcontent\nF.1 AdditionalImage\nFigure16: Moreresultswithadditionalimage.\nF.2 AdditionalDepth\nFigure17: Moreresultswithadditionaldepth.\nF.3 AdditionalEdge\nFigure18: Moreresultswithadditionaledge.\n21\n\n",
    "total_pages": 21,
    "pages": [
      {
        "page": 1,
        "content": "InstanceAssemble: Layout-Aware Image Generation\nvia Instance Assembling Attention\nQiangXiang1,2,ShuangSun2,BingleiLi1,3,\nDejiaSong2,HuaxiaLi2,YiboChen2, XuTang2,YaoHu2, JunpingZhang1∗\n1ShanghaiKeyLaboratoryofIntelligentInformationProcessing,\nCollegeofComputerScienceandArtificialIntelligence,FudanUniversity\n2XiaohongshuInc. 3ShanghaiInnovationInstitute\n{qxiang24, blli24}@m.fudan.edu.cn, jpzhang@fudan.edu.cn,\n{sunshuang1, dejiasong, lihuaxia, zhaohaibo, tangshen, xiahou}@xiaohongshu.com\nFigure1:Layout-awareimagegenerationresultbyInstanceAssemble.Weshowimagegeneration\nresultunderpreciselayoutcontrol,rangingfromsimpletointricate,sparsetodenselayouts.\nAbstract\nDiffusionmodelshavedemonstratedremarkablecapabilitiesingeneratinghigh-\nqualityimages. RecentadvancementsinLayout-to-Image(L2I)generationhave\nleveragedpositionalconditionsandtextualdescriptionstofacilitatepreciseand\ncontrollableimagesynthesis. Despiteoverallprogress,currentL2Imethodsstill\nexhibitsuboptimalperformance. Therefore,weproposeInstanceAssemble,anovel\narchitecturethatincorporateslayoutconditionsviainstance-assemblingattention,\nenabling position control with bounding boxes (bbox) and multimodal content\ncontrolincludingtextsandadditionalvisualcontent. Ourmethodachievesflexible\nadaptiontoexistingDiT-basedT2Imodelsthroughlight-weightedLoRAmodules.\nAdditionally,weproposeaLayout-to-Imagebenchmark,Denselayout,acompre-\nhensivebenchmarkforlayout-to-imagegeneration,containing5kimageswith90k\ninstancesintotal. WefurtherintroduceLayoutGroundingScore(LGS),aninter-\npretableevaluationmetrictomorepreciselyassesstheaccuracyofL2Igeneration.\nExperimentsdemonstratethatourInstanceAssemblemethodachievesstate-of-the-\nartperformanceundercomplexlayoutconditions,whileexhibitingstrongcompati-\nbilitywithdiversestyleLoRAmodules. Thecodeandpretrainedmodelsarepub-\nliclyavailableathttps://github.com/FireRedTeam/InstanceAssemble.\n∗Correspondingauthor.\n39thConferenceonNeuralInformationProcessingSystems(NeurIPS2025).\n5202\ntcO\n82\n]VC.sc[\n2v19661.9052:viXra"
      },
      {
        "page": 2,
        "content": "1 Introduction\nDiffusionmodels[22]haverevolutionizedimagegenerationtask,witharchitectureslikeDiffusion\nTransformer(DiT)[40]offeringsuperiorqualityovertraditionalUNet-basedframeworks. Recent\nimplementationssuchasStableDiffusion3/3.5[15,49]andFlux.1[4]furtherenhancetext-to-image\nalignment, paving the way for advancements in layout-controlled generation. Layout-to-Image\n(L2I)generationisataskthatfocusesoncreatingimagesunderlayoutconditions,allowingusers\ntodefinespatialpositionsandsemanticcontentofeachinstanceexplicitly. Thistaskfacesseveral\nsignificantchallenges:(i)ensuringpreciselayoutalignmentwhilemaintaininghighimagequality,(ii)\npreservingobjectpositionsandsemanticattributesaccuratelyduringtheiterativedenoisingprocess\nofdiffusionmodels,and(iii)supportingvarioustypesofreferenceconditions,suchastexts,images\nandstructureinformation. Thesechallengeshighlightthecomplexityofachievingrobustandflexible\nlayout-controlledimagegeneration.\nExistingL2Imethodscanbebroadlycategorizedintotraining-freeandtraining-basedapproaches,\nbothpossessingdistinctadvantagesandlimitations. Training-freemethods[55,7,13,8,5,27]rely\nonheuristictechniqueswithoutmodifyingthebasemodel. However,thesemethodsoftenexhibit\ndegradedperformanceincomplexlayouts,demonstratehighsensitivitytohyperparametertuning,\nandsufferfromslowinferencespeed,whichmakethemlesspracticalforreal-worldapplications.\nIncontrast,training-basedmethods[63,53,64,29,61]involvetrainingspecificlayoutmodulesto\nimprovelayoutalignment,whichintroducesasignificantamountofextraparametersandincreases\ntrainingcomplexityandresourcerequirements. Additionally, existingL2Ievaluationmetricsex-\nhibitinaccuracies,suchasfalseacceptanceandlocalizationerrors. Theseidentifiedshortcomings\nnecessitatealgorithminnovationforeffectiveandefficientlayout-controlledimagegeneration.\nTherefore, we propose InstanceAssemble, a novel framework that systematically tackles these\nissuesthroughinnovativedesignandefficientimplementation. Ourapproachintroducesacascaded\nInstanceAssemblestructure,whichemploysamultimodalinteractionparadigmtoprocessglobal\nprompts and instance-wise layout conditions sequentially. By leveraging the Assemble-MMDiT\narchitecture,weapplyanindependentattentionmechanismtothesemanticcontentofeachinstance,\nthusenablingeffectivehandlingofdenseandcomplexlayouts. Furthermore,weadoptLoRA[23]\nforlightweightadaptation,addingonly71MparameterstoSD3-Medium(2B)and102MtoFlux.1\n(11.8B).Ourmethodenablespositioncontrolwithboundingboxesandmultimodalcontentcontrol\nincludingtextsandadditionalvisualcontent. Thislightweightdesignpreservesthecapabilitiesof\nthebasemodelwhileenhancingflexibilityandefficiency. Wealsointroduceanovelmetriccalled\nLayoutGroundingScore(LGS)toensureaccurateevaluationforL2Igeneration,alongsideatest\ndatasetDenseLayout. Thismetricprovidesaconsistentbenchmarkforassessinglayoutalignment.\nOurmethodachievesstate-of-the-artperformanceacrossbenchmarksanddemonstratesrobustlayout\nalignmentunderawidevarietyofscenarios,rangingfromsimpletointricate,sparsetodenselayouts.\nNotably,despitebeingtrainedonsparselayouts(≤ 10instances),ourapproachmaintainsrobust\ngeneralization capability on dense layouts (≥ 10 instances), confirming the effectiveness of our\nproposedInstanceAssemble. Themaincontributionsarelistedbelow.\n1. WeproposeacascadedInstanceAssemblestructurethatprocessesglobaltextpromptsand\nlayoutconditionssequentially,enablingrobusthandlingofcomplexlayoutsthroughanindependent\nattentionmechanism.\n2. ByleveragingLoRA[23],weachieveefficientadaptationwithminimalextraparameters\n(3.46%onSD3-Mediumand0.84%onFlux.1),supportingpositioncontrolwithmultimodalcontent\ncontrolwhilepreservingcapabilitiesofbasemodel.\n3. WeproposeanewtestdatasetDenseLayoutandanovelmetricLayoutGroundingScore\n(LGS)forLayout-to-Imageevaluation. Experimentalresultsdemonstratethatourapproachachieves\nstate-of-the-artperformanceandrobustcapabilityundercomplexanddenselayoutconditions.\n2 RelatedWork\nText-to-ImageGenerationText-to-imagesynthesis[47,45,42,40,30,17,6,1,44,3]haswitnessed\nrapidprogresswiththedevelopmentofdiffusionmodels.Initialworks[44,3,47,45]utilizeUNet[46]\nas the denoising architecture, leveraging cross-attention mechanisms to inject text-conditioning\n2"
      },
      {
        "page": 3,
        "content": "Global Prompt 1. Global Prompt – Image 2. Instances - Image\nGlobal Prompt Global Prompt\n“\na\no\nVa\nf\ni\nsh\naq\nsi\nu\nus kat\nnr\nao\nie\nlr g.i\nh\nc\ntT\nIa ,hl\ne\nn\nr\nsNm\nim\ntoo don\nin\naiu\nnu\nnsm gme\ne\ncen\non\net\nnt\nl\nai\nCo\ns\noc\nh\na\noa\nnt\nr\nte ssd\net\ne\n.a\nni ”tn\nu\nt\ne\nredocnEredocnE\nEAVtxeT\nL Enay co odut\ner\nxN\nImage MMDiT\n1In\n2I sm taag\nn\n3e\nces\n4\nAs Ms Mem Dib Tle- 1In\n2I sm taag\nn\n3e\nces\n4\nredoceD\nEAV\nTextual Instance Content\nAssemble-MMDiT\n“A tall, pointed white structure\nw s “ s d “ hi p e B itAt h t r sa h e a o ttma r i n ouh i l z rejo c s e i e Sr a cosi l a e aft pz n q l i aco f d u an i e f, tt n a s i h ia i t gw ol a r r ue r\na\nl i i ra s ltb d a ee a a e n sh n t r e w Ld s dr i as t a t ee t h t a pd h\ny\na e t i on t u cb od p o e tr r\nu\ne p , eo n ta a . dn a k ” .z te. ”e” redocnE txeT FM oL uP\nrier\negamI mroNreyaLadA latent 1 tacno&C .joV rKQ P\n.\nnttA\n1 elbmessA FF&mroNreyaL\nPointe hd\no\nrr so eof DenseSample secnatsnI 1234 mroNreyaLadA 1234 2 tacno&C .jV oK rQ P\n.\n2\nAssemble-Attn\n1234 FF&mroNreyaL 1234\nHistorical monument Legend Concatenation Element-wise Addition Frozen Trainable(LoRA)\nFigure2: TheproposedInstanceAssemblepipeline. Variouslayoutconditionsareprocessedbythe\nLayoutEncodertoobtaininstancetokens,whichguidetheimagegenerationviaAssemble-MMDiT.\nInAssemble-MMDiT,theinstancetokensinteractwithimagetokensthroughtheAssembling-Attn.\nsignals. Recently, researches [6, 15, 49, 4, 34] have used the Multimodal Diffusion Transformer\n(MMDiT)architecture,markingasignificantimprovement.\nLayout-to-ImageGenerationLayout-to-Imagegenerationenablesimagegenerationunderlayout\nconditions,whichisdefinedasspatialpositionswithtextualdescriptions. Existingapproachescanbe\nbroadlycategorizedintotraining-freeandtraining-basedparadigms.\nTraining-freemethodsleveragepretrainedtext-to-imagediffusionmodelswithoutadditionaltrain-\ning. Acommonstrategyinvolvesgradient-basedguidanceduringdenoisingtoalignwithlayout\nconditions[55,12,41,13,18,50]. Also,therearemethodsthatdirectlymanipulatelatentsthrough\nwell-defined replacing or merging operations [8, 48, 2] or enforce layout alignment via spatially\nconstrainedattentionmasks[5,20]. GrounDiT[27]exploitssemanticsharinginDiT:acroppednoisy\npatchandthefullimagebecomesemanticcloneswhendenoisedtogether,enablinglayout-to-image\ngeneration by jointly denoising instance regions with their corresponding image context. Other\napproachesgenerateeachinstanceseparatelyandemployinpaintingtechniquestocomposethefinal\nimage[37]. However,thesemethodsdemonstratedecentperformanceprimarilyonsimpleandsparse\nlayouts,whiletheiraccuracydecreasesinmorecomplexlayouts. Somemethodsrequirehyperparam-\netertuningspecifictodifferentlayoutconditions,reducingtheiradaptability. Furthermore,additional\ngradient computations or latent manipulations result in slow inference speed, thus limiting their\napplicabilityinreal-worldscenarios.\nTraining-basedmethodsexplicitlyincorporatelayoutconditioningthrougharchitecturalmodifica-\ntions. Mostapproachesinjectspatialconstraintsviacross-attention[63,57,36,25,59,16,54]or\nself-attention[10,53,28].Someworksproposededicatedlayoutencodingmodules[9,64,65,58,62]\noradoptatwo-stagepipelinethatgeneratesimagesafterpredictingadepthmapwithlayoutcondi-\ntions[14,66]. Otherworksleverageautoregressiveimagegenerationmodels[19]. Thesemethods\nsufferhighcomputationalcostsduetoexcessiveparameters.\n3 Method\nPreliminariesRecentstate-of-the-arttext-to-imagemodelssuchasSD3[15]andFlux[4]adoptthe\nMultimodalDiffusionTransformer(MMDiT)asthebackboneforgeneration. Unliketraditional\nUNet-basedcross-attentionapproaches,MMDiTstreatimageandtextmodalitiesinasymmetric\nmanner, which leads to stronger prompt alignment and controllability. These models are trained\nunder the flow matching framework [33], which formulates generation as learning a continuous\nvelocityfieldthattransportsnoisetodata. GivenacleanlatentxandGaussiannoiseϵ∼N(0,I),an\n3"
      },
      {
        "page": 4,
        "content": "interpolatedlatentisdefinedas\nz =(1−t)x+tϵ, t∈[0,1]. (1)\nt\nThe training objective minimizes the squared error between the predicted velocity and the target\nvelocity(ϵ−x):\nh i\nL =E ∥v (z ,t,y)−(ϵ−x)∥2 , (2)\nFM ϵ∼N(0,I),x,t θ t 2\nwherev isimplementedwithanMMDiTbackbone.\nθ\nProblem Definition Layout-to-Image generation aims to synthesize images with precise control\nthroughaglobalpromptpandinstance-wiselayoutconditionsL. ThelayoutconditionscompriseN\ninstances{l }N ,whereeachinstancel isdefinedbyitsspatialpositionb andcontentc :\ni i=1 i i i\nL={l ,...,l }, wherel =(c ,b ). (3)\n1 N i i i\nInourframework,spatialpositionsarerepresentedasboundingboxes,whileinstancecontentcanbe\nspecifiedthroughmultiplemodalities: textualinstancecontentandadditionalvisualinstancecontent,\nincludingreferenceimages,depthmapsandedgemaps.\nWeproposeInstanceAssemble,aframeworkwithaLayoutEncodertoencodethelayoutconditions\nandAssemble-MMDiTtoeffectivelyintegratetheencodedlayoutconditionswithimagefeatures.\n3.1 LayoutEncoder\nWeuseaLayoutEncoder(Fig.2left-bottompanel)toencodeeachinstancel ,andthetokensare\ni\ndenotedashL =[hl1,...,hlN]whichrepresentsthelayoutinformationofeachinstance. Giventhe\nspatialpositionoftheinstance(boundingbox),wefirstenhancethespatialrepresentationthrough\nDenseSample. Givenaboundingboxb =(x ,y ,w,h)∈[0,1]4withtop-leftcoordinates(x ,y )\ni 1 1 1 1\nandsize(w,h),wegenerateK2uniformlyspacedpoints:\n(cid:26)(cid:18) (cid:19)(cid:12) (cid:27)\nw h (cid:12)\nP i = x 1+k x· K,y 1+k y· K (cid:12) (cid:12)k x,k y ∈{0,...,K−1} (4)\nThen,followingGLIGEN[28],wecomputethetextualinstancetokensas:\nhi =MLP([τ(c ),Fourier(P )]), (5)\nl i i\nwhere τ represents the text encoder, Fourier(·) denotes Fourier embedding [51], [·, ·] denotes\nconcatenationalongthefeaturedimension,andMLPisamulti-layerperception.\nAdditionally,wecanuseadditionalvisualinstancecontenttobetterimproveperformance. Given\nthevisualinstancecontent,wefirstextractfeaturesusingtheVAEencoderofthebasemodel,then\nprojectthemtotheunifiedinstancetokenspacethroughaMLP:\nhi =MLP(VAE(c )). (6)\nl i\n3.2 Assemble-MMDiT\nWeobservethatapplyingattentionbetweenallimagetokensandinstancetokensresultsinsuboptimal\nperformanceundercomplexlayoutconditions(e.g.,overlapping,tinyobjects). Toaddressthis,we\nintroduce Assemble-MMDiT (Fig. 2, right-bottom panel), which enhances the location of each\ninstancewhilemaintainingcompositionalcoherencewithotherinstances. Ourmethodprocesses\neachinstanceindependentlythroughattentionmoduleswithitsassociatedimagetokens,followedby\nweightedfeatureassembling.\nFormally,givenimagetokensh∈RC×W×H (whereC denotesthelatentchannelsizeand[W,H]\nthelatentsize)andinstancetokenshl ∈ RC×N,weapplyAdaLayerNorm[56],followedbyour\nproposedAssembling-Attn,asshowninFig.2(right-bottompanel). Wecroptheimagetokenshz by\nthebboxb ofeachinstanceandgethz =hz[b ]∈RC×w×h. Then,weprojectthecroppedimage\ni li i\ntokenshz\nli\nandtheircorrespondinginstancetokensl iintoqueries(Qzli,Qli),keys(Kzli,Kli),and\nvalues(Vzli,Vli),andthenapplyattention:\nhz li′ ,hl i′ =Attention(cid:0) [Qzli,Qli],[Kzli,Kli],[Vzli,Vli](cid:1) . (7)\n4"
      },
      {
        "page": 5,
        "content": "where[·,·]denotesconcatenationalongthetokendimension. Theupdatedtokensareassembled\nacross instances. Let M ∈ NW×H represent the instance density map, calculating the counts of\ninstances.\nTheassembledimagetokenshz′ andinstancetokenshl′\narecomputedas:\nN\nhz′ :hz′\n[:,i,j]=\n1 X hz′\n[:,i,j], wherei∈[0,W −1],j ∈[0,H −1]\nM[i,j] lk (8)\nk=1\nhl′ :hl′ [:,k] =hl k′ .\nAs illustrated in Fig. 3, the top row demonstrates that\nourassemblingmechanismensuresinstancetokensattend\no l\ne\nc\nee\nxf\non f\nf\nprl t ey\nr\nlci\ne\nint\nct\nco\ni iv\ntb tr\ne\nsl le a\nal\npl yc ye akv\notg\ni.a uaun T tlit\nd\nch pi\ne\noem\nos\nnsma\ntg\ni\nrg\ntl\noi ie od\no\nlbd nr (e al bseg\nl\n.\noi pr tIo to nrn ow os mcm, orw re\np\nn\nov th\nt\nwree\nt\naoar )se l\nk\nts r,eu et\nn\ng\nsn h\nus\ner ae lntt tl\no\nsea th rt\nif\nae ne\no\ntd\nc\ni\nlm\no\nor ue\nn\nce sg ac woi lho\nin\nza in\nt\nan\nt\nhs\nh\nti\no\nia s\ne\noumr\ni\nne\nr\nt\ntuoyal\n/w\ntpmos re pc n la at bs on lI\nG\ne inrr co or ns s( is\" tB er ni cti is eh sS (\"h do or gth \"a mir\" isi sn inw g)r .onglocation)orsemantic tuoyal\no/w\nBritish ShorthairAmerican robin Maltipoo dog water\nFurthermore,topreservethegenerationcapabilityofthe Figure3: (Top)instance-imageattention\noriginalmodelandmitigateconditionalconflictsbetween mapw/layout. (Middle)globalprompt-\nglobal prompt and layout conditions, we employ a cas- imageattentionmapw/layout.(Bottom)\ncadedmechanismasshowninFig.2(right-abovepanel). globalprompt-imageattentionmapw/o\nIn our design, the global text prompt and image latents layout.\narepassedthroughoriginalMMDiTfirst,thentheimage\ntokens along with instance tokens are processed by our\nAssemble-MMDiTmodule. Thefirststepcapturesglobalcontextandensuresgenerationquality,\nwhilethesecondstepensuresinstancelayoutalignment. Besides,wetrainAssemble-MMDiTwith\nLoRA,significantlyreducingboththetrainingcostandinferencecosts.\n‘cRoanldld iye\nt\nt iCe oac nrt\n:’\nSAMbox co‘nTdoiwteiro’n: detect\ndetect co‘nWdinitdioown’:\n‘Blaccko nsdhiotlidoenr: bag’ detect\n(a) Rally Car? (b) Black shoulder bag? (c) Window? (d) Tower?\nCropVQA: Yes –> 1 CropVQA: No –> 0 SAMIoU: 0.77 BinaryIoU: 1\nDetectIoU: 0.60 DetectIoU: 0.54 DetectIoU: 0.00 DetectIoU: 0.81\nFigure4: Failurecasesofothermetrics. (a)falseacceptanceinCropVQA,(b)falserejectionin\nCropVQA,(c)localizationerrorinSAMIoU,and(d)discontinuousinBinaryIoU.\n3.3 Benchmark: DenseLayoutandLayoutGroundingScore\nTheLayout-to-Imagetaskaimstogenerateimagesthatalignpreciselywithprovidedlayouts,eval-\nuatingbothspatialaccuracyandsemanticconsistency(e.g.,color,texture,andshape,ifprovided).\nTheexistingmetrics(AP/AR)forobjectdetection[10,28,55,53]aresuboptimal. Theyassumea\nfixedcategorysetandrelyoninappropriateprecision/recallforbinarylayoutoutcomes. VLM-based\ncroppedVQAmethods[61]sufferfalseacceptance(Fig.4(a))andfalserejection(Fig.4(b)). While\nspatial-onlymetricslikeSAMIoU[11]ignoreappearanceconsistency(Fig.4(c)),GroundingDINO-\nbased[35]binaryIoUthresholds[64,54]failtocapturecontinuouslayoutprecision(Fig.4(d)). Thus,\nweproposeLayoutGroundingScore(LGS),whichintegratesbothspatialaccuracyandsemantic\naccuracy:\n1. SpatialAccuracy(DetectIoU):wedetectallinstancesviaanoff-the-shelfdetector[35],\ncomputetheIoUagainstconditionbbox, andreporttheglobalmeanIoUacrossallinstancesfor\nequalweighting.\n2. SemanticAccuracy: forinstanceswithIoU>0.5,wecropthepredictedregionandassessthe\nsemanticaccuracybyitsattributeconsistency(color,texture,shape)viaVLM-basedVQA[60].\nLGSsupportsopen-setevaluation,usesDetectIoUtoevaluatespatialaccuracyanddecouplesthe\nspatial and semantic check to avoid CropVQA [61] failures (shown in Fig. 4). Furthermore, we\n5"
      },
      {
        "page": 6,
        "content": "Table1: QuantitativecomparisonbetweenourSD3-basedInstanceAssembleandotherL2I\nmethodsonLayoutSAM-Eval. ⋆ TheCropVQAscoreisproposedinCreatilayout[61]andthe\nscoreofInstanceDiff,MIGCandCreatiLayoutisborrowedfromCreatiLayout[61].\nCropVQA⋆ LayoutGroundingScore GlobalQuality\nLayoutSAM-Eval\nspatial↑color↑ texture↑shape↑ mIoU↑ color↑ texture↑shape↑ VQA↑ Pick↑ CLIP↑\nRealImages(UpperBound) 98.95 98.45 98.90 98.80 88.85 88.07 88.71 88.62\nInstanceDiff(SD1.5) 87.99 69.16 72.78 71.08 78.16 63.14 66.82 65.86 86.42 21.16 11.73\nMIGC(SD1.4) 85.66 66.97 71.24 69.06 62.87 50.70 52.99 51.77 88.97 20.69 12.56\nHICO(realisticVisionV51) 90.92 69.82 73.25 71.69 70.68 53.16 55.71 54.61 86.53 21.77 9.47\nCreatiLayout(SD3-M) 92.67 74.45 77.21 75.93 45.82 38.44 39.68 39.24 92.74 21.71 13.82\nInstanceAssemble(ours)(SD3-M) 94.97 77.53 80.72 80.11 78.88 63.89 66.27 65.86 93.12 21.79 12.76\nTable2: QuantitativecomparisonbetweenourInstanceAssembleandotherL2Imethodson\nDenseLayout.\nLayoutGroundingScore GlobalQuality\nDenseLayout\nmIoU↑ color↑ texture↑shape↑ VQA↑ Pick↑ CLIP↑\nRealImages(UpperBound) 92.35 76.52 80.78 79.78\nInstanceDiff(SD1.5) 47.31 29.48 33.36 32.43 88.79 20.87 11.73\nMIGC(SD1.4) 34.39 22.10 23.99 23.45 91.18 20.74 12.81\nHICO(realisticVisionV51) 22.42 10.52 11.69 11.46 74.42 20.51 8.16\nCreatiLayout(SD3-Medium) 15.54 11.69 12.34 12.17 93.42 21.88 12.89\nInstanceAssemble(ours)(SD3-Medium) 52.07 33.77 36.21 35.81 93.54 21.68 12.58\nRegional-Flux(Flux.1-Dev) 14.06 11.34 11.91 11.84 92.94 22.67 10.66\nRAG(Flux.1-Dev) 17.23 14.22 14.62 14.55 92.16 22.28 11.01\nInstanceAssemble(ours)(Flux.1-Dev) 43.42 27.60 29.50 29.14 93.36 21.98 11.38\nInstanceAssemble(ours)(Flux.1-Schnell) 45.33 27.73 30.06 29.62 93.52 21.72 10.78\nintroduceDenseLayout,adenseevaluationdatasetforL2I,whichconsistsof5kimageswith90k\ninstances(18.1perimage). TheimagesinDenseLayoutaregeneratedbyFlux.1-Dev, taggedby\nRAM++[24],detectedbyGroundingDINO[35],recaptionedbyQwen2.5-VL[43],andfilteredto\nretainthosewith≥15instances,thusprovidingdenselayoutconditions.\n3.4 TrainingandInference\nDuringtraining,wefreezetheparametersofthebasemodelandonlyupdatetheproposedLayout\nEncoder and Assemble-MMDiT module. We denote the adding parameters by θ′. The training\nobjectiveisgivenby\nL=E ϵ∼N(0,I),x,t,p,Lh(cid:13) (cid:13)v {θ,θ′}(cid:0) z t,t,p,L(cid:1) −(ϵ−x)(cid:13) (cid:13)2 2i , (9)\nwherez =(1−t)x+tϵ. Duringinference,layout-conditioneddenoisingisappliedduringthefirst\nt\n30%ofdiffusionsteps,asthelayoutprimarilyformsinearlystages[28,64].\n4 Experiments\n4.1 ExperimentalSetup\nImplementationDetailsThetextual-onlyInstanceAssembleistrainedonSD3-Medium[15]and\nFlux.1-Dev[4]andtheversionwithadditionalvisualinstancecontentisonlytrainedonSD3-Medium.\nWefreezethepretrainedMMDiTbackboneandonlyadapttheLayoutEncoderandLoRAmodules\nofAssemble-MMDiT.Assemble-MMDiTisinitializedfrompretrainedweights, andLoRAwith\nrank=4isapplied. IntheSD3-basedmodelallAssemble-MMDiTblocksareadapted,whileinthe\nFlux-basedmodelweadapteightblocks(sevendouble-blocksandonesingleblock)duetoresource\nconstraints. Duringinference,theLoRA-basedAssemble-MMDiTisactivatedforthefirst30%of\ndenoisingsteps,whiletheglobalprompt–imagephaseusesthefrozenbackbone. Thisdesignyields\n71M(SD3-M)and102M(Flux.1-Dev)additionalparametersforthetextual-onlysetting;thevariant\nwithadditionalvisualinstancecontent(SD3-M)introduces85Mparameters. Allmodelsaretrained\nonLayoutSAM[61]at1024×1024withProdigy,for380Kiterations(batchsize2)onSD3-Mand\n6"
      },
      {
        "page": 7,
        "content": "Table4: Parameteradditionandtimeefficiencyundersparseanddenselayoutconditions. We\nevaluatedon10%oftheLayoutSAM-EvalandDenseLayoutdatasetsat1024×1024resolution. ⋆ are\noptimizedfor512×512resolution,sotheirresultsarereportedatthisscale.\nParameterAddition TimeEfficiency(s)(relativeruntimeincrease(%))\n(relativeparameteraddition(%)) SparseLayout DenseLayout\nInstanceDiff⋆(SD1.5) 369M(43%) 14.37(+771%) 44.81(+2754%)\nMIGC(SD1.4) 57M(6.64%) 14.41(+25.4%) 21.58(+87.5%)\nHICO⋆(realisticVisionV51) 361M(33.9%) 4.11(+92.9%) 9.93(+320%)\nCreatiLayout(SD3-M) 1.2B(64.0%) 4.37(+14.4%) 4.42(+14.8%)\nRegional-Flux(Flux.1-Dev) - 15.29(+113%) 37.47(+418%)\nRAG(Flux.1-Dev) - 15.69(+119%) 21.14(+192%)\nInstanceAssemble(ours)(SD3-M) 71M(3.46%) 7.19(+88.2%) 13.38(+248%)\nInstanceAssemble(ours)(Flux.1-Dev) 102M(0.84%) 8.21(+14.3%) 10.28(+41.9%)\nInstanceAssemble(ours)(Flux.1-Schnell) 102M(0.84%) 1.41(+8.46%) 1.70(+28.8%)\n300Kiterations(batchsize1)onFlux.1-Dev,using8×H800GPUs(7daysforSD3-M;5daysfor\nFlux.1-Dev).\nEvaluationDatasetWeuseLayoutSAM-Eval[61]toevaluateperformanceonfine-grainedopen-set\nsparseL2Idataset,containing5kimagesand19kinstancesintotal(3.8instancesperimage). To\nassess performance on fine-grained open-set dense L2I evaluation dataset, we use the proposed\nDenseLayout, which consists of 5kimages and 90k instancesin total (18.1 instancesper image).\nFollowingconventionalpractice,wealsoevaluateoncoarse-grainedclose-setL2Ievaluationdataset\nCOCO[31]. WecombineCOCO-StuffandCOCO-InstanceannotationstocreateourCOCO-Layout\nevaluationdataset,containing5kimagesand57kinstancesintotal(11.5instancesperimage).\nEvaluationMetricWeevaluatetheaccuracyofL2IgenerationusingourproposedLGSmetricalong\nwithCropVQAproposedbyCreatiLayout[61],measuringspatialandsemanticaccuracy. Wealso\nemploymultipleestablishedmetricstomeasureoverallimagequalityandglobalpromptalignment,\nincludingVQAScore[32],PickScore[26]andCLIPScore[21].\n4.2 EvaluationonL2IwithTextual-OnlyContent\nFine-GrainedOpen-SetSparseL2IGenerationTab.1presentsthequantitativeresultsofInstance-\nAssemble on LayoutSAM-Eval [61], reporting results using our proposed LGS, CropVQA [61]\nandglobalqualitymetrics. OurproposedInstanceAssemblenotonlyachievesSOTAinspatialand\nsemanticaccuracyofeachinstance,butalsodemonstratessuperiorglobalquality.\nFine-GrainedOpen-SetDenseL2IGener-\nation Tab. 2 presents results on DenseLay- Table 3: Comparison between our SD3-based\nout. WiththesameSD3-Mediumbackbone, InstanceAssemble and other L2I methods on\nInstanceAssemble significantly outperforms COCO-Layout. SinceCOCOdon\"thavedetailed\nCreatiLayout(mIoU:52.07vs. 15.54)while descriptionforeachinstance,wecannotevaluatethe\nmaintaining comparable global quality. On attributeaccuracyandonlyreportthespatialaccu-\nFlux.1,italsoyieldslargegainsoverRegional- racy-mIoU.\nFluxandRAG(e.g.,mIoU:43.42vs.17.23for\nRAG),showingthatourcascadedAssemble- LGS GlobalQuality\nCOCO-Layout\nAttndesigngeneralizeswellacrossbackbones. mIoU↑ VQA↑ Pick↑ CLIP↑\nComparedtoearlierUNet-basedapproaches RealImages(UpperBound) 49.14\nsuchasInstanceDiffandMIGC,ourmethod\nInstanceDiff(SD1.5) 30.39 75.77 20.75 24.41\nachieveshigherspatialandsemanticaccuracy MIGC(SD1.4) 27.36 70.32 20.20 23.58\n(mIoU:52.07vs.47.31)withoutsacrificingre- HICO(realisticVisionV51) 18.88 50.61 20.38 20.72\nCreatiLayout(SD3-M) 7.12 87.79 21.22 25.59\nalism. Overall,InstanceAssembleestablishes\nInstanceAssemble(ours)(SD3-M) 27.85 89.06 21.58 25.68\nconsistentimprovementsinlayoutalignment\nwhileensuringhighimagequalityunderchal-\nlengingdenselayouts.\nCoarse-GrainedClosed-SetL2IGenerationTab.3presentsthequantitativeresultofInstance-\nAssembleonCOCO-Layout. OurproposedInstanceAssemblesurpassespreviousmethodsinoverall\nimagequalitybutlagsslightlybehindInstanceDiff[53]inlayoutprecision. Weattributethisgapto:\n7"
      },
      {
        "page": 8,
        "content": "Layout InstanceDiff MIGC HICO CreatiLayout Regional-Flux RAG Ours\nFigure5: QualitativecomparisonofInstanceAssemblewithothermethods.\n(i)InstanceDiff’sfine-grainedCOCOtrainingdatawithper-entityattributeannotations,and(ii)its\nentity-wisegenerationstrategy,whichimprovesprecisionatsignificantcomputationalcost(Tab.4).\nQualitativeComparisonThecomparativeresultsinFig.5\ndemonstrate that our proposed InstanceAssemble method\ntext\nachieves superior spatial precision and instance-caption 4te 6.x 1ture text+image\ntext+depth\nalignment compared to baseline methods. For example, 4 3 . 7color text+edge\n34.6\ninthethirdrow,bothInstanceDiff[53]andMIGC[64]gen- 32.8\neratemorethanoneshoes; HICO[9]failstogeneratethe shape 23.0 21.9\n45.8\nspecifiedNewBalanceshoe;Regional-Flux[5]doesnotad- 34.4 22.9 11.5 10.9\nhere to the layout conditions; and the shoe generated by 11.5 25.2 35.6 46.0 5 6 m.4 IoU\nRAG[8]isnotproperlyfusedwiththebackground. Incon-\n84.9\n10.7\ntrast,ourmethodgeneratesthecorrectinstance,accurately 87.9 20.0\n90.8 11.7\nplacedandseamlesslyintegratedwiththescene. VQA\n93.7 20.7 12.7\nTime Efficiency and Parameter Addition We compare 21.4 13.7\nCLIP\ntimeefficiencyandparameteradditionwithotherL2Imeth-\nPick2 2 .1\nods,asshowninTab.4. OurmethodachievesSOTAperfor-\nmanceonlayoutalignmentwithacceptabletimeefficiency Figure6: QuantitativeresultsofIn-\nandminimalparameteraddition. stanceAssemble with additional vi-\nsualinstancecontent.\n4.3 EvaluationonL2IwithAdditionalVisualContent\nWeevaluatethreeadditionalvisualinstancecontent: image,\ndepth,andedge(seeFig.6). Unsurprisingly,usingimage\nasadditionalinstancecontentyieldsthebestperformance, asitprovidesrichvisualinformation.\nAlthough depth and edge capture texture and shape features, their performance remains inferior\ncomparedtoimageinstancecontent. Nevertheless,visualmodalitiesoutperformtextual-onlyinstance\n8"
      },
      {
        "page": 9,
        "content": "Table5: AblationstudyonourproposedcomponentsonDenseLayout. \"Assemble\"referstothe\npresenceoftheAssemble-Attnmodule(architecturaldesign). \"Cascaded\"indicatestheinteraction\norder: (✔)meansglobalprompt–imageinteractionfollowedbyinstance–imageinteraction(cascaded\nstructure),while(✘)meansbothareappliedinparallel. \"LoRA\"specifiesthetrainingstrategyforthe\nAssemble-MMDiTmodule: (✔)indicatestrainingwithLoRA,while(✘)indicatesfullfine-tuning.\n\"DenseSample\"denoteswhethertheDenseSamplespatialencodingisused.\nLayoutGroundingScore\nAssemble Cascaded LoRA DenseSample\nmIoU↑ color↑ texture↑shape↑ VQA↑\n✘ ✘ ✘ ✘ 11.69 9.16 9.68 9.56 93.75\n✔ ✘ ✘ ✘ 43.98 24.19 26.95 26.75 84.57\n✔ ✔ ✘ ✘ 45.96 29.61 31.50 31.09 92.71\n✔ ✔ ✔ ✘ 51.28 32.68 34.94 34.58 93.33\n✔ ✔ ✔ ✔ 52.07 33.77 36.21 35.81 93.54\ncontent. Qualitativecomparisons(Fig.7)furtherdemonstratethatvisualinstancecontentleadsto\nsuperiortextureandshapealignmentcomparedtotext.\nLayout Text+Image Text+Depth Text+Edge Text\nFigure7: QualitativeresultsofInstanceAssemblewithadditionalvisualinstancecontent.\n4.4 AblationStudy\nWe evaluated the contribution of each proposed component on SD3-M based InstanceAssemble\ninTab.5. Thebasemodel(SD3-Mediumwithoutadditionalmodules)yieldsaverylowLayout\nGrounding Score, indicating poor layout and content control when instance information is not\nexplicitlymodeled. TheintroductionofAssemble-Attnmoduleelevatesspatialaccuracy(mIoUto\n43.98)andboostssemanticmetrics(color/texture/shapeto24.19/26.95/26.75). Thecascadeddesign\n(✔:prompt–imagefollowedbyinstance–image;✘:parallel)resolvesglobalqualitydegradationwhile\nmaintaininglayoutalignment.UsingLoRAtotrainAssemble-MMDiTimprovesperformancefortwo\nreasons: (1)itretainsthebasemodel’scapabilitiescomparedtothefullyfine-tunedversion,and(2)\nitenableseffectivelayoutcontrolwithfarfewertrainableparametersbyintroducingonlylightweight\nlow-rankmatricesonattentionprojections. Finally,DenseSamplefurtherenhancesspatialaccuracy,\ninstancesemanticaccuracyandimagequality. Together,theserefinementsprogressivelycollectively\noptimizelayouttoimagemodelingwithoutcompromisinggenerationability.\n4.5 Applications\nWedemonstratethatInstanceAssembleisversatileandapplicabletovarioustasks. Itseamlessly\nintegrateswithdomain-specificLoRAmodulesformulti-domainstyletransferwhilemaintaining\nlayoutconsistency,asshowninFig.8. OurproposedInstanceAssemblecancooperatewithdistilled\nmodelssuchasFlux.1-Schnell[4],asillustratedinTab.2,achievinggeometriclayoutcontroland\ndetailedsynthesis. Ourapproachdemonstratesbothstyleadaptabilityandcomputationalefficiency,\nmakingitwell-suitedforcontrollablegenerativedesignapplications.\n9"
      },
      {
        "page": 10,
        "content": "D3etUC\ngnitniap\nliO\nilbihG\nFigure8: TheadaptionofCute3D[52]/OilPainting[39]/Ghibli[38]LoRAwithourmethods.\nThe proposed InstanceAssemble successfully adapts diverse style lora and maintaining superior\nlayoutalignment.\n5 Conclusion\nWe present InstanceAssemble, a novel approach for Layout-to-Image generation. Our method\nachievesstate-of-the-artlayoutalignmentwhilemaintaininghigh-qualitygenerationcapabilitiesof\nDiT-basedarchitectures. WevalidateInstanceAssembleacrosstextualinstancecontentandadditional\nvisualinstancecontent,demonstratingitsversatilityandrobustness. Ourlayoutcontrolschemealso\nsuccessfullyadaptsdiversestyleLoRAswhilemaintainingsuperiorlayoutalignment,demonstrating\ncross-domaingeneralizationcapability. Futhermore,weintroduceLayoutGroundingScoremetric\nandaDenseLayoutevaluationdatasettovalidateperformanceundercomplexlayoutconditions.\nLimitationsandFutureWorkWhileourworkadvancescontrollablegenerationbyunifyingprecise\nlayoutcontrolwiththeexpressivepowerofdiffusionmodels,severallimitationsremain. First,our\ndesigncurrentlyrequiressequentialAssemble-MMDiTcalls,whichmayincurinefficiency;exploring\nparallelizationstrategiesisanimportantdirection. Second,althoughourapproachiseffectiveundera\nwiderangeoflayouts,imagefidelitycandegradeinextremelydenseorhighlycomplexcases.\nBroaderImpactsInstanceAssembleexpandsthefrontierofstructuredvisualsynthesisbyproviding\nfine-grainedlayoutcontrolandhigh-qualitymultimodalgeneration. However,itspowerfulgenerative\ncapabilities may also introduce risks. In particular, malicious use could enable the creation of\nmisleadingordeceptivelayouts,exacerbatingthespreadofdisinformation. Themodelmayalso\nraiseprivacyconcernsifappliedtosensitivedata,andlikemanygenerativesystems,itinheritsand\nmayamplifysocietalbiasespresentintrainingcorpora. Weencourageresponsibledeploymentand\ncontinuedinvestigationintosafeguardsthatmitigatetheseriskswhileenablingbeneficialapplications\nindesign,education,andaccessibility.\n10"
      },
      {
        "page": 11,
        "content": "AcknowledgmentsandDisclosureofFunding\nThisresearchwassupportedbytheNationalNaturalScienceFoundationofChina(NSFC62576103,\n62176059). ThecomputationswereconductedusingtheCFFFplatformatFudanUniversity. Partof\nthisworkwascarriedoutduringaninternshipatXiaohongshu.\nReferences\n[1] F.Bao,S.Nie,K.Xue,Y.Cao,C.Li,H.Su,andJ.Zhu. AllareWorthWords: AViTBackbone\nforDiffusionModels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand\nPatternRecognition,pages22669–22679.IEEE,2023.\n[2] O.Bar-Tal, L.Yariv, Y.Lipman, andT.Dekel. MultiDiffusion: FusingDiffusionPathsfor\nControlled Image Generation. In Proceedings of the International Conference on Machine\nLearning,volume202ofProceedingsofMachineLearningResearch,pages1737–1752.PMLR,\n2023.\n[3] J.Betker,G.Goh,L.Jing,T.Brooks,J.Wang,L.Li,L.Ouyang,J.Zhuang,J.Lee,Y.Guo,\netal. Improvingimagegenerationwithbettercaptions,2023.\n[4] BlackForestLabs. Flux. https://github.com/black-forest-labs/flux,2024.\n[5] A.Chen,J.Xu,W.Zheng,G.Dai,Y.Wang,R.Zhang,H.Wang,andS.Zhang. Training-free\nRegionalPromptingforDiffusionTransformers,2024.URLhttps://arxiv.org/abs/2411.\n02395.\n[6] J.Chen,J.Yu,C.Ge,L.Yao,E.Xie,Z.Wang,J.T.Kwok,P.Luo,H.Lu,andZ.Li. PixArt-\nα: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis. In\nProceedingsoftheInternationalConferenceonLearningRepresentations.OpenReview.net,\n2024.\n[7] M.Chen,I.Laina,andA.Vedaldi.Training-FreeLayoutControlwithCross-AttentionGuidance.\nInWinterConferenceonApplicationsofComputerVision,pages5331–5341.IEEE,2024.\n[8] Z. Chen, Y. Li, H. Wang, Z. Chen, Z. Jiang, J. Li, Q. Wang, J. Yang, and Y. Tai. Region-\nAwareText-to-ImageGenerationviaHardBindingandSoftRefinement,2024. URLhttps:\n//arxiv.org/abs/2411.06558.\n[9] B.Cheng,Y.Ma,L.Wu,S.Liu,A.Ma,X.Wu,D.Leng,andY.Yin. HiCo: HierarchicalCon-\ntrollableDiffusionModelforLayout-to-imageGeneration. InAdvancesinNeuralInformation\nProcessingSystems,2024.\n[10] J.Cheng,X.Liang,X.Shi,T.He,T.Xiao,andM.Li. LayoutDiffuse: AdaptingFoundational\nDiffusionModelsforLayout-to-ImageGeneration,2023. URLhttps://arxiv.org/abs/\n2302.08908.\n[11] J. Cheng, Z. Zhao, T. He, T. Xiao, Z. Zhang, and Y. Zhou. Rethinking The Training And\nEvaluationofRich-ContextLayout-to-ImageGeneration. InAdvancesinNeuralInformation\nProcessingSystems,2024.\n[12] G. Couairon, M. Careil, M. Cord, S. Lathuilière, and J. Verbeek. Zero-shot spatial layout\nconditioningfortext-to-imagediffusionmodels. InProceedingsoftheIEEE/CVFInternational\nConferenceonComputerVision,pages2174–2183.IEEE,2023.\n[13] O. Dahary, O. Patashnik, K. Aberman, and D. Cohen-Or. Be Yourself: Bounded Attention\nfor Multi-subject Text-to-Image Generation. In Proceedings of the European Conference\nonComputerVision, volume15072ofLectureNotesinComputerScience, pages432–448.\nSpringer,2024.\n[14] deweiZhou,J.Xie,Z.Yang,andY.Yang. 3DIS:Depth-DrivenDecoupledImageSynthesis\nforUniversalMulti-InstanceGeneration. InProceedingsoftheInternationalConferenceon\nLearningRepresentations,2025. URLhttps://openreview.net/forum?id=MagmwodCAB.\n11"
      },
      {
        "page": 12,
        "content": "[15] P.Esser,S.Kulal,A.Blattmann,R.Entezari,J.Müller,H.Saini,Y.Levi,D.Lorenz,A.Sauer,\nF.Boesel,D.Podell,T.Dockhorn,Z.English,andR.Rombach. ScalingRectifiedFlowTrans-\nformersforHigh-ResolutionImageSynthesis. InProceedingsoftheInternationalConference\nonMachineLearning.OpenReview.net,2024.\n[16] Y. Feng, B. Gong, D. Chen, Y. Shen, Y. Liu, and J. Zhou. Ranni: Taming Text-to-Image\nDiffusionforAccurateInstructionFollowing. InProceedingsoftheIEEE/CVFConferenceon\nComputerVisionandPatternRecognition,pages4744–4753.IEEE,2024.\n[17] P.Gao,L.Zhuo,D.Liu,R.Du,X.Luo,L.Qiu,Y.Zhang,C.Lin,R.Huang,S.Geng,R.Zhang,\nJ.Xi,W.Shao,Z.Jiang,T.Yang,W.Ye,H.Tong,J.He,Y.Qiao,andH.Li. Lumina-T2X:\nTransformingTextintoAnyModality,Resolution,andDurationviaFlow-basedLargeDiffusion\nTransformers,2024. URLhttps://arxiv.org/abs/2405.05945.\n[18] B.Gong,S.Huang,Y.Feng,S.Zhang,Y.Li,andY.Liu. Check,Locate,Rectify: ATraining-\nFreeLayoutCalibrationSystemforText-to-ImageGeneration.InProceedingsoftheIEEE/CVF\nConferenceonComputerVisionandPatternRecognition,pages6624–6634.IEEE,2024.\n[19] R.He,B.Cheng,Y.Ma,Q.Jia,S.Liu,A.Ma,X.Wu,L.Wu,D.Leng,andY.Yin. PlanGen:\nTowardsUnifiedLayoutPlanningandImageGenerationinAuto-RegressiveVisionLanguage\nModels,2025. URLhttps://arxiv.org/abs/2503.10127.\n[20] Y.He,R.Salakhutdinov,andJ.Z.Kolter. Localizedtext-to-imagegenerationforfreeviacross\nattentioncontrol,2023. URLhttps://arxiv.org/abs/2306.14636.\n[21] J.Hessel,A.Holtzman,M.Forbes,R.L.Bras,andY.Choi. CLIPScore: AReference-free\nEvaluationMetricforImageCaptioning. InProceedingsoftheEmpiricalMethodsinNatural\nLanguageProcessing,pages7514–7528.AssociationforComputationalLinguistics,2021.\n[22] J.Ho,A.Jain,andP.Abbeel. DenoisingDiffusionProbabilisticModels. InNeurIPS,2020.\n[23] E.J.Hu,Y.Shen,P.Wallis,Z.Allen-Zhu,Y.Li,S.Wang,L.Wang,andW.Chen. LoRA:Low-\nRankAdaptationofLargeLanguageModels. InProceedingsoftheInternationalConference\nonLearningRepresentations.OpenReview.net,2022.\n[24] X.Huang,Y.-J.Huang,Y.Zhang,W.Tian,R.Feng,Y.Zhang,Y.Xie,Y.Li,andL.Zhang.\nOpen-SetImageTaggingwithMulti-GrainedTextSupervision,2023. URLhttps://arxiv.\norg/abs/2310.15200.\n[25] C.Jia,M.Luo,Z.Dang,G.Dai,X.Chang,M.Wang,andJ.Wang. SSMG:Spatial-Semantic\nMapGuidedDiffusionModelforFree-FormLayout-to-ImageGeneration. InProceedingsof\ntheAAAIConferenceonArtificialIntelligence,pages2480–2488.AAAIPress,2024.\n[26] Y.Kirstain,A.Polyak,U.Singer,S.Matiana,J.Penna,andO.Levy. Pick-a-Pic: AnOpen\nDatasetofUserPreferencesforText-to-ImageGeneration. InAdvancesinNeuralInformation\nProcessingSystems,2023.\n[27] Y.Lee,T.Yoon,andM.Sung. GrounDiT:GroundingDiffusionTransformersviaNoisyPatch\nTransplantation. InAdvancesinNeuralInformationProcessingSystems,2024.\n[28] Y.Li,H.Liu,Q.Wu,F.Mu,J.Yang,J.Gao,C.Li,andY.J.Lee. GLIGEN:Open-SetGrounded\nText-to-ImageGeneration. InProceedingsoftheIEEE/CVFConferenceonComputerVision\nandPatternRecognition,pages22511–22521.IEEE,2023.\n[29] Y. Li, M. Keuper, D. Zhang, and A. Khoreva. Adversarial Supervision Makes Layout-to-\nImageDiffusionModelsThrive. InProceedingsoftheInternationalConferenceonLearning\nRepresentations.OpenReview.net,2024.\n[30] Z.Li, J.Zhang, Q.Lin, J.Xiong, Y.Long, X.Deng, Y.Zhang, X.Liu, M.Huang, Z.Xiao,\nD.Chen,J.He,J.Li,W.Li,C.Zhang,R.Quan,J.Lu,J.Huang,X.Yuan,X.Zheng,Y.Li,\nJ.Zhang,C.Zhang,M.Chen,J.Liu,Z.Fang,W.Wang,J.Xue,Y.Tao,J.Zhu,K.Liu,S.Lin,\nY.Sun,Y.Li,D.Wang,M.Chen,Z.Hu,X.Xiao,Y.Chen,Y.Liu,W.Liu,D.Wang,Y.Yang,\nJ.Jiang,andQ.Lu. Hunyuan-DiT:APowerfulMulti-ResolutionDiffusionTransformerwith\nFine-GrainedChineseUnderstanding,2024. URLhttps://arxiv.org/abs/2405.08748.\n12"
      },
      {
        "page": 13,
        "content": "[31] T.Lin,M.Maire,S.J.Belongie,J.Hays,P.Perona,D.Ramanan,P.Dollár,andC.L.Zitnick.\nMicrosoftCOCO:CommonObjectsinContext. InProceedingsoftheEuropeanConferenceon\nComputerVision,volume8693ofLectureNotesinComputerScience,pages740–755.Springer,\n2014.\n[32] Z. Lin, D. Pathak, B. Li, J. Li, X. Xia, G. Neubig, P. Zhang, and D. Ramanan. Evaluating\nText-to-VisualGenerationwithImage-to-TextGeneration. InProceedingsoftheEuropean\nConferenceonComputerVision,volume15067ofLectureNotesinComputerScience,pages\n366–384.Springer,2024.\n[33] Y.Lipman,R.T.Q.Chen,H.Ben-Hamu,M.Nickel,andM.Le. Flowmatchingforgenera-\ntivemodeling. InProceedingsoftheInternationalConferenceonLearningRepresentations.\nOpenReview.net,2023.\n[34] B.Liu,E.Akhgari,A.Visheratin,A.Kamko,L.Xu,S.Shrirao,C.Lambert,J.Souza,S.Doshi,\nand D. Li. Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large\nLanguageModels,2024. URLhttps://arxiv.org/abs/2409.10695.\n[35] S.Liu,Z.Zeng,T.Ren,F.Li,H.Zhang,J.Yang,Q.Jiang,C.Li,J.Yang,H.Su,J.Zhu,and\nL.Zhang. GroundingDINO:MarryingDINOwithGroundedPre-trainingforOpen-SetObject\nDetection. InProceedingsoftheEuropeanConferenceonComputerVision,volume15105of\nLectureNotesinComputerScience,pages38–55.Springer,2024.\n[36] Z. Lv, Y. Wei, W. Zuo, and K. K. Wong. PLACE: Adaptive Layout-Semantic Fusion for\nSemanticImageSynthesis. InProceedingsoftheIEEE/CVFConferenceonComputerVision\nandPatternRecognition,pages9264–9274.IEEE,2024.\n[37] M.Ohanyan,H.Manukyan,Z.Wang,S.Navasardyan,andH.Shi. Zero-Painter: Training-Free\nLayoutControlforText-to-ImageSynthesis. InProceedingsoftheIEEE/CVFConferenceon\nComputerVisionandPatternRecognition,pages8764–8774.IEEE,2024.\n[38] openfree. flux-chatgpt-ghibli-lora. https://huggingface.co/openfree/\nflux-chatgpt-ghibli-lora,2025.\n[39] PatrickStarrrr. FLUX - Oil painting. https://civitai.com/models/1455014/\nchatgpt-4o-renderer?modelVersionId=1697982,2024.\n[40] W.PeeblesandS.Xie. ScalableDiffusionModelswithTransformers. InProceedingsofthe\nIEEE/CVFInternationalConferenceonComputerVision,pages4172–4182.IEEE,2023.\n[41] Q.Phung,S.Ge,andJ.Huang. GroundedText-to-ImageSynthesiswithAttentionRefocusing.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages7932–7942.IEEE,2024.\n[42] D.Podell,Z.English,K.Lacey,A.Blattmann,T.Dockhorn,J.Müller,J.Penna,andR.Rom-\nbach. SDXL:ImprovingLatentDiffusionModelsforHigh-ResolutionImageSynthesis. In\nProceedingsoftheInternationalConferenceonLearningRepresentations.OpenReview.net,\n2024.\n[43] QwenTeam. Qwen2.5-VL,January2025. URLhttps://qwenlm.github.io/blog/qwen2.\n5-vl/.\n[44] A.Ramesh,P.Dhariwal,A.Nichol,C.Chu,andM.Chen. HierarchicalText-ConditionalImage\nGenerationwithCLIPLatents,2022. URLhttps://arxiv.org/abs/2204.06125.\n[45] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-Resolution Image\nSynthesis with Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on\nComputerVisionandPatternRecognition,pages10674–10685.IEEE,2022.\n[46] O.Ronneberger,P.Fischer,andT.Brox. U-Net: ConvolutionalNetworksforBiomedicalImage\nSegmentation. InMedicalImageComputingandComputer-AssistedIntervention,volume9351\nofLectureNotesinComputerScience,pages234–241.Springer,2015.\n13"
      },
      {
        "page": 14,
        "content": "[47] C.Saharia,W.Chan,S.Saxena,L.Li,J.Whang,E.L.Denton,S.K.S.Ghasemipour,R.G.\nLopes,B.K.Ayan,T.Salimans,J.Ho,D.J.Fleet,andM.Norouzi.PhotorealisticText-to-Image\nDiffusionModelswithDeepLanguageUnderstanding. InAdvancesinNeuralInformation\nProcessingSystems,2022.\n[48] T.ShirakawaandS.Uchida. NoiseCollage: ALayout-AwareText-to-ImageDiffusionModel\nBased on Noise Cropping and Merging. In Proceedings of the IEEE/CVF Conference on\nComputerVisionandPatternRecognition,pages8921–8930.IEEE,2024.\n[49] stability.ai. Stable Diffusion 3.5. https://stability.ai/news/\nintroducing-stable-diffusion-3-5,Nov2024.\n[50] A.Taghipour,M.Ghahremani,M.Bennamoun,A.M.Rekavandi,H.Laga,andF.Boussaid.\nBoxIttoBindIt: UnifiedLayoutControlandAttributeBindinginT2IDiffusionModels,2024.\nURLhttps://arxiv.org/abs/2402.17910.\n[51] M.Tancik,P.P.Srinivasan,B.Mildenhall,S.Fridovich-Keil,N.Raghavan,U.Singhal,R.Ra-\nmamoorthi,J.T.Barron,andR.Ng. FourierFeaturesLetNetworksLearnHighFrequency\nFunctions in Low Dimensional Domains. In Advances in Neural Information Processing\nSystems,2020.\n[52] vjleoliu. ChatGPT-4o Renderer. https://civitai.com/models/1455014/\nchatgpt-4o-renderer?modelVersionId=1697982,2025.\n[53] X.Wang,T.Darrell,S.S.Rambhatla,R.Girdhar,andI.Misra. InstanceDiffusion: Instance-\nLevelControlforImageGeneration. InProceedingsoftheIEEE/CVFConferenceonComputer\nVisionandPatternRecognition,pages6232–6242.IEEE,2024.\n[54] Y.Wu,X.Zhou,B.Ma,X.Su,K.Ma,andX.Wang. IFAdapter: InstanceFeatureControlfor\nGroundedText-to-ImageGeneration,2024. URLhttps://arxiv.org/abs/2409.08240.\n[55] J.Xie,Y.Li,Y.Huang,H.Liu,W.Zhang,Y.Zheng,andM.Z.Shou. BoxDiff: Text-to-Image\nSynthesiswithTraining-FreeBox-ConstrainedDiffusion. InProceedingsoftheIEEE/CVF\nInternationalConferenceonComputerVision,pages7418–7427.IEEE,2023.\n[56] J.Xu,X.Sun,Z.Zhang,G.Zhao,andJ.Lin.UnderstandingandImprovingLayerNormalization.\nInAdvancesinNeuralInformationProcessingSystems,pages4383–4393,2019.\n[57] H.Xue,Z.Huang,Q.Sun,L.Song,andW.Zhang. FreestyleLayout-to-ImageSynthesis. In\nProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages\n14256–14266.IEEE,2023.\n[58] B.Yang,Y.Luo,Z.Chen,G.Wang,X.Liang,andL.Lin.LAW-Diffusion:ComplexSceneGen-\nerationbyDiffusionwithLayouts. InProceedingsoftheIEEE/CVFInternationalConference\nonComputerVision,pages22612–22622.IEEE,2023.\n[59] Z. Yang, J. Wang, Z. Gan, L. Li, K. Lin, C. Wu, N. Duan, Z. Liu, C. Liu, M. Zeng, and\nL.Wang.ReCo:Region-ControlledText-to-ImageGeneration.InProceedingsoftheIEEE/CVF\nConferenceonComputerVisionandPatternRecognition,pages14246–14255.IEEE,2023.\n[60] Y.Yao, T.Yu, A.Zhang, C.Wang, J.Cui, H.Zhu, T.Cai, H.Li, W.Zhao, Z.He, Q.Chen,\nH. Zhou, Z. Zou, H. Zhang, S. Hu, Z. Zheng, J. Zhou, J. Cai, X. Han, G. Zeng, D. Li,\nZ. Liu, and M. Sun. MiniCPM-V: A GPT-4V Level MLLM on Your Phone, 2024. URL\nhttps://arxiv.org/abs/2408.01800.\n[61] H.Zhang,D.Hong,Y.Wang,J.Shao,X.Wu,Z.Wu,andY.-G.Jiang. CreatiLayout: Siamese\nMultimodal Diffusion Transformer for Creative Layout-to-Image Generation, 2025. URL\nhttps://arxiv.org/abs/2412.03859.\n[62] L.Zhang,A.Rao,andM.Agrawala. Addingconditionalcontroltotext-to-imagediffusion\nmodels. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages\n3813–3824.IEEE,2023.\n14"
      },
      {
        "page": 15,
        "content": "[63] G.Zheng,X.Zhou,X.Li,Z.Qi,Y.Shan,andX.Li. LayoutDiffusion: ControllableDiffusion\nModel for Layout-to-Image Generation. In Proceedings of the IEEE/CVF Conference on\nComputerVisionandPatternRecognition,pages22490–22499.IEEE,2023.\n[64] D.Zhou,Y.Li,F.Ma,X.Zhang,andY.Yang. MIGC:Multi-InstanceGenerationControllerfor\nText-to-ImageSynthesis. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand\nPatternRecognition,pages6818–6828.IEEE,2024.\n[65] D. Zhou, Y. Li, F. Ma, Z. Yang, and Y. Yang. MIGC++: Advanced Multi-Instance Gener-\nationControllerforImageSynthesis. IEEETransactionsonPatternAnalysisandMachine\nIntelligence,47(3):1714–1728,2025.\n[66] D. Zhou, J. Xie, Z. Yang, and Y. Yang. 3DIS-FLUX: simple and efficient multi-instance\ngenerationwithDiTrendering,2025. URLhttps://arxiv.org/abs/2501.05131.\n15"
      },
      {
        "page": 16,
        "content": "Supplementary Material\nA Early-StageLayoutControlinAssemble-MMDiT\nWeapplytheAssemble-MMDiTlayoutcontrolmoduleexclusivelyduringtheinitial30%ofthe\ndenoisingtrajectoryanddeactivateitfortheremaining70%. Thistwo-stagestrategyprovidesrobust\nlow-frequencystructuralguidanceintheearlystagestofacilitatelayoutalignment,whileallowing\nsubsequentunconstrainedrefinementofhigh-frequencydetailsduringlaterdenoisingphases.\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.0 0.2 0.4 0.6 0.8 1.0\nControl Ratio\nerocS\n3.0\n:detceleS\nAblation on Control Ratio\n10\n8\n6\nmIoU (LGS) 4\nColor (LGS)\nTexture (LGS)\nShape (LGS)\nVQA\n)s(\nemiT\nTime (s)\nFigure9: Impactoftheproportionofdiffusionstepsincorporatinglayoutconditioningongeneration\nquality.\nAsillustratedinFigure9,restrictinglayoutcontroltolessthan30%ofthediffusionprocessresults\nininsufficientlayoutalignmentwiththetargetboundingboxes. Incontrast,extendingcontrolbeyond\nthisoptimalthresholdleadstoadeclineinoutputquality. Furthermore,increasingtheproportionof\nlayout-guidedstepsresultsinsignificantadditionalcomputationalcost.\nB AdditionalAblationStudies\nB.1 EffectofBboxEncodingandDenseSample\nTo clarify the role of bounding box encoding and DenseSample, we further ablated the SD3-M\nbasedInstanceAssemblemodelonDenseLayout. Boundingboxembeddingsguidecorrectobject\nplacement,whileDenseSampleprovidesadditionalimprovementsinspatialaccuracyandinstance-\nlevelsemantics. TheresultsinTable6demonstratethatbothcomponentscontributetotheoverall\nperformance.\nTable6: AblationonboundingboxencodingandDenseSample.\nSetting mIoU↑ color↑ texture↑ shape↑ VQA↑\nw/obboxencoding,w/oDenseSample 51.22 32.15 34.04 33.53 93.30\nw/bboxencoding,w/oDenseSample 51.28 32.68 34.94 34.58 93.33\nw/bboxencoding,w/DenseSample 52.07 33.77 36.21 35.81 93.54\nB.2 ComparisonwithAttentionMask-basedRegionInjection\nWealsocompareourAssemble-Attndesignwithattentionmask-basedregioninjection. Whileboth\ncanbeviewedasregion-wiseattentionmechanisms,attentionmasksoperategloballyandmaycause\n16"
      },
      {
        "page": 17,
        "content": "semanticleakageinoverlappingregions. Ourmethodinsteadappliesinstance-wiseself-attention\noncroppedlatentregionsandthenfusestheupdatedfeaturesviatheAssemblestep,whichismore\neffective in dense layouts. As shown in Table 7, our design achieves superior instance attribute\nconsistencyandahigherVQAscorecomparedtotheattentionmaskbaseline.\nTable7: Comparisonbetweenattentionmask-basedinjectionandourAssemble-Attn.\nMethod spatial↑ color↑ texture↑ shape↑ VQA↑\nSD3-Medium(basemodel) 77.49 60.28 62.55 60.38 93.30\nAttentionmask(SD3-M) 94.11 74.28 77.58 76.54 91.53\nInstanceAssemble(ours,SD3-M) 94.97 77.53 80.72 80.11 93.12\nC Underlyingdataforradar-chartvisualizations\nInSections4.3,weutilizedaradarcharttodepicteachquantitativevariablealongequi-angularaxes,\nprovidinganintuitivecomparison. Thisvisualizationhighlightsthemultifacetedsuperiorityofour\nmethod. Here, wepresentthecorrespondingrawevaluationresultsintabularform. Specifically,\nTab.8correspondstoFig.6,thusensuringaclearmappingbetweeneachradar-chartsubfigureand\nitsunderlyingdata.\nTable8: QuantitativeresultsofadditionalvisualcontentonDenseLayout.\nLayoutGroundingScore GlobalQuality\nDenseLayout\nmIoU↑ color↑ texture↑shape↑ VQA↑ Pick↑ CLIP↑\nRealImages(UpperBound) 92.35 76.52 80.78 79.78\ntext 43.72 26.57 28.56 28.39 93.37 21.63 12.45\ntext+image 55.29 42.15 44.50 44.24 91.66 22.05 12.95\ntext+depth 49.64 28.25 31.82 31.62 92.83 21.28 13.25\ntext+edge 50.73 29.45 33.92 33.84 90.13 21.26 13.55\nD MoreDetailsonDenseLayoutEvaluationDataset\nD.1 ConstructionPipelineofDenseLayoutDataset\nThe DenseLayout dataset is constructed through a multi-stage pipeline designed to extract high-\ndensity and semantically-rich layout information from synthetic images. The pipeline includes\nfollowingsteps:\n1. ImageGenerationusingFlux.1-Dev[4]\nA diverse set of synthetic images is generated using Flux.1-Dev, a text-to-image model.\nTheinputpromptsaregenerictextualdescriptions,sampledfromtheLayoutSAMdataset,\nwhichisbasedonSA-1B.Theimagesareresizedtomaintainthesameaspectratioasthe\noriginalSA-1Bimages,withthelongeredgesetto1024pixels. Thisstepprovidesavisually\ncomplexbaseforextractinglayoutstructures.\n2. Multi-labelTaggingusingRAM++[24]\nThegeneratedimagesaretaggedusingRAM++,thenext-generationmodelofRAM,which\nsupportsopen-setrecognition. Thesetagsofferhigh-levelsemanticguidanceforsubsequent\ngrounding.\n3. ObjectDetectionviaGroundingDINO[35]\nUsingtheimageanditspredictedtagsasinput,GroundingDINOperformsopen-setobject\ndetection.Itoutputsboundingboxesandclasslabelsforalldetectedentities.Thedetectionis\nconfiguredwithabox_thresholdof0.35andatext_thresholdof0.25. Eachdetected\nboundingboxistreatedasaninstanceboundingbox, andthecorrespondingpredicted\nlabelisrecordedastheinstancedescription.\n4. DetailedCaptioningwithQwen2.5-VL[43]\nEachboundingboxregioniscroppedfromtheoriginalimageandfedintoQwen2.5-VL\n17"
      },
      {
        "page": 18,
        "content": "togenerateafine-grainedcaption. Theseregion-levelcaptionsarestoredasthedetailed\ndescriptionforeachinstance,enrichingthesemanticinformationbeyondcategorylabels.\n5. DensityFiltering\nToensurehighlayoutcomplexity,onlyimageswith15ormoredetectedinstances(asoutput\nbyGroundingDINO)areretained. Thisresultsinadenselayoutdistributionsuitablefor\nlayout-conditionedgenerationtasks. ThedistributionofinstancecountisshowninFig.10.\nFinally,theDenseLayoutdatasetcontains5,000imagesand90,339instances,withanaverageof\n18.1instancesperimage.\n1200\n1000\n800\n600\n400\n200\n0 15 20 25 30 35 40 45 50\nNumber of Instances per Image\ntnuoC\negamI\nDenseLayout: Instance Count Distribution\nMean: 18.1\nMedian: 17.0 Std: 3.7\nFigure10: InstancecountdistributionperimageinDenseLayout.\nAnnotationFormat. Theannotationforeachimageconsistsof:\n• global_caption: theoriginalpromptusedforimagegeneration.\n• image_info: metadataoftheimage,includingheightandwidth.\n• instance_info: alistofinstances,eachwith:\n– bbox: theboundingboxoftheinstance,formattingas[x ,y ,x ,y ].\n1 1 2 2\n– description: thecategorylabelpredictedbyGroundingDINO.\n– detail_description: a fine-grained caption generated by Qwen2.5-VL for the\ncroppedregion.\nD.2 SamplesofDenseLayoutDataset\n\"instance_info\": [\n{\n\"bbox\": [129,489,283,642],\n\"description\": \"nightstand\",\n\"detail_description\": \"The nightstand is dark brown,\ncompact, with a drawer.\"\n},\n{\n\"bbox\": [306,170,430,339],\n\"description\": \"picture frame\",\n\"detail_description\": \"Brown wooden frame containing a\nwatercolor painting of green leaves on a white\nbackground.\"\n},\n{\n\"bbox\": [603,170,727,340],\n\"description\": \"picture frame\",\n\"detail_description\": \"A simple brown wooden frame holds\na botanical print with detailed leaves and stems.\"\n},\n...\n]\nFigure11: AsampleofDenseLayoutanditsannotation.\n18"
      },
      {
        "page": 19,
        "content": "This is a photo showcasing a traditional Chinese-style pavilion and This is a photo depicting a traditional Asian floating market scene. In This is a photo showcasing a Chinese-style building and a statue. The\nboats on the river. The pavilion is located on the right side of the the picture, two women are sitting in their respective boats, each building is brightly colored, with a red roof and a yellow door, and the\npicture, with a beautifully decorated roof and a golden plaque hanging wearing a traditional conical hat, and dressed in brightly colored statue is located in front of the building, standing on a pedestal. The\nin the middle. In front of the pavilion is a row of wooden boats, each traditional clothing. The woman on the left is wearing a blue top and a surrounding environment is a spacious square, with several pedestrians\nwith a red flag hanging on it, and people are gathered on the boat dock. dark skirt, while the woman on the right is wearing a yellow top and a walking around the building. The background is a clear blue sky and lush\nThe river is calm, and the sky is clear, with a few clouds leisurely light-colored skirt. Their boats are filled with a variety of goods, trees.\ndrifting by. including fresh flowers, fruits, and other food items. The boats are\nadorned with bright colored cloths, adding a festive atmosphere to the\nscene. The entire scene is captured under natural light, presenting a\ntranquil and vibrant market atmosphere.\nThis is a photo showcasing a modern interior design style, with the This is a realistic-style photograph depicting a city street scene after This is a photograph showcasing a famous archway in a city. The archway\nfocus on a spacious and bright room. The room is furnished with wooden a flood. In the photo, vehicles and pedestrians are struggling to is a reddish-brown structure, adorned with golden decorations and\nfurniture, including a long wooden table and a matching wooden bench. On navigate through the flooded streets. A yellow tricycle is parked in the sculptures, and its design is very intricate. The archway is located on\nthe table, there are some books and a laptop, while on the bench, there middle of the street, surrounded by vehicles covered with blue a wide street, with pedestrians and cyclists weaving through the road.\nis a black desk lamp. The room's lighting comes from several minimalist waterproof cloth. Pedestrians are using umbrellas to shield themselves The sky is clear, with a few clouds scattered in the blue sky. The\nchandeliers and a large window, through which you can see the green from the rain, some are pushing bicycles, while others are walking. The buildings in the background have a Mediterranean style, with the domes\nplants outside. The floor is covered with a red carpet, and there are buildings on both sides of the street are submerged in water, and the of some buildings visible in the distance.\nseveral decorations on the floor, including a red vase and a decorative wires and poles above the street are also submerged, adding to the\nsculpture. The entire scene is illuminated by natural light, creating a severity of the flooding. The entire scene is shrouded in a gloomy\nwarm and comfortable atmosphere. atmosphere, with no sunlight piercing through the clouds.\nT f t b r a a ph oo h a r ri or e r f e es fe n e s sg g w c ei .r r a o ns o a n c v t Iu s d a e ia nn s r r n d l a s e gp f, a d h r n s p ao oi d m a w t nt , a r i to ts l k t r t l e h as os h e d nh fu e r d qo r r i e uw tf e h n n ic ha o s la ec a u f e s e r s r ai b e e o t nn up , n r dg ir s t e le e b e pa ds v o o s e ie e t f , ar nn r h cu gt a t a er si l w h n fa ,n i e d ul g w t l t o h f t l ha o e h ra e d t n e un rv e r c rd ei n a e s as v d . k lc ii b i y a sd u t I ap i i n i te ag l o s m, r d n t o re i a h f sw oe n l e i pi wn g l ht s w b l eh oa , o a e r fn o c d ea d i d k . w n e g wg oy c n r ir oe l o ta dl u s u hs el d t n s no i r d wl w n u , ha f g c in ec t t td no a u h e cl r e i eo l e cn sr a s h l ,. r i ot g a l uh wO e n l de in r d s s t ,h T m d e d a p sx th a e e ap ai g p c f tr is n i o i te n i c r g es ei f t a u rs ds i s t r ni c i e so ga e a v n l n e i a. ap t f n n sh i e dT so s g l a h t c u e fe wo u r m h l i l e e e os ns p n l rc dh t i t m au oo u n s e ll ww r , t p sc e a . dt ,a i eu s l l n T sr ti o o c h ie hn c n l e g rg a g u ni o t d a ss ut e r i l , gh d o n t s he b g a au i e r nr wi n , a dr hn i o it t h s s tu ce h o t hn hr e l a d ed i d t e e to c i u c wd hr e n e o a e n g r lb o t o a ly lf e a f t s i r e e ga c a d ax h o r rq tc f o f w eu h s i i i fu t s g t as rr h , u h di oc e r ot mh w e c re , a i o n o l t h m es uw t h o p dt ti a l l a st r a d e wt ih . i x iu d s n te et T o g g hs h h l e se e e a o ca h m m on if s n c e ld no c r t o ec u o r r su l s i f s p s c u i t l no u a ,n r n e da T w g s s i a be eh i l u n v ni t a r c fe cs h s r l er h s o u wa ei a u d l ss a n i p , m n d n es a o d e g de e d d et np e m r ss jh r e b e t oo n t y d ro yt a i- f io l s b as n e r nt s gs s v i sy t h t e c l a to r r k oe i hw u a n r ec c l b b s a t u tu ss u b i hi a ui r u l el n nn e i d d d sg s l i si h . d n qn b ia i g ug e n T n s a n em h g ri c .o e s a en h d n , e Te s o d t s hr q f sh en u m oe f a d o m o su r i d ec r kr e f e e yb f r an p a i e n rt e in s r ee o s e g r p s p n l w, l cq a t a a e lu v s li ea e a s kt t ar d r is o re c c n ,. w h u ge r i i r ,x e wT t t t t s ih h e a se t te c i or . h g t n mi s r u eo T aq e r w r h u y a a a e fa l l rc er s l eo s we l s s m q a t . sp u wi b y io a hs s l T ts r i , e h te e ts s e id ep w , r n i a i e go sc t f i h a o o r nu e s ,\nadding a warm and sacred atmosphere to the interior. clouds scattered in the blue sky.\nFigure12: RepresentativesamplesfromDenseLayoutdatasetdemonstrating: (a)High-densityscene\nwith≥15instances,(b)Complexinstancerelationshipswithpreciseattributespecifications.\n19"
      },
      {
        "page": 20,
        "content": "E Moreresultswithtextual-onlycontent\nE.1 InstanceAssemblebasedonSD3-Medium\nx\nFigure13: MoreresultsofInstanceAssemblebasedonSD3-Mediumwithtextual-onlycontent.\nE.2 InstanceAssemblebasedonFlux.1-Dev\nFigure14: MoreresultsofInstanceAssemblebasedonFlux.1-Devwithtextual-onlycontent.\nE.3 InstanceAssemblebasedonFlux.1-Schnell\nFigure15: MoreresultsofInstanceAssemblebasedonFlux.1-Schnellwithtextual-onlycontent.\n20"
      },
      {
        "page": 21,
        "content": "F Moreresultswithadditionalvisualcontent\nF.1 AdditionalImage\nFigure16: Moreresultswithadditionalimage.\nF.2 AdditionalDepth\nFigure17: Moreresultswithadditionaldepth.\nF.3 AdditionalEdge\nFigure18: Moreresultswithadditionaledge.\n21"
      }
    ]
  },
  "pdf_url": "/uploads/1b8f85e87a9a5e09e54e2e792a68f0ce.pdf"
}